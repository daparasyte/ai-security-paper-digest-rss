<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>AI Security Paper Digest</title><link>https://kentaroh-toyoda.github.io/ai-security-paper-digest-rss/rss.xml</link><description>Curated papers on AI security from OpenAlex and ArXiv</description><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><lastBuildDate>Thu, 21 Aug 2025 22:21:58 +0000</lastBuildDate><item><title>Dark Miner: Defend against undesirable generation for text-to-image diffusion models</title><link>https://arxiv.org/abs/2409.17682</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Addresses undesired generation in text-to-image models due to unfiltered training data&lt;/li&gt;&lt;li&gt;Proposes Dark Miner with mining, verifying, and circumventing stages&lt;/li&gt;&lt;li&gt;Reduces generation probabilities of target concepts more effectively&lt;/li&gt;&lt;li&gt;Shows better defense against adversarial attacks while preserving generation capability&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Zheling Meng', 'Bo Peng', 'Xiaochuan Jin', 'Yue Jiang', 'Wei Wang', 'Jing Dong', 'Tieniu Tan']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial prompting', 'robustness', 'text-to-image', 'safety', 'security']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2409.17682</guid><pubDate>Thu, 21 Aug 2025 00:00:00 +0000</pubDate></item><item><title>Adversarial Generation and Collaborative Evolution of Safety-Critical Scenarios for Autonomous Vehicles</title><link>https://arxiv.org/abs/2508.14527</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces ScenGE framework for generating adversarial driving scenarios using LLMs&lt;/li&gt;&lt;li&gt;Combines meta-scenario generation and complex scenario evolution&lt;/li&gt;&lt;li&gt;Improves collision detection by 31.96% over baselines&lt;/li&gt;&lt;li&gt;Validated through real-world tests and human evaluation&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jiangfan Liu', 'Yongkang Guo', 'Fangzhi Zhong', 'Tianyuan Zhang', 'Zonglei Jing', 'Siyuan Liang', 'Jiakai Wang', 'Mingchuan Zhang', 'Aishan Liu', 'Xianglong Liu']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial generation', 'safety evaluation', 'autonomous vehicles', 'red teaming', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2508.14527</guid><pubDate>Thu, 21 Aug 2025 00:00:00 +0000</pubDate></item><item><title>Two Birds with One Stone: Multi-Task Detection and Attribution of LLM-Generated Text</title><link>https://arxiv.org/abs/2508.14190</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces DA-MTL, a multi-task learning framework for detecting LLM-generated text and attributing it to specific models&lt;/li&gt;&lt;li&gt;Evaluates performance across multiple languages and LLM sources&lt;/li&gt;&lt;li&gt;Analyzes cross-modal and cross-lingual patterns&lt;/li&gt;&lt;li&gt;Assesses robustness against adversarial obfuscation techniques&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Zixin Rao', 'Youssef Mohamed', 'Shang Liu', 'Zeyan Liu']&lt;/li&gt;&lt;li&gt;Tags: ['LLM detection', 'authorship attribution', 'multi-task learning', 'adversarial robustness', 'cross-lingual']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2508.14190</guid><pubDate>Thu, 21 Aug 2025 00:00:00 +0000</pubDate></item><item><title>Zero-knowledge LLM hallucination detection and mitigation through fine-grained cross-model consistency</title><link>https://arxiv.org/abs/2508.14314</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces Finch-Zk, a black-box framework for detecting and mitigating LLM hallucinations&lt;/li&gt;&lt;li&gt;Uses cross-model consistency checks to reveal fine-grained inaccuracies&lt;/li&gt;&lt;li&gt;Applies targeted corrections to problematic segments while preserving accurate content&lt;/li&gt;&lt;li&gt;Demonstrates significant improvements in detection and mitigation on benchmark datasets&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Aman Goel', 'Daniel Schwartz', 'Yanjun Qi']&lt;/li&gt;&lt;li&gt;Tags: ['safety evaluation', 'robustness', 'adversarial prompting', 'alignment', 'hallucination detection']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2508.14314</guid><pubDate>Thu, 21 Aug 2025 00:00:00 +0000</pubDate></item><item><title>Is The Watermarking Of LLM-Generated Code Robust?</title><link>https://arxiv.org/abs/2403.17983</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Evaluates the robustness of watermarking techniques on LLM-generated code&lt;/li&gt;&lt;li&gt;Shows that simple semantic-preserving transformations can remove watermarks&lt;/li&gt;&lt;li&gt;Develops an AST-based algorithm for applying randomized transformations&lt;/li&gt;&lt;li&gt;Finds true positive rates dropping below 50% after modifications&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Tarun Suresh', 'Shubham Ugare', 'Gagandeep Singh', 'Sasa Misailovic']&lt;/li&gt;&lt;li&gt;Tags: ['red teaming', 'robustness', 'watermarking', 'code generation', 'LLM security']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2403.17983</guid><pubDate>Thu, 21 Aug 2025 00:00:00 +0000</pubDate></item><item><title>Zero-knowledge LLM hallucination detection and mitigation through fine-grained cross-model consistency</title><link>https://arxiv.org/abs/2508.14314</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces Finch-Zk, a black-box framework for detecting and mitigating LLM hallucinations&lt;/li&gt;&lt;li&gt;Uses cross-model consistency checks to reveal fine-grained inaccuracies&lt;/li&gt;&lt;li&gt;Applies targeted corrections to problematic segments while preserving accurate content&lt;/li&gt;&lt;li&gt;Demonstrates significant improvements in detection and mitigation on benchmark datasets&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Aman Goel', 'Daniel Schwartz', 'Yanjun Qi']&lt;/li&gt;&lt;li&gt;Tags: ['safety evaluation', 'robustness', 'adversarial prompting', 'alignment', 'hallucination detection']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2508.14314</guid><pubDate>Thu, 21 Aug 2025 00:00:00 +0000</pubDate></item><item><title>Two Birds with One Stone: Multi-Task Detection and Attribution of LLM-Generated Text</title><link>https://arxiv.org/abs/2508.14190</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces DA-MTL, a multi-task learning framework for detecting LLM-generated text and attributing it to specific models&lt;/li&gt;&lt;li&gt;Evaluates performance across multiple languages and LLM sources&lt;/li&gt;&lt;li&gt;Analyzes cross-modal and cross-lingual patterns&lt;/li&gt;&lt;li&gt;Assesses robustness against adversarial obfuscation techniques&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Zixin Rao', 'Youssef Mohamed', 'Shang Liu', 'Zeyan Liu']&lt;/li&gt;&lt;li&gt;Tags: ['LLM detection', 'authorship attribution', 'multi-task learning', 'adversarial robustness', 'cross-lingual']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2508.14190</guid><pubDate>Thu, 21 Aug 2025 00:00:00 +0000</pubDate></item><item><title>Universal and Transferable Adversarial Attack on Large Language Models Using Exponentiated Gradient Descent</title><link>https://arxiv.org/abs/2508.14853</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a new method for generating jailbreak attacks on LLMs using exponentiated gradient descent&lt;/li&gt;&lt;li&gt;Optimizes relaxed one-hot encodings of adversarial suffix tokens with Bregman projection&lt;/li&gt;&lt;li&gt;Achieves higher success rates and faster convergence than state-of-the-art baselines&lt;/li&gt;&lt;li&gt;Demonstrates universal and transferable adversarial suffixes across multiple LLMs&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Sajib Biswas', 'Mao Nishino', 'Samuel Jacob Chacko', 'Xiuwen Liu']&lt;/li&gt;&lt;li&gt;Tags: ['jailbreak', 'adversarial attack', 'LLM', 'red teaming', 'prompt injection']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2508.14853</guid><pubDate>Thu, 21 Aug 2025 00:00:00 +0000</pubDate></item><item><title>Foe for Fraud: Transferable Adversarial Attacks in Credit Card Fraud Detection</title><link>https://arxiv.org/abs/2508.14699</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Investigates adversarial attacks on ML models for credit card fraud detection&lt;/li&gt;&lt;li&gt;Tests gradient-based attacks in both black and white box settings&lt;/li&gt;&lt;li&gt;Demonstrates transferability of attacks between different models&lt;/li&gt;&lt;li&gt;Highlights the need for robust defenses in financial ML applications&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jan Lum Fok', 'Qingwen Zeng', 'Shiping Chen', 'Oscar Fawkes', 'Huaming Chen']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial attacks', 'robustness', 'security', 'transferability', 'fraud detection']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2508.14699</guid><pubDate>Thu, 21 Aug 2025 00:00:00 +0000</pubDate></item><item><title>Zero-knowledge LLM hallucination detection and mitigation through fine-grained cross-model consistency</title><link>https://arxiv.org/abs/2508.14314</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces Finch-Zk, a black-box framework for detecting and mitigating LLM hallucinations&lt;/li&gt;&lt;li&gt;Uses cross-model consistency checks to reveal fine-grained inaccuracies&lt;/li&gt;&lt;li&gt;Applies targeted corrections to problematic segments while preserving accurate content&lt;/li&gt;&lt;li&gt;Demonstrates significant improvements in detection and mitigation on benchmark datasets&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Aman Goel', 'Daniel Schwartz', 'Yanjun Qi']&lt;/li&gt;&lt;li&gt;Tags: ['safety evaluation', 'robustness', 'adversarial prompting', 'alignment', 'hallucination detection']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2508.14314</guid><pubDate>Thu, 21 Aug 2025 00:00:00 +0000</pubDate></item><item><title>Incident Analysis for AI Agents</title><link>https://arxiv.org/abs/2508.14231</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes an incident analysis framework for AI agents using systems safety approaches&lt;/li&gt;&lt;li&gt;Identifies three types of incident factors: system-related, contextual, and cognitive&lt;/li&gt;&lt;li&gt;Recommends retaining activity logs, system docs, and tool info for investigation&lt;/li&gt;&lt;li&gt;Aims to improve understanding and prevention of AI agent incidents&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Carson Ezell', 'Xavier Roberts-Gaal', 'Alan Chan']&lt;/li&gt;&lt;li&gt;Tags: ['red teaming', 'prompt injection', 'safety evaluation', 'security standards', 'incident response']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2508.14231</guid><pubDate>Thu, 21 Aug 2025 00:00:00 +0000</pubDate></item><item><title>CCFC: Core &amp; Core-Full-Core Dual-Track Defense for LLM Jailbreak Protection</title><link>https://arxiv.org/abs/2508.14128</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces CCFC, a dual-track defense framework for LLM jailbreak protection&lt;/li&gt;&lt;li&gt;Uses core-only and core-full-core tracks to handle different types of attacks&lt;/li&gt;&lt;li&gt;Reduces attack success rates by 50-75% without sacrificing response quality&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jiaming Hu', 'Haoyu Wang', 'Debarghya Mukherjee', 'Ioannis Ch. Paschalidis']&lt;/li&gt;&lt;li&gt;Tags: ['jailbreaking', 'prompt_injection', 'adversarial_prompting', 'red_teaming', 'safety_evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2508.14128</guid><pubDate>Thu, 21 Aug 2025 00:00:00 +0000</pubDate></item><item><title>Special-Character Adversarial Attacks on Open-Source Language Model</title><link>https://arxiv.org/abs/2508.14070</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Demonstrates vulnerability of LLMs to special-character adversarial attacks&lt;/li&gt;&lt;li&gt;Shows that character-level manipulations can compromise model security&lt;/li&gt;&lt;li&gt;Highlights the need for robust defenses against such attacks&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Ephraiem Sarabamoun']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial attacks', 'red teaming', 'LLM security', 'prompt injection']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2508.14070</guid><pubDate>Thu, 21 Aug 2025 00:00:00 +0000</pubDate></item><item><title>The Agent Behavior: Model, Governance and Challenges in the AI Digital Age</title><link>https://arxiv.org/abs/2508.14415</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes the 'Network Behavior Lifecycle' model to analyze agent behavior stages&lt;/li&gt;&lt;li&gt;Introduces 'Agent for Agent (A4A)' and 'Human-Agent Behavioral Disparity (HABD)' models&lt;/li&gt;&lt;li&gt;Verifies models through real-world red team and blue team security cases&lt;/li&gt;&lt;li&gt;Discusses future research in dynamic governance and meta-protocols&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Qiang Zhang', 'Pei Yan', 'Yijia Xu', 'Chuanpo Fu', 'Yong Fang', 'Yang Liu']&lt;/li&gt;&lt;li&gt;Tags: ['agent behavior', 'security', 'red teaming', 'governance', 'trustworthy AI']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2508.14415</guid><pubDate>Thu, 21 Aug 2025 00:00:00 +0000</pubDate></item></channel></rss>