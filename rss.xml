<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>AI Security Paper Digest</title><link>https://kentaroh-toyoda.github.io/ai-security-paper-digest-rss/rss.xml</link><description>Curated papers on AI security from ArXiv</description><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><lastBuildDate>Fri, 29 Aug 2025 22:23:05 +0000</lastBuildDate><item><title>When Memory Becomes a Vulnerability: Towards Multi-turn Jailbreak Attacks against Text-to-Image Generation Systems</title><link>https://arxiv.org/abs/2504.20376</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces Inception, a multi-turn jailbreak attack exploiting memory in T2I systems&lt;/li&gt;&lt;li&gt;Proposes segmentation and recursion modules for crafting adversarial prompts&lt;/li&gt;&lt;li&gt;Builds VisionFlow to emulate T2I safety mechanisms&lt;/li&gt;&lt;li&gt;Achieves 20% higher attack success rate than SOTA and validates on real platforms&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Shiqian Zhao', 'Jiayang Liu', 'Yiming Li', 'Runyi Hu', 'Xiaojun Jia', 'Wenshu Fan', 'Xinfeng Li', 'Jie Zhang', 'Wei Dong', 'Tianwei Zhang', 'Luu Anh Tuan']&lt;/li&gt;&lt;li&gt;Tags: ['jailbreaking', 'adversarial_prompting', 'text_to_image', 'multi_turn', 'memory_exploitation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2504.20376</guid><pubDate>Fri, 29 Aug 2025 00:00:00 +0000</pubDate></item><item><title>Exploring Typographic Visual Prompts Injection Threats in Cross-Modality Generation Models</title><link>https://arxiv.org/abs/2503.11519</link><description>&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Other&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Hao Cheng', 'Erjia Xiao', 'Yichi Wang', 'Lingfeng Zhang', 'Qiang Zhang', 'Jiahang Cao', 'Kaidi Xu', 'Mengshu Sun', 'Xiaoshuai Hao', 'Jindong Gu', 'Renjing Xu']&lt;/li&gt;&lt;li&gt;Tags: []&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2503.11519</guid><pubDate>Fri, 29 Aug 2025 00:00:00 +0000</pubDate></item><item><title>GUARD: Guideline Upholding Test through Adaptive Role-play and Jailbreak Diagnostics for LLMs</title><link>https://arxiv.org/abs/2508.20325</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces GUARD, a testing method for LLM compliance with ethics guidelines&lt;/li&gt;&lt;li&gt;Uses adaptive role-play and jailbreak diagnostics to identify guideline violations&lt;/li&gt;&lt;li&gt;Empirically validated on multiple LLMs including GPT-3.5, GPT-4, and Claude-3.7&lt;/li&gt;&lt;li&gt;Can transfer jailbreak diagnostics to vision-language models&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Haibo Jin', 'Ruoxi Chen', 'Peiyan Zhang', 'Andy Zhou', 'Yang Zhang', 'Haohan Wang']&lt;/li&gt;&lt;li&gt;Tags: ['red teaming', 'jailbreaking', 'safety evaluation', 'compliance testing', 'adversarial prompting']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2508.20325</guid><pubDate>Fri, 29 Aug 2025 00:00:00 +0000</pubDate></item><item><title>Towards Mechanistic Defenses Against Typographic Attacks in CLIP</title><link>https://arxiv.org/abs/2508.20570</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Identifies attention heads in CLIP vision encoder that process typographic information&lt;/li&gt;&lt;li&gt;Introduces defense by ablating specific attention heads without fine-tuning&lt;/li&gt;&lt;li&gt;Improves robustness against typographic attacks by up to 19.6%&lt;/li&gt;&lt;li&gt;Releases dyslexic CLIP models as drop-in replacements for safety-critical applications&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Lorenz Hufe', 'Constantin Venhoff', 'Maximilian Dreyer', 'Sebastian Lapuschkin', 'Wojciech Samek']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial attacks', 'CLIP', 'typographic attacks', 'safety', 'defense mechanism']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2508.20570</guid><pubDate>Fri, 29 Aug 2025 00:00:00 +0000</pubDate></item><item><title>Exploring Typographic Visual Prompts Injection Threats in Cross-Modality Generation Models</title><link>https://arxiv.org/abs/2503.11519</link><description>&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Other&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Hao Cheng', 'Erjia Xiao', 'Yichi Wang', 'Lingfeng Zhang', 'Qiang Zhang', 'Jiahang Cao', 'Kaidi Xu', 'Mengshu Sun', 'Xiaoshuai Hao', 'Jindong Gu', 'Renjing Xu']&lt;/li&gt;&lt;li&gt;Tags: []&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2503.11519</guid><pubDate>Fri, 29 Aug 2025 00:00:00 +0000</pubDate></item><item><title>Forewarned is Forearmed: Pre-Synthesizing Jailbreak-like Instructions to Enhance LLM Safety Guardrail to Potential Attacks</title><link>https://arxiv.org/abs/2508.20038</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces IMAGINE framework for generating jailbreak-like instructions&lt;/li&gt;&lt;li&gt;Uses embedding space distribution analysis for iterative optimization&lt;/li&gt;&lt;li&gt;Augments safety alignment data to reduce attack success rates&lt;/li&gt;&lt;li&gt;Validated on Qwen2.5, Llama3.1, and Llama3.2 models&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Sheng Liu', 'Qiang Sheng', 'Danding Wang', 'Yang Li', 'Guang Yang', 'Juan Cao']&lt;/li&gt;&lt;li&gt;Tags: ['jailbreak', 'red teaming', 'adversarial prompting', 'safety', 'data augmentation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2508.20038</guid><pubDate>Fri, 29 Aug 2025 00:00:00 +0000</pubDate></item><item><title>CoCoTen: Detecting Adversarial Inputs to Large Language Models through Latent Space Features of Contextual Co-occurrence Tensors</title><link>https://arxiv.org/abs/2508.02997</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes CoCoTen method for detecting adversarial and jailbreak prompts in LLMs&lt;/li&gt;&lt;li&gt;Achieves F1 score of 0.83 with only 0.5% labeled data&lt;/li&gt;&lt;li&gt;Significant speed improvements over baseline models (2.3-128.4x)&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Sri Durga Sai Sowmya Kadali', 'Evangelos E. Papalexakis']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial detection', 'jailbreak', 'prompt injection', 'LLM security', 'safety evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2508.02997</guid><pubDate>Fri, 29 Aug 2025 00:00:00 +0000</pubDate></item><item><title>Agent-to-Agent Theory of Mind: Testing Interlocutor Awareness among Large Language Models</title><link>https://arxiv.org/abs/2506.22957</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Formalizes interlocutor awareness in LLMs as the ability to identify and adapt to dialogue partners&lt;/li&gt;&lt;li&gt;Evaluates awareness across reasoning, style, and alignment preferences&lt;/li&gt;&lt;li&gt;Demonstrates both enhanced collaboration and new safety vulnerabilities (reward hacking, jailbreaks)&lt;/li&gt;&lt;li&gt;Highlights need for safeguards in multi-agent LLM deployments&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Younwoo Choi', 'Changling Li', 'Yongjin Yang', 'Zhijing Jin']&lt;/li&gt;&lt;li&gt;Tags: ['red teaming', 'jailbreaking', 'alignment', 'safety evaluation', 'multi-agent systems']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.22957</guid><pubDate>Fri, 29 Aug 2025 00:00:00 +0000</pubDate></item><item><title>Token Buncher: Shielding LLMs from Harmful Reinforcement Learning Fine-Tuning</title><link>https://arxiv.org/abs/2508.20697</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Demonstrates RL-based fine-tuning is more effective for harmful misuse than SFT&lt;/li&gt;&lt;li&gt;Proposes TokenBuncher defense using entropy-as-reward RL and Token Noiser&lt;/li&gt;&lt;li&gt;Reduces model response uncertainty to prevent RL exploitation&lt;/li&gt;&lt;li&gt;Maintains benign task performance and finetunability&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Weitao Feng', 'Lixu Wang', 'Tianyi Wei', 'Jie Zhang', 'Chongyang Gao', 'Sinong Zhan', 'Peizhuo Lv', 'Wei Dong']&lt;/li&gt;&lt;li&gt;Tags: ['LLM security', 'red teaming', 'reinforcement learning', 'fine-tuning defense', 'alignment']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2508.20697</guid><pubDate>Fri, 29 Aug 2025 00:00:00 +0000</pubDate></item><item><title>Poison Once, Refuse Forever: Weaponizing Alignment for Injecting Bias in LLMs</title><link>https://arxiv.org/abs/2508.20333</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces Subversive Alignment Injection (SAI), a data poisoning attack that exploits LLM alignment to inject bias&lt;/li&gt;&lt;li&gt;Demonstrates evasion of state-of-the-art poisoning defenses including forensics and robust aggregation&lt;/li&gt;&lt;li&gt;Shows significant bias induction in chat applications and NLP tasks with as little as 1% poisoned data&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Md Abdullah Al Mamun', 'Ihsen Alouani', 'Nael Abu-Ghazaleh']&lt;/li&gt;&lt;li&gt;Tags: ['data poisoning', 'alignment', 'safety', 'red teaming', 'adversarial attacks']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2508.20333</guid><pubDate>Fri, 29 Aug 2025 00:00:00 +0000</pubDate></item><item><title>Robustness Assessment and Enhancement of Text Watermarking for Google's SynthID</title><link>https://arxiv.org/abs/2508.20228</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Assessed robustness of SynthID-Text against meaning-preserving attacks&lt;/li&gt;&lt;li&gt;Proposed SynGuard combining semantic and lexical watermarking&lt;/li&gt;&lt;li&gt;Showed 11.1% F1 score improvement in watermark recovery&lt;/li&gt;&lt;li&gt;Code and datasets are publicly available&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Xia Han', 'Qi Li', 'Jianbing Ni', 'Mohammad Zulkernine']&lt;/li&gt;&lt;li&gt;Tags: ['watermarking', 'robustness', 'adversarial attacks', 'semantic alignment', 'LLM security']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2508.20228</guid><pubDate>Fri, 29 Aug 2025 00:00:00 +0000</pubDate></item><item><title>Lethe: Purifying Backdoored Large Language Models with Knowledge Dilution</title><link>https://arxiv.org/abs/2508.21004</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces Lethe, a method to eliminate backdoor behaviors in LLMs using knowledge dilution&lt;/li&gt;&lt;li&gt;Combines internal model merging with a clean model and external prompt evidence injection&lt;/li&gt;&lt;li&gt;Demonstrates effectiveness against 8 backdoor attacks across 5 LLMs with up to 98% reduction in attack success&lt;/li&gt;&lt;li&gt;Maintains model utility while being cost-efficient and robust against adaptive attacks&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Chen Chen', 'Yuchen Sun', 'Jiaxin Gao', 'Xueluan Gong', 'Qian Wang', 'Ziyao Wang', 'Yongsen Zheng', 'Kwok-Yan Lam']&lt;/li&gt;&lt;li&gt;Tags: ['backdoor', 'security', 'LLM', 'defense', 'knowledge dilution']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2508.21004</guid><pubDate>Fri, 29 Aug 2025 00:00:00 +0000</pubDate></item><item><title>Turning the Spell Around: Lightweight Alignment Amplification via Rank-One Safety Injection</title><link>https://arxiv.org/abs/2508.20766</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces Rank-One Safety Injection (ROSI) to amplify safety alignment in LLMs&lt;/li&gt;&lt;li&gt;Steers activations towards refusal-mediating subspace via rank-one weight modification&lt;/li&gt;&lt;li&gt;Increases safety refusal rates while preserving model utility&lt;/li&gt;&lt;li&gt;White-box, fine-tuning-free method using a small set of instruction pairs&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Harethah Abu Shairah', 'Hasan Abed Al Kader Hammoud', 'George Turkiyyah', 'Bernard Ghanem']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'safety evaluation', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2508.20766</guid><pubDate>Fri, 29 Aug 2025 00:00:00 +0000</pubDate></item><item><title>ConspirED: A Dataset for Cognitive Traits of Conspiracy Theories and Large Language Model Safety</title><link>https://arxiv.org/abs/2508.20468</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduced ConspirED dataset for evaluating cognitive traits of conspiracy theories&lt;/li&gt;&lt;li&gt;Developed models to identify conspiratorial traits in text&lt;/li&gt;&lt;li&gt;Evaluated LLM robustness against conspiratorial inputs&lt;/li&gt;&lt;li&gt;Found models mirror conspiratorial reasoning even when deflecting misinformation&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Luke Bates', 'Max Glockner', 'Preslav Nakov', 'Iryna Gurevych']&lt;/li&gt;&lt;li&gt;Tags: ['safety evaluation', 'adversarial prompting', 'model alignment', 'dataset', 'text modality']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2508.20468</guid><pubDate>Fri, 29 Aug 2025 00:00:00 +0000</pubDate></item><item><title>GUARD: Guideline Upholding Test through Adaptive Role-play and Jailbreak Diagnostics for LLMs</title><link>https://arxiv.org/abs/2508.20325</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces GUARD, a testing method for LLM compliance with ethics guidelines&lt;/li&gt;&lt;li&gt;Uses adaptive role-play and jailbreak diagnostics to identify guideline violations&lt;/li&gt;&lt;li&gt;Empirically validated on multiple LLMs including GPT-3.5, GPT-4, and Claude-3.7&lt;/li&gt;&lt;li&gt;Can transfer jailbreak diagnostics to vision-language models&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Haibo Jin', 'Ruoxi Chen', 'Peiyan Zhang', 'Andy Zhou', 'Yang Zhang', 'Haohan Wang']&lt;/li&gt;&lt;li&gt;Tags: ['red teaming', 'jailbreaking', 'safety evaluation', 'compliance testing', 'adversarial prompting']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2508.20325</guid><pubDate>Fri, 29 Aug 2025 00:00:00 +0000</pubDate></item><item><title>Turning the Spell Around: Lightweight Alignment Amplification via Rank-One Safety Injection</title><link>https://arxiv.org/abs/2508.20766</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces Rank-One Safety Injection (ROSI) to amplify safety alignment in LLMs&lt;/li&gt;&lt;li&gt;Steers activations towards refusal-mediating subspace via rank-one weight modification&lt;/li&gt;&lt;li&gt;Increases safety refusal rates while preserving model utility&lt;/li&gt;&lt;li&gt;White-box, fine-tuning-free method using a small set of instruction pairs&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Harethah Abu Shairah', 'Hasan Abed Al Kader Hammoud', 'George Turkiyyah', 'Bernard Ghanem']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'safety evaluation', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2508.20766</guid><pubDate>Fri, 29 Aug 2025 00:00:00 +0000</pubDate></item><item><title>Token Buncher: Shielding LLMs from Harmful Reinforcement Learning Fine-Tuning</title><link>https://arxiv.org/abs/2508.20697</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Demonstrates RL-based fine-tuning is more effective for harmful misuse than SFT&lt;/li&gt;&lt;li&gt;Proposes TokenBuncher defense using entropy-as-reward RL and Token Noiser&lt;/li&gt;&lt;li&gt;Reduces model response uncertainty to prevent RL exploitation&lt;/li&gt;&lt;li&gt;Maintains benign task performance and finetunability&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Weitao Feng', 'Lixu Wang', 'Tianyi Wei', 'Jie Zhang', 'Chongyang Gao', 'Sinong Zhan', 'Peizhuo Lv', 'Wei Dong']&lt;/li&gt;&lt;li&gt;Tags: ['LLM security', 'red teaming', 'reinforcement learning', 'fine-tuning defense', 'alignment']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2508.20697</guid><pubDate>Fri, 29 Aug 2025 00:00:00 +0000</pubDate></item><item><title>Poison Once, Refuse Forever: Weaponizing Alignment for Injecting Bias in LLMs</title><link>https://arxiv.org/abs/2508.20333</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces Subversive Alignment Injection (SAI), a data poisoning attack that exploits LLM alignment to inject bias&lt;/li&gt;&lt;li&gt;Demonstrates evasion of state-of-the-art poisoning defenses including forensics and robust aggregation&lt;/li&gt;&lt;li&gt;Shows significant bias induction in chat applications and NLP tasks with as little as 1% poisoned data&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Md Abdullah Al Mamun', 'Ihsen Alouani', 'Nael Abu-Ghazaleh']&lt;/li&gt;&lt;li&gt;Tags: ['data poisoning', 'alignment', 'safety', 'red teaming', 'adversarial attacks']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2508.20333</guid><pubDate>Fri, 29 Aug 2025 00:00:00 +0000</pubDate></item><item><title>Agent-to-Agent Theory of Mind: Testing Interlocutor Awareness among Large Language Models</title><link>https://arxiv.org/abs/2506.22957</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Formalizes interlocutor awareness in LLMs as the ability to identify and adapt to dialogue partners&lt;/li&gt;&lt;li&gt;Evaluates awareness across reasoning, style, and alignment preferences&lt;/li&gt;&lt;li&gt;Demonstrates both enhanced collaboration and new safety vulnerabilities (reward hacking, jailbreaks)&lt;/li&gt;&lt;li&gt;Highlights need for safeguards in multi-agent LLM deployments&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Younwoo Choi', 'Changling Li', 'Yongjin Yang', 'Zhijing Jin']&lt;/li&gt;&lt;li&gt;Tags: ['red teaming', 'jailbreaking', 'alignment', 'safety evaluation', 'multi-agent systems']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.22957</guid><pubDate>Fri, 29 Aug 2025 00:00:00 +0000</pubDate></item><item><title>RevPRAG: Revealing Poisoning Attacks in Retrieval-Augmented Generation through LLM Activation Analysis</title><link>https://arxiv.org/abs/2411.18948</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces RevPRAG, a detection pipeline for RAG poisoning attacks&lt;/li&gt;&lt;li&gt;Uses LLM activation analysis to distinguish poisoned responses&lt;/li&gt;&lt;li&gt;Achieves 98% true positive rate with &lt;1% false positives&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Xue Tan', 'Hao Luan', 'Mingyu Luo', 'Xiaoyan Sun', 'Ping Chen', 'Jun Dai']&lt;/li&gt;&lt;li&gt;Tags: ['data_poisoning', 'security', 'rag', 'detection', 'llm_activations']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2411.18948</guid><pubDate>Fri, 29 Aug 2025 00:00:00 +0000</pubDate></item><item><title>JADES: A Universal Framework for Jailbreak Assessment via Decompositional Scoring</title><link>https://arxiv.org/abs/2508.20848</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces JADES, a decompositional scoring framework for evaluating jailbreak attempts&lt;/li&gt;&lt;li&gt;Validates on new JailbreakQR benchmark with high human agreement&lt;/li&gt;&lt;li&gt;Re-evaluates existing attacks revealing overestimated success rates&lt;/li&gt;&lt;li&gt;Incorporates optional fact-checking to detect hallucinations&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Junjie Chu', 'Mingjie Li', 'Ziqing Yang', 'Ye Leng', 'Chenhao Lin', 'Chao Shen', 'Michael Backes', 'Yun Shen', 'Yang Zhang']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'jailbreaking', 'evaluation framework', 'benchmarking', 'adversarial prompting']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2508.20848</guid><pubDate>Fri, 29 Aug 2025 00:00:00 +0000</pubDate></item><item><title>Multi-Agent Penetration Testing AI for the Web</title><link>https://arxiv.org/abs/2508.20816</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces MAPTA, a multi-agent system for web security assessment using LLMs&lt;/li&gt;&lt;li&gt;Achieves high success rates on XBOW benchmark (76.9% overall)&lt;/li&gt;&lt;li&gt;Discovers real-world vulnerabilities leading to CVEs&lt;/li&gt;&lt;li&gt;Analyzes cost efficiency of the approach&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Isaac David', 'Arthur Gervais']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'adversarial prompting', 'security evaluation', 'web security']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2508.20816</guid><pubDate>Fri, 29 Aug 2025 00:00:00 +0000</pubDate></item><item><title>Turning the Spell Around: Lightweight Alignment Amplification via Rank-One Safety Injection</title><link>https://arxiv.org/abs/2508.20766</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces Rank-One Safety Injection (ROSI) to amplify safety alignment in LLMs&lt;/li&gt;&lt;li&gt;Steers activations towards refusal-mediating subspace via rank-one weight modification&lt;/li&gt;&lt;li&gt;Increases safety refusal rates while preserving model utility&lt;/li&gt;&lt;li&gt;White-box, fine-tuning-free method using a small set of instruction pairs&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Harethah Abu Shairah', 'Hasan Abed Al Kader Hammoud', 'George Turkiyyah', 'Bernard Ghanem']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'safety evaluation', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2508.20766</guid><pubDate>Fri, 29 Aug 2025 00:00:00 +0000</pubDate></item><item><title>Rethinking Testing for LLM Applications: Characteristics, Challenges, and a Lightweight Interaction Protocol</title><link>https://arxiv.org/abs/2508.20737</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Decomposes LLM applications into three layers: System Shell, Prompt Orchestration, LLM Inference Core&lt;/li&gt;&lt;li&gt;Analyzes applicability of traditional testing methods across layers&lt;/li&gt;&lt;li&gt;Proposes collaborative testing strategies and AICL protocol for LLM application testing&lt;/li&gt;&lt;li&gt;Emphasizes integration of pre-deployment validation and runtime monitoring&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Wei Ma', 'Yixiao Yang', 'Qiang Hu', 'Shi Ying', 'Zhi Jin', 'Bo Du', 'Zhenchang Xing', 'Tianlin Li', 'Junjie Shi', 'Yang Liu', 'Linxiao Jiang']&lt;/li&gt;&lt;li&gt;Tags: ['safety evaluation', 'testing', 'LLM', 'robustness', 'quality assurance']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2508.20737</guid><pubDate>Fri, 29 Aug 2025 00:00:00 +0000</pubDate></item><item><title>Towards Mechanistic Defenses Against Typographic Attacks in CLIP</title><link>https://arxiv.org/abs/2508.20570</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Identifies attention heads in CLIP vision encoder that process typographic information&lt;/li&gt;&lt;li&gt;Introduces defense by ablating specific attention heads without fine-tuning&lt;/li&gt;&lt;li&gt;Improves robustness against typographic attacks by up to 19.6%&lt;/li&gt;&lt;li&gt;Releases dyslexic CLIP models as drop-in replacements for safety-critical applications&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Lorenz Hufe', 'Constantin Venhoff', 'Maximilian Dreyer', 'Sebastian Lapuschkin', 'Wojciech Samek']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial attacks', 'CLIP', 'typographic attacks', 'safety', 'defense mechanism']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2508.20570</guid><pubDate>Fri, 29 Aug 2025 00:00:00 +0000</pubDate></item><item><title>Poison Once, Refuse Forever: Weaponizing Alignment for Injecting Bias in LLMs</title><link>https://arxiv.org/abs/2508.20333</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces Subversive Alignment Injection (SAI), a data poisoning attack that exploits LLM alignment to inject bias&lt;/li&gt;&lt;li&gt;Demonstrates evasion of state-of-the-art poisoning defenses including forensics and robust aggregation&lt;/li&gt;&lt;li&gt;Shows significant bias induction in chat applications and NLP tasks with as little as 1% poisoned data&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Md Abdullah Al Mamun', 'Ihsen Alouani', 'Nael Abu-Ghazaleh']&lt;/li&gt;&lt;li&gt;Tags: ['data poisoning', 'alignment', 'safety', 'red teaming', 'adversarial attacks']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2508.20333</guid><pubDate>Fri, 29 Aug 2025 00:00:00 +0000</pubDate></item><item><title>GUARD: Guideline Upholding Test through Adaptive Role-play and Jailbreak Diagnostics for LLMs</title><link>https://arxiv.org/abs/2508.20325</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces GUARD, a testing method for LLM compliance with ethics guidelines&lt;/li&gt;&lt;li&gt;Uses adaptive role-play and jailbreak diagnostics to identify guideline violations&lt;/li&gt;&lt;li&gt;Empirically validated on multiple LLMs including GPT-3.5, GPT-4, and Claude-3.7&lt;/li&gt;&lt;li&gt;Can transfer jailbreak diagnostics to vision-language models&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Haibo Jin', 'Ruoxi Chen', 'Peiyan Zhang', 'Andy Zhou', 'Yang Zhang', 'Haohan Wang']&lt;/li&gt;&lt;li&gt;Tags: ['red teaming', 'jailbreaking', 'safety evaluation', 'compliance testing', 'adversarial prompting']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2508.20325</guid><pubDate>Fri, 29 Aug 2025 00:00:00 +0000</pubDate></item><item><title>Surveying the Operational Cybersecurity and Supply Chain Threat Landscape when Developing and Deploying AI Systems</title><link>https://arxiv.org/abs/2508.20307</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Surveys operational cybersecurity and supply chain threats in AI systems&lt;/li&gt;&lt;li&gt;Highlights manipulation of AI outputs as a key attack vector&lt;/li&gt;&lt;li&gt;Emphasizes need for tailored security frameworks across AI lifecycle&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Survey&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Michael R Smith', 'Joe Ingram']&lt;/li&gt;&lt;li&gt;Tags: ['cybersecurity', 'supply_chain', 'AI_security', 'adversarial_attacks', 'data_poisoning']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2508.20307</guid><pubDate>Fri, 29 Aug 2025 00:00:00 +0000</pubDate></item><item><title>Network-Level Prompt and Trait Leakage in Local Research Agents</title><link>https://arxiv.org/abs/2508.20282</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Demonstrates network-level metadata attack on WRAs to infer prompts and traits&lt;/li&gt;&lt;li&gt;Uses IP addresses and timings to fingerprint user queries&lt;/li&gt;&lt;li&gt;Shows high accuracy in prompt and trait recovery&lt;/li&gt;&lt;li&gt;Discusses mitigation strategies with minimal utility impact&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Hyejun Jeong', 'Mohammadreze Teymoorianfard', 'Abhinav Kumar', 'Amir Houmansadr', 'Eugene Badasarian']&lt;/li&gt;&lt;li&gt;Tags: ['privacy attacks', 'adversarial attacks', 'network inference', 'LLM security', 'metadata attacks']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2508.20282</guid><pubDate>Fri, 29 Aug 2025 00:00:00 +0000</pubDate></item><item><title>Governable AI: Provable Safety Under Extreme Threat Models</title><link>https://arxiv.org/abs/2508.20411</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes Governable AI (GAI) framework using cryptographic enforcement for provable safety&lt;/li&gt;&lt;li&gt;Introduces Rule Enforcement Module (REM) and Secure Super-Platform (GSSP) for tamper-resistant governance&lt;/li&gt;&lt;li&gt;Provides formal proofs of security properties under extreme threat models&lt;/li&gt;&lt;li&gt;Demonstrates effectiveness through prototype evaluation in high-stakes scenarios&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Donglin Wang', 'Weiyun Liang', 'Chunyuan Chen', 'Jing Xu', 'Yulong Fu']&lt;/li&gt;&lt;li&gt;Tags: ['AI safety', 'Cryptographic mechanisms', 'Formal verification', 'Existential risks', 'Governance framework']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2508.20411</guid><pubDate>Fri, 29 Aug 2025 00:00:00 +0000</pubDate></item><item><title>IntentionReasoner: Facilitating Adaptive LLM Safeguards through Intent Reasoning and Selective Query Refinement</title><link>https://arxiv.org/abs/2508.20151</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces IntentionReasoner, a safeguard mechanism for LLMs using intent reasoning and query refinement&lt;/li&gt;&lt;li&gt;Constructs a comprehensive dataset of 163k annotated queries for training the guard model&lt;/li&gt;&lt;li&gt;Applies multi-reward optimization combining heuristics and reward models&lt;/li&gt;&lt;li&gt;Demonstrates improved safety, reduced over-refusal, and better response quality in benchmarks and jailbreak scenarios&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yuanzhe Shen', 'Zisu Huang', 'Zhengkang Guo', 'Yide Liu', 'Guanxu Chen', 'Ruicheng Yin', 'Xiaoqing Zheng', 'Xuanjing Huang']&lt;/li&gt;&lt;li&gt;Tags: ['jailbreak', 'safety', 'LLM', 'intent analysis', 'query refinement', 'adversarial prompting', 'safety evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2508.20151</guid><pubDate>Fri, 29 Aug 2025 00:00:00 +0000</pubDate></item></channel></rss>