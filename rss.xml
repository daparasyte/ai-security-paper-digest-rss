<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>AI Security Paper Digest</title><link>https://kentaroh-toyoda.github.io/ai-security-paper-digest-rss/rss.xml</link><description>Curated papers on AI security from OpenAlex and ArXiv</description><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><lastBuildDate>Wed, 02 Jul 2025 22:21:58 +0000</lastBuildDate><item><title>Just Noticeable Difference for Large Multimodal Models</title><link>https://arxiv.org/abs/2507.00490</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces LMM-JND to quantify visual blind spots in multimodal models&lt;/li&gt;&lt;li&gt;Constructs VPA-JND dataset with 21.5k images and 489k stimuli across 12 distortion types&lt;/li&gt;&lt;li&gt;Evaluates state-of-the-art models like GPT-4o and InternVL2.5 showing significant performance gaps vs human vision&lt;/li&gt;&lt;li&gt;Highlights security concerns from predictable visual perception defects&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Zijian Chen', 'Yuan Tian', 'Yuze Sun', 'Wei Sun', 'Zicheng Zhang', 'Weisi Lin', 'Guangtao Zhai', 'Wenjun Zhang']&lt;/li&gt;&lt;li&gt;Tags: ['safety evaluation', 'robustness', 'multimodal', 'visual perception', 'security']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2507.00490</guid><pubDate>Wed, 02 Jul 2025 00:00:00 +0000</pubDate></item><item><title>SPADE: Structured Prompting Augmentation for Dialogue Enhancement in Machine-Generated Text Detection</title><link>https://arxiv.org/abs/2503.15044</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes SPADE framework for detecting synthetic dialogues using structured prompting&lt;/li&gt;&lt;li&gt;Creates 14 new dialogue datasets and benchmarks 8 MGT detection models&lt;/li&gt;&lt;li&gt;Improves generalization performance with mixed datasets from augmentation&lt;/li&gt;&lt;li&gt;Simulates online dialogue detection and studies chat history impact on accuracy&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Haoyi Li', 'Angela Yifei Yuan', 'Soyeon Caren Han', 'Christopher Leckie']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial prompting', 'data poisoning', 'safety evaluation', 'robustness', 'red teaming']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2503.15044</guid><pubDate>Wed, 02 Jul 2025 00:00:00 +0000</pubDate></item><item><title>Pitfalls of Evaluating Language Models with Open Benchmarks</title><link>https://arxiv.org/abs/2507.00460</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Exposed vulnerabilities in open benchmarks by creating 'cheating' models trained on test data&lt;/li&gt;&lt;li&gt;Demonstrated high performance on HELM benchmark despite poor generalization&lt;/li&gt;&lt;li&gt;Advocated for private/dynamic benchmarks to prevent exploitation&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Md. Najib Hasan (Wichita State University)', 'Mohammad Fakhruddin Babar (Washington State University)', 'Souvika Sarkar (Wichita State University)', 'Monowar Hasan (Washington State University)', 'Santu Karmaker (University of Central Florida)']&lt;/li&gt;&lt;li&gt;Tags: ['security standards', 'safety evaluation', 'robustness', 'red teaming']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2507.00460</guid><pubDate>Wed, 02 Jul 2025 00:00:00 +0000</pubDate></item><item><title>AudioTrust: Benchmarking the Multifaceted Trustworthiness of Audio Large Language Models</title><link>https://arxiv.org/abs/2505.16211</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduction of AudioTrust benchmark for evaluating trustworthiness of Audio Large Language Models (ALLMs)&lt;/li&gt;&lt;li&gt;Assesses six dimensions: fairness, hallucination, safety, privacy, robustness, authentication&lt;/li&gt;&lt;li&gt;Features 18 experimental setups and 4,420 audio/text samples&lt;/li&gt;&lt;li&gt;Includes 9 audio-specific evaluation metrics and large-scale automated scoring&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Kai Li', 'Can Shen', 'Yile Liu', 'Jirui Han', 'Kelong Zheng', 'Xuechao Zou', 'Zhe Wang', 'Xingjian Du', 'Shun Zhang', 'Hanjun Luo', 'Yingbin Jin', 'Xinxin Xing', 'Ziyang Ma', 'Yue Liu', 'Xiaojun Jia', 'Yifan Zhang', 'Junfeng Fang', 'Kun Wang', 'Yibo Yan', 'Haoyang Li', 'Yiming Li', 'Xiaobin Zhuang', 'Yang Liu', 'Haibo Hu', 'Zhizheng Wu', 'Xiaolin Hu', 'Eng-Siong Chng', 'XiaoFeng Wang', 'Wenyuan Xu', 'Wei Dong', 'Xinfeng Li']&lt;/li&gt;&lt;li&gt;Tags: ['safety', 'privacy', 'robustness', 'authentication', 'fairness', 'hallucination', 'benchmarking', 'evaluation framework']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2505.16211</guid><pubDate>Wed, 02 Jul 2025 00:00:00 +0000</pubDate></item><item><title>Reasoning as an Adaptive Defense for Safety</title><link>https://arxiv.org/abs/2507.00971</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces TARS (Training Adaptive Reasoners for Safety), a reinforcement learning approach to train LLMs for safety&lt;/li&gt;&lt;li&gt;Utilizes chain-of-thought reasoning and adaptive compute allocation based on prompt ambiguity&lt;/li&gt;&lt;li&gt;Demonstrates improved robustness against white-box (GCG) and black-box (PAIR) attacks&lt;/li&gt;&lt;li&gt;Emphasizes balanced training with harmful, harmless, and ambiguous prompts&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Taeyoun Kim', 'Fahim Tajwar', 'Aditi Raghunathan', 'Aviral Kumar']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'safety evaluation', 'robustness', 'adversarial prompting', 'jailbreaking']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2507.00971</guid><pubDate>Wed, 02 Jul 2025 00:00:00 +0000</pubDate></item><item><title>CAVALRY-V: A Large-Scale Generator Framework for Adversarial Attacks on Video MLLMs</title><link>https://arxiv.org/abs/2507.00817</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces CAVALRY-V framework for adversarial attacks on video MLLMs&lt;/li&gt;&lt;li&gt;Dual-objective loss disrupts text logits and visual representations&lt;/li&gt;&lt;li&gt;Two-stage generator with pre-training and fine-tuning for efficiency&lt;/li&gt;&lt;li&gt;Significant performance improvements over baseline attacks on multiple models&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jiaming Zhang', 'Rui Hu', 'Qing Guo', 'Wei Yang Bryan Lim']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial attacks', 'video MLLMs', 'red teaming', 'robustness', 'multimodal']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2507.00817</guid><pubDate>Wed, 02 Jul 2025 00:00:00 +0000</pubDate></item><item><title>SAFER: Probing Safety in Reward Models with Sparse Autoencoder</title><link>https://arxiv.org/abs/2507.00665</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces SAFER framework using sparse autoencoders to interpret reward model features&lt;/li&gt;&lt;li&gt;Applies to safety-oriented preference datasets to identify safety-relevant features&lt;/li&gt;&lt;li&gt;Demonstrates targeted data poisoning/denoising to affect safety alignment&lt;/li&gt;&lt;li&gt;Maintains general chat performance while manipulating safety alignment&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Sihang Li', 'Wei Shi', 'Ziyuan Xie', 'Tao Liang', 'Guojun Ma', 'Xiang Wang']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'safety evaluation', 'data poisoning', 'reward model', 'interpretability']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2507.00665</guid><pubDate>Wed, 02 Jul 2025 00:00:00 +0000</pubDate></item><item><title>BadViM: Backdoor Attack against Vision Mamba</title><link>https://arxiv.org/abs/2507.00577</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces BadViM, a backdoor attack framework targeting Vision Mamba&lt;/li&gt;&lt;li&gt;Uses Resonant Frequency Trigger (RFT) for stealthy, distributed triggers&lt;/li&gt;&lt;li&gt;Employs Hidden State Alignment loss to manipulate internal model representations&lt;/li&gt;&lt;li&gt;Demonstrates high attack success and resilience against common defenses&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yinghao Wu', 'Liyan Zhang']&lt;/li&gt;&lt;li&gt;Tags: ['data poisoning', 'adversarial attacks', 'model extraction', 'privacy attacks', 'security standards']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2507.00577</guid><pubDate>Wed, 02 Jul 2025 00:00:00 +0000</pubDate></item><item><title>PNAct: Crafting Backdoor Attacks in Safe Reinforcement Learning</title><link>https://arxiv.org/abs/2507.00485</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces PNAct, a backdoor attack framework for Safe RL&lt;/li&gt;&lt;li&gt;Demonstrates vulnerability through positive and negative action samples&lt;/li&gt;&lt;li&gt;Evaluates attack effectiveness with established metrics&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Weiran Guo', 'Guanjun Liu', 'Ziyuan Zhou', 'Ling Wang']&lt;/li&gt;&lt;li&gt;Tags: ['backdoor attacks', 'safe RL', 'red teaming', 'data poisoning']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2507.00485</guid><pubDate>Wed, 02 Jul 2025 00:00:00 +0000</pubDate></item><item><title>Linearly Decoding Refused Knowledge in Aligned Language Models</title><link>https://arxiv.org/abs/2507.00239</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Study shows that refused knowledge in aligned LMs can be decoded via linear probes on hidden states&lt;/li&gt;&lt;li&gt;Jailbreak prompts reveal that information persists even after instruction-tuning&lt;/li&gt;&lt;li&gt;Probes trained on base models sometimes transfer to aligned versions&lt;/li&gt;&lt;li&gt;Decoded information correlates with model-generated comparisons&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Aryan Shrivastava', 'Ari Holtzman']&lt;/li&gt;&lt;li&gt;Tags: ['jailbreaking', 'adversarial_prompting', 'model_extraction', 'alignment', 'safety_evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2507.00239</guid><pubDate>Wed, 02 Jul 2025 00:00:00 +0000</pubDate></item><item><title>VSF-Med:A Vulnerability Scoring Framework for Medical Vision-Language Models</title><link>https://arxiv.org/abs/2507.00052</link><description>&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Other&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Binesh Sadanandan', 'Vahid Behzadan']&lt;/li&gt;&lt;li&gt;Tags: []&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2507.00052</guid><pubDate>Wed, 02 Jul 2025 00:00:00 +0000</pubDate></item><item><title>ROSE: Toward Reality-Oriented Safety Evaluation of Large Language Models</title><link>https://arxiv.org/abs/2507.00026</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes ROSE framework for reality-oriented safety evaluation of LLMs&lt;/li&gt;&lt;li&gt;Uses multi-objective reinforcement learning to generate adversarial prompts&lt;/li&gt;&lt;li&gt;Aims to improve topic diversity and real-world context in safety testing&lt;/li&gt;&lt;li&gt;Demonstrates improved vulnerability detection in state-of-the-art LLMs&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jiale Ding', 'Xiang Zheng', 'Cong Wang', 'Wei-Bin Lee', 'Xingjun Ma', 'Yu-Gang Jiang']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial prompting', 'safety evaluation', 'red teaming', 'LLM security', 'reinforcement learning']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2507.00026</guid><pubDate>Wed, 02 Jul 2025 00:00:00 +0000</pubDate></item><item><title>Vision Transformer with Adversarial Indicator Token against Adversarial Attacks in Radio Signal Classifications</title><link>https://arxiv.org/abs/2507.00015</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposed a Vision Transformer with Adversarial Indicator (AdvI) token for detecting adversarial attacks in radio signal classification&lt;/li&gt;&lt;li&gt;Combined adversarial training and detection using the AdvI token in a unified model&lt;/li&gt;&lt;li&gt;Demonstrated improved performance against white-box attacks like FGSM, PGD, and BIM&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Lu Zhang', 'Sangarapillai Lambotharan', 'Gan Zheng', 'Guisheng Liao', 'Xuekang Liu', 'Fabio Roli', 'Carsten Maple']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial defense', 'vision transformer', 'radio signal classification', 'adversarial training', 'attention mechanism']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2507.00015</guid><pubDate>Wed, 02 Jul 2025 00:00:00 +0000</pubDate></item><item><title>Enhancing LLM Agent Safety via Causal Influence Prompting</title><link>https://arxiv.org/abs/2507.00979</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces CIP technique using causal influence diagrams (CIDs) for LLM agent safety&lt;/li&gt;&lt;li&gt;Three key steps: initialize CID, guide interactions, refine based on outcomes&lt;/li&gt;&lt;li&gt;Demonstrates enhanced safety in code execution and mobile device control tasks&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Dongyoon Hahm', 'Woogyeol Jin', 'June Suk Choi', 'Sungsoo Ahn', 'Kimin Lee']&lt;/li&gt;&lt;li&gt;Tags: ['safety', 'causal influence', 'LLM agents', 'decision-making', 'risk mitigation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2507.00979</guid><pubDate>Wed, 02 Jul 2025 00:00:00 +0000</pubDate></item><item><title>SafeMobile: Chain-level Jailbreak Detection and Automated Evaluation for Multimodal Mobile Agents</title><link>https://arxiv.org/abs/2507.00841</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces SafeMobile for detecting jailbreaks in multimodal mobile agents&lt;/li&gt;&lt;li&gt;Uses behavioral sequence analysis and LLM-based automated evaluation&lt;/li&gt;&lt;li&gt;Validated in high-risk tasks with improved risky behavior recognition&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Siyuan Liang', 'Tianmeng Fang', 'Zhe Liu', 'Aishan Liu', 'Yan Xiao', 'Jinyuan He', 'Ee-Chien Chang', 'Xiaochun Cao']&lt;/li&gt;&lt;li&gt;Tags: ['jailbreaking', 'multimodal agents', 'automated evaluation', 'risk detection']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2507.00841</guid><pubDate>Wed, 02 Jul 2025 00:00:00 +0000</pubDate></item><item><title>Thinking About Thinking: SAGE-nano's Inverse Reasoning for Self-Aware Language Models</title><link>https://arxiv.org/abs/2507.00092</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces inverse reasoning for LLM self-reflection and explanation&lt;/li&gt;&lt;li&gt;Develops a metacognitive framework to reverse attention flow&lt;/li&gt;&lt;li&gt;Evaluates reasoning transparency and performance&lt;/li&gt;&lt;li&gt;Contributes to AI safety through interpretability&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Basab Jha', 'Firoj Paudel', 'Ujjwal Puri', 'Zhang Yuting', 'Choi Donghyuk', 'Wang Junhao']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'safety evaluation', 'transparency']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2507.00092</guid><pubDate>Wed, 02 Jul 2025 00:00:00 +0000</pubDate></item></channel></rss>