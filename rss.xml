<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>AI Security Paper Digest</title><link>https://kentaroh-toyoda.github.io/ai-security-paper-digest-rss/rss.xml</link><description>Summarized AI security papers from arXiv</description><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><lastBuildDate>Fri, 23 May 2025 22:13:16 +0000</lastBuildDate><item><title>On the Clean Generalization and Robust Overfitting in Adversarial Training from Two Theoretical Views: Representation Complexity and Training Dynamics</title><link>https://arxiv.org/abs/2306.01271</link><description>• This paper investigates the phenomenon of clean generalization and robust overfitting (CGRO) in adversarial training, analyzing why deep neural networks trained with adversarial examples generalize well on clean data but may overfit to adversarial examples. The study provides theoretical and empirical insights into the representation complexity and training dynamics underlying adversarial robustness, directly addressing issues of robustness under attack and adversarial training.&lt;br/&gt;&lt;br/&gt;Tags: adversarial training, robustness, overfitting, representation complexity, training dynamics, theoretical analysis&lt;br/&gt;Authors: Binghui Li, Yuanzhi Li&lt;br/&gt;Relevance: 5 / 5&lt;br/&gt;&lt;br/&gt;&lt;a href='https://arxiv.org/abs/2306.01271'&gt;Read on arXiv&lt;/a&gt;</description><pubDate>Fri, 23 May 2025 22:13:16 +0000</pubDate></item><item><title>AudioTrust: Benchmarking the Multifaceted Trustworthiness of Audio Large Language Models</title><link>https://arxiv.org/abs/2505.16211</link><description>• The paper introduces AudioTrust, a benchmark for evaluating the trustworthiness of Audio Large Language Models (ALLMs) across multiple dimensions, including safety, privacy, robustness, and authentication.&lt;br/&gt;• It specifically addresses risks and high-risk scenarios unique to audio-based AI models, and includes robustness and safety as core evaluation criteria.&lt;br/&gt;• The benchmark is designed to reveal limitations and boundaries of ALLMs in scenarios relevant to secure and trustworthy deployment.&lt;br/&gt;&lt;br/&gt;Tags: robustness, safety, audio LLMs, trustworthiness, evaluation, AI security&lt;br/&gt;Authors: Kai Li, Can Shen, Yile Liu, Jirui Han, Kelong Zheng, Xuechao Zou, Zhe Wang, Xingjian Du, Shun Zhang, Hanjun Luo, Yingbin Jin, Xinxin Xing, Ziyang Ma, Yue Liu, Xiaojun Jia, Yifan Zhang, Junfeng Fang, Kun Wang, Yibo Yan, Haoyang Li, Yiming Li, Xiaobin Zhuang, Yang Liu, Haibo Hu, Zhuo Chen, Zhizheng Wu, Xiaolin Hu, Eng-Siong Chng, XiaoFeng Wang, Wenyuan Xu, Wei Dong, Xinfeng Li&lt;br/&gt;Relevance: 4 / 5&lt;br/&gt;&lt;br/&gt;&lt;a href='https://arxiv.org/abs/2505.16211'&gt;Read on arXiv&lt;/a&gt;</description><pubDate>Fri, 23 May 2025 22:13:16 +0000</pubDate></item><item><title>ASVspoof2019 vs. ASVspoof5: Assessment and Comparison</title><link>https://arxiv.org/abs/2505.15911</link><description>• This paper compares two ASVspoof challenges, which focus on spoofing attacks against automatic speaker verification (ASV) systems and the robustness of countermeasures. It analyzes the impact of database mismatches on the difficulty of detecting spoofed versus genuine speech, directly addressing adversarial attacks and robustness under attack in the context of AI-driven speaker verification.&lt;br/&gt;&lt;br/&gt;Tags: spoofing, adversarial attacks, robustness, speaker verification, countermeasures&lt;br/&gt;Authors: Avishai Weizman, Yehuda Ben-Shimol, Itshak Lapidot&lt;br/&gt;Relevance: 4 / 5&lt;br/&gt;&lt;br/&gt;&lt;a href='https://arxiv.org/abs/2505.15911'&gt;Read on arXiv&lt;/a&gt;</description><pubDate>Fri, 23 May 2025 22:13:16 +0000</pubDate></item><item><title>From Evaluation to Defense: Advancing Safety in Video Large Language Models</title><link>https://arxiv.org/abs/2505.16643</link><description>• This paper introduces VideoSafetyBench, a large-scale benchmark for evaluating the safety of video-based large language models (Video LLMs), and proposes a dual-stage defense framework (VideoSafety-R1) to improve their robustness against multimodal attacks and safety risks. The work systematically studies vulnerabilities in Video LLMs, demonstrating that integrating video modalities can degrade safety and expose new attack surfaces. The proposed methods focus on harm perception and defensive reasoning to actively mitigate risks.&lt;br/&gt;&lt;br/&gt;Tags: AI security, multimodal attacks, LLM safety, robustness, defense, video LLMs&lt;br/&gt;Authors: Yiwei Sun, Peiqi Jiang, Chuanbin Liu, Luohao Lin, Zhiying Lu, Hongtao Xie&lt;br/&gt;Relevance: 5 / 5&lt;br/&gt;&lt;br/&gt;&lt;a href='https://arxiv.org/abs/2505.16643'&gt;Read on arXiv&lt;/a&gt;</description><pubDate>Fri, 23 May 2025 22:13:16 +0000</pubDate></item><item><title>Beyond Face Swapping: A Diffusion-Based Digital Human Benchmark for Multimodal Deepfake Detection</title><link>https://arxiv.org/abs/2505.16512</link><description>• The paper introduces a new large-scale dataset (DigiFakeAV) for detecting deepfakes generated by advanced diffusion models, highlighting the increasing difficulty of distinguishing synthetic digital humans from real ones. It also proposes a new detection method (DigiShield) that fuses spatiotemporal and cross-modal features to improve robustness against these sophisticated forgeries. The work directly addresses the challenge of AI-generated media misuse and the robustness of detection systems against evolving adversarial content.&lt;br/&gt;&lt;br/&gt;Tags: deepfake detection, AI-generated media, adversarial robustness, multimodal forgery, diffusion models&lt;br/&gt;Authors: Jiaxin Liu, Jia Wang, Saihui Hou, Min Ren, Huijia Wu, Zhaofeng He&lt;br/&gt;Relevance: 4 / 5&lt;br/&gt;&lt;br/&gt;&lt;a href='https://arxiv.org/abs/2505.16512'&gt;Read on arXiv&lt;/a&gt;</description><pubDate>Fri, 23 May 2025 22:13:16 +0000</pubDate></item><item><title>AdvReal: Adversarial Patch Generation Framework with Application to Adversarial Safety Evaluation of Object Detection Systems</title><link>https://arxiv.org/abs/2505.16402</link><description>• This paper presents a framework for generating adversarial patches to evaluate and attack object detection systems, particularly in the context of autonomous vehicles.&lt;br/&gt;• It addresses the robustness of deep learning-based perception methods against adversarial attacks in both digital and physical environments.&lt;br/&gt;• The work includes methods for enhancing the realism and effectiveness of adversarial samples, and evaluates their impact on multiple object detection models.&lt;br/&gt;&lt;br/&gt;Tags: adversarial attacks, object detection, autonomous vehicles, robustness, physical adversarial examples&lt;br/&gt;Authors: Yuanhao Huang, Yilong Ren, Jinlei Wang, Lujia Huo, Xuesong Bai, Jinchuan Zhang, Haiyan Yu&lt;br/&gt;Relevance: 5 / 5&lt;br/&gt;&lt;br/&gt;&lt;a href='https://arxiv.org/abs/2505.16402'&gt;Read on arXiv&lt;/a&gt;</description><pubDate>Fri, 23 May 2025 22:13:16 +0000</pubDate></item><item><title>Accelerating Targeted Hard-Label Adversarial Attacks in Low-Query Black-Box Settings</title><link>https://arxiv.org/abs/2505.16313</link><description>• The paper introduces a new method (TEA) for accelerating targeted hard-label adversarial attacks in low-query black-box settings. It focuses on generating adversarial examples that can fool deep neural networks into misclassifying images into specific target classes, even when only the final prediction is accessible. The method leverages edge information from target images to improve attack efficiency and effectiveness, which is directly related to adversarial robustness and attack strategies in AI security.&lt;br/&gt;&lt;br/&gt;Tags: adversarial attacks, black-box attacks, robustness, image classification, AI security&lt;br/&gt;Authors: Arjhun Swaminathan, Mete Akg\"un&lt;br/&gt;Relevance: 5 / 5&lt;br/&gt;&lt;br/&gt;&lt;a href='https://arxiv.org/abs/2505.16313'&gt;Read on arXiv&lt;/a&gt;</description><pubDate>Fri, 23 May 2025 22:13:16 +0000</pubDate></item><item><title>Erased or Dormant? Rethinking Concept Erasure Through Reversibility</title><link>https://arxiv.org/abs/2505.16174</link><description>• This paper investigates the effectiveness and limitations of concept erasure techniques in diffusion models, specifically examining whether such methods truly eliminate the ability to generate undesired or harmful concepts or merely suppress them superficially. The authors analyze the robustness and reversibility of concept erasure, revealing that erased concepts can often be reactivated with minimal effort, which has direct implications for the security and misuse prevention of generative AI models.&lt;br/&gt;&lt;br/&gt;Tags: concept erasure, model robustness, generative models, AI misuse prevention, security&lt;br/&gt;Authors: Ping Liu, Chi Zhang&lt;br/&gt;Relevance: 4 / 5&lt;br/&gt;&lt;br/&gt;&lt;a href='https://arxiv.org/abs/2505.16174'&gt;Read on arXiv&lt;/a&gt;</description><pubDate>Fri, 23 May 2025 22:13:16 +0000</pubDate></item><item><title>TRAIL: Transferable Robust Adversarial Images via Latent diffusion</title><link>https://arxiv.org/abs/2505.16166</link><description>• This paper proposes TRAIL, a framework for generating transferable adversarial images using latent diffusion models. It addresses the challenge of creating adversarial attacks that are robust and transferable across different deep learning models, which is a core concern in AI security. The method adapts diffusion models to generate adversarial samples that are both effective at misleading victim models and realistic, improving the practicality of black-box attacks.&lt;br/&gt;&lt;br/&gt;Tags: adversarial attacks, transferability, diffusion models, black-box attacks, robustness&lt;br/&gt;Authors: Yuhao Xue, Zhifei Zhang, Xinyang Jiang, Yifei Shen, Junyao Gao, Wentao Gu, Jiale Zhao, Miaojing Shi, Cairong Zhao&lt;br/&gt;Relevance: 5 / 5&lt;br/&gt;&lt;br/&gt;&lt;a href='https://arxiv.org/abs/2505.16166'&gt;Read on arXiv&lt;/a&gt;</description><pubDate>Fri, 23 May 2025 22:13:16 +0000</pubDate></item><item><title>BadDepth: Backdoor Attacks Against Monocular Depth Estimation in the Physical World</title><link>https://arxiv.org/abs/2505.16154</link><description>• This paper introduces BadDepth, a novel backdoor attack method targeting monocular depth estimation (MDE) models, which are widely used in autonomous driving and robotics. The attack manipulates depth maps at the object level and is shown to be effective both digitally and in the physical world, highlighting vulnerabilities in MDE models to backdoor attacks.&lt;br/&gt;&lt;br/&gt;Tags: backdoor attacks, model robustness, adversarial attacks, depth estimation, AI security&lt;br/&gt;Authors: Ji Guo, Long Zhou, Zhijin Wang, Jiaming He, Qiyang Song, Aiguo Chen, Wenbo Jiang&lt;br/&gt;Relevance: 5 / 5&lt;br/&gt;&lt;br/&gt;&lt;a href='https://arxiv.org/abs/2505.16154'&gt;Read on arXiv&lt;/a&gt;</description><pubDate>Fri, 23 May 2025 22:13:16 +0000</pubDate></item><item><title>Challenger: Affordable Adversarial Driving Video Generation</title><link>https://arxiv.org/abs/2505.15880</link><description>• The paper introduces Challenger, a framework for generating photorealistic adversarial driving videos designed to stress-test autonomous driving (AD) systems.&lt;br/&gt;• It focuses on creating physically plausible adversarial scenarios that increase collision rates in state-of-the-art AD models, demonstrating transferability of adversarial behaviors.&lt;br/&gt;• The work directly addresses adversarial attacks and robustness under attack in the context of AI-driven autonomous vehicles.&lt;br/&gt;&lt;br/&gt;Tags: adversarial attacks, robustness, autonomous driving, AI security, simulation&lt;br/&gt;Authors: Zhiyuan Xu, Bohan Li, Huan-ang Gao, Mingju Gao, Yong Chen, Ming Liu, Chenxu Yan, Hang Zhao, Shuo Feng, Hao Zhao&lt;br/&gt;Relevance: 5 / 5&lt;br/&gt;&lt;br/&gt;&lt;a href='https://arxiv.org/abs/2505.15880'&gt;Read on arXiv&lt;/a&gt;</description><pubDate>Fri, 23 May 2025 22:13:16 +0000</pubDate></item><item><title>PandaGuard: Systematic Evaluation of LLM Safety against Jailbreaking Attacks</title><link>https://arxiv.org/abs/2505.13862</link><description>• This paper introduces PandaGuard, a framework for systematically evaluating the safety of large language models (LLMs) against jailbreak attacks. It implements multiple attack and defense methods, benchmarks their interactions across many LLMs, and analyzes vulnerabilities, defense effectiveness, and judgment consistency. The work directly addresses adversarial attacks (jailbreaks) and robustness under attack, providing tools and insights for AI security.&lt;br/&gt;&lt;br/&gt;Tags: jailbreak, adversarial attacks, LLM safety, robustness, benchmark, evaluation framework&lt;br/&gt;Authors: Guobin Shen, Dongcheng Zhao, Linghao Feng, Xiang He, Jihang Wang, Sicheng Shen, Haibo Tong, Yiting Dong, Jindong Li, Xiang Zheng, Yi Zeng&lt;br/&gt;Relevance: 5 / 5&lt;br/&gt;&lt;br/&gt;&lt;a href='https://arxiv.org/abs/2505.13862'&gt;Read on arXiv&lt;/a&gt;</description><pubDate>Fri, 23 May 2025 22:13:16 +0000</pubDate></item><item><title>An End-to-End Model For Logits Based Large Language Models Watermarking</title><link>https://arxiv.org/abs/2505.02344</link><description>• This paper proposes a novel end-to-end logits perturbation method for watermarking text generated by large language models (LLMs). The method aims to improve robustness against text modifications (such as paraphrasing) while maintaining text quality, addressing challenges in tracing the source of AI-generated content and protecting against unauthorized use or copyright infringement. The approach is evaluated for robustness under attack scenarios where the watermarked text is altered.&lt;br/&gt;&lt;br/&gt;Tags: watermarking, LLM, robustness, source tracing, copyright protection, adversarial robustness&lt;br/&gt;Authors: Kahim Wong, Jicheng Zhou, Jiantao Zhou, Yain-Whar Si&lt;br/&gt;Relevance: 3 / 5&lt;br/&gt;&lt;br/&gt;&lt;a href='https://arxiv.org/abs/2505.02344'&gt;Read on arXiv&lt;/a&gt;</description><pubDate>Fri, 23 May 2025 22:13:16 +0000</pubDate></item><item><title>Firewalls to Secure Dynamic LLM Agentic Networks</title><link>https://arxiv.org/abs/2502.01822</link><description>• The paper addresses security challenges in networks of LLM agents, focusing on vulnerabilities such as prompt injection, jailbreaks, and manipulation. It proposes a firewall-inspired framework that derives task-specific rules to control agent communication, aiming to prevent external attacks and reduce privacy and security risks. The framework is evaluated against diverse attacks, demonstrating improved robustness.&lt;br/&gt;&lt;br/&gt;Tags: LLM security, prompt injection, jailbreak prevention, agent communication, firewalls, robustness, privacy&lt;br/&gt;Authors: Sahar Abdelnabi, Amr Gomaa, Eugene Bagdasarian, Per Ola Kristensson, Reza Shokri&lt;br/&gt;Relevance: 5 / 5&lt;br/&gt;&lt;br/&gt;&lt;a href='https://arxiv.org/abs/2502.01822'&gt;Read on arXiv&lt;/a&gt;</description><pubDate>Fri, 23 May 2025 22:13:16 +0000</pubDate></item><item><title>On the Lack of Robustness of Binary Function Similarity Systems</title><link>https://arxiv.org/abs/2412.04163</link><description>• This paper investigates the robustness of machine learning models used for binary function similarity against adversarial attacks. It introduces a black-box greedy attack that modifies the control flow of functions to compromise these models, demonstrating high attack success rates. The work highlights the trade-off between model performance and robustness, emphasizing the need for further research in securing such systems.&lt;br/&gt;&lt;br/&gt;Tags: adversarial attacks, robustness, binary function similarity, machine learning security&lt;br/&gt;Authors: Gianluca Capozzi, Tong Tang, Jie Wan, Ziqi Yang, Daniele Cono D'Elia, Giuseppe Antonio Di Luna, Lorenzo Cavallaro, Leonardo Querzoni&lt;br/&gt;Relevance: 5 / 5&lt;br/&gt;&lt;br/&gt;&lt;a href='https://arxiv.org/abs/2412.04163'&gt;Read on arXiv&lt;/a&gt;</description><pubDate>Fri, 23 May 2025 22:13:16 +0000</pubDate></item><item><title>Discovering Spoofing Attempts on Language Model Watermarks</title><link>https://arxiv.org/abs/2410.02693</link><description>• This paper addresses the security of language model watermarks, specifically focusing on detecting spoofing attacks where adversaries forge watermarks to misattribute text. The authors propose statistical methods to distinguish spoofed from genuine watermarked text, analyze the limitations of current spoofing methods, and provide mitigation strategies. This directly relates to AI security concerns such as model misuse, adversarial attacks, and robustness of attribution mechanisms.&lt;br/&gt;&lt;br/&gt;Tags: watermarking, spoofing attacks, LLM security, adversarial attacks, attribution, robustness&lt;br/&gt;Authors: Thibaud Gloaguen, Nikola Jovanovi\'c, Robin Staab, Martin Vechev&lt;br/&gt;Relevance: 5 / 5&lt;br/&gt;&lt;br/&gt;&lt;a href='https://arxiv.org/abs/2410.02693'&gt;Read on arXiv&lt;/a&gt;</description><pubDate>Fri, 23 May 2025 22:13:16 +0000</pubDate></item><item><title>SuperPure: Efficient Purification of Localized and Distributed Adversarial Patches via Super-Resolution GAN Models</title><link>https://arxiv.org/abs/2505.16318</link><description>• The paper addresses AI security by proposing a new defense (SuperPure) against adversarial patch attacks on vision-based machine learning models. It specifically tackles both localized and distributed adversarial patches, which are methods of attacking AI systems by manipulating input data to evade detection or cause misclassification. The paper evaluates robustness under attack, discusses improvements over existing defenses, and analyzes resistance to white-box attacks.&lt;br/&gt;&lt;br/&gt;Tags: adversarial attacks, robustness, defense methods, computer vision, GANs&lt;br/&gt;Authors: Hossein Khalili, Seongbin Park, Venkat Bollapragada, Nader Sehatbakhsh&lt;br/&gt;Relevance: 5 / 5&lt;br/&gt;&lt;br/&gt;&lt;a href='https://arxiv.org/abs/2505.16318'&gt;Read on arXiv&lt;/a&gt;</description><pubDate>Fri, 23 May 2025 22:13:16 +0000</pubDate></item><item><title>MAPS: A Multilingual Benchmark for Global Agent Performance and Security</title><link>https://arxiv.org/abs/2505.15935</link><description>• The paper introduces MAPS, a multilingual benchmark suite for evaluating agentic AI systems, including their security and robustness across languages.&lt;br/&gt;• It specifically incorporates the Agent Security Benchmark and analyzes how multilingual contexts impact agent performance and security.&lt;br/&gt;• Empirical results highlight degradation in both performance and security in non-English settings, and the paper provides recommendations for improving security in multilingual agentic AI.&lt;br/&gt;&lt;br/&gt;Tags: AI security, multilingual robustness, agentic AI, benchmarking, LLM security&lt;br/&gt;Authors: Omer Hofman, Oren Rachmil, Shamik Bose, Vikas Pahuja, Jonathan Brokman, Toshiya Shimizu, Trisha Starostina, Kelly Marchisio, Seraphina Goldfarb-Tarrant, Roman Vainshtein&lt;br/&gt;Relevance: 4 / 5&lt;br/&gt;&lt;br/&gt;&lt;a href='https://arxiv.org/abs/2505.15935'&gt;Read on arXiv&lt;/a&gt;</description><pubDate>Fri, 23 May 2025 22:13:16 +0000</pubDate></item><item><title>Adversarially Robust Spiking Neural Networks with Sparse Connectivity</title><link>https://arxiv.org/abs/2505.15833</link><description>• This paper presents a method for creating spiking neural networks (SNNs) that are both energy/memory efficient and adversarially robust. The authors introduce a conversion algorithm that leverages sparse connectivity and weights from robustly pretrained artificial neural networks to produce SNNs that maintain high performance and robustness against adversarial attacks. The work explicitly addresses adversarial robustness in the context of neural network deployment.&lt;br/&gt;&lt;br/&gt;Tags: adversarial robustness, spiking neural networks, sparse connectivity, energy efficiency, robustness under attack&lt;br/&gt;Authors: Mathias Schmolli, Maximilian Baronig, Robert Legenstein, Ozan \"Ozdenizci&lt;br/&gt;Relevance: 4 / 5&lt;br/&gt;&lt;br/&gt;&lt;a href='https://arxiv.org/abs/2505.15833'&gt;Read on arXiv&lt;/a&gt;</description><pubDate>Fri, 23 May 2025 22:13:16 +0000</pubDate></item><item><title>Invisible Prompts, Visible Threats: Malicious Font Injection in External Resources for Large Language Models</title><link>https://arxiv.org/abs/2505.16957</link><description>• The paper investigates security vulnerabilities in Large Language Models (LLMs) related to hidden adversarial prompts delivered via malicious font injection in external resources. It demonstrates how attackers can manipulate code-to-glyph mapping to inject invisible prompts, leading to malicious content relay and sensitive data leakage, effectively bypassing LLM safety mechanisms. The study highlights the need for improved security in LLMs when handling external content.&lt;br/&gt;&lt;br/&gt;Tags: adversarial attacks, prompt injection, LLM security, external resource vulnerabilities, font injection, data leakage&lt;br/&gt;Authors: Junjie Xiong, Changjia Zhu, Shuhang Lin, Chong Zhang, Yongfeng Zhang, Yao Liu, Lingyao Li&lt;br/&gt;Relevance: 5 / 5&lt;br/&gt;&lt;br/&gt;&lt;a href='https://arxiv.org/abs/2505.16957'&gt;Read on arXiv&lt;/a&gt;</description><pubDate>Fri, 23 May 2025 22:13:16 +0000</pubDate></item><item><title>Backdoor Cleaning without External Guidance in MLLM Fine-tuning</title><link>https://arxiv.org/abs/2505.16916</link><description>• The paper addresses security risks in Multimodal Large Language Models (MLLMs) related to backdoor attacks introduced during fine-tuning. It proposes a novel defense method (BYE) that detects and removes backdoor samples without external guidance, aiming to improve robustness against such attacks.&lt;br/&gt;&lt;br/&gt;Tags: backdoor attacks, MLLM security, robustness, fine-tuning, defense mechanisms&lt;br/&gt;Authors: Xuankun Rong, Wenke Huang, Jian Liang, Jinhe Bi, Xun Xiao, Yiming Li, Bo Du, Mang Ye&lt;br/&gt;Relevance: 5 / 5&lt;br/&gt;&lt;br/&gt;&lt;a href='https://arxiv.org/abs/2505.16916'&gt;Read on arXiv&lt;/a&gt;</description><pubDate>Fri, 23 May 2025 22:13:16 +0000</pubDate></item><item><title>CAIN: Hijacking LLM-Humans Conversations via a Two-Stage Malicious System Prompt Generation and Refining Framework</title><link>https://arxiv.org/abs/2505.16888</link><description>• The paper introduces CAIN, a framework for generating malicious system prompts that hijack LLM-human conversations, causing LLMs to produce harmful answers to targeted questions while remaining benign on others.&lt;br/&gt;• It demonstrates a novel adversarial attack on LLMs, showing significant impact on both open-source and commercial models in a black-box setting.&lt;br/&gt;• The work highlights the security risks of prompt manipulation and the need for improved robustness in LLMs.&lt;br/&gt;&lt;br/&gt;Tags: adversarial attacks, prompt injection, LLM misuse, robustness, AI security&lt;br/&gt;Authors: Viet Pham, Thai Le&lt;br/&gt;Relevance: 5 / 5&lt;br/&gt;&lt;br/&gt;&lt;a href='https://arxiv.org/abs/2505.16888'&gt;Read on arXiv&lt;/a&gt;</description><pubDate>Fri, 23 May 2025 22:13:16 +0000</pubDate></item><item><title>CoTSRF: Utilize Chain of Thought as Stealthy and Robust Fingerprint of Large Language Models</title><link>https://arxiv.org/abs/2505.16785</link><description>• This paper introduces CoTSRF, a novel method for fingerprinting large language models (LLMs) using their chain-of-thought responses. The approach aims to provide stealthy and robust identification of LLMs behind suspect applications, addressing concerns about abusive usage and model misuse. The method involves extracting unique CoT features from LLM outputs and verifying fingerprints to attribute suspect models, which is directly relevant to AI security topics such as model misuse detection and model attribution.&lt;br/&gt;&lt;br/&gt;Tags: LLM fingerprinting, model attribution, AI misuse detection, robustness, AI security&lt;br/&gt;Authors: Zhenzhen Ren, GuoBiao Li, Sheng Li, Zhenxing Qian, Xinpeng Zhang&lt;br/&gt;Relevance: 4 / 5&lt;br/&gt;&lt;br/&gt;&lt;a href='https://arxiv.org/abs/2505.16785'&gt;Read on arXiv&lt;/a&gt;</description><pubDate>Fri, 23 May 2025 22:13:16 +0000</pubDate></item><item><title>When Safety Detectors Aren't Enough: A Stealthy and Effective Jailbreak Attack on LLMs via Steganographic Techniques</title><link>https://arxiv.org/abs/2505.16765</link><description>• This paper introduces StegoAttack, a novel jailbreak attack on large language models (LLMs) that uses steganographic techniques to stealthily bypass safety detectors.&lt;br/&gt;• It systematically surveys jailbreak methods with a focus on stealth, proposes a new attack that hides harmful queries within benign text, and demonstrates high success rates against multiple safety-aligned LLMs.&lt;br/&gt;• The work directly addresses LLM misuse, jailbreaks, evasion of safety mechanisms, and robustness under attack.&lt;br/&gt;&lt;br/&gt;Tags: jailbreak attacks, LLM security, steganography, adversarial attacks, evasion, robustness&lt;br/&gt;Authors: Jianing Geng, Biao Yi, Zekun Fei, Tongxi Wu, Lihai Nie, Zheli Liu&lt;br/&gt;Relevance: 5 / 5&lt;br/&gt;&lt;br/&gt;&lt;a href='https://arxiv.org/abs/2505.16765'&gt;Read on arXiv&lt;/a&gt;</description><pubDate>Fri, 23 May 2025 22:13:16 +0000</pubDate></item><item><title>Robust LLM Fingerprinting via Domain-Specific Watermarks</title><link>https://arxiv.org/abs/2505.16723</link><description>• The paper proposes a domain-specific watermarking technique for fingerprinting large language models (LLMs), aiming to ensure model provenance and ownership detection. It addresses the robustness and stealthiness of watermarks under real-world deployment scenarios, which are relevant to preventing model theft and unauthorized use.&lt;br/&gt;&lt;br/&gt;Tags: model provenance, watermarking, LLM fingerprinting, model theft prevention, robustness&lt;br/&gt;Authors: Thibaud Gloaguen, Robin Staab, Nikola Jovanovi\'c, Martin Vechev&lt;br/&gt;Relevance: 4 / 5&lt;br/&gt;&lt;br/&gt;&lt;a href='https://arxiv.org/abs/2505.16723'&gt;Read on arXiv&lt;/a&gt;</description><pubDate>Fri, 23 May 2025 22:13:16 +0000</pubDate></item><item><title>BitHydra: Towards Bit-flip Inference Cost Attack against Large Language Models</title><link>https://arxiv.org/abs/2505.16670</link><description>• This paper introduces 'BitHydra', a novel bit-flip attack targeting large language models (LLMs) to induce maximum inference cost by manipulating model parameters. The attack demonstrates high efficiency and transferability, highlighting a new vulnerability in LLMs related to inference cost manipulation via parameter tampering.&lt;br/&gt;&lt;br/&gt;Tags: bit-flip attack, LLM security, model robustness, adversarial attacks, inference cost attack&lt;br/&gt;Authors: Xiaobei Yan, Yiming Li, Zhaoxin Fan, Han Qiu, Tianwei Zhang&lt;br/&gt;Relevance: 5 / 5&lt;br/&gt;&lt;br/&gt;&lt;a href='https://arxiv.org/abs/2505.16670'&gt;Read on arXiv&lt;/a&gt;</description><pubDate>Fri, 23 May 2025 22:13:16 +0000</pubDate></item><item><title>BadVLA: Towards Backdoor Attacks on Vision-Language-Action Models via Objective-Decoupled Optimization</title><link>https://arxiv.org/abs/2505.16640</link><description>• This paper introduces BadVLA, a novel backdoor attack method targeting Vision-Language-Action (VLA) models, which are used in robotic control. The work systematically investigates the security vulnerabilities of VLA models to backdoor attacks, demonstrating high attack success rates and robustness against common defenses. The study highlights the urgent need for secure and trustworthy design practices for embodied AI systems.&lt;br/&gt;&lt;br/&gt;Tags: backdoor attacks, AI security, vision-language-action models, robotics, robustness, model vulnerabilities&lt;br/&gt;Authors: Xueyang Zhou, Guiyao Tie, Guowen Zhang, Hechang Wang, Pan Zhou, Lichao Sun&lt;br/&gt;Relevance: 5 / 5&lt;br/&gt;&lt;br/&gt;&lt;a href='https://arxiv.org/abs/2505.16640'&gt;Read on arXiv&lt;/a&gt;</description><pubDate>Fri, 23 May 2025 22:13:16 +0000</pubDate></item><item><title>CTRAP: Embedding Collapse Trap to Safeguard Large Language Models from Harmful Fine-Tuning</title><link>https://arxiv.org/abs/2505.16559</link><description>• This paper introduces CTRAP, a defense mechanism for large language models (LLMs) that safeguards against harmful fine-tuning attacks. The method embeds a 'collapse trap' during model alignment, which triggers model degradation if malicious fine-tuning is detected, thereby neutralizing the model's utility for attackers. The approach is designed to remain inactive during benign fine-tuning, preserving legitimate use. The paper provides empirical evidence of effectiveness against various attack scenarios.&lt;br/&gt;&lt;br/&gt;Tags: harmful fine-tuning, LLM security, adversarial attacks, robustness, defense mechanisms&lt;br/&gt;Authors: Biao Yi, Tiansheng Huang, Baolei Zhang, Tong Li, Lihai Nie, Zheli Liu, Li Shen&lt;br/&gt;Relevance: 5 / 5&lt;br/&gt;&lt;br/&gt;&lt;a href='https://arxiv.org/abs/2505.16559'&gt;Read on arXiv&lt;/a&gt;</description><pubDate>Fri, 23 May 2025 22:13:16 +0000</pubDate></item><item><title>Privacy-Aware Cyberterrorism Network Analysis using Graph Neural Networks and Federated Learning</title><link>https://arxiv.org/abs/2505.16371</link><description>• The paper proposes a privacy-aware federated graph neural network framework for analyzing cyberterrorism networks.&lt;br/&gt;• It incorporates differential privacy, homomorphic encryption, and defenses against gradient poisoning (a type of adversarial attack) in federated learning.&lt;br/&gt;• The framework is evaluated for robustness under adversarial client behavior, specifically addressing resilience to attacks such as gradient poisoning.&lt;br/&gt;&lt;br/&gt;Tags: federated learning, graph neural networks, differential privacy, homomorphic encryption, gradient poisoning, adversarial robustness, cybersecurity&lt;br/&gt;Authors: Anas Ali, Mubashar Husain, Peter Hans&lt;br/&gt;Relevance: 4 / 5&lt;br/&gt;&lt;br/&gt;&lt;a href='https://arxiv.org/abs/2505.16371'&gt;Read on arXiv&lt;/a&gt;</description><pubDate>Fri, 23 May 2025 22:13:16 +0000</pubDate></item><item><title>All You Need is "Leet": Evading Hate-speech Detection AI</title><link>https://arxiv.org/abs/2505.16263</link><description>• The paper presents black-box adversarial attacks that generate perturbations to evade deep learning-based hate speech detection models, demonstrating high evasion rates while preserving the original meaning of the text. This directly addresses the robustness of AI systems under adversarial attack.&lt;br/&gt;&lt;br/&gt;Tags: adversarial attacks, evasion, robustness, AI security, hate speech detection&lt;br/&gt;Authors: Sampanna Yashwant Kahu, Naman Ahuja&lt;br/&gt;Relevance: 5 / 5&lt;br/&gt;&lt;br/&gt;&lt;a href='https://arxiv.org/abs/2505.16263'&gt;Read on arXiv&lt;/a&gt;</description><pubDate>Fri, 23 May 2025 22:13:16 +0000</pubDate></item><item><title>Does Localization Inform Unlearning? A Rigorous Examination of Local Parameter Attribution for Knowledge Unlearning in Language Models</title><link>https://arxiv.org/abs/2505.16252</link><description>• This paper examines methods for knowledge unlearning in large language models, specifically evaluating the effectiveness of localized parameter updates for removing specific knowledge. Knowledge unlearning is directly relevant to AI security as it can be used to mitigate the retention of sensitive or harmful information, address model misuse, or comply with data removal requests. The paper provides a rigorous evaluation of unlearning techniques, which are important for ensuring that models do not retain or leak unwanted or dangerous information.&lt;br/&gt;&lt;br/&gt;Tags: knowledge unlearning, language models, parameter attribution, AI security, model robustness&lt;br/&gt;Authors: Hwiyeong Lee, Uiji Hwang, Hyelim Lim, Taeuk Kim&lt;br/&gt;Relevance: 4 / 5&lt;br/&gt;&lt;br/&gt;&lt;a href='https://arxiv.org/abs/2505.16252'&gt;Read on arXiv&lt;/a&gt;</description><pubDate>Fri, 23 May 2025 22:13:16 +0000</pubDate></item><item><title>Three Minds, One Legend: Jailbreak Large Reasoning Model with Adaptive Stacked Ciphers</title><link>https://arxiv.org/abs/2505.16241</link><description>• This paper introduces SEAL, a novel jailbreak attack targeting Large Reasoning Models (LRMs) using adaptive stacked ciphers to bypass safety mechanisms and alignment. The work evaluates the attack's effectiveness against state-of-the-art models, demonstrating high success rates and highlighting vulnerabilities in advanced reasoning models.&lt;br/&gt;&lt;br/&gt;Tags: jailbreak, adversarial attacks, LLM misuse, robustness under attack, AI security&lt;br/&gt;Authors: Viet-Anh Nguyen, Shiqian Zhao, Gia Dao, Runyi Hu, Yi Xie, Luu Anh Tuan&lt;br/&gt;Relevance: 5 / 5&lt;br/&gt;&lt;br/&gt;&lt;a href='https://arxiv.org/abs/2505.16241'&gt;Read on arXiv&lt;/a&gt;</description><pubDate>Fri, 23 May 2025 22:13:16 +0000</pubDate></item><item><title>Ranking Free RAG: Replacing Re-ranking with Selection in RAG for Sensitive Domains</title><link>https://arxiv.org/abs/2505.16014</link><description>• This paper introduces METEORA, a rationale-driven selection method for Retrieval-Augmented Generation (RAG) pipelines, specifically designed to improve robustness and safety in sensitive domains.&lt;br/&gt;• The method includes a Verifier LLM to detect and filter poisoned or misleading content, directly addressing adversarial attacks and data poisoning.&lt;br/&gt;• Evaluation demonstrates significant improvements in resilience to poisoning attacks compared to existing defenses, making the work highly relevant to AI security.&lt;br/&gt;&lt;br/&gt;Tags: RAG, adversarial attacks, poisoning, robustness, LLM security, safe generation&lt;br/&gt;Authors: Yash Saxena, Anpur Padia, Mandar S Chaudhary, Kalpa Gunaratna, Srinivasan Parthasarathy, Manas Gaur&lt;br/&gt;Relevance: 5 / 5&lt;br/&gt;&lt;br/&gt;&lt;a href='https://arxiv.org/abs/2505.16014'&gt;Read on arXiv&lt;/a&gt;</description><pubDate>Fri, 23 May 2025 22:13:16 +0000</pubDate></item><item><title>ATR-Bench: A Federated Learning Benchmark for Adaptation, Trust, and Reasoning</title><link>https://arxiv.org/abs/2505.16850</link><description>• The paper introduces ATR-Bench, a federated learning benchmark that explicitly evaluates trustworthiness in adversarial or unreliable environments. It benchmarks methods for adaptation to heterogeneous clients and addresses challenges related to adversarial threats in federated learning, which are core concerns in AI security.&lt;br/&gt;&lt;br/&gt;Tags: federated learning, adversarial environments, trustworthiness, AI security, benchmark&lt;br/&gt;Authors: Tajamul Ashraf, Mohammed Mohsen Peerzada, Moloud Abdar, Yutong Xie, Yuyin Zhou, Xiaofeng Liu, Iqra Altaf Gillani, Janibul Bashir&lt;br/&gt;Relevance: 4 / 5&lt;br/&gt;&lt;br/&gt;&lt;a href='https://arxiv.org/abs/2505.16850'&gt;Read on arXiv&lt;/a&gt;</description><pubDate>Fri, 23 May 2025 22:13:16 +0000</pubDate></item><item><title>Mitigating Fine-tuning Risks in LLMs via Safety-Aware Probing Optimization</title><link>https://arxiv.org/abs/2505.16737</link><description>• This paper addresses the degradation of safety in large language models (LLMs) during fine-tuning, even on benign data. It proposes a safety-aware probing optimization framework to mitigate risks of harmful content generation, directly targeting AI safety and robustness against misuse or unintended behaviors.&lt;br/&gt;&lt;br/&gt;Tags: LLM safety, fine-tuning risks, harmful content mitigation, robustness, AI safety&lt;br/&gt;Authors: Chengcan Wu, Zhixin Zhang, Zeming Wei, Yihao Zhang, Meng Sun&lt;br/&gt;Relevance: 5 / 5&lt;br/&gt;&lt;br/&gt;&lt;a href='https://arxiv.org/abs/2505.16737'&gt;Read on arXiv&lt;/a&gt;</description><pubDate>Fri, 23 May 2025 22:13:16 +0000</pubDate></item><item><title>Finetuning-Activated Backdoors in LLMs</title><link>https://arxiv.org/abs/2505.16567</link><description>• This paper introduces a novel attack called Finetuning-Activated Backdoor (FAB) targeting Large Language Models (LLMs). The attack involves poisoning LLMs so that they behave benignly until a downstream user finetunes them, at which point malicious behaviors are triggered. The paper demonstrates the effectiveness and robustness of this attack across multiple models and behaviors, directly addressing a new AI security threat vector related to model poisoning and backdoors.&lt;br/&gt;&lt;br/&gt;Tags: backdoor attacks, model poisoning, LLM security, finetuning, adversarial attacks&lt;br/&gt;Authors: Thibaud Gloaguen, Mark Vero, Robin Staab, Martin Vechev&lt;br/&gt;Relevance: 5 / 5&lt;br/&gt;&lt;br/&gt;&lt;a href='https://arxiv.org/abs/2505.16567'&gt;Read on arXiv&lt;/a&gt;</description><pubDate>Fri, 23 May 2025 22:13:16 +0000</pubDate></item><item><title>Implicit Jailbreak Attacks via Cross-Modal Information Concealment on Vision-Language Models</title><link>https://arxiv.org/abs/2505.16446</link><description>• This paper introduces a novel implicit jailbreak attack framework (IJA) targeting vision-language models (MLLMs). The attack stealthily embeds malicious instructions into images using steganography and combines them with benign-looking prompts, bypassing cross-modal alignment defenses. The method is evaluated on commercial models like GPT-4o and Gemini-1.5 Pro, achieving high attack success rates. The work directly addresses AI security concerns such as jailbreaks, adversarial attacks, and robustness under attack.&lt;br/&gt;&lt;br/&gt;Tags: jailbreak attacks, adversarial attacks, vision-language models, steganography, AI security, robustness&lt;br/&gt;Authors: Zhaoxin Wang, Handing Wang, Cong Tian, Yaochu Jin&lt;br/&gt;Relevance: 5 / 5&lt;br/&gt;&lt;br/&gt;&lt;a href='https://arxiv.org/abs/2505.16446'&gt;Read on arXiv&lt;/a&gt;</description><pubDate>Fri, 23 May 2025 22:13:16 +0000</pubDate></item><item><title>Performance Guaranteed Poisoning Attacks in Federated Learning: A Sliding Mode Approach</title><link>https://arxiv.org/abs/2505.16403</link><description>• The paper proposes a novel poisoning attack (FedSA) on federated learning systems, leveraging sliding mode control theory to precisely and stealthily degrade global model performance. The attack allows adversaries to control the extent of model compromise, demonstrating effectiveness with fewer malicious clients and maintaining stealth.&lt;br/&gt;&lt;br/&gt;Tags: poisoning attacks, federated learning, model robustness, adversarial attacks, AI security&lt;br/&gt;Authors: Huazi Pan, Yanjun Zhang, Leo Yu Zhang, Scott Adams, Abbas Kouzani, Suiyang Khoo&lt;br/&gt;Relevance: 5 / 5&lt;br/&gt;&lt;br/&gt;&lt;a href='https://arxiv.org/abs/2505.16403'&gt;Read on arXiv&lt;/a&gt;</description><pubDate>Fri, 23 May 2025 22:13:16 +0000</pubDate></item><item><title>LAGO: Few-shot Crosslingual Embedding Inversion Attacks via Language Similarity-Aware Graph Optimization</title><link>https://arxiv.org/abs/2505.16008</link><description>• The paper introduces LAGO, a method for few-shot cross-lingual embedding inversion attacks, which exploit privacy vulnerabilities in multilingual NLP systems. It models linguistic relationships to improve the effectiveness and transferability of these attacks, highlighting privacy and security risks in multilingual embeddings.&lt;br/&gt;&lt;br/&gt;Tags: embedding inversion, privacy attacks, cross-lingual NLP, adversarial attacks, AI security&lt;br/&gt;Authors: Wenrui Yu, Yiyi Chen, Johannes Bjerva, Sokol Kosta, Qiongxiu Li&lt;br/&gt;Relevance: 5 / 5&lt;br/&gt;&lt;br/&gt;&lt;a href='https://arxiv.org/abs/2505.16008'&gt;Read on arXiv&lt;/a&gt;</description><pubDate>Fri, 23 May 2025 22:13:16 +0000</pubDate></item><item><title>Interpretability Illusions with Sparse Autoencoders: Evaluating Robustness of Concept Representations</title><link>https://arxiv.org/abs/2505.16004</link><description>• This paper investigates the robustness of concept representations in sparse autoencoders (SAEs) used for interpreting large language models (LLMs), specifically focusing on their vulnerability to adversarial input perturbations. The authors develop an evaluation framework to test how easily adversarial attacks can manipulate these concept-based interpretations without affecting the LLM's outputs. Their findings highlight that SAE concept representations are fragile and susceptible to adversarial manipulation, raising concerns for their use in model monitoring and oversight.&lt;br/&gt;&lt;br/&gt;Tags: adversarial attacks, robustness, interpretability, LLM security&lt;br/&gt;Authors: Aaron J. Li, Suraj Srinivas, Usha Bhalla, Himabindu Lakkaraju&lt;br/&gt;Relevance: 4 / 5&lt;br/&gt;&lt;br/&gt;&lt;a href='https://arxiv.org/abs/2505.16004'&gt;Read on arXiv&lt;/a&gt;</description><pubDate>Fri, 23 May 2025 22:13:16 +0000</pubDate></item><item><title>SafeKey: Amplifying Aha-Moment Insights for Safety Reasoning</title><link>https://arxiv.org/abs/2505.16186</link><description>• This paper addresses the safety of Large Reasoning Models (LRMs) against harmful queries and adversarial attacks, specifically focusing on improving generalization to unseen jailbreak prompts.&lt;br/&gt;• It introduces SafeKey, a method to enhance safety reasoning within LRMs, and demonstrates improved robustness against jailbreak attacks and harmful prompts.&lt;br/&gt;• The work includes experiments on safety benchmarks and analyzes how the proposed method improves internal model representations for safety.&lt;br/&gt;&lt;br/&gt;Tags: AI safety, jailbreaks, adversarial attacks, LLM misuse, robustness, red teaming&lt;br/&gt;Authors: Kaiwen Zhou, Xuandong Zhao, Gaowen Liu, Jayanth Srinivasa, Aosong Feng, Dawn Song, Xin Eric Wang&lt;br/&gt;Relevance: 5 / 5&lt;br/&gt;&lt;br/&gt;&lt;a href='https://arxiv.org/abs/2505.16186'&gt;Read on arXiv&lt;/a&gt;</description><pubDate>Fri, 23 May 2025 22:13:16 +0000</pubDate></item><item><title>LLM-Powered AI Agent Systems and Their Applications in Industry</title><link>https://arxiv.org/abs/2505.16120</link><description>• The paper surveys LLM-powered agent systems, their evolution, and industrial applications. It explicitly discusses primary challenges, including security vulnerabilities, and proposes potential solutions to mitigate these concerns.&lt;br/&gt;&lt;br/&gt;Tags: LLM security, agent systems, security vulnerabilities, industrial applications&lt;br/&gt;Authors: Guannan Liang, Qianqian Tong&lt;br/&gt;Relevance: 2 / 5&lt;br/&gt;&lt;br/&gt;&lt;a href='https://arxiv.org/abs/2505.16120'&gt;Read on arXiv&lt;/a&gt;</description><pubDate>Fri, 23 May 2025 22:13:16 +0000</pubDate></item></channel></rss>