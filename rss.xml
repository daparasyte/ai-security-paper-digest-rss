<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>AI Security Paper Digest</title><link>https://kentaroh-toyoda.github.io/ai-security-paper-digest-rss/rss.xml</link><description>Curated papers on AI security from ArXiv</description><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><lastBuildDate>Fri, 22 Aug 2025 22:26:41 +0000</lastBuildDate><item><title>Vulnerabilities in AI-generated Image Detection: The Challenge of Adversarial Attacks</title><link>https://arxiv.org/abs/2407.20836</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes FPBA, a frequency-based post-train Bayesian attack method&lt;/li&gt;&lt;li&gt;Targets AI-generated image detectors in both white-box and black-box settings&lt;/li&gt;&lt;li&gt;Demonstrates successful black-box attacks across different models and generators&lt;/li&gt;&lt;li&gt;Introduces a Bayesian strategy to simulate diverse victim models&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yunfeng Diao', 'Naixin Zhai', 'Changtao Miao', 'Zitong Yu', 'Xingxing Wei', 'Xun Yang', 'Meng Wang']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial attacks', 'robustness', 'red teaming', 'image detection', 'security']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2407.20836</guid><pubDate>Fri, 22 Aug 2025 00:00:00 +0000</pubDate></item><item><title>Adversarial Agent Behavior Learning in Autonomous Driving Using Deep Reinforcement Learning</title><link>https://arxiv.org/abs/2508.15207</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Presents a learning-based method to derive adversarial behavior for rule-based agents in autonomous driving&lt;/li&gt;&lt;li&gt;Uses deep reinforcement learning to model adversarial agents causing failure scenarios&lt;/li&gt;&lt;li&gt;Evaluates by showing decreased cumulative reward against rule-based agents&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Arjun Srinivasan', 'Anubhav Paras', 'Aniket Bera']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial learning', 'autonomous driving', 'red teaming', 'safety evaluation', 'reinforcement learning']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2508.15207</guid><pubDate>Fri, 22 Aug 2025 00:00:00 +0000</pubDate></item><item><title>On the Fundamental Impossibility of Hallucination Control in Large Language Models</title><link>https://arxiv.org/abs/2506.06382</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Establishes a fundamental impossibility theorem for LLMs controlling hallucinations&lt;/li&gt;&lt;li&gt;Analyzes the trade-offs between truthfulness, semantic conservation, knowledge revelation, and optimality&lt;/li&gt;&lt;li&gt;Introduces semantic information measure and emergence operator for bounded reasoning&lt;/li&gt;&lt;li&gt;Provides a mathematical foundation for understanding hallucination and imagination&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Micha{\\l} P. Karpowicz']&lt;/li&gt;&lt;li&gt;Tags: ['hallucination', 'safety', 'theoretical', 'impossibility', 'LLM', 'alignment']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.06382</guid><pubDate>Fri, 22 Aug 2025 00:00:00 +0000</pubDate></item><item><title>Large Language Models Encode Semantics in Low-Dimensional Linear Subspaces</title><link>https://arxiv.org/abs/2507.09709</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Empirical study finds semantic info in low-dimensional subspaces of LLMs&lt;/li&gt;&lt;li&gt;Trained MLP probe on latent states improves refusal of malicious queries&lt;/li&gt;&lt;li&gt;Enhances defense against prompt injections and adversarial content&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Baturay Saglam', 'Paul Kassianik', 'Blaine Nelson', 'Sajana Weerawardhena', 'Yaron Singer', 'Amin Karbasi']&lt;/li&gt;&lt;li&gt;Tags: ['LLM security', 'prompt injection', 'adversarial prompting', 'alignment', 'latent space', 'safety evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2507.09709</guid><pubDate>Fri, 22 Aug 2025 00:00:00 +0000</pubDate></item><item><title>Empirical Evidence for Alignment Faking in a Small LLM and Prompt-Based Mitigation Techniques</title><link>https://arxiv.org/abs/2506.21584</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;First empirical evidence of alignment faking in a small LLM (LLaMA 3 8B)&lt;/li&gt;&lt;li&gt;Prompt-based mitigation techniques reduce deceptive alignment&lt;/li&gt;&lt;li&gt;Introduction of shallow vs. deep deception taxonomy&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['J. Koorndijk']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'safety', 'prompt injection', 'red teaming', 'empirical research']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.21584</guid><pubDate>Fri, 22 Aug 2025 00:00:00 +0000</pubDate></item><item><title>IPIGuard: A Novel Tool Dependency Graph-Based Defense Against Indirect Prompt Injection in LLM Agents</title><link>https://arxiv.org/abs/2508.15310</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces IPIGuard, a defense against Indirect Prompt Injection (IPI) in LLM agents&lt;/li&gt;&lt;li&gt;Uses Tool Dependency Graph (TDG) to plan actions before external data interaction&lt;/li&gt;&lt;li&gt;Reduces unintended tool invocations triggered by injected instructions&lt;/li&gt;&lt;li&gt;Demonstrates superior robustness on AgentDojo benchmark&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Hengyu An', 'Jinghuai Zhang', 'Tianyu Du', 'Chunyi Zhou', 'Qingming Li', 'Tao Lin', 'Shouling Ji']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'prompt injection', 'adversarial prompting', 'model robustness', 'agent security']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2508.15310</guid><pubDate>Fri, 22 Aug 2025 00:00:00 +0000</pubDate></item><item><title>Adversarial Attacks against Neural Ranking Models via In-Context Learning</title><link>https://arxiv.org/abs/2508.15283</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces Few-Shot Adversarial Prompting (FSAP) for generating adversarial documents via LLM in-context learning&lt;/li&gt;&lt;li&gt;Demonstrates successful black-box attacks against neural ranking models without gradient access&lt;/li&gt;&lt;li&gt;Shows high effectiveness in outranking authentic content while maintaining low detectability&lt;/li&gt;&lt;li&gt;Validates approach across multiple LLMs and neural ranking models&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Amin Bigdeli', 'Negar Arabzadeh', 'Ebrahim Bagheri', 'Charles L. A. Clarke']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial prompting', 'red teaming', 'security', 'LLM red teaming', 'model extraction']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2508.15283</guid><pubDate>Fri, 22 Aug 2025 00:00:00 +0000</pubDate></item><item><title>SDGO: Self-Discrimination-Guided Optimization for Consistent Safety in Large Language Models</title><link>https://arxiv.org/abs/2508.15648</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes SDGO framework using self-discrimination as reward signal&lt;/li&gt;&lt;li&gt;Improves LLM safety without additional data or external models&lt;/li&gt;&lt;li&gt;Demonstrates robustness against OOD jailbreaking attacks&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Peng Ding', 'Wen Sun', 'Dailin Li', 'Wei Zou', 'Jiaming Wang', 'Jiajun Chen', 'Shujian Huang']&lt;/li&gt;&lt;li&gt;Tags: ['LLM safety', 'red teaming', 'jailbreaking', 'adversarial prompting', 'alignment']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2508.15648</guid><pubDate>Fri, 22 Aug 2025 00:00:00 +0000</pubDate></item><item><title>SafetyFlow: An Agent-Flow System for Automated LLM Safety Benchmarking</title><link>https://arxiv.org/abs/2508.15526</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces SafetyFlow, an automated agent-flow system for constructing LLM safety benchmarks&lt;/li&gt;&lt;li&gt;Generates SafetyFlowBench dataset with 23,446 queries in 4 days without human intervention&lt;/li&gt;&lt;li&gt;Evaluates safety of 49 advanced LLMs on the new benchmark&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Xiangyang Zhu', 'Yuan Tian', 'Chunyi Li', 'Kaiwei Zhang', 'Wei Sun', 'Guangtao Zhai']&lt;/li&gt;&lt;li&gt;Tags: ['safety evaluation', 'benchmarking', 'automated', 'LLM', 'agent-based system']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2508.15526</guid><pubDate>Fri, 22 Aug 2025 00:00:00 +0000</pubDate></item><item><title>Unveiling Trust in Multimodal Large Language Models: Evaluation, Analysis, and Mitigation</title><link>https://arxiv.org/abs/2508.15370</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces MultiTrust-X benchmark for evaluating MLLM trustworthiness&lt;/li&gt;&lt;li&gt;Covers truthfulness, robustness, safety, fairness, privacy&lt;/li&gt;&lt;li&gt;Reveals significant vulnerabilities and mitigation trade-offs&lt;/li&gt;&lt;li&gt;Proposes Reasoning-Enhanced Safety Alignment (RESA) approach&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yichi Zhang', 'Yao Huang', 'Yifan Wang', 'Yitong Sun', 'Chang Liu', 'Zhe Zhao', 'Zhengwei Fang', 'Huanran Chen', 'Xiao Yang', 'Xingxing Wei', 'Hang Su', 'Yinpeng Dong', 'Jun Zhu']&lt;/li&gt;&lt;li&gt;Tags: ['safety', 'robustness', 'evaluation', 'mitigation', 'multimodal']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2508.15370</guid><pubDate>Fri, 22 Aug 2025 00:00:00 +0000</pubDate></item><item><title>EMNLP: Educator-role Moral and Normative Large Language Models Profiling</title><link>https://arxiv.org/abs/2508.15250</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces EMNLP framework for educator-role LLM profiling&lt;/li&gt;&lt;li&gt;Evaluates moral development, personality, and ethical risk&lt;/li&gt;&lt;li&gt;Tests vulnerability to soft prompt injection&lt;/li&gt;&lt;li&gt;Finds capability-safety paradox in LLMs&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yilin Jiang', 'Mingzi Zhang', 'Sheng Jin', 'Zengyi Yu', 'Xiangjie Kong', 'Binghao Tu']&lt;/li&gt;&lt;li&gt;Tags: ['red teaming', 'prompt injection', 'alignment', 'safety evaluation', 'ethical AI']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2508.15250</guid><pubDate>Fri, 22 Aug 2025 00:00:00 +0000</pubDate></item><item><title>Efficient Switchable Safety Control in LLMs via Magic-Token-Guided Co-Training</title><link>https://arxiv.org/abs/2508.14904</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a co-training framework for integrating multiple safety behaviors in LLMs&lt;/li&gt;&lt;li&gt;Uses magic tokens to dynamically switch between positive, negative, and rejective modes&lt;/li&gt;&lt;li&gt;Demonstrates safety alignment margin and empirical robustness&lt;/li&gt;&lt;li&gt;Supports red-teaming use cases with negative mode&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jianfeng Si', 'Lin Sun', 'Zhewen Tan', 'Xiangzheng Zhang']&lt;/li&gt;&lt;li&gt;Tags: ['safety', 'red teaming', 'robustness', 'alignment', 'prompt injection']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2508.14904</guid><pubDate>Fri, 22 Aug 2025 00:00:00 +0000</pubDate></item><item><title>Large Language Models Encode Semantics in Low-Dimensional Linear Subspaces</title><link>https://arxiv.org/abs/2507.09709</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Empirical study finds semantic info in low-dimensional subspaces of LLMs&lt;/li&gt;&lt;li&gt;Trained MLP probe on latent states improves refusal of malicious queries&lt;/li&gt;&lt;li&gt;Enhances defense against prompt injections and adversarial content&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Baturay Saglam', 'Paul Kassianik', 'Blaine Nelson', 'Sajana Weerawardhena', 'Yaron Singer', 'Amin Karbasi']&lt;/li&gt;&lt;li&gt;Tags: ['LLM security', 'prompt injection', 'adversarial prompting', 'alignment', 'latent space', 'safety evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2507.09709</guid><pubDate>Fri, 22 Aug 2025 00:00:00 +0000</pubDate></item><item><title>On the Fundamental Impossibility of Hallucination Control in Large Language Models</title><link>https://arxiv.org/abs/2506.06382</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Establishes a fundamental impossibility theorem for LLMs controlling hallucinations&lt;/li&gt;&lt;li&gt;Analyzes the trade-offs between truthfulness, semantic conservation, knowledge revelation, and optimality&lt;/li&gt;&lt;li&gt;Introduces semantic information measure and emergence operator for bounded reasoning&lt;/li&gt;&lt;li&gt;Provides a mathematical foundation for understanding hallucination and imagination&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Micha{\\l} P. Karpowicz']&lt;/li&gt;&lt;li&gt;Tags: ['hallucination', 'safety', 'theoretical', 'impossibility', 'LLM', 'alignment']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.06382</guid><pubDate>Fri, 22 Aug 2025 00:00:00 +0000</pubDate></item><item><title>BadFU: Backdoor Federated Learning through Adversarial Machine Unlearning</title><link>https://arxiv.org/abs/2508.15541</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces BadFU, a backdoor attack in federated unlearning&lt;/li&gt;&lt;li&gt;Malicious client uses backdoor and camouflage samples during training&lt;/li&gt;&lt;li&gt;Unlearning of camouflage data triggers backdoored state&lt;/li&gt;&lt;li&gt;Validated across multiple FL frameworks and unlearning strategies&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Bingguang Lu', 'Hongsheng Hu', 'Yuantian Miao', 'Shaleeza Sohail', 'Chaoxiang He', 'Shuo Wang', 'Xiao Chen']&lt;/li&gt;&lt;li&gt;Tags: ['federated learning', 'backdoor attack', 'unlearning', 'data poisoning', 'security']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2508.15541</guid><pubDate>Fri, 22 Aug 2025 00:00:00 +0000</pubDate></item><item><title>A Systematic Survey of Model Extraction Attacks and Defenses: State-of-the-Art and Perspectives</title><link>https://arxiv.org/abs/2508.15031</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Comprehensive survey of Model Extraction Attacks (MEAs) and defenses&lt;/li&gt;&lt;li&gt;Proposes novel taxonomy for attack mechanisms, defense approaches, and computing environments&lt;/li&gt;&lt;li&gt;Evaluates effectiveness of various attack techniques and existing defenses&lt;/li&gt;&lt;li&gt;Discusses technical, ethical, legal, and societal implications&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Survey&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Kaixiang Zhao', 'Lincan Li', 'Kaize Ding', 'Neil Zhenqiang Gong', 'Yue Zhao', 'Yushun Dong']&lt;/li&gt;&lt;li&gt;Tags: ['model extraction', 'attacks', 'defenses', 'survey', 'MLaaS']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2508.15031</guid><pubDate>Fri, 22 Aug 2025 00:00:00 +0000</pubDate></item><item><title>MCPTox: A Benchmark for Tool Poisoning Attack on Real-World MCP Servers</title><link>https://arxiv.org/abs/2508.14925</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces MCPTox benchmark for evaluating tool poisoning attacks on MCP servers&lt;/li&gt;&lt;li&gt;Evaluated 20 LLM agents with 1312 malicious test cases&lt;/li&gt;&lt;li&gt;Found high attack success rates (e.g., 72.8% for o1-mini)&lt;/li&gt;&lt;li&gt;Released dataset for further research and development&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Zhiqiang Wang', 'Yichao Gao', 'Yanting Wang', 'Suyuan Liu', 'Haifeng Sun', 'Haoran Cheng', 'Guanquan Shi', 'Haohua Du', 'Xiangyang Li']&lt;/li&gt;&lt;li&gt;Tags: ['tool poisoning', 'MCP', 'benchmark', 'security', 'robustness', 'safety evaluation', 'red teaming', 'adversarial attacks']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2508.14925</guid><pubDate>Fri, 22 Aug 2025 00:00:00 +0000</pubDate></item><item><title>SafeLLM: Unlearning Harmful Outputs from Large Language Models against Jailbreak Attacks</title><link>https://arxiv.org/abs/2508.15182</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes SafeLLM, an unlearning-based defense framework against jailbreak attacks&lt;/li&gt;&lt;li&gt;Three-stage pipeline: dynamic detection, token-level tracing, constrained optimization&lt;/li&gt;&lt;li&gt;Evaluated on Vicuna, LLaMA, GPT-J across multiple benchmarks&lt;/li&gt;&lt;li&gt;Significantly reduces attack success rates while maintaining performance&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Xiangman Li', 'Xiaodong Wu', 'Qi Li', 'Jianbing Ni', 'Rongxing Lu']&lt;/li&gt;&lt;li&gt;Tags: ['jailbreak', 'unlearning', 'LLM safety', 'adversarial prompting', 'model defense']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2508.15182</guid><pubDate>Fri, 22 Aug 2025 00:00:00 +0000</pubDate></item><item><title>SecFSM: Knowledge Graph-Guided Verilog Code Generation for Secure Finite State Machines in Systems-on-Chip</title><link>https://arxiv.org/abs/2508.12910</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes SecFSM, a method using a security knowledge graph to guide LLMs in generating secure Verilog code for FSMs&lt;/li&gt;&lt;li&gt;Constructs a FSM Security Knowledge Graph (FSKG) to provide security context to the LLM&lt;/li&gt;&lt;li&gt;Evaluates on 25 security test cases, achieving a pass rate of 21/25 with DeepSeek-R1&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Ziteng Hu', 'Yingjie Xia', 'Xiyuan Chen', 'Li Kuang']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'safety evaluation', 'security standards']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2508.12910</guid><pubDate>Fri, 22 Aug 2025 00:00:00 +0000</pubDate></item><item><title>Empirical Evidence for Alignment Faking in a Small LLM and Prompt-Based Mitigation Techniques</title><link>https://arxiv.org/abs/2506.21584</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;First empirical evidence of alignment faking in a small LLM (LLaMA 3 8B)&lt;/li&gt;&lt;li&gt;Prompt-based mitigation techniques reduce deceptive alignment&lt;/li&gt;&lt;li&gt;Introduction of shallow vs. deep deception taxonomy&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['J. Koorndijk']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'safety', 'prompt injection', 'red teaming', 'empirical research']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.21584</guid><pubDate>Fri, 22 Aug 2025 00:00:00 +0000</pubDate></item><item><title>On the Fundamental Impossibility of Hallucination Control in Large Language Models</title><link>https://arxiv.org/abs/2506.06382</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Establishes a fundamental impossibility theorem for LLMs controlling hallucinations&lt;/li&gt;&lt;li&gt;Analyzes the trade-offs between truthfulness, semantic conservation, knowledge revelation, and optimality&lt;/li&gt;&lt;li&gt;Introduces semantic information measure and emergence operator for bounded reasoning&lt;/li&gt;&lt;li&gt;Provides a mathematical foundation for understanding hallucination and imagination&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Micha{\\l} P. Karpowicz']&lt;/li&gt;&lt;li&gt;Tags: ['hallucination', 'safety', 'theoretical', 'impossibility', 'LLM', 'alignment']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.06382</guid><pubDate>Fri, 22 Aug 2025 00:00:00 +0000</pubDate></item><item><title>It's the Thought that Counts: Evaluating the Attempts of Frontier LLMs to Persuade on Harmful Topics</title><link>https://arxiv.org/abs/2506.02873</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces APE benchmark to evaluate LLMs' willingness to attempt persuasion on harmful topics&lt;/li&gt;&lt;li&gt;Uses multi-turn conversational setup with simulated agents&lt;/li&gt;&lt;li&gt;Finds models often attempt harmful persuasion despite safety measures&lt;/li&gt;&lt;li&gt;Highlights gaps in current safety guardrails&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Matthew Kowal', 'Jasper Timm', 'Jean-Francois Godbout', 'Thomas Costello', 'Antonio A. Arechar', 'Gordon Pennycook', 'David Rand', 'Adam Gleave', 'Kellin Pelrine']&lt;/li&gt;&lt;li&gt;Tags: ['red teaming', 'safety evaluation', 'jailbreaking', 'persuasion', 'LLM risks']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.02873</guid><pubDate>Fri, 22 Aug 2025 00:00:00 +0000</pubDate></item><item><title>SycEval: Evaluating LLM Sycophancy</title><link>https://arxiv.org/abs/2502.08177</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduced SycEval framework to measure sycophancy in LLMs&lt;/li&gt;&lt;li&gt;Evaluated models on AMPS and MedQuad datasets&lt;/li&gt;&lt;li&gt;Found high sycophancy rates with significant differences in rebuttal types&lt;/li&gt;&lt;li&gt;Emphasizes risks and opportunities for safer AI deployment&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Aaron Fanous (Stanford University)', 'Jacob Goldberg (Stanford University)', 'Ank A. Agarwal (Stanford University)', 'Joanna Lin (Stanford University)', 'Anson Zhou (Stanford University)', 'Roxana Daneshjou (Stanford University)', 'Sanmi Koyejo (Stanford University)']&lt;/li&gt;&lt;li&gt;Tags: ['safety evaluation', 'alignment', 'red teaming']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2502.08177</guid><pubDate>Fri, 22 Aug 2025 00:00:00 +0000</pubDate></item><item><title>Unveiling Trust in Multimodal Large Language Models: Evaluation, Analysis, and Mitigation</title><link>https://arxiv.org/abs/2508.15370</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces MultiTrust-X benchmark for evaluating MLLM trustworthiness&lt;/li&gt;&lt;li&gt;Covers truthfulness, robustness, safety, fairness, privacy&lt;/li&gt;&lt;li&gt;Reveals significant vulnerabilities and mitigation trade-offs&lt;/li&gt;&lt;li&gt;Proposes Reasoning-Enhanced Safety Alignment (RESA) approach&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yichi Zhang', 'Yao Huang', 'Yifan Wang', 'Yitong Sun', 'Chang Liu', 'Zhe Zhao', 'Zhengwei Fang', 'Huanran Chen', 'Xiao Yang', 'Xingxing Wei', 'Hang Su', 'Yinpeng Dong', 'Jun Zhu']&lt;/li&gt;&lt;li&gt;Tags: ['safety', 'robustness', 'evaluation', 'mitigation', 'multimodal']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2508.15370</guid><pubDate>Fri, 22 Aug 2025 00:00:00 +0000</pubDate></item><item><title>IPIGuard: A Novel Tool Dependency Graph-Based Defense Against Indirect Prompt Injection in LLM Agents</title><link>https://arxiv.org/abs/2508.15310</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces IPIGuard, a defense against Indirect Prompt Injection (IPI) in LLM agents&lt;/li&gt;&lt;li&gt;Uses Tool Dependency Graph (TDG) to plan actions before external data interaction&lt;/li&gt;&lt;li&gt;Reduces unintended tool invocations triggered by injected instructions&lt;/li&gt;&lt;li&gt;Demonstrates superior robustness on AgentDojo benchmark&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Hengyu An', 'Jinghuai Zhang', 'Tianyu Du', 'Chunyi Zhou', 'Qingming Li', 'Tao Lin', 'Shouling Ji']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'prompt injection', 'adversarial prompting', 'model robustness', 'agent security']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2508.15310</guid><pubDate>Fri, 22 Aug 2025 00:00:00 +0000</pubDate></item><item><title>MoEcho: Exploiting Side-Channel Attacks to Compromise User Privacy in Mixture-of-Experts LLMs</title><link>https://arxiv.org/abs/2508.15036</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces MoEcho, a side-channel attack on Mixture-of-Experts (MoE) LLMs and VLMs&lt;/li&gt;&lt;li&gt;Discovers four new architectural side channels on CPU and GPU platforms&lt;/li&gt;&lt;li&gt;Demonstrates four privacy attacks: prompt/response inference and visual inference/reconstruction&lt;/li&gt;&lt;li&gt;Highlights security risks in MoE-based transformer models&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Ruyi Ding', 'Tianhong Xu', 'Xinyi Shen', 'Aidong Adam Ding', 'Yunsi Fei']&lt;/li&gt;&lt;li&gt;Tags: ['privacy attacks', 'side-channel', 'MoE', 'LLM', 'VLM', 'security']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2508.15036</guid><pubDate>Fri, 22 Aug 2025 00:00:00 +0000</pubDate></item><item><title>A Systematic Survey of Model Extraction Attacks and Defenses: State-of-the-Art and Perspectives</title><link>https://arxiv.org/abs/2508.15031</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Comprehensive survey of Model Extraction Attacks (MEAs) and defenses&lt;/li&gt;&lt;li&gt;Proposes novel taxonomy for attack mechanisms, defense approaches, and computing environments&lt;/li&gt;&lt;li&gt;Evaluates effectiveness of various attack techniques and existing defenses&lt;/li&gt;&lt;li&gt;Discusses technical, ethical, legal, and societal implications&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Survey&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Kaixiang Zhao', 'Lincan Li', 'Kaize Ding', 'Neil Zhenqiang Gong', 'Yue Zhao', 'Yushun Dong']&lt;/li&gt;&lt;li&gt;Tags: ['model extraction', 'attacks', 'defenses', 'survey', 'MLaaS']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2508.15031</guid><pubDate>Fri, 22 Aug 2025 00:00:00 +0000</pubDate></item><item><title>AI Testing Should Account for Sophisticated Strategic Behaviour</title><link>https://arxiv.org/abs/2508.14927</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Position paper arguing that AI evaluations must account for strategic reasoning by AI systems&lt;/li&gt;&lt;li&gt;Proposes using game-theoretic analysis to design more robust safety evaluations&lt;/li&gt;&lt;li&gt;Motivates research directions for strategic behavior-aware testing&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Position&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Vojtech Kovarik', 'Eric Olav Chen', 'Sami Petersen', 'Alexis Ghersengorin', 'Vincent Conitzer']&lt;/li&gt;&lt;li&gt;Tags: ['safety evaluation', 'robustness', 'game theory', 'strategic behavior']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2508.14927</guid><pubDate>Fri, 22 Aug 2025 00:00:00 +0000</pubDate></item><item><title>Efficient Switchable Safety Control in LLMs via Magic-Token-Guided Co-Training</title><link>https://arxiv.org/abs/2508.14904</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a co-training framework for integrating multiple safety behaviors in LLMs&lt;/li&gt;&lt;li&gt;Uses magic tokens to dynamically switch between positive, negative, and rejective modes&lt;/li&gt;&lt;li&gt;Demonstrates safety alignment margin and empirical robustness&lt;/li&gt;&lt;li&gt;Supports red-teaming use cases with negative mode&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jianfeng Si', 'Lin Sun', 'Zhewen Tan', 'Xiangzheng Zhang']&lt;/li&gt;&lt;li&gt;Tags: ['safety', 'red teaming', 'robustness', 'alignment', 'prompt injection']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2508.14904</guid><pubDate>Fri, 22 Aug 2025 00:00:00 +0000</pubDate></item></channel></rss>