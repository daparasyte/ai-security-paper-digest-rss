<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>AI Security Paper Digest</title><link>https://kentaroh-toyoda.github.io/ai-security-paper-digest-rss/rss.xml</link><description>Curated papers on AI security from OpenAlex and ArXiv</description><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><lastBuildDate>Fri, 18 Jul 2025 22:22:20 +0000</lastBuildDate><item><title>Prompt-driven Transferable Adversarial Attack on Person Re-Identification with Attribute-aware Textual Inversion</title><link>https://arxiv.org/abs/2502.19697</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces Attribute-aware Prompt Attack (AP-Attack) using VLMs for transferable adversarial attacks on person re-identification&lt;/li&gt;&lt;li&gt;Leverages textual inversion to disrupt attribute-specific semantic features&lt;/li&gt;&lt;li&gt;Achieves state-of-the-art transferability with 22.9% mean Drop Rate improvement&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yuan Bian', 'Min Liu', 'Yunqi Yi', 'Xueping Wang', 'Yaonan Wang']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial prompting', 'prompt injection', 'model robustness', 'transferable attacks', 'vision-language models']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2502.19697</guid><pubDate>Fri, 18 Jul 2025 00:00:00 +0000</pubDate></item><item><title>Secure Multifaceted-RAG for Enterprise: Hybrid Knowledge Retrieval with Security Filtering</title><link>https://arxiv.org/abs/2504.13425</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes SecMulti-RAG framework for secure enterprise RAG&lt;/li&gt;&lt;li&gt;Combines internal docs, expert knowledge, and filtered external LLMs&lt;/li&gt;&lt;li&gt;Uses local open-source model to prevent data leakage&lt;/li&gt;&lt;li&gt;Evaluates security and performance in automotive report generation&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Grace Byun', 'Shinsun Lee', 'Nayoung Choi', 'Jinho D. Choi']&lt;/li&gt;&lt;li&gt;Tags: ['security', 'data_privacy', 'RAG', 'LLM', 'enterprise', 'prompt_filtering']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2504.13425</guid><pubDate>Fri, 18 Jul 2025 00:00:00 +0000</pubDate></item><item><title>JailDAM: Jailbreak Detection with Adaptive Memory for Vision-Language Model</title><link>https://arxiv.org/abs/2504.03770</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces JailDAM, a test-time adaptive framework for detecting jailbreak attacks in multimodal LLMs&lt;/li&gt;&lt;li&gt;Addresses key challenges of existing methods: white-box requirements, computational overhead, and labeled data scarcity&lt;/li&gt;&lt;li&gt;Uses policy-driven unsafe knowledge representations with dynamic memory updates&lt;/li&gt;&lt;li&gt;Demonstrates state-of-the-art performance on VLM jailbreak benchmarks&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yi Nian', 'Shenzhe Zhu', 'Yuehan Qin', 'Li Li', 'Ziyi Wang', 'Chaowei Xiao', 'Yue Zhao']&lt;/li&gt;&lt;li&gt;Tags: ['jailbreak detection', 'adaptive memory', 'vision-language models', 'security', 'red teaming']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2504.03770</guid><pubDate>Fri, 18 Jul 2025 00:00:00 +0000</pubDate></item><item><title>Risks of ignoring uncertainty propagation in AI-augmented security pipelines</title><link>https://arxiv.org/abs/2407.14540</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Addresses uncertainty propagation in AI-augmented security pipelines&lt;/li&gt;&lt;li&gt;Provides formal underpinnings and simulation for quantifying uncertainty&lt;/li&gt;&lt;li&gt;Evaluates with a case study and discusses generalizability&lt;/li&gt;&lt;li&gt;Presents recommendations for AI system evaluation policies&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Emanuele Mezzi', 'Aurora Papotti', 'Fabio Massacci', 'Katja Tuma']&lt;/li&gt;&lt;li&gt;Tags: ['safety evaluation', 'uncertainty propagation', 'robustness', 'security standards']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2407.14540</guid><pubDate>Fri, 18 Jul 2025 00:00:00 +0000</pubDate></item><item><title>Automating Steering for Safe Multimodal Large Language Models</title><link>https://arxiv.org/abs/2507.13255</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces AutoSteer, an inference-time intervention framework for MLLMs&lt;/li&gt;&lt;li&gt;Components include Safety Awareness Score, adaptive safety prober, and Refusal Head&lt;/li&gt;&lt;li&gt;Reduces Attack Success Rate (ASR) for textual, visual, and cross-modal threats&lt;/li&gt;&lt;li&gt;Maintains general model abilities while improving safety&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Lyucheng Wu', 'Mengru Wang', 'Ziwen Xu', 'Tri Cao', 'Nay Oo', 'Bryan Hooi', 'Shumin Deng']&lt;/li&gt;&lt;li&gt;Tags: ['safety', 'multimodal', 'adversarial', 'intervention', 'MLLM', 'AutoSteer']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2507.13255</guid><pubDate>Fri, 18 Jul 2025 00:00:00 +0000</pubDate></item><item><title>Prompt Injection 2.0: Hybrid AI Threats</title><link>https://arxiv.org/abs/2507.13169</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Analyzes Prompt Injection 2.0 attacks combining with traditional web security vulnerabilities&lt;/li&gt;&lt;li&gt;Evaluates existing mitigation technologies against hybrid AI-web threats&lt;/li&gt;&lt;li&gt;Presents architectural solutions for prompt isolation and runtime security&lt;/li&gt;&lt;li&gt;Demonstrates failures of traditional security controls against AI-enhanced attacks&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jeremy McHugh', 'Kristina \\v{S}ekrst', 'Jon Cefalu']&lt;/li&gt;&lt;li&gt;Tags: ['prompt injection', 'hybrid threats', 'XSS', 'CSRF', 'LLM security']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2507.13169</guid><pubDate>Fri, 18 Jul 2025 00:00:00 +0000</pubDate></item><item><title>Manipulation Attacks by Misaligned AI: Risk Analysis and Safety Case Framework</title><link>https://arxiv.org/abs/2507.12872</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces a safety case framework for manipulation attacks by misaligned AI&lt;/li&gt;&lt;li&gt;Framework structured around inability, control, and trustworthiness arguments&lt;/li&gt;&lt;li&gt;Provides evidence requirements and evaluation methodologies for each argument&lt;/li&gt;&lt;li&gt;Aims to integrate manipulation risk into AI safety governance&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Rishane Dassanayake', 'Mario Demetroudi', 'James Walpole', 'Lindley Lentati', 'Jason R. Brown', 'Edward James Young']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'safety evaluation', 'security standards', 'robustness', 'risk analysis']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2507.12872</guid><pubDate>Fri, 18 Jul 2025 00:00:00 +0000</pubDate></item><item><title>Benchmarking Deception Probes via Black-to-White Performance Boosts</title><link>https://arxiv.org/abs/2507.12691</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces black-to-white performance boost metric to evaluate deception probes&lt;/li&gt;&lt;li&gt;Compares white-box vs black-box monitoring effectiveness&lt;/li&gt;&lt;li&gt;Finds weak but encouraging performance improvements from existing probes&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Avi Parrack', 'Carlo Leonardo Attubato', 'Stefan Heimersheim']&lt;/li&gt;&lt;li&gt;Tags: ['red teaming', 'safety evaluation', 'adversarial', 'probes', 'deception']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2507.12691</guid><pubDate>Fri, 18 Jul 2025 00:00:00 +0000</pubDate></item></channel></rss>