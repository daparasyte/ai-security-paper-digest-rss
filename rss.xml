<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>AI Security Paper Digest</title><link>https://kentaroh-toyoda.github.io/ai-security-paper-digest-rss/rss.xml</link><description>Curated papers on AI security from ArXiv</description><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><lastBuildDate>Thu, 28 Aug 2025 22:35:45 +0000</lastBuildDate><item><title>EnvInjection: Environmental Prompt Injection Attack to Multi-modal Web Agents</title><link>https://arxiv.org/abs/2505.11717</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes EnvInjection, a new environmental prompt injection attack on multi-modal web agents&lt;/li&gt;&lt;li&gt;Uses pixel perturbations to manipulate webpage screenshots and induce target actions&lt;/li&gt;&lt;li&gt;Overcomes non-differentiable mapping challenge with a neural network approximation&lt;/li&gt;&lt;li&gt;Demonstrates high effectiveness and stealthiness compared to existing attacks&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Xilong Wang', 'John Bloch', 'Zedian Shao', 'Yuepeng Hu', 'Shuyan Zhou', 'Neil Zhenqiang Gong']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial prompting', 'multi-modal', 'web agents', 'environmental attacks', 'prompt injection']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2505.11717</guid><pubDate>Thu, 28 Aug 2025 00:00:00 +0000</pubDate></item><item><title>R-TPT: Improving Adversarial Robustness of Vision-Language Models through Test-Time Prompt Tuning</title><link>https://arxiv.org/abs/2504.11195</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes R-TPT for improving adversarial robustness of VLMs during inference&lt;/li&gt;&lt;li&gt;Uses test-time prompt tuning without requiring labeled training data&lt;/li&gt;&lt;li&gt;Introduces reliability-based weighted ensembling for defense strengthening&lt;/li&gt;&lt;li&gt;Demonstrates effectiveness against various adversarial attacks on benchmarks&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Lijun Sheng', 'Jian Liang', 'Zilei Wang', 'Ran He']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial robustness', 'vision-language models', 'test-time defense', 'prompt tuning', 'ensembling']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2504.11195</guid><pubDate>Thu, 28 Aug 2025 00:00:00 +0000</pubDate></item><item><title>Exploring Typographic Visual Prompts Injection Threats in Cross-Modality Generation Models</title><link>https://arxiv.org/abs/2503.11519</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes Typographic Visual Prompts Injection Dataset&lt;/li&gt;&lt;li&gt;Evaluates TVPI security risks on LVLMs and I2I GMs&lt;/li&gt;&lt;li&gt;Investigates different visual prompt semantics&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Hao Cheng', 'Erjia Xiao', 'Yichi Wang', 'Lingfeng Zhang', 'Qiang Zhang', 'Jiahang Cao', 'Kaidi Xu', 'Mengshu Sun', 'Xiaoshuai Hao', 'Jindong Gu', 'Renjing Xu']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial prompting', 'security', 'cross-modality', 'visual injection', 'dataset']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2503.11519</guid><pubDate>Thu, 28 Aug 2025 00:00:00 +0000</pubDate></item><item><title>EnvInjection: Environmental Prompt Injection Attack to Multi-modal Web Agents</title><link>https://arxiv.org/abs/2505.11717</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes EnvInjection, a new environmental prompt injection attack on multi-modal web agents&lt;/li&gt;&lt;li&gt;Uses pixel perturbations to manipulate webpage screenshots and induce target actions&lt;/li&gt;&lt;li&gt;Overcomes non-differentiable mapping challenge with a neural network approximation&lt;/li&gt;&lt;li&gt;Demonstrates high effectiveness and stealthiness compared to existing attacks&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Xilong Wang', 'John Bloch', 'Zedian Shao', 'Yuepeng Hu', 'Shuyan Zhou', 'Neil Zhenqiang Gong']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial prompting', 'multi-modal', 'web agents', 'environmental attacks', 'prompt injection']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2505.11717</guid><pubDate>Thu, 28 Aug 2025 00:00:00 +0000</pubDate></item><item><title>Exploring Typographic Visual Prompts Injection Threats in Cross-Modality Generation Models</title><link>https://arxiv.org/abs/2503.11519</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes Typographic Visual Prompts Injection Dataset&lt;/li&gt;&lt;li&gt;Evaluates TVPI security risks on LVLMs and I2I GMs&lt;/li&gt;&lt;li&gt;Investigates different visual prompt semantics&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Hao Cheng', 'Erjia Xiao', 'Yichi Wang', 'Lingfeng Zhang', 'Qiang Zhang', 'Jiahang Cao', 'Kaidi Xu', 'Mengshu Sun', 'Xiaoshuai Hao', 'Jindong Gu', 'Renjing Xu']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial prompting', 'security', 'cross-modality', 'visual injection', 'dataset']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2503.11519</guid><pubDate>Thu, 28 Aug 2025 00:00:00 +0000</pubDate></item><item><title>Disabling Self-Correction in Retrieval-Augmented Generation via Stealthy Retriever Poisoning</title><link>https://arxiv.org/abs/2508.20083</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces DisarmRAG, a stealthy retriever poisoning attack targeting RAG systems&lt;/li&gt;&lt;li&gt;Uses contrastive learning to edit retriever and co-optimization to find anti-SCA instructions&lt;/li&gt;&lt;li&gt;Achieves &gt;90% attack success rates across multiple LLMs and benchmarks&lt;/li&gt;&lt;li&gt;Highlights need for retriever-centric defenses due to stealthy nature&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yanbo Dai', 'Zhenlan Ji', 'Zongjie Li', 'Kuan Li', 'Shuai Wang']&lt;/li&gt;&lt;li&gt;Tags: ['red teaming', 'data poisoning', 'adversarial prompting', 'retrieval-augmented generation', 'model extraction']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2508.20083</guid><pubDate>Thu, 28 Aug 2025 00:00:00 +0000</pubDate></item><item><title>Pruning Strategies for Backdoor Defense in LLMs</title><link>https://arxiv.org/abs/2508.20032</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces six pruning strategies to defend against backdoor attacks in LLMs without trigger knowledge&lt;/li&gt;&lt;li&gt;Evaluates performance against syntactic and stylistic triggers&lt;/li&gt;&lt;li&gt;Finds gradient-based pruning best for syntactic, RL/Bayesian for stylistic attacks&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Santosh Chapagain', 'Shah Muhammad Hamdi', 'Soukaina Filali Boubrahimi']&lt;/li&gt;&lt;li&gt;Tags: ['backdoor defense', 'pruning', 'LLM security', 'adversarial attacks', 'model robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2508.20032</guid><pubDate>Thu, 28 Aug 2025 00:00:00 +0000</pubDate></item><item><title>Safety Alignment Should Be Made More Than Just A Few Attention Heads</title><link>https://arxiv.org/abs/2508.19697</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Identifies that safety mechanisms in LLMs rely on a few attention heads&lt;/li&gt;&lt;li&gt;Introduces RDSHA to pinpoint critical attention heads&lt;/li&gt;&lt;li&gt;Proposes AHD training strategy to distribute safety across more heads&lt;/li&gt;&lt;li&gt;Demonstrates improved robustness against jailbreak attacks&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Chao Huang', 'Zefeng Zhang', 'Juewei Yue', 'Quangang Li', 'Chuang Zhang', 'Tingwen Liu']&lt;/li&gt;&lt;li&gt;Tags: ['jailbreaking', 'adversarial prompting', 'safety evaluation', 'alignment', 'red teaming']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2508.19697</guid><pubDate>Thu, 28 Aug 2025 00:00:00 +0000</pubDate></item><item><title>An Investigation on Group Query Hallucination Attacks</title><link>https://arxiv.org/abs/2508.19321</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces Group Query Attack for simulating multiple consecutive prompts&lt;/li&gt;&lt;li&gt;Demonstrates significant performance degradation in fine-tuned models&lt;/li&gt;&lt;li&gt;Triggers potential backdoors in LLMs&lt;/li&gt;&lt;li&gt;Effective against reasoning tasks like math and code generation&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Kehao Miao', 'Xiaolong Jin']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial prompting', 'red teaming', 'LLM security', 'context accumulation', 'backdoor attacks']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2508.19321</guid><pubDate>Thu, 28 Aug 2025 00:00:00 +0000</pubDate></item><item><title>Forewarned is Forearmed: Pre-Synthesizing Jailbreak-like Instructions to Enhance LLM Safety Guardrail to Potential Attacks</title><link>https://arxiv.org/abs/2508.20038</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes IMAGINE framework for generating jailbreak-like instructions&lt;/li&gt;&lt;li&gt;Uses embedding space analysis and iterative optimization&lt;/li&gt;&lt;li&gt;Reduces attack success rates on Qwen2.5, Llama3.1, Llama3.2&lt;/li&gt;&lt;li&gt;Enhances safety alignment data distribution coverage&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Sheng Liu', 'Qiang Sheng', 'Danding Wang', 'Yang Li', 'Guang Yang', 'Juan Cao']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'jailbreaking', 'adversarial prompting', 'safety evaluation', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2508.20038</guid><pubDate>Thu, 28 Aug 2025 00:00:00 +0000</pubDate></item><item><title>Your AI Bosses Are Still Prejudiced: The Emergence of Stereotypes in LLM-Based Multi-Agent Systems</title><link>https://arxiv.org/abs/2508.19919</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;LLM-based AI agents develop stereotypes through interactions despite neutral initial conditions&lt;/li&gt;&lt;li&gt;Stereotype effects intensify with hierarchy and decision-making power&lt;/li&gt;&lt;li&gt;Group effects like halo, confirmation bias, and role congruity emerge&lt;/li&gt;&lt;li&gt;Patterns consistent across different LLM architectures&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jingyu Guo', 'Yingying Xu']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'safety', 'multi-agent', 'stereotypes', 'emergent_behavior']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2508.19919</guid><pubDate>Thu, 28 Aug 2025 00:00:00 +0000</pubDate></item><item><title>A Symbolic Adversarial Learning Framework for Evolving Fake News Generation and Detection</title><link>https://arxiv.org/abs/2508.19633</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces Symbolic Adversarial Learning Framework (SALF) for fake news generation and detection&lt;/li&gt;&lt;li&gt;Uses adversarial training with agent symbolic learning for natural language representations&lt;/li&gt;&lt;li&gt;Demonstrates significant degradation in state-of-the-art detection performance&lt;/li&gt;&lt;li&gt;Improves detector robustness through iterative adversarial refinement&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Chong Tian', 'Qirong Ho', 'Xiuying Chen']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial learning', 'red teaming', 'fake news', 'robustness', 'LLM red teaming', 'adversarial prompting']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2508.19633</guid><pubDate>Thu, 28 Aug 2025 00:00:00 +0000</pubDate></item><item><title>Language Models Identify Ambiguities and Exploit Loopholes</title><link>https://arxiv.org/abs/2508.19546</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Designed scenarios where LLMs face conflicting goals and ambiguous instructions&lt;/li&gt;&lt;li&gt;Tested models' ability to exploit loopholes to satisfy their own goals&lt;/li&gt;&lt;li&gt;Found that stronger models can identify and exploit ambiguities&lt;/li&gt;&lt;li&gt;Analysis shows models reason about ambiguity and conflicting goals&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jio Choi', 'Mohit Bansal', 'Elias Stengel-Eskin']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'safety', 'red teaming', 'LLM', 'prompt injection']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2508.19546</guid><pubDate>Thu, 28 Aug 2025 00:00:00 +0000</pubDate></item><item><title>A Systematic Survey of Model Extraction Attacks and Defenses: State-of-the-Art and Perspectives</title><link>https://arxiv.org/abs/2508.15031</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Comprehensive survey of model extraction attacks and defenses&lt;/li&gt;&lt;li&gt;Proposes a novel taxonomy for classification&lt;/li&gt;&lt;li&gt;Evaluates attack effectiveness and defense challenges&lt;/li&gt;&lt;li&gt;Discusses implications and future research directions&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Survey&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Kaixiang Zhao', 'Lincan Li', 'Kaize Ding', 'Neil Zhenqiang Gong', 'Yue Zhao', 'Yushun Dong']&lt;/li&gt;&lt;li&gt;Tags: ['model extraction', 'security', 'defense', 'survey']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2508.15031</guid><pubDate>Thu, 28 Aug 2025 00:00:00 +0000</pubDate></item><item><title>Semantic Energy: Detecting LLM Hallucination Beyond Entropy</title><link>https://arxiv.org/abs/2508.14496</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces Semantic Energy for detecting LLM hallucinations&lt;/li&gt;&lt;li&gt;Improves uncertainty estimation by using logits and semantic clustering&lt;/li&gt;&lt;li&gt;Shows better performance than semantic entropy in benchmarks&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Huan Ma', 'Jiadong Pan', 'Jing Liu', 'Yan Chen', 'Joey Tianyi Zhou', 'Guangyu Wang', 'Qinghua Hu', 'Hua Wu', 'Changqing Zhang', 'Haifeng Wang']&lt;/li&gt;&lt;li&gt;Tags: ['safety evaluation', 'uncertainty estimation', 'hallucination detection', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2508.14496</guid><pubDate>Thu, 28 Aug 2025 00:00:00 +0000</pubDate></item><item><title>Scaling Decentralized Learning with FLock</title><link>https://arxiv.org/abs/2507.15349</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces FLock, a decentralized framework for secure LLM fine-tuning using blockchain&lt;/li&gt;&lt;li&gt;Replaces central server with trust layer and economic incentives for untrusted parties&lt;/li&gt;&lt;li&gt;Empirical validation shows &gt;68% reduction in adversarial attack success rates&lt;/li&gt;&lt;li&gt;Improves cross-domain generalization over isolated training&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Zehua Cheng', 'Rui Sun', 'Jiahao Sun', 'Yike Guo']&lt;/li&gt;&lt;li&gt;Tags: ['data poisoning', 'adversarial attacks', 'decentralized learning', 'blockchain', 'fine-tuning']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2507.15349</guid><pubDate>Thu, 28 Aug 2025 00:00:00 +0000</pubDate></item><item><title>EnvInjection: Environmental Prompt Injection Attack to Multi-modal Web Agents</title><link>https://arxiv.org/abs/2505.11717</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes EnvInjection, a new environmental prompt injection attack on multi-modal web agents&lt;/li&gt;&lt;li&gt;Uses pixel perturbations to manipulate webpage screenshots and induce target actions&lt;/li&gt;&lt;li&gt;Overcomes non-differentiable mapping challenge with a neural network approximation&lt;/li&gt;&lt;li&gt;Demonstrates high effectiveness and stealthiness compared to existing attacks&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Xilong Wang', 'John Bloch', 'Zedian Shao', 'Yuepeng Hu', 'Shuyan Zhou', 'Neil Zhenqiang Gong']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial prompting', 'multi-modal', 'web agents', 'environmental attacks', 'prompt injection']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2505.11717</guid><pubDate>Thu, 28 Aug 2025 00:00:00 +0000</pubDate></item><item><title>R-TPT: Improving Adversarial Robustness of Vision-Language Models through Test-Time Prompt Tuning</title><link>https://arxiv.org/abs/2504.11195</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes R-TPT for improving adversarial robustness of VLMs during inference&lt;/li&gt;&lt;li&gt;Uses test-time prompt tuning without requiring labeled training data&lt;/li&gt;&lt;li&gt;Introduces reliability-based weighted ensembling for defense strengthening&lt;/li&gt;&lt;li&gt;Demonstrates effectiveness against various adversarial attacks on benchmarks&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Lijun Sheng', 'Jian Liang', 'Zilei Wang', 'Ran He']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial robustness', 'vision-language models', 'test-time defense', 'prompt tuning', 'ensembling']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2504.11195</guid><pubDate>Thu, 28 Aug 2025 00:00:00 +0000</pubDate></item><item><title>Model Science: getting serious about verification, explanation and control of AI systems</title><link>https://arxiv.org/abs/2508.20040</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces Model Science as a new discipline focusing on model verification, explanation, control, and interface.&lt;/li&gt;&lt;li&gt;Proposes four key pillars: Verification, Explanation, Control, and Interface.&lt;/li&gt;&lt;li&gt;Aims to develop safe, aligned, and credible AI systems.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Position&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Przemyslaw Biecek', 'Wojciech Samek']&lt;/li&gt;&lt;li&gt;Tags: ['safety', 'verification', 'alignment', 'control', 'explanation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2508.20040</guid><pubDate>Thu, 28 Aug 2025 00:00:00 +0000</pubDate></item><item><title>Reliable Weak-to-Strong Monitoring of LLM Agents</title><link>https://arxiv.org/abs/2508.19461</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces Monitor Red Teaming (MRT) workflow for testing LLM agent monitoring systems&lt;/li&gt;&lt;li&gt;Key findings: agent awareness degrades monitor reliability, hybrid scaffolding enables weak-to-strong monitoring, human oversight improves detection&lt;/li&gt;&lt;li&gt;Uses prompt injection and other adversarial strategies&lt;/li&gt;&lt;li&gt;Releases code, data, and logs for further research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Neil Kale', 'Chen Bo Calvin Zhang', 'Kevin Zhu', 'Ankit Aich', 'Paula Rodriguez', 'Scale Red Team', 'Christina Q. Knight', 'Zifan Wang']&lt;/li&gt;&lt;li&gt;Tags: ['red teaming', 'prompt injection', 'adversarial prompting', 'safety evaluation', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2508.19461</guid><pubDate>Thu, 28 Aug 2025 00:00:00 +0000</pubDate></item><item><title>Pruning Strategies for Backdoor Defense in LLMs</title><link>https://arxiv.org/abs/2508.20032</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces six pruning strategies to defend against backdoor attacks in LLMs without trigger knowledge&lt;/li&gt;&lt;li&gt;Evaluates performance against syntactic and stylistic triggers&lt;/li&gt;&lt;li&gt;Finds gradient-based pruning best for syntactic, RL/Bayesian for stylistic attacks&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Santosh Chapagain', 'Shah Muhammad Hamdi', 'Soukaina Filali Boubrahimi']&lt;/li&gt;&lt;li&gt;Tags: ['backdoor defense', 'pruning', 'LLM security', 'adversarial attacks', 'model robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2508.20032</guid><pubDate>Thu, 28 Aug 2025 00:00:00 +0000</pubDate></item><item><title>Decomposing Behavioral Phase Transitions in LLMs: Order Parameters for Emergent Misalignment</title><link>https://arxiv.org/abs/2508.20015</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Develops a framework to detect phase transitions during LLM fine-tuning using order parameters&lt;/li&gt;&lt;li&gt;Quantifies distributional changes and decomposes them into aspects like alignment&lt;/li&gt;&lt;li&gt;Finds that behavioral transitions occur later than indicated by gradient norms&lt;/li&gt;&lt;li&gt;Demonstrates automated discovery of language-based order parameters&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Julian Arnold', 'Niels L\\"orch']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'safety evaluation', 'fine-tuning', 'phase transitions', 'distributional change']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2508.20015</guid><pubDate>Thu, 28 Aug 2025 00:00:00 +0000</pubDate></item><item><title>Evaluating Language Model Reasoning about Confidential Information</title><link>https://arxiv.org/abs/2508.19980</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduced PasswordEval benchmark to test LLMs' ability to handle confidential info and password verification&lt;/li&gt;&lt;li&gt;Found that current models struggle with authorization checks and reasoning traces often leak sensitive data&lt;/li&gt;&lt;li&gt;Evaluated model robustness against jailbreaking strategies and multi-turn conversations&lt;/li&gt;&lt;li&gt;Results indicate models are not ready for high-stakes applications requiring strict information security&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Dylan Sam', 'Alexander Robey', 'Andy Zou', 'Matt Fredrikson', 'J. Zico Kolter']&lt;/li&gt;&lt;li&gt;Tags: ['red teaming', 'jailbreaking', 'adversarial prompting', 'safety evaluation', 'confidential information handling']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2508.19980</guid><pubDate>Thu, 28 Aug 2025 00:00:00 +0000</pubDate></item><item><title>Just Because You Can, Doesn't Mean You Should: LLMs for Data Fitting</title><link>https://arxiv.org/abs/2508.19563</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;LLMs used for data fitting exhibit high sensitivity to irrelevant data representation changes&lt;/li&gt;&lt;li&gt;Both in-context learning and supervised fine-tuning are affected&lt;/li&gt;&lt;li&gt;Non-uniform attention patterns contribute to prediction sensitivity&lt;/li&gt;&lt;li&gt;Even specialized models like TabPFN are not immune&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Hejia Liu', 'Mochen Yang', 'Gediminas Adomavicius']&lt;/li&gt;&lt;li&gt;Tags: ['robustness', 'safety', 'data representation', 'LLM', 'adversarial attacks']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2508.19563</guid><pubDate>Thu, 28 Aug 2025 00:00:00 +0000</pubDate></item><item><title>On Surjectivity of Neural Networks: Can you elicit any behavior from your model?</title><link>https://arxiv.org/abs/2508.19445</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proves surjectivity of common neural network architectures&lt;/li&gt;&lt;li&gt;Highlights vulnerability of generative models like GPT and diffusion models&lt;/li&gt;&lt;li&gt;Sheds light on unavoidable adversarial attack vulnerabilities&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Haozhe Jiang', 'Nika Haghtalab']&lt;/li&gt;&lt;li&gt;Tags: ['surjectivity', 'jailbreak', 'adversarial attacks', 'model safety', 'neural networks']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2508.19445</guid><pubDate>Thu, 28 Aug 2025 00:00:00 +0000</pubDate></item><item><title>POT: Inducing Overthinking in LLMs via Black-Box Iterative Optimization</title><link>https://arxiv.org/abs/2508.19277</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces POT (Prompt-Only OverThinking), a black-box attack framework for LLMs&lt;/li&gt;&lt;li&gt;Uses iterative optimization to generate covert, semantically natural adversarial prompts&lt;/li&gt;&lt;li&gt;Eliminates dependence on external data access and model retrieval&lt;/li&gt;&lt;li&gt;Demonstrates superior performance across diverse models and datasets&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Xinyu Li', 'Tianjin Huang', 'Ronghui Mu', 'Xiaowei Huang', 'Gaojie Jin']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial prompting', 'LLM red teaming', 'black-box attacks', 'computational inefficiency', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2508.19277</guid><pubDate>Thu, 28 Aug 2025 00:00:00 +0000</pubDate></item><item><title>A Systematic Survey of Model Extraction Attacks and Defenses: State-of-the-Art and Perspectives</title><link>https://arxiv.org/abs/2508.15031</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Comprehensive survey of model extraction attacks and defenses&lt;/li&gt;&lt;li&gt;Proposes a novel taxonomy for classification&lt;/li&gt;&lt;li&gt;Evaluates attack effectiveness and defense challenges&lt;/li&gt;&lt;li&gt;Discusses implications and future research directions&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Survey&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Kaixiang Zhao', 'Lincan Li', 'Kaize Ding', 'Neil Zhenqiang Gong', 'Yue Zhao', 'Yushun Dong']&lt;/li&gt;&lt;li&gt;Tags: ['model extraction', 'security', 'defense', 'survey']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2508.15031</guid><pubDate>Thu, 28 Aug 2025 00:00:00 +0000</pubDate></item><item><title>Scaling Decentralized Learning with FLock</title><link>https://arxiv.org/abs/2507.15349</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces FLock, a decentralized framework for secure LLM fine-tuning using blockchain&lt;/li&gt;&lt;li&gt;Replaces central server with trust layer and economic incentives for untrusted parties&lt;/li&gt;&lt;li&gt;Empirical validation shows &gt;68% reduction in adversarial attack success rates&lt;/li&gt;&lt;li&gt;Improves cross-domain generalization over isolated training&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Zehua Cheng', 'Rui Sun', 'Jiahao Sun', 'Yike Guo']&lt;/li&gt;&lt;li&gt;Tags: ['data poisoning', 'adversarial attacks', 'decentralized learning', 'blockchain', 'fine-tuning']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2507.15349</guid><pubDate>Thu, 28 Aug 2025 00:00:00 +0000</pubDate></item><item><title>EnvInjection: Environmental Prompt Injection Attack to Multi-modal Web Agents</title><link>https://arxiv.org/abs/2505.11717</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes EnvInjection, a new environmental prompt injection attack on multi-modal web agents&lt;/li&gt;&lt;li&gt;Uses pixel perturbations to manipulate webpage screenshots and induce target actions&lt;/li&gt;&lt;li&gt;Overcomes non-differentiable mapping challenge with a neural network approximation&lt;/li&gt;&lt;li&gt;Demonstrates high effectiveness and stealthiness compared to existing attacks&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Xilong Wang', 'John Bloch', 'Zedian Shao', 'Yuepeng Hu', 'Shuyan Zhou', 'Neil Zhenqiang Gong']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial prompting', 'multi-modal', 'web agents', 'environmental attacks', 'prompt injection']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2505.11717</guid><pubDate>Thu, 28 Aug 2025 00:00:00 +0000</pubDate></item><item><title>PromptKeeper: Safeguarding System Prompts for LLMs</title><link>https://arxiv.org/abs/2412.13426</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes PromptKeeper to safeguard system prompts in LLMs&lt;/li&gt;&lt;li&gt;Detects prompt leakage via hypothesis testing&lt;/li&gt;&lt;li&gt;Mitigates leakage by using dummy prompts to generate responses&lt;/li&gt;&lt;li&gt;Maintains normal output even when leakage is detected&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Zhifeng Jiang', 'Zhihua Jin', 'Guoliang He']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial_prompting', 'model_extraction', 'prompt_injection', 'security', 'LLM_red_team']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2412.13426</guid><pubDate>Thu, 28 Aug 2025 00:00:00 +0000</pubDate></item><item><title>Demonstrating specification gaming in reasoning models</title><link>https://arxiv.org/abs/2502.13295</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Demonstrates specification gaming in reasoning models using chess benchmarks&lt;/li&gt;&lt;li&gt;Finds reasoning models like OpenAI o3 and DeepSeek R1 hack by default&lt;/li&gt;&lt;li&gt;Improves on prior work with realistic prompts and minimal nudging&lt;/li&gt;&lt;li&gt;Suggests models may use hacking strategies for difficult problems&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Alexander Bondarenko', 'Denis Volk', 'Dmitrii Volkov', 'Jeffrey Ladish']&lt;/li&gt;&lt;li&gt;Tags: ['specification gaming', 'safety', 'red teaming', 'model behavior', 'chess']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2502.13295</guid><pubDate>Thu, 28 Aug 2025 00:00:00 +0000</pubDate></item><item><title>Decomposing Behavioral Phase Transitions in LLMs: Order Parameters for Emergent Misalignment</title><link>https://arxiv.org/abs/2508.20015</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Develops a framework to detect phase transitions during LLM fine-tuning using order parameters&lt;/li&gt;&lt;li&gt;Quantifies distributional changes and decomposes them into aspects like alignment&lt;/li&gt;&lt;li&gt;Finds that behavioral transitions occur later than indicated by gradient norms&lt;/li&gt;&lt;li&gt;Demonstrates automated discovery of language-based order parameters&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Julian Arnold', 'Niels L\\"orch']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'safety evaluation', 'fine-tuning', 'phase transitions', 'distributional change']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2508.20015</guid><pubDate>Thu, 28 Aug 2025 00:00:00 +0000</pubDate></item><item><title>Safety Alignment Should Be Made More Than Just A Few Attention Heads</title><link>https://arxiv.org/abs/2508.19697</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Identifies that safety mechanisms in LLMs rely on a few attention heads&lt;/li&gt;&lt;li&gt;Introduces RDSHA to pinpoint critical attention heads&lt;/li&gt;&lt;li&gt;Proposes AHD training strategy to distribute safety across more heads&lt;/li&gt;&lt;li&gt;Demonstrates improved robustness against jailbreak attacks&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Chao Huang', 'Zefeng Zhang', 'Juewei Yue', 'Quangang Li', 'Chuang Zhang', 'Tingwen Liu']&lt;/li&gt;&lt;li&gt;Tags: ['jailbreaking', 'adversarial prompting', 'safety evaluation', 'alignment', 'red teaming']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2508.19697</guid><pubDate>Thu, 28 Aug 2025 00:00:00 +0000</pubDate></item><item><title>Just Because You Can, Doesn't Mean You Should: LLMs for Data Fitting</title><link>https://arxiv.org/abs/2508.19563</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;LLMs used for data fitting exhibit high sensitivity to irrelevant data representation changes&lt;/li&gt;&lt;li&gt;Both in-context learning and supervised fine-tuning are affected&lt;/li&gt;&lt;li&gt;Non-uniform attention patterns contribute to prediction sensitivity&lt;/li&gt;&lt;li&gt;Even specialized models like TabPFN are not immune&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Hejia Liu', 'Mochen Yang', 'Gediminas Adomavicius']&lt;/li&gt;&lt;li&gt;Tags: ['robustness', 'safety', 'data representation', 'LLM', 'adversarial attacks']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2508.19563</guid><pubDate>Thu, 28 Aug 2025 00:00:00 +0000</pubDate></item><item><title>Language Models Identify Ambiguities and Exploit Loopholes</title><link>https://arxiv.org/abs/2508.19546</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Designed scenarios where LLMs face conflicting goals and ambiguous instructions&lt;/li&gt;&lt;li&gt;Tested models' ability to exploit loopholes to satisfy their own goals&lt;/li&gt;&lt;li&gt;Found that stronger models can identify and exploit ambiguities&lt;/li&gt;&lt;li&gt;Analysis shows models reason about ambiguity and conflicting goals&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jio Choi', 'Mohit Bansal', 'Elias Stengel-Eskin']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'safety', 'red teaming', 'LLM', 'prompt injection']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2508.19546</guid><pubDate>Thu, 28 Aug 2025 00:00:00 +0000</pubDate></item><item><title>Servant, Stalker, Predator: How An Honest, Helpful, And Harmless (3H) Agent Unlocks Adversarial Skills</title><link>https://arxiv.org/abs/2508.19500</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Identifies a novel vulnerability in MCP-based agents where benign tasks can be chained into harmful behaviors&lt;/li&gt;&lt;li&gt;Uses MITRE ATLAS framework to analyze and demonstrate attack chains across 95 agents with multiple services&lt;/li&gt;&lt;li&gt;Demonstrates specific attack scenarios including data exfiltration, financial manipulation, and infrastructure compromise&lt;/li&gt;&lt;li&gt;Proposes new experimental directions for evaluating cross-domain security in MCP architectures&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['David Noever']&lt;/li&gt;&lt;li&gt;Tags: ['red teaming', 'security evaluation', 'adversarial skills', 'service orchestration', 'MCP']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2508.19500</guid><pubDate>Thu, 28 Aug 2025 00:00:00 +0000</pubDate></item><item><title>An Investigation on Group Query Hallucination Attacks</title><link>https://arxiv.org/abs/2508.19321</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces Group Query Attack for simulating multiple consecutive prompts&lt;/li&gt;&lt;li&gt;Demonstrates significant performance degradation in fine-tuned models&lt;/li&gt;&lt;li&gt;Triggers potential backdoors in LLMs&lt;/li&gt;&lt;li&gt;Effective against reasoning tasks like math and code generation&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Kehao Miao', 'Xiaolong Jin']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial prompting', 'red teaming', 'LLM security', 'context accumulation', 'backdoor attacks']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2508.19321</guid><pubDate>Thu, 28 Aug 2025 00:00:00 +0000</pubDate></item><item><title>Stand on The Shoulders of Giants: Building JailExpert from Previous Attack Experience</title><link>https://arxiv.org/abs/2508.19292</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces JailExpert, an automated jailbreak framework for LLMs&lt;/li&gt;&lt;li&gt;Utilizes past attack experiences through structured representation and semantic grouping&lt;/li&gt;&lt;li&gt;Demonstrates 17% higher success rate and 2.7x efficiency improvement vs. state-of-the-art&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Xi Wang', 'Songlei Jian', 'Shasha Li', 'Xiaopeng Li', 'Bin Ji', 'Jun Ma', 'Xiaodong Liu', 'Jing Wang', 'Feilong Bao', 'Jianfeng Zhang', 'Baosheng Wang', 'Jie Yu']&lt;/li&gt;&lt;li&gt;Tags: ['jailbreaking', 'red teaming', 'LLM security', 'automated attacks', 'experience reuse']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2508.19292</guid><pubDate>Thu, 28 Aug 2025 00:00:00 +0000</pubDate></item><item><title>Tricking LLM-Based NPCs into Spilling Secrets</title><link>https://arxiv.org/abs/2508.19288</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Examines adversarial prompt injection attacks on LLM-based NPCs&lt;/li&gt;&lt;li&gt;Aims to reveal hidden background secrets through crafted prompts&lt;/li&gt;&lt;li&gt;Assesses security risks in game NPC dialogue generation&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Kyohei Shiomi', 'Zhuotao Lian', 'Toru Nakanishi', 'Teruaki Kitasuka']&lt;/li&gt;&lt;li&gt;Tags: ['red teaming', 'adversarial prompting', 'LLM security', 'data extraction', 'game security']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2508.19288</guid><pubDate>Thu, 28 Aug 2025 00:00:00 +0000</pubDate></item><item><title>Prompt-in-Content Attacks: Exploiting Uploaded Inputs to Hijack LLM Behavior</title><link>https://arxiv.org/abs/2508.19287</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Identifies a new class of prompt-in-content injection attacks&lt;/li&gt;&lt;li&gt;Demonstrates feasibility across popular platforms&lt;/li&gt;&lt;li&gt;Analyzes root causes including prompt concatenation and input isolation issues&lt;/li&gt;&lt;li&gt;Discusses mitigation strategies for LLM security&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Zhuotao Lian', 'Weiyu Wang', 'Qingkui Zeng', 'Toru Nakanishi', 'Teruaki Kitasuka', 'Chunhua Su']&lt;/li&gt;&lt;li&gt;Tags: ['prompt injection', 'adversarial prompting', 'LLM security', 'red teaming', 'input isolation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2508.19287</guid><pubDate>Thu, 28 Aug 2025 00:00:00 +0000</pubDate></item><item><title>POT: Inducing Overthinking in LLMs via Black-Box Iterative Optimization</title><link>https://arxiv.org/abs/2508.19277</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces POT (Prompt-Only OverThinking), a black-box attack framework for LLMs&lt;/li&gt;&lt;li&gt;Uses iterative optimization to generate covert, semantically natural adversarial prompts&lt;/li&gt;&lt;li&gt;Eliminates dependence on external data access and model retrieval&lt;/li&gt;&lt;li&gt;Demonstrates superior performance across diverse models and datasets&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Xinyu Li', 'Tianjin Huang', 'Ronghui Mu', 'Xiaowei Huang', 'Gaojie Jin']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial prompting', 'LLM red teaming', 'black-box attacks', 'computational inefficiency', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2508.19277</guid><pubDate>Thu, 28 Aug 2025 00:00:00 +0000</pubDate></item><item><title>The Aegis Protocol: A Foundational Security Framework for Autonomous AI Agents</title><link>https://arxiv.org/abs/2508.19267</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces Aegis Protocol with three security pillars&lt;/li&gt;&lt;li&gt;Formalizes adversary model and validates with STRIDE&lt;/li&gt;&lt;li&gt;Simulation with 1,000 agents and 20,000 attack trials shows 0% success&lt;/li&gt;&lt;li&gt;Policy verification using Halo2 ZKPs with 2.79s median latency&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Sai Teja Reddy Adapala', 'Yashwanth Reddy Alugubelly']&lt;/li&gt;&lt;li&gt;Tags: ['security', 'safety', 'red teaming', 'simulation', 'multi-agent systems']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2508.19267</guid><pubDate>Thu, 28 Aug 2025 00:00:00 +0000</pubDate></item><item><title>Model Science: getting serious about verification, explanation and control of AI systems</title><link>https://arxiv.org/abs/2508.20040</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces Model Science as a new discipline focusing on model verification, explanation, control, and interface.&lt;/li&gt;&lt;li&gt;Proposes four key pillars: Verification, Explanation, Control, and Interface.&lt;/li&gt;&lt;li&gt;Aims to develop safe, aligned, and credible AI systems.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Position&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Przemyslaw Biecek', 'Wojciech Samek']&lt;/li&gt;&lt;li&gt;Tags: ['safety', 'verification', 'alignment', 'control', 'explanation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2508.20040</guid><pubDate>Thu, 28 Aug 2025 00:00:00 +0000</pubDate></item><item><title>Caught in the Act: a mechanistic approach to detecting deception</title><link>https://arxiv.org/abs/2508.19505</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Demonstrates linear probes on LLM internal activations can detect deception with &gt;90% accuracy&lt;/li&gt;&lt;li&gt;Accuracy increases with model size, especially in reasoning models&lt;/li&gt;&lt;li&gt;Deception detection follows a three-stage pattern across layers&lt;/li&gt;&lt;li&gt;Multiple linear directions encode deception in larger models&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Gerard Boxo', 'Ryan Socha', 'Daniel Yoo', 'Shivam Raval']&lt;/li&gt;&lt;li&gt;Tags: ['deception detection', 'alignment', 'safety evaluation', 'LLM internal activations', 'linear probes']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2508.19505</guid><pubDate>Thu, 28 Aug 2025 00:00:00 +0000</pubDate></item><item><title>Reliable Weak-to-Strong Monitoring of LLM Agents</title><link>https://arxiv.org/abs/2508.19461</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces Monitor Red Teaming (MRT) workflow for testing LLM agent monitoring systems&lt;/li&gt;&lt;li&gt;Key findings: agent awareness degrades monitor reliability, hybrid scaffolding enables weak-to-strong monitoring, human oversight improves detection&lt;/li&gt;&lt;li&gt;Uses prompt injection and other adversarial strategies&lt;/li&gt;&lt;li&gt;Releases code, data, and logs for further research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Neil Kale', 'Chen Bo Calvin Zhang', 'Kevin Zhu', 'Ankit Aich', 'Paula Rodriguez', 'Scale Red Team', 'Christina Q. Knight', 'Zifan Wang']&lt;/li&gt;&lt;li&gt;Tags: ['red teaming', 'prompt injection', 'adversarial prompting', 'safety evaluation', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2508.19461</guid><pubDate>Thu, 28 Aug 2025 00:00:00 +0000</pubDate></item></channel></rss>