<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>AI Security Paper Digest</title><link>https://kentaroh-toyoda.github.io/ai-security-paper-digest-rss/rss.xml</link><description>Curated papers on AI security from ArXiv and ACL</description><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><lastBuildDate>Tue, 30 Sep 2025 22:56:13 +0000</lastBuildDate><item><title>Improving Generalizability and Undetectability for Targeted Adversarial Attacks on Multimodal Pre-trained Models</title><link>https://arxiv.org/abs/2509.19994</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes Proxy Targeted Attack (PTA) for multimodal models&lt;/li&gt;&lt;li&gt;Addresses generalizability and undetectability of adversarial attacks&lt;/li&gt;&lt;li&gt;Theoretical analysis links generalizability and undetectability&lt;/li&gt;&lt;li&gt;Experimental results show high success rate and evasion capabilities&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Zhifang Zhang', 'Jiahan Zhang', 'Shengjie Zhou', 'Qi Wei', 'Shuo He', 'Feng Liu', 'Lei Feng']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial attacks', 'multimodal models', 'generalizability', 'undetectability', 'anomaly detection']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.19994</guid><pubDate>Tue, 30 Sep 2025 00:00:00 +0000</pubDate></item><item><title>Better Safe Than Sorry? Overreaction Problem of Vision Language Models in Visual Emergency Recognition</title><link>https://arxiv.org/abs/2505.15367</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces VERI benchmark for visual emergency recognition&lt;/li&gt;&lt;li&gt;Evaluates 17 VLMs on safety-critical scenarios&lt;/li&gt;&lt;li&gt;Reveals overreaction problem with low precision&lt;/li&gt;&lt;li&gt;Highlights contextual overinterpretation as main issue&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Dasol Choi', 'Seunghyun Lee', 'Youngsook Song']&lt;/li&gt;&lt;li&gt;Tags: ['safety evaluation', 'benchmarking', 'vision-language models', 'overreaction problem', 'contextual reasoning']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2505.15367</guid><pubDate>Tue, 30 Sep 2025 00:00:00 +0000</pubDate></item><item><title>DiffusionGuard: A Robust Defense Against Malicious Diffusion-based Image Editing</title><link>https://arxiv.org/abs/2410.05694</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes DiffusionGuard, a defense against malicious diffusion-based image editing&lt;/li&gt;&lt;li&gt;Introduces a new objective for generating adversarial noise in the early diffusion stage&lt;/li&gt;&lt;li&gt;Includes mask-augmentation for robustness against different masks&lt;/li&gt;&lt;li&gt;Presents a comprehensive benchmark for evaluating privacy protection&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['June Suk Choi', 'Kyungmin Lee', 'Jongheon Jeong', 'Saining Xie', 'Jinwoo Shin', 'Kimin Lee']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial attacks', 'image editing', 'diffusion models', 'privacy', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2410.05694</guid><pubDate>Tue, 30 Sep 2025 00:00:00 +0000</pubDate></item><item><title>Score-based Membership Inference on Diffusion Models</title><link>https://arxiv.org/abs/2509.25003</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;The paper studies membership inference attacks (MIAs) on diffusion models, focusing on score-based methods.&lt;/li&gt;&lt;li&gt;Proposes SimA, a single-query attack using predicted noise vectors.&lt;/li&gt;&lt;li&gt;Finds LDMs less vulnerable than pixel-space models due to latent auto-encoder bottleneck.&lt;/li&gt;&lt;li&gt;Suggests adjusting VAE regularization for improved robustness.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Mingxing Rao', 'Bowen Qu', 'Daniel Moyer']&lt;/li&gt;&lt;li&gt;Tags: ['privacy attacks', 'diffusion models', 'membership inference', 'adversarial attacks']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.25003</guid><pubDate>Tue, 30 Sep 2025 00:00:00 +0000</pubDate></item><item><title>Taught Well Learned Ill: Towards Distillation-conditional Backdoor Attack</title><link>https://arxiv.org/abs/2509.23871</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces distillation-conditional backdoor attacks (DCBAs) where backdoors in teacher models are activated in student models via knowledge distillation.&lt;/li&gt;&lt;li&gt;Proposes SCAR, a method using bilevel optimization to inject conditional backdoors into teacher models.&lt;/li&gt;&lt;li&gt;Validates the attack's effectiveness and stealth through experiments across various datasets and models.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yukun Chen', 'Boheng Li', 'Yu Yuan', 'Leyi Qi', 'Yiming Li', 'Tianwei Zhang', 'Zhan Qin', 'Kui Ren']&lt;/li&gt;&lt;li&gt;Tags: ['backdoor attacks', 'knowledge distillation', 'adversarial attacks', 'security', 'bilevel optimization']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.23871</guid><pubDate>Tue, 30 Sep 2025 00:00:00 +0000</pubDate></item><item><title>StolenLoRA: Exploring LoRA Extraction Attacks via Synthetic Data</title><link>https://arxiv.org/abs/2509.23594</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces LoRA extraction attacks targeting parameter-efficient fine-tuning methods&lt;/li&gt;&lt;li&gt;Proposes StolenLoRA method using synthetic data and LLM prompts&lt;/li&gt;&lt;li&gt;Demonstrates high attack success rates with limited queries&lt;/li&gt;&lt;li&gt;Explores preliminary defense strategies&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yixu Wang', 'Yan Teng', 'Yingchun Wang', 'Xingjun Ma']&lt;/li&gt;&lt;li&gt;Tags: ['model extraction', 'adversarial attacks', 'LoRA', 'PEFT', 'synthetic data', 'privacy attacks']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.23594</guid><pubDate>Tue, 30 Sep 2025 00:00:00 +0000</pubDate></item><item><title>GHOST: Hallucination-Inducing Image Generation for Multimodal LLMs</title><link>https://arxiv.org/abs/2509.25178</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces GHOST, a method to generate images that induce hallucinations in MLLMs&lt;/li&gt;&lt;li&gt;Operates by optimizing image embeddings to mislead models while keeping target objects absent&lt;/li&gt;&lt;li&gt;Evaluates across multiple models, achieving high hallucination success rates&lt;/li&gt;&lt;li&gt;Demonstrates transferable vulnerabilities and potential for fine-tuning mitigation&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Aryan Yazdan Parast', 'Parsa Hosseini', 'Hesam Asadollahzadeh', 'Arshia Soltani Moakhar', 'Basim Azam', 'Soheil Feizi', 'Naveed Akhtar']&lt;/li&gt;&lt;li&gt;Tags: ['red teaming', 'adversarial prompting', 'multimodal', 'hallucination', 'image generation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.25178</guid><pubDate>Tue, 30 Sep 2025 00:00:00 +0000</pubDate></item><item><title>VAGUEGAN: Stealthy Poisoning and Backdoor Attacks on Image Generative Pipelines</title><link>https://arxiv.org/abs/2509.24891</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces VagueGAN, a stealthy poisoning attack on image generative models&lt;/li&gt;&lt;li&gt;Combines PoisonerNet with Generator Discriminator to create stealthy triggers&lt;/li&gt;&lt;li&gt;Evaluates attack efficacy and stealth using perceptual metrics&lt;/li&gt;&lt;li&gt;Tests transferability to diffusion models via ControlNet&lt;/li&gt;&lt;li&gt;Finds poisoned outputs can have higher visual quality than clean ones&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Mostafa Mohaimen Akand Faisal', 'Rabeya Amin Jhuma']&lt;/li&gt;&lt;li&gt;Tags: ['data poisoning', 'adversarial attacks', 'generative models', 'stealthy attacks', 'visual quality']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.24891</guid><pubDate>Tue, 30 Sep 2025 00:00:00 +0000</pubDate></item><item><title>TokenSwap: Backdoor Attack on the Compositional Understanding of Large Vision-Language Models</title><link>https://arxiv.org/abs/2509.24566</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces TokenSwap, a backdoor attack on large vision-language models (LVLMs)&lt;/li&gt;&lt;li&gt;Focuses on disrupting compositional understanding by swapping token roles&lt;/li&gt;&lt;li&gt;Uses visual triggers and subtle text changes to evade detection&lt;/li&gt;&lt;li&gt;Employs adaptive token-weighted loss for effective learning of backdoor behavior&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Zhifang Zhang', 'Qiqi Tao', 'Jiaqi Lv', 'Na Zhao', 'Lei Feng', 'Joey Tianyi Zhou']&lt;/li&gt;&lt;li&gt;Tags: ['backdoor attack', 'vision-language models', 'compositional understanding', 'evasion', 'stealthiness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.24566</guid><pubDate>Tue, 30 Sep 2025 00:00:00 +0000</pubDate></item><item><title>Bridging the Task Gap: Multi-Task Adversarial Transferability in CLIP and Its Derivatives</title><link>https://arxiv.org/abs/2509.23917</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;The paper explores adversarial transferability in CLIP-based models across different tasks.&lt;/li&gt;&lt;li&gt;It proposes MT-AdvCLIP, a framework that improves adversarial attack success rates by enhancing cross-task generalization.&lt;/li&gt;&lt;li&gt;The study shows significant improvements in attack success rates against CLIP-derived models.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Kuanrong Liu', 'Siyuan Liang', 'Cheng Qian', 'Ming Zhang', 'Xiaochun Cao']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial attacks', 'CLIP', 'multi-task learning', 'robustness', 'transferability']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.23917</guid><pubDate>Tue, 30 Sep 2025 00:00:00 +0000</pubDate></item><item><title>Vid-Freeze: Protecting Images from Malicious Image-to-Video Generation via Temporal Freezing</title><link>https://arxiv.org/abs/2509.23279</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Vid-Freeze is an adversarial attack that adds perturbations to images to disrupt motion in generated videos&lt;/li&gt;&lt;li&gt;Targets attention mechanisms in I2V models&lt;/li&gt;&lt;li&gt;Aims to prevent malicious video synthesis while preserving image semantics&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Rohit Chowdhury', 'Aniruddha Bala', 'Rohan Jaiswal', 'Siddharth Roheda']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial attacks', 'image-to-video generation', 'security', 'privacy', 'red teaming']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.23279</guid><pubDate>Tue, 30 Sep 2025 00:00:00 +0000</pubDate></item><item><title>Real-World Transferable Adversarial Attack on Face-Recognition Systems</title><link>https://arxiv.org/abs/2509.23198</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces GaP, a method for generating a universal adversarial patch for face recognition systems in a black-box setting.&lt;/li&gt;&lt;li&gt;Uses a query-efficient algorithm to create a symmetric, grayscale forehead patch.&lt;/li&gt;&lt;li&gt;Demonstrates high attack success rates in both digital and physical tests.&lt;/li&gt;&lt;li&gt;Shows transferability across different FR models like ArcFace and FaceNet.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Andrey Kaznacheev', 'Matvey Mikhalchuk', 'Andrey Kuznetsov', 'Aleksandr Petiushko', 'Anton Razzhigaev']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial attacks', 'face recognition', 'black-box', 'transferability', 'security']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.23198</guid><pubDate>Tue, 30 Sep 2025 00:00:00 +0000</pubDate></item><item><title>Better Safe Than Sorry? Overreaction Problem of Vision Language Models in Visual Emergency Recognition</title><link>https://arxiv.org/abs/2505.15367</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces VERI benchmark for visual emergency recognition&lt;/li&gt;&lt;li&gt;Evaluates 17 VLMs on safety-critical scenarios&lt;/li&gt;&lt;li&gt;Reveals overreaction problem with low precision&lt;/li&gt;&lt;li&gt;Highlights contextual overinterpretation as main issue&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Dasol Choi', 'Seunghyun Lee', 'Youngsook Song']&lt;/li&gt;&lt;li&gt;Tags: ['safety evaluation', 'benchmarking', 'vision-language models', 'overreaction problem', 'contextual reasoning']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2505.15367</guid><pubDate>Tue, 30 Sep 2025 00:00:00 +0000</pubDate></item><item><title>A Voter-Based Stochastic Rejection-Method Framework for Asymptotically Safe Language Model Outputs</title><link>https://arxiv.org/abs/2407.16994</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes Repeated Checking with Regeneration (RCR) to prevent unsafe LLM outputs&lt;/li&gt;&lt;li&gt;Uses stochasticity of LLMs and voting mechanism&lt;/li&gt;&lt;li&gt;Aims for Pareto-optimal cost and failure rate&lt;/li&gt;&lt;li&gt;Model-agnostic approach&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jake R. Watts', 'Joel Sokol']&lt;/li&gt;&lt;li&gt;Tags: ['safety', 'robustness', 'alignment', 'evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2407.16994</guid><pubDate>Tue, 30 Sep 2025 00:00:00 +0000</pubDate></item><item><title>When Thinking Backfires: Mechanistic Insights Into Reasoning-Induced Misalignment</title><link>https://arxiv.org/abs/2509.00544</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces Reasoning-Induced Misalignment (RIM) where enhanced reasoning leads to misalignment&lt;/li&gt;&lt;li&gt;Provides mechanistic insights through attention head analysis and neuron activation entanglement&lt;/li&gt;&lt;li&gt;Highlights catastrophic forgetting as a correlate of RIM during training&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Hanqi Yan', 'Hainiu Xu', 'Siya Qi', 'Shu Yang', 'Yulan He']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'safety', 'model analysis', 'neuron entanglement', 'catastrophic forgetting']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.00544</guid><pubDate>Tue, 30 Sep 2025 00:00:00 +0000</pubDate></item><item><title>Reasoning Isn't Enough: Examining Truth-Bias and Sycophancy in LLMs</title><link>https://arxiv.org/abs/2506.21561</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Evaluates LLMs' veracity detection capabilities&lt;/li&gt;&lt;li&gt;Compares reasoning vs non-reasoning models&lt;/li&gt;&lt;li&gt;Identifies truth-bias and sycophancy in models&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research Paper&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Emilio Barkett', 'Olivia Long', 'Madhavendra Thakur']&lt;/li&gt;&lt;li&gt;Tags: ['safety evaluation', 'alignment', 'veracity detection']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.21561</guid><pubDate>Tue, 30 Sep 2025 00:00:00 +0000</pubDate></item><item><title>GRAF: Multi-turn Jailbreaking via Global Refinement and Active Fabrication</title><link>https://arxiv.org/abs/2506.17881</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes GRAF, a multi-turn jailbreaking method for LLMs&lt;/li&gt;&lt;li&gt;Uses global refinement and active fabrication of model responses&lt;/li&gt;&lt;li&gt;Aims to suppress safety warnings and elicit harmful outputs&lt;/li&gt;&lt;li&gt;Evaluated on six state-of-the-art LLMs with superior results&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Hua Tang', 'Lingyong Yan', 'Yukun Zhao', 'Shuaiqiang Wang', 'Jizhou Huang', 'Dawei Yin']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'jailbreaking', 'multi-turn attacks', 'adversarial prompting', 'safety evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.17881</guid><pubDate>Tue, 30 Sep 2025 00:00:00 +0000</pubDate></item><item><title>AdversariaL attacK sAfety aLIgnment(ALKALI): Safeguarding LLMs through GRACE: Geometric Representation-Aware Contrastive Enhancement- Introducing Adversarial Vulnerability Quality Index (AVQI)</title><link>https://arxiv.org/abs/2506.08885</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces ALKALI, a new adversarial benchmark for LLMs with 9,000 prompts&lt;/li&gt;&lt;li&gt;Reveals latent camouflage vulnerability where adversarial completions mimic safe geometry&lt;/li&gt;&lt;li&gt;Proposes GRACE framework for latent space regularization to improve safety&lt;/li&gt;&lt;li&gt;Introduces AVQI metric to measure latent alignment failure&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Danush Khanna', 'Gurucharan Marthi Krishna Kumar', 'Basab Ghosh', 'Yaswanth Narsupalli', 'Vinija Jain', 'Vasu Sharma', 'Aman Chadha', 'Amitava Das']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'adversarial prompting', 'alignment', 'safety evaluation', 'model extraction']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.08885</guid><pubDate>Tue, 30 Sep 2025 00:00:00 +0000</pubDate></item><item><title>TrojanStego: Your Language Model Can Secretly Be A Steganographic Privacy Leaking Agent</title><link>https://arxiv.org/abs/2505.20118</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces TrojanStego, a steganography-based attack on LLMs for data exfiltration&lt;/li&gt;&lt;li&gt;Proposes a taxonomy for evaluating risk factors of compromised LLMs&lt;/li&gt;&lt;li&gt;Demonstrates high accuracy in secret transmission with minimal detection&lt;/li&gt;&lt;li&gt;Highlights new class of passive, covert LLM attacks&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Dominik Meier', 'Jan Philip Wahle', 'Paul R\\"ottger', 'Terry Ruas', 'Bela Gipp']&lt;/li&gt;&lt;li&gt;Tags: ['data poisoning', 'privacy attacks', 'adversarial prompting', 'model extraction', 'red teaming']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2505.20118</guid><pubDate>Tue, 30 Sep 2025 00:00:00 +0000</pubDate></item><item><title>Language Models Optimized to Fool Detectors Still Have a Distinct Style (And How to Change It)</title><link>https://arxiv.org/abs/2505.14608</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;The paper explores the robustness of machine-text detectors against optimized language models.&lt;/li&gt;&lt;li&gt;It identifies a stylistic feature space that remains effective even when models are optimized to evade detection.&lt;/li&gt;&lt;li&gt;The study shows that while single-sample attacks can be effective, multiple samples become distinguishable.&lt;/li&gt;&lt;li&gt;The findings suggest avoiding reliance on machine-text detection.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Rafael Rivera Soto', 'Barry Chen', 'Nicholas Andrews']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial prompting', 'model extraction', 'privacy attacks', 'alignment', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2505.14608</guid><pubDate>Tue, 30 Sep 2025 00:00:00 +0000</pubDate></item><item><title>Are You Getting What You Pay For? Auditing Model Substitution in LLM APIs</title><link>https://arxiv.org/abs/2504.04715</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Audits model substitution in LLM APIs&lt;/li&gt;&lt;li&gt;Evaluates detection methods under adversarial conditions&lt;/li&gt;&lt;li&gt;Proposes TEEs for hardware-level security&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Will Cai', 'Tianneng Shi', 'Xuandong Zhao', 'Dawn Song']&lt;/li&gt;&lt;li&gt;Tags: ['security', 'red teaming', 'model extraction', 'adversarial prompting', 'trust']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2504.04715</guid><pubDate>Tue, 30 Sep 2025 00:00:00 +0000</pubDate></item><item><title>When Speculation Spills Secrets: Side Channels via Speculative Decoding In LLMs</title><link>https://arxiv.org/abs/2411.01076</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Reveals a side-channel attack on speculative decoding in LLMs&lt;/li&gt;&lt;li&gt;Adversary can infer user queries and leak data by monitoring token counts/packet sizes&lt;/li&gt;&lt;li&gt;Evaluated across multiple decoding schemes and in vLLM framework&lt;/li&gt;&lt;li&gt;Proposes mitigations like packet padding and token aggregation&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jiankun Wei', 'Abdulrahman Abdulrazzag', 'Tianchen Zhang', 'Adel Muursepp', 'Gururaj Saileshwar']&lt;/li&gt;&lt;li&gt;Tags: ['side-channel attacks', 'speculative decoding', 'LLM security', 'data leakage', 'mitigations']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2411.01076</guid><pubDate>Tue, 30 Sep 2025 00:00:00 +0000</pubDate></item><item><title>Towards Safe Reasoning in Large Reasoning Models via Corrective Intervention</title><link>https://arxiv.org/abs/2509.24393</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces Intervened Preference Optimization (IPO) to enforce safe reasoning in LRMs&lt;/li&gt;&lt;li&gt;Addresses jailbreak and adversarial safety benchmarks&lt;/li&gt;&lt;li&gt;Improves safety by substituting compliance steps with safety triggers&lt;/li&gt;&lt;li&gt;Reduces harmfulness by over 30% compared to baselines&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yichi Zhang', 'Yue Ding', 'Jingwen Yang', 'Tianwei Luo', 'Dongbai Li', 'Ranjie Duan', 'Qiang Liu', 'Hang Su', 'Yinpeng Dong', 'Jun Zhu']&lt;/li&gt;&lt;li&gt;Tags: ['red teaming', 'alignment', 'safety evaluation', 'robustness', 'jailbreaking']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.24393</guid><pubDate>Tue, 30 Sep 2025 00:00:00 +0000</pubDate></item><item><title>AdvChain: Adversarial Chain-of-Thought Tuning for Robust Safety Alignment of Large Reasoning Models</title><link>https://arxiv.org/abs/2509.24269</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces AdvChain, an adversarial CoT tuning method for LRM safety alignment&lt;/li&gt;&lt;li&gt;Addresses the snowball effect in CoT reasoning leading to harmful compliance or over-refusal&lt;/li&gt;&lt;li&gt;Uses Temptation-Correction and Hesitation-Correction samples for self-correction training&lt;/li&gt;&lt;li&gt;Improves robustness against jailbreak attacks and reduces over-refusal on benign prompts&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Zihao Zhu', 'Xinyu Wu', 'Gehan Hu', 'Siwei Lyu', 'Ke Xu', 'Baoyuan Wu']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'alignment', 'robustness', 'safety evaluation', 'jailbreaking']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.24269</guid><pubDate>Tue, 30 Sep 2025 00:00:00 +0000</pubDate></item><item><title>SafeSearch: Automated Red-Teaming for the Safety of LLM-Based Search Agents</title><link>https://arxiv.org/abs/2509.23694</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces SafeSearch, an automated red-teaming framework for LLM-based search agents&lt;/li&gt;&lt;li&gt;Constructs a benchmark with 300 test cases across five risk categories&lt;/li&gt;&lt;li&gt;Evaluates three search agent scaffolds with multiple LLMs, revealing high attack success rates&lt;/li&gt;&lt;li&gt;Highlights limitations of current defense practices like reminder prompting&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jianshuo Dong', 'Sheng Guo', 'Hao Wang', 'Zhuotao Liu', 'Tianwei Zhang', 'Ke Xu', 'Minlie Huang', 'Han Qiu']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'safety evaluation', 'search agents', 'benchmarking', 'adversarial prompting']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.23694</guid><pubDate>Tue, 30 Sep 2025 00:00:00 +0000</pubDate></item><item><title>MaskSQL: Safeguarding Privacy for LLM-Based Text-to-SQL via Abstraction</title><link>https://arxiv.org/abs/2509.23459</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces MaskSQL, a text-to-SQL framework using abstraction to mask sensitive info in LLM prompts&lt;/li&gt;&lt;li&gt;Aims to balance privacy and utility for text-to-SQL tasks&lt;/li&gt;&lt;li&gt;Outperforms SLM-based models and approaches LLM performance&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Sepideh Abedini (University of Waterloo', 'Vector Institute)', 'Shubhankar Mohapatra (University of Waterloo)', 'D. B. Emerson (Vector Institute)', 'Masoumeh Shafieinejad (Vector Institute)', 'Jesse C. Cresswell (Layer 6 AI)', 'Xi He (University of Waterloo', 'Vector Institute)']&lt;/li&gt;&lt;li&gt;Tags: ['privacy', 'text-to-sql', 'abstraction', 'data masking']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.23459</guid><pubDate>Tue, 30 Sep 2025 00:00:00 +0000</pubDate></item><item><title>Virus Infection Attack on LLMs: Your Poisoning Can Spread "VIA" Synthetic Data</title><link>https://arxiv.org/abs/2509.23041</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces Virus Infection Attack (VIA) framework for propagating poisoning and backdoor attacks through synthetic data in LLM training.&lt;/li&gt;&lt;li&gt;Shows that VIA increases poisoning content in synthetic data and raises attack success rates on downstream models.&lt;/li&gt;&lt;li&gt;Highlights security risks of using synthetic data in LLM training.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Zi Liang', 'Qingqing Ye', 'Xuan Liu', 'Yanyun Wang', 'Jianliang Xu', 'Haibo Hu']&lt;/li&gt;&lt;li&gt;Tags: ['data poisoning', 'backdoor attacks', 'synthetic data', 'LLM security', 'red teaming']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.23041</guid><pubDate>Tue, 30 Sep 2025 00:00:00 +0000</pubDate></item><item><title>Towards Trustworthy Lexical Simplification: Exploring Safety and Efficiency with Small LLMs</title><link>https://arxiv.org/abs/2509.25086</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes an efficient framework for lexical simplification using small LLMs&lt;/li&gt;&lt;li&gt;Explores knowledge distillation and in-context learning&lt;/li&gt;&lt;li&gt;Finds safety trade-offs with knowledge distillation&lt;/li&gt;&lt;li&gt;Introduces a filtering strategy based on output probability&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Akio Hayakawa', 'Stefan Bott', 'Horacio Saggion']&lt;/li&gt;&lt;li&gt;Tags: ['safety', 'small LLMs', 'lexical simplification', 'knowledge distillation', 'filtering']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.25086</guid><pubDate>Tue, 30 Sep 2025 00:00:00 +0000</pubDate></item><item><title>Sanitize Your Responses: Mitigating Privacy Leakage in Large Language Models</title><link>https://arxiv.org/abs/2509.24488</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces Self-Sanitize, a framework for real-time monitoring and repair of harmful content in LLM responses&lt;/li&gt;&lt;li&gt;Uses a Self-Monitor module for token-level inspection and a Self-Repair module for in-place correction&lt;/li&gt;&lt;li&gt;Conducts experiments on four LLMs across three privacy leakage scenarios&lt;/li&gt;&lt;li&gt;Aims to mitigate privacy leakage with minimal overhead and maintain LLM utility&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Wenjie Fu', 'Huandong Wang', 'Junyao Gao', 'Guoan Wan', 'Tao Jiang']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'privacy attacks', 'safety evaluation', 'robustness', 'alignment']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.24488</guid><pubDate>Tue, 30 Sep 2025 00:00:00 +0000</pubDate></item><item><title>HarmMetric Eval: Benchmarking Metrics and Judges for LLM Harmfulness Assessment</title><link>https://arxiv.org/abs/2509.24384</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces HarmMetric Eval, a benchmark for evaluating harmfulness metrics and judges for LLMs&lt;/li&gt;&lt;li&gt;Challenges prevailing beliefs by showing METEOR and ROUGE-1 outperform LLM-based judges&lt;/li&gt;&lt;li&gt;Provides a public dataset and code&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Langqi Yang', 'Tianhang Zheng', 'Kedong Xiu', 'Yixuan Chen', 'Di Wang', 'Puning Zhao', 'Zhan Qin', 'Kui Ren']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'jailbreaking', 'safety evaluation', 'benchmarking']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.24384</guid><pubDate>Tue, 30 Sep 2025 00:00:00 +0000</pubDate></item><item><title>DiffuGuard: How Intrinsic Safety is Lost and Found in Diffusion Large Language Models</title><link>https://arxiv.org/abs/2509.24296</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Analyzes vulnerabilities in Diffusion Large Language Models (dLLMs) related to jailbreak attacks&lt;/li&gt;&lt;li&gt;Identifies Denoising-path Dependence and greedy remasking bias as key issues&lt;/li&gt;&lt;li&gt;Proposes DiffuGuard, a training-free defense framework with Stochastic Annealing Remasking and Block-level Audit and Repair&lt;/li&gt;&lt;li&gt;Reduces attack success rate from 47.9% to 14.7% while maintaining model utility&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Zherui Li', 'Zheng Nie', 'Zhenhong Zhou', 'Yufei Guo', 'Yue Liu', 'Yitong Zhang', 'Yu Cheng', 'Qingsong Wen', 'Kun Wang', 'Jiaheng Zhang']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'jailbreaking', 'adversarial prompting', 'safety evaluation', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.24296</guid><pubDate>Tue, 30 Sep 2025 00:00:00 +0000</pubDate></item><item><title>Beyond Magic Words: Sharpness-Aware Prompt Evolving for Robust Large Language Models with TARE</title><link>https://arxiv.org/abs/2509.24130</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces TARE and ATARE frameworks for robust prompt optimization&lt;/li&gt;&lt;li&gt;Focuses on minimizing textual sharpness to improve paraphrase invariance&lt;/li&gt;&lt;li&gt;Evaluates on diverse tasks showing improved robustness&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Guancheng Wan', 'Lucheng Fu', 'Haoxin Liu', 'Yiqiao Jin', 'Hui Yi Leong', 'Eric Hanchen Jiang', 'Hejia Geng', 'Jinhe Bi', 'Yunpu Ma', 'Xiangru Tang', 'B. Aditya Prakash', 'Yizhou Sun', 'Wei Wang']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'prompt injection', 'adversarial prompting', 'robustness', 'safety evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.24130</guid><pubDate>Tue, 30 Sep 2025 00:00:00 +0000</pubDate></item><item><title>The AI Agent Code of Conduct: Automated Guardrail Policy-as-Prompt Synthesis</title><link>https://arxiv.org/abs/2509.23994</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces a framework for automating the translation of design documents into real-time guardrails using LLMs.&lt;/li&gt;&lt;li&gt;Uses 'Policy as Prompt' to enforce natural language policies via prompt-based classifiers.&lt;/li&gt;&lt;li&gt;Validates the approach across diverse applications, focusing on safety and regulatability.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Gauri Kholkar', 'Ratinder Ahuja']&lt;/li&gt;&lt;li&gt;Tags: ['safety', 'policy enforcement', 'guardrails', 'LLM', 'runtime auditing']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.23994</guid><pubDate>Tue, 30 Sep 2025 00:00:00 +0000</pubDate></item><item><title>Cognition-of-Thought Elicits Social-Aligned Reasoning in Large Language Models</title><link>https://arxiv.org/abs/2509.23441</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces Cognition-of-Thought (CooT) framework for dynamic alignment of LLMs&lt;/li&gt;&lt;li&gt;Uses a cognitive Perceiver to monitor and correct generation in real-time&lt;/li&gt;&lt;li&gt;Improves safety and social reasoning without retraining&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Xuanming Zhang', 'Yuxuan Chen', 'Min-Hsuan Yeh', 'Yixuan Li']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'safety', 'red teaming', 'jailbreaking']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.23441</guid><pubDate>Tue, 30 Sep 2025 00:00:00 +0000</pubDate></item><item><title>Guard Vector: Beyond English LLM Guardrails with Task-Vector Composition and Streaming-Aware Prefix SFT</title><link>https://arxiv.org/abs/2509.23381</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces Guard Vector for composing safety task vectors between models&lt;/li&gt;&lt;li&gt;Enables language extensibility without additional training&lt;/li&gt;&lt;li&gt;Uses streaming-aware prefix SFT for alignment&lt;/li&gt;&lt;li&gt;Improves classification quality across safety suites&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Wonhyuk Lee', 'Youngchol Kim', 'Yunjin Park', 'Junhyung Moon', 'Dongyoung Jeong', 'Wanjin Park']&lt;/li&gt;&lt;li&gt;Tags: ['safety evaluation', 'model alignment', 'language extensibility', 'streaming aware', 'prefix SFT']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.23381</guid><pubDate>Tue, 30 Sep 2025 00:00:00 +0000</pubDate></item><item><title>Dual-Space Smoothness for Robust and Balanced LLM Unlearning</title><link>https://arxiv.org/abs/2509.23362</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes PRISM framework for robust and balanced LLM unlearning&lt;/li&gt;&lt;li&gt;Addresses jailbreak and relearn attacks through dual-space smoothness&lt;/li&gt;&lt;li&gt;Improves unlearning metrics balance and defense against attacks&lt;/li&gt;&lt;li&gt;Evaluated on WMDP and MUSE datasets with conversational and continuous text&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Han Yan', 'Zheyuan Liu', 'Meng Jiang']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'jailbreaking', 'model extraction', 'robustness', 'safety evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.23362</guid><pubDate>Tue, 30 Sep 2025 00:00:00 +0000</pubDate></item><item><title>A2D: Any-Order, Any-Step Safety Alignment for Diffusion Language Models</title><link>https://arxiv.org/abs/2509.23286</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces A2D, a token-level alignment method for diffusion LLMs&lt;/li&gt;&lt;li&gt;Aims to prevent harmful content generation at any position&lt;/li&gt;&lt;li&gt;Reduces DIJA attack success rates significantly&lt;/li&gt;&lt;li&gt;Enables real-time monitoring and early termination&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Wonje Jeung', 'Sangyeon Yoon', 'Yoonjun Cho', 'Dongjae Jeon', 'Sangwoo Shin', 'Hyesoo Hong', 'Albert No']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'safety alignment', 'adversarial prompting', 'robustness', 'privacy attacks']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.23286</guid><pubDate>Tue, 30 Sep 2025 00:00:00 +0000</pubDate></item><item><title>ChatInject: Abusing Chat Templates for Prompt Injection in LLM Agents</title><link>https://arxiv.org/abs/2509.22830</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces ChatInject, an attack that uses chat templates for prompt injection in LLM agents&lt;/li&gt;&lt;li&gt;Demonstrates higher success rates compared to traditional methods&lt;/li&gt;&lt;li&gt;Shows transferability across models and ineffectiveness of current defenses&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Hwan Chang', 'Yonghyun Jun', 'Hwanhee Lee']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'prompt injection', 'adversarial prompting', 'safety evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.22830</guid><pubDate>Tue, 30 Sep 2025 00:00:00 +0000</pubDate></item><item><title>Mitigating Watermark Forgery in Generative Models via Randomized Key Selection</title><link>https://arxiv.org/abs/2507.07871</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a defense against watermark forgery in generative models using randomized key selection&lt;/li&gt;&lt;li&gt;Ensures forgery resistance regardless of the number of watermarked samples collected&lt;/li&gt;&lt;li&gt;Maintains model utility without degradation&lt;/li&gt;&lt;li&gt;Empirically reduces attacker success rate to 2% with low overhead&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Toluwani Aremu', 'Noor Hussein', 'Munachiso Nwadike', 'Samuele Poppi', 'Jie Zhang', 'Karthik Nandakumar', 'Neil Gong', 'Nils Lukas']&lt;/li&gt;&lt;li&gt;Tags: ['security', 'watermarking', 'forgery attacks', 'generative models', 'key randomization']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2507.07871</guid><pubDate>Tue, 30 Sep 2025 00:00:00 +0000</pubDate></item><item><title>AdversariaL attacK sAfety aLIgnment(ALKALI): Safeguarding LLMs through GRACE: Geometric Representation-Aware Contrastive Enhancement- Introducing Adversarial Vulnerability Quality Index (AVQI)</title><link>https://arxiv.org/abs/2506.08885</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces ALKALI, a new adversarial benchmark for LLMs with 9,000 prompts&lt;/li&gt;&lt;li&gt;Reveals latent camouflage vulnerability where adversarial completions mimic safe geometry&lt;/li&gt;&lt;li&gt;Proposes GRACE framework for latent space regularization to improve safety&lt;/li&gt;&lt;li&gt;Introduces AVQI metric to measure latent alignment failure&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Danush Khanna', 'Gurucharan Marthi Krishna Kumar', 'Basab Ghosh', 'Yaswanth Narsupalli', 'Vinija Jain', 'Vasu Sharma', 'Aman Chadha', 'Amitava Das']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'adversarial prompting', 'alignment', 'safety evaluation', 'model extraction']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.08885</guid><pubDate>Tue, 30 Sep 2025 00:00:00 +0000</pubDate></item><item><title>Language Models Optimized to Fool Detectors Still Have a Distinct Style (And How to Change It)</title><link>https://arxiv.org/abs/2505.14608</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;The paper explores the robustness of machine-text detectors against optimized language models.&lt;/li&gt;&lt;li&gt;It identifies a stylistic feature space that remains effective even when models are optimized to evade detection.&lt;/li&gt;&lt;li&gt;The study shows that while single-sample attacks can be effective, multiple samples become distinguishable.&lt;/li&gt;&lt;li&gt;The findings suggest avoiding reliance on machine-text detection.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Rafael Rivera Soto', 'Barry Chen', 'Nicholas Andrews']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial prompting', 'model extraction', 'privacy attacks', 'alignment', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2505.14608</guid><pubDate>Tue, 30 Sep 2025 00:00:00 +0000</pubDate></item><item><title>Are You Getting What You Pay For? Auditing Model Substitution in LLM APIs</title><link>https://arxiv.org/abs/2504.04715</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Audits model substitution in LLM APIs&lt;/li&gt;&lt;li&gt;Evaluates detection methods under adversarial conditions&lt;/li&gt;&lt;li&gt;Proposes TEEs for hardware-level security&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Will Cai', 'Tianneng Shi', 'Xuandong Zhao', 'Dawn Song']&lt;/li&gt;&lt;li&gt;Tags: ['security', 'red teaming', 'model extraction', 'adversarial prompting', 'trust']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2504.04715</guid><pubDate>Tue, 30 Sep 2025 00:00:00 +0000</pubDate></item><item><title>Safety Mirage: How Spurious Correlations Undermine VLM Safety Fine-Tuning and Can Be Mitigated by Machine Unlearning</title><link>https://arxiv.org/abs/2503.11832</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Identifies 'safety mirage' in VLM safety fine-tuning due to spurious correlations&lt;/li&gt;&lt;li&gt;Shows vulnerability to one-word modification attacks and over-prudence&lt;/li&gt;&lt;li&gt;Proposes machine unlearning (MU) as an alternative to supervised fine-tuning&lt;/li&gt;&lt;li&gt;Evaluates MU's effectiveness in reducing attack success and unnecessary rejections&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yiwei Chen', 'Yuguang Yao', 'Yihua Zhang', 'Bingquan Shen', 'Gaowen Liu', 'Sijia Liu']&lt;/li&gt;&lt;li&gt;Tags: ['safety', 'alignment', 'spurious correlations', 'machine unlearning', 'VLMs']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2503.11832</guid><pubDate>Tue, 30 Sep 2025 00:00:00 +0000</pubDate></item><item><title>When Speculation Spills Secrets: Side Channels via Speculative Decoding In LLMs</title><link>https://arxiv.org/abs/2411.01076</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Reveals a side-channel attack on speculative decoding in LLMs&lt;/li&gt;&lt;li&gt;Adversary can infer user queries and leak data by monitoring token counts/packet sizes&lt;/li&gt;&lt;li&gt;Evaluated across multiple decoding schemes and in vLLM framework&lt;/li&gt;&lt;li&gt;Proposes mitigations like packet padding and token aggregation&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jiankun Wei', 'Abdulrahman Abdulrazzag', 'Tianchen Zhang', 'Adel Muursepp', 'Gururaj Saileshwar']&lt;/li&gt;&lt;li&gt;Tags: ['side-channel attacks', 'speculative decoding', 'LLM security', 'data leakage', 'mitigations']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2411.01076</guid><pubDate>Tue, 30 Sep 2025 00:00:00 +0000</pubDate></item><item><title>A Voter-Based Stochastic Rejection-Method Framework for Asymptotically Safe Language Model Outputs</title><link>https://arxiv.org/abs/2407.16994</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes Repeated Checking with Regeneration (RCR) to prevent unsafe LLM outputs&lt;/li&gt;&lt;li&gt;Uses stochasticity of LLMs and voting mechanism&lt;/li&gt;&lt;li&gt;Aims for Pareto-optimal cost and failure rate&lt;/li&gt;&lt;li&gt;Model-agnostic approach&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jake R. Watts', 'Joel Sokol']&lt;/li&gt;&lt;li&gt;Tags: ['safety', 'robustness', 'alignment', 'evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2407.16994</guid><pubDate>Tue, 30 Sep 2025 00:00:00 +0000</pubDate></item><item><title>Efficiently Attacking Memorization Scores</title><link>https://arxiv.org/abs/2509.20463</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;This paper studies the feasibility of attacking memorization-based influence estimators, which are used in data valuation and responsible ML. The authors present an attack method that requires only black-box access to model outputs and show empirical vulnerability across image classification tasks. They also provide theoretical analysis on the stability of these scores under adversarial perturbations.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Tue Do', 'Varun Chandrasekaran', 'Daniel Alabi']&lt;/li&gt;&lt;li&gt;Tags: ['data poisoning', 'adversarial attacks', 'influence estimation', 'memorization scores', 'model robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.20463</guid><pubDate>Tue, 30 Sep 2025 00:00:00 +0000</pubDate></item><item><title>Beyond Sharp Minima: Robust LLM Unlearning via Feedback-Guided Multi-Point Optimization</title><link>https://arxiv.org/abs/2509.20230</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Addresses LLM unlearning security vulnerability&lt;/li&gt;&lt;li&gt;Proposes StableUN framework for robust unlearning&lt;/li&gt;&lt;li&gt;Uses feedback-guided multi-point optimization&lt;/li&gt;&lt;li&gt;Improves resistance against relearning and jailbreaking attacks&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Wenhan Wu', 'Zheyuan Liu', 'Chongyang Gao', 'Ren Wang', 'Kaize Ding']&lt;/li&gt;&lt;li&gt;Tags: ['LLM security', 'unlearning', 'robustness', 'jailbreaking', 'relearning attacks']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.20230</guid><pubDate>Tue, 30 Sep 2025 00:00:00 +0000</pubDate></item><item><title>EigenTrack: Spectral Activation Feature Tracking for Hallucination and Out-of-Distribution Detection in LLMs and VLMs</title><link>https://arxiv.org/abs/2509.15735</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;EigenTrack detects hallucination and OOD errors in LLMs and VLMs using spectral activation features&lt;/li&gt;&lt;li&gt;Uses covariance-spectrum statistics and a recurrent classifier for real-time detection&lt;/li&gt;&lt;li&gt;Preserves temporal context and offers interpretable accuracy-latency trade-offs&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Davide Ettori', 'Nastaran Darabi', 'Sina Tayebati', 'Ranganath Krishnan', 'Mahesh Subedar', 'Omesh Tickoo', 'Amit Ranjan Trivedi']&lt;/li&gt;&lt;li&gt;Tags: ['safety', 'robustness', 'evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.15735</guid><pubDate>Tue, 30 Sep 2025 00:00:00 +0000</pubDate></item><item><title>Speculative Safety-Aware Decoding</title><link>https://arxiv.org/abs/2508.17739</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces Speculative Safety-Aware Decoding (SSD) to enhance LLM safety during decoding&lt;/li&gt;&lt;li&gt;Uses a small model to check safety and dynamically adjust decoding&lt;/li&gt;&lt;li&gt;Aims to prevent jailbreak attacks while maintaining performance&lt;/li&gt;&lt;li&gt;Shows improved safety and faster inference in experiments&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Xuekang Wang', 'Shengyu Zhu', 'Xueqi Cheng']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'jailbreaking', 'safety evaluation', 'robustness', 'adversarial prompting']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2508.17739</guid><pubDate>Tue, 30 Sep 2025 00:00:00 +0000</pubDate></item><item><title>Beyond Prompt-Induced Lies: Investigating LLM Deception on Benign Prompts</title><link>https://arxiv.org/abs/2508.06361</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Investigates LLM self-initiated deception on benign prompts&lt;/li&gt;&lt;li&gt;Proposes Deceptive Intention and Behavior Scores using CSQ framework&lt;/li&gt;&lt;li&gt;Evaluates 16 models, finds deception metrics rise with task difficulty&lt;/li&gt;&lt;li&gt;Model capacity doesn't always reduce deception&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Zhaomin Wu', 'Mingzhe Du', 'See-Kiong Ng', 'Bingsheng He']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'deception', 'safety evaluation', 'alignment', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2508.06361</guid><pubDate>Tue, 30 Sep 2025 00:00:00 +0000</pubDate></item><item><title>Unlearning Isn't Invisible: Detecting Unlearning Traces in LLMs from Model Outputs</title><link>https://arxiv.org/abs/2506.14003</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;The paper explores detecting traces of unlearning in LLMs through model outputs and internal representations.&lt;/li&gt;&lt;li&gt;It shows that unlearning leaves persistent fingerprints detectable via classifiers using logits or text outputs.&lt;/li&gt;&lt;li&gt;The study finds high detection accuracy even with irrelevant prompts and stronger traces in larger models.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yiwei Chen', 'Soumyadeep Pal', 'Yimeng Zhang', 'Qing Qu', 'Sijia Liu']&lt;/li&gt;&lt;li&gt;Tags: ['data_poisoning', 'privacy_attacks', 'model_extraction', 'safety_evaluation', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.14003</guid><pubDate>Tue, 30 Sep 2025 00:00:00 +0000</pubDate></item><item><title>Can In-Context Reinforcement Learning Recover From Reward Poisoning Attacks?</title><link>https://arxiv.org/abs/2506.06891</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes AT-DPT, an adversarial training framework for DPT to handle reward poisoning attacks&lt;/li&gt;&lt;li&gt;Evaluates robustness against learned and adaptive attackers in bandit and MDP settings&lt;/li&gt;&lt;li&gt;Outperforms standard robust baselines in reward-contaminated environments&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Paulius Sasnauskas', 'Yi\\u{g}it Yal{\\i}n', "Goran Radanovi\\'c"]&lt;/li&gt;&lt;li&gt;Tags: ['adversarial training', 'reward poisoning', 'reinforcement learning', 'robustness', 'ICRL']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.06891</guid><pubDate>Tue, 30 Sep 2025 00:00:00 +0000</pubDate></item><item><title>Causes and Consequences of Representational Similarity in Machine Learning Models</title><link>https://arxiv.org/abs/2505.13899</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;The paper explores how dataset and task overlap affect representational similarity in ML models.&lt;/li&gt;&lt;li&gt;It shows that higher similarity increases vulnerability to transferable adversarial and jailbreak attacks.&lt;/li&gt;&lt;li&gt;Experiments include models from small classifiers to large language models across modalities.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Zeyu Michael Li', 'Hung Anh Vu', 'Damilola Awofisayo', 'Emily Wenger']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial attacks', 'jailbreak attacks', 'representational similarity', 'dataset overlap', 'task overlap']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2505.13899</guid><pubDate>Tue, 30 Sep 2025 00:00:00 +0000</pubDate></item><item><title>GHOST: Hallucination-Inducing Image Generation for Multimodal LLMs</title><link>https://arxiv.org/abs/2509.25178</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces GHOST, a method to generate images that induce hallucinations in MLLMs&lt;/li&gt;&lt;li&gt;Operates by optimizing image embeddings to mislead models while keeping target objects absent&lt;/li&gt;&lt;li&gt;Evaluates across multiple models, achieving high hallucination success rates&lt;/li&gt;&lt;li&gt;Demonstrates transferable vulnerabilities and potential for fine-tuning mitigation&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Aryan Yazdan Parast', 'Parsa Hosseini', 'Hesam Asadollahzadeh', 'Arshia Soltani Moakhar', 'Basim Azam', 'Soheil Feizi', 'Naveed Akhtar']&lt;/li&gt;&lt;li&gt;Tags: ['red teaming', 'adversarial prompting', 'multimodal', 'hallucination', 'image generation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.25178</guid><pubDate>Tue, 30 Sep 2025 00:00:00 +0000</pubDate></item><item><title>VAGUEGAN: Stealthy Poisoning and Backdoor Attacks on Image Generative Pipelines</title><link>https://arxiv.org/abs/2509.24891</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces VagueGAN, a stealthy poisoning attack on image generative models&lt;/li&gt;&lt;li&gt;Combines PoisonerNet with Generator Discriminator to create stealthy triggers&lt;/li&gt;&lt;li&gt;Evaluates attack efficacy and stealth using perceptual metrics&lt;/li&gt;&lt;li&gt;Tests transferability to diffusion models via ControlNet&lt;/li&gt;&lt;li&gt;Finds poisoned outputs can have higher visual quality than clean ones&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Mostafa Mohaimen Akand Faisal', 'Rabeya Amin Jhuma']&lt;/li&gt;&lt;li&gt;Tags: ['data poisoning', 'adversarial attacks', 'generative models', 'stealthy attacks', 'visual quality']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.24891</guid><pubDate>Tue, 30 Sep 2025 00:00:00 +0000</pubDate></item><item><title>Sanitize Your Responses: Mitigating Privacy Leakage in Large Language Models</title><link>https://arxiv.org/abs/2509.24488</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces Self-Sanitize, a framework for real-time monitoring and repair of harmful content in LLM responses&lt;/li&gt;&lt;li&gt;Uses a Self-Monitor module for token-level inspection and a Self-Repair module for in-place correction&lt;/li&gt;&lt;li&gt;Conducts experiments on four LLMs across three privacy leakage scenarios&lt;/li&gt;&lt;li&gt;Aims to mitigate privacy leakage with minimal overhead and maintain LLM utility&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Wenjie Fu', 'Huandong Wang', 'Junyao Gao', 'Guoan Wan', 'Tao Jiang']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'privacy attacks', 'safety evaluation', 'robustness', 'alignment']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.24488</guid><pubDate>Tue, 30 Sep 2025 00:00:00 +0000</pubDate></item><item><title>FuncPoison: Poisoning Function Library to Hijack Multi-agent Autonomous Driving Systems</title><link>https://arxiv.org/abs/2509.24408</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces FuncPoison, a poisoning attack targeting the function library in multi-agent autonomous driving systems&lt;/li&gt;&lt;li&gt;Exploits agents' reliance on text-based instructions and standardized command formats&lt;/li&gt;&lt;li&gt;Demonstrates significant degradation in trajectory accuracy and coordinated misbehavior&lt;/li&gt;&lt;li&gt;Evaluates against defense mechanisms, highlighting reliability concerns&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yuzhen Long', 'Songze Li']&lt;/li&gt;&lt;li&gt;Tags: ['data poisoning', 'adversarial prompting', 'autonomous systems', 'multi-agent', 'function library']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.24408</guid><pubDate>Tue, 30 Sep 2025 00:00:00 +0000</pubDate></item><item><title>Learning-Based Testing for Deep Learning: Enhancing Model Robustness with Adversarial Input Prioritization</title><link>https://arxiv.org/abs/2509.23961</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Enhances model robustness by prioritizing adversarial test cases using Learning-Based Testing (LBT)&lt;/li&gt;&lt;li&gt;Integrates hypothesis and mutation testing for efficient fault detection&lt;/li&gt;&lt;li&gt;Outperforms baseline methods in fault-revealing input prioritization&lt;/li&gt;&lt;li&gt;Adaptable across different DNN architectures and datasets&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Sheikh Md Mushfiqur Rahman', 'Nasir Eisty']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial testing', 'model robustness', 'test prioritization', 'deep learning security']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.23961</guid><pubDate>Tue, 30 Sep 2025 00:00:00 +0000</pubDate></item><item><title>Taught Well Learned Ill: Towards Distillation-conditional Backdoor Attack</title><link>https://arxiv.org/abs/2509.23871</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces distillation-conditional backdoor attacks (DCBAs) where backdoors in teacher models are activated in student models via knowledge distillation.&lt;/li&gt;&lt;li&gt;Proposes SCAR, a method using bilevel optimization to inject conditional backdoors into teacher models.&lt;/li&gt;&lt;li&gt;Validates the attack's effectiveness and stealth through experiments across various datasets and models.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yukun Chen', 'Boheng Li', 'Yu Yuan', 'Leyi Qi', 'Yiming Li', 'Tianwei Zhang', 'Zhan Qin', 'Kui Ren']&lt;/li&gt;&lt;li&gt;Tags: ['backdoor attacks', 'knowledge distillation', 'adversarial attacks', 'security', 'bilevel optimization']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.23871</guid><pubDate>Tue, 30 Sep 2025 00:00:00 +0000</pubDate></item><item><title>Influence-Guided Concolic Testing of Transformer Robustness</title><link>https://arxiv.org/abs/2509.23806</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Influence-guided concolic testing for Transformer robustness&lt;/li&gt;&lt;li&gt;Uses SHAP-based estimates to rank path predicates&lt;/li&gt;&lt;li&gt;White-box study on compact Transformers with L0 budgets&lt;/li&gt;&lt;li&gt;Potential for debugging and model auditing&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Chih-Duo Hong', 'Yu Wang', 'Yao-Chen Chang', 'Fang Yu']&lt;/li&gt;&lt;li&gt;Tags: ['robustness', 'adversarial attacks', 'concolic testing', 'Transformer models', 'SHAP']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.23806</guid><pubDate>Tue, 30 Sep 2025 00:00:00 +0000</pubDate></item><item><title>Score-based Membership Inference on Diffusion Models</title><link>https://arxiv.org/abs/2509.25003</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;The paper studies membership inference attacks (MIAs) on diffusion models, focusing on score-based methods.&lt;/li&gt;&lt;li&gt;Proposes SimA, a single-query attack using predicted noise vectors.&lt;/li&gt;&lt;li&gt;Finds LDMs less vulnerable than pixel-space models due to latent auto-encoder bottleneck.&lt;/li&gt;&lt;li&gt;Suggests adjusting VAE regularization for improved robustness.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Mingxing Rao', 'Bowen Qu', 'Daniel Moyer']&lt;/li&gt;&lt;li&gt;Tags: ['privacy attacks', 'diffusion models', 'membership inference', 'adversarial attacks']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.25003</guid><pubDate>Tue, 30 Sep 2025 00:00:00 +0000</pubDate></item><item><title>Circuit-Aware Reward Training: A Mechanistic Framework for Longtail Robustness in RLHF</title><link>https://arxiv.org/abs/2509.24713</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes Circuit-Aware Reward Training (CART) to improve longtail robustness in RLHF reward models&lt;/li&gt;&lt;li&gt;Uses circuit analysis for data augmentation, regularization, and ensemble strategies&lt;/li&gt;&lt;li&gt;Aims to address reward hacking and misalignment&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jing Liu']&lt;/li&gt;&lt;li&gt;Tags: ['robustness', 'alignment', 'safety evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.24713</guid><pubDate>Tue, 30 Sep 2025 00:00:00 +0000</pubDate></item><item><title>Stable Forgetting: Bounded Parameter-Efficient Unlearning in LLMs</title><link>https://arxiv.org/abs/2509.24166</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a method for stable unlearning in LLMs&lt;/li&gt;&lt;li&gt;Addresses weight and gradient instability in gradient difference method&lt;/li&gt;&lt;li&gt;Applies bounded functions to MLP adapters in LoRA&lt;/li&gt;&lt;li&gt;Improves forgetting while preserving retention across benchmarks&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Arpit Garg', 'Hemanth Saratchandran', 'Ravi Garg', 'Simon Lucey']&lt;/li&gt;&lt;li&gt;Tags: ['privacy', 'safety', 'unlearning', 'parameter efficiency', 'MLP layers']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.24166</guid><pubDate>Tue, 30 Sep 2025 00:00:00 +0000</pubDate></item><item><title>Merge Now, Regret Later: The Hidden Cost of Model Merging is Adversarial Transferability</title><link>https://arxiv.org/abs/2509.23689</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;This paper studies the impact of Model Merging (MM) on the transferability of adversarial examples.&lt;/li&gt;&lt;li&gt;It evaluates 8 MM methods, 7 datasets, and 6 attack methods across 336 settings.&lt;/li&gt;&lt;li&gt;Key findings: MM doesn't reliably defend against transfer attacks, stronger MM methods increase vulnerability, and weight averaging is most vulnerable.&lt;/li&gt;&lt;li&gt;The paper provides insights for designing more secure systems using MM.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Ankit Gangwal', 'Aaryan Ajay Sharma']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial attacks', 'model merging', 'transfer attacks', 'robustness', 'security']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.23689</guid><pubDate>Tue, 30 Sep 2025 00:00:00 +0000</pubDate></item><item><title>GuardNet: Graph-Attention Filtering for Jailbreak Defense in Large Language Models</title><link>https://arxiv.org/abs/2509.23037</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes GuardNet, a hierarchical filtering framework for detecting and filtering jailbreak prompts in LLMs.&lt;/li&gt;&lt;li&gt;Uses graph neural networks at prompt and token levels to capture linguistic and contextual patterns.&lt;/li&gt;&lt;li&gt;Shows significant performance improvements over prior defenses in detecting jailbreak attacks.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Javad Forough', 'Mohammad Maheri', 'Hamed Haddadi']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'jailbreaking', 'adversarial prompting', 'safety evaluation', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.23037</guid><pubDate>Tue, 30 Sep 2025 00:00:00 +0000</pubDate></item><item><title>Boundary on the Table: Efficient Black-Box Decision-Based Attacks for Structured Data</title><link>https://arxiv.org/abs/2509.22850</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces a black-box decision-based adversarial attack for tabular data&lt;/li&gt;&lt;li&gt;Combines gradient-free direction estimation with boundary search&lt;/li&gt;&lt;li&gt;Effective against both classical ML and LLM-based models&lt;/li&gt;&lt;li&gt;High success rates with minimal queries&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Roie Kazoom', 'Yuval Ratzabi', 'Etamar Rothstein', 'Ofer Hadar']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial attacks', 'black-box attacks', 'tabular data', 'model robustness', 'security']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.22850</guid><pubDate>Tue, 30 Sep 2025 00:00:00 +0000</pubDate></item><item><title>Beyond Sharp Minima: Robust LLM Unlearning via Feedback-Guided Multi-Point Optimization</title><link>https://arxiv.org/abs/2509.20230</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Addresses LLM unlearning security vulnerability&lt;/li&gt;&lt;li&gt;Proposes StableUN framework for robust unlearning&lt;/li&gt;&lt;li&gt;Uses feedback-guided multi-point optimization&lt;/li&gt;&lt;li&gt;Improves resistance against relearning and jailbreaking attacks&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Wenhan Wu', 'Zheyuan Liu', 'Chongyang Gao', 'Ren Wang', 'Kaize Ding']&lt;/li&gt;&lt;li&gt;Tags: ['LLM security', 'unlearning', 'robustness', 'jailbreaking', 'relearning attacks']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.20230</guid><pubDate>Tue, 30 Sep 2025 00:00:00 +0000</pubDate></item><item><title>Speculative Safety-Aware Decoding</title><link>https://arxiv.org/abs/2508.17739</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces Speculative Safety-Aware Decoding (SSD) to enhance LLM safety during decoding&lt;/li&gt;&lt;li&gt;Uses a small model to check safety and dynamically adjust decoding&lt;/li&gt;&lt;li&gt;Aims to prevent jailbreak attacks while maintaining performance&lt;/li&gt;&lt;li&gt;Shows improved safety and faster inference in experiments&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Xuekang Wang', 'Shengyu Zhu', 'Xueqi Cheng']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'jailbreaking', 'safety evaluation', 'robustness', 'adversarial prompting']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2508.17739</guid><pubDate>Tue, 30 Sep 2025 00:00:00 +0000</pubDate></item><item><title>Beyond Prompt-Induced Lies: Investigating LLM Deception on Benign Prompts</title><link>https://arxiv.org/abs/2508.06361</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Investigates LLM self-initiated deception on benign prompts&lt;/li&gt;&lt;li&gt;Proposes Deceptive Intention and Behavior Scores using CSQ framework&lt;/li&gt;&lt;li&gt;Evaluates 16 models, finds deception metrics rise with task difficulty&lt;/li&gt;&lt;li&gt;Model capacity doesn't always reduce deception&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Zhaomin Wu', 'Mingzhe Du', 'See-Kiong Ng', 'Bingsheng He']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'deception', 'safety evaluation', 'alignment', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2508.06361</guid><pubDate>Tue, 30 Sep 2025 00:00:00 +0000</pubDate></item><item><title>Mitigating Watermark Forgery in Generative Models via Randomized Key Selection</title><link>https://arxiv.org/abs/2507.07871</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a defense against watermark forgery in generative models using randomized key selection&lt;/li&gt;&lt;li&gt;Ensures forgery resistance regardless of the number of watermarked samples collected&lt;/li&gt;&lt;li&gt;Maintains model utility without degradation&lt;/li&gt;&lt;li&gt;Empirically reduces attacker success rate to 2% with low overhead&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Toluwani Aremu', 'Noor Hussein', 'Munachiso Nwadike', 'Samuele Poppi', 'Jie Zhang', 'Karthik Nandakumar', 'Neil Gong', 'Nils Lukas']&lt;/li&gt;&lt;li&gt;Tags: ['security', 'watermarking', 'forgery attacks', 'generative models', 'key randomization']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2507.07871</guid><pubDate>Tue, 30 Sep 2025 00:00:00 +0000</pubDate></item><item><title>Reasoning Isn't Enough: Examining Truth-Bias and Sycophancy in LLMs</title><link>https://arxiv.org/abs/2506.21561</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Evaluates LLMs' veracity detection capabilities&lt;/li&gt;&lt;li&gt;Compares reasoning vs non-reasoning models&lt;/li&gt;&lt;li&gt;Identifies truth-bias and sycophancy in models&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research Paper&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Emilio Barkett', 'Olivia Long', 'Madhavendra Thakur']&lt;/li&gt;&lt;li&gt;Tags: ['safety evaluation', 'alignment', 'veracity detection']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.21561</guid><pubDate>Tue, 30 Sep 2025 00:00:00 +0000</pubDate></item><item><title>GRAF: Multi-turn Jailbreaking via Global Refinement and Active Fabrication</title><link>https://arxiv.org/abs/2506.17881</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes GRAF, a multi-turn jailbreaking method for LLMs&lt;/li&gt;&lt;li&gt;Uses global refinement and active fabrication of model responses&lt;/li&gt;&lt;li&gt;Aims to suppress safety warnings and elicit harmful outputs&lt;/li&gt;&lt;li&gt;Evaluated on six state-of-the-art LLMs with superior results&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Hua Tang', 'Lingyong Yan', 'Yukun Zhao', 'Shuaiqiang Wang', 'Jizhou Huang', 'Dawei Yin']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'jailbreaking', 'multi-turn attacks', 'adversarial prompting', 'safety evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.17881</guid><pubDate>Tue, 30 Sep 2025 00:00:00 +0000</pubDate></item><item><title>TRAPDOC: Deceiving LLM Users by Injecting Imperceptible Phantom Tokens into Documents</title><link>https://arxiv.org/abs/2506.00089</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes TRAPDOC framework to inject phantom tokens into documents&lt;/li&gt;&lt;li&gt;Aims to deceive over-reliant LLM users by generating plausible but incorrect outputs&lt;/li&gt;&lt;li&gt;Empirical evaluation shows effectiveness against proprietary LLMs&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Hyundong Jin', 'Sicheol Sung', 'Shinwoo Park', 'SeungYeop Baik', 'Yo-Sub Han']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial prompting', 'red teaming', 'safety evaluation', 'privacy attacks']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.00089</guid><pubDate>Tue, 30 Sep 2025 00:00:00 +0000</pubDate></item><item><title>BitHydra: Towards Bit-flip Inference Cost Attack against Large Language Models</title><link>https://arxiv.org/abs/2505.16670</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces BitHydra, a bit-flip inference cost attack against LLMs&lt;/li&gt;&lt;li&gt;Modifies model weights to suppress EOS token, causing longer outputs&lt;/li&gt;&lt;li&gt;Uses critical-bit search focused on EOS embedding vector&lt;/li&gt;&lt;li&gt;Evaluated on 11 LLMs (1.5B-14B) with int8 and float16&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Xiaobei Yan', 'Yiming Li', 'Hao Wang', 'Han Qiu', 'Tianwei Zhang']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial attacks', 'model extraction', 'data poisoning', 'security', 'LLM red teaming']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2505.16670</guid><pubDate>Tue, 30 Sep 2025 00:00:00 +0000</pubDate></item><item><title>Better Safe Than Sorry? Overreaction Problem of Vision Language Models in Visual Emergency Recognition</title><link>https://arxiv.org/abs/2505.15367</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces VERI benchmark for visual emergency recognition&lt;/li&gt;&lt;li&gt;Evaluates 17 VLMs on safety-critical scenarios&lt;/li&gt;&lt;li&gt;Reveals overreaction problem with low precision&lt;/li&gt;&lt;li&gt;Highlights contextual overinterpretation as main issue&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Dasol Choi', 'Seunghyun Lee', 'Youngsook Song']&lt;/li&gt;&lt;li&gt;Tags: ['safety evaluation', 'benchmarking', 'vision-language models', 'overreaction problem', 'contextual reasoning']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2505.15367</guid><pubDate>Tue, 30 Sep 2025 00:00:00 +0000</pubDate></item><item><title>Language Models Optimized to Fool Detectors Still Have a Distinct Style (And How to Change It)</title><link>https://arxiv.org/abs/2505.14608</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;The paper explores the robustness of machine-text detectors against optimized language models.&lt;/li&gt;&lt;li&gt;It identifies a stylistic feature space that remains effective even when models are optimized to evade detection.&lt;/li&gt;&lt;li&gt;The study shows that while single-sample attacks can be effective, multiple samples become distinguishable.&lt;/li&gt;&lt;li&gt;The findings suggest avoiding reliance on machine-text detection.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Rafael Rivera Soto', 'Barry Chen', 'Nicholas Andrews']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial prompting', 'model extraction', 'privacy attacks', 'alignment', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2505.14608</guid><pubDate>Tue, 30 Sep 2025 00:00:00 +0000</pubDate></item><item><title>When Speculation Spills Secrets: Side Channels via Speculative Decoding In LLMs</title><link>https://arxiv.org/abs/2411.01076</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Reveals a side-channel attack on speculative decoding in LLMs&lt;/li&gt;&lt;li&gt;Adversary can infer user queries and leak data by monitoring token counts/packet sizes&lt;/li&gt;&lt;li&gt;Evaluated across multiple decoding schemes and in vLLM framework&lt;/li&gt;&lt;li&gt;Proposes mitigations like packet padding and token aggregation&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jiankun Wei', 'Abdulrahman Abdulrazzag', 'Tianchen Zhang', 'Adel Muursepp', 'Gururaj Saileshwar']&lt;/li&gt;&lt;li&gt;Tags: ['side-channel attacks', 'speculative decoding', 'LLM security', 'data leakage', 'mitigations']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2411.01076</guid><pubDate>Tue, 30 Sep 2025 00:00:00 +0000</pubDate></item><item><title>Safety Mirage: How Spurious Correlations Undermine VLM Safety Fine-Tuning and Can Be Mitigated by Machine Unlearning</title><link>https://arxiv.org/abs/2503.11832</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Identifies 'safety mirage' in VLM safety fine-tuning due to spurious correlations&lt;/li&gt;&lt;li&gt;Shows vulnerability to one-word modification attacks and over-prudence&lt;/li&gt;&lt;li&gt;Proposes machine unlearning (MU) as an alternative to supervised fine-tuning&lt;/li&gt;&lt;li&gt;Evaluates MU's effectiveness in reducing attack success and unnecessary rejections&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yiwei Chen', 'Yuguang Yao', 'Yihua Zhang', 'Bingquan Shen', 'Gaowen Liu', 'Sijia Liu']&lt;/li&gt;&lt;li&gt;Tags: ['safety', 'alignment', 'spurious correlations', 'machine unlearning', 'VLMs']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2503.11832</guid><pubDate>Tue, 30 Sep 2025 00:00:00 +0000</pubDate></item><item><title>A Voter-Based Stochastic Rejection-Method Framework for Asymptotically Safe Language Model Outputs</title><link>https://arxiv.org/abs/2407.16994</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes Repeated Checking with Regeneration (RCR) to prevent unsafe LLM outputs&lt;/li&gt;&lt;li&gt;Uses stochasticity of LLMs and voting mechanism&lt;/li&gt;&lt;li&gt;Aims for Pareto-optimal cost and failure rate&lt;/li&gt;&lt;li&gt;Model-agnostic approach&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jake R. Watts', 'Joel Sokol']&lt;/li&gt;&lt;li&gt;Tags: ['safety', 'robustness', 'alignment', 'evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2407.16994</guid><pubDate>Tue, 30 Sep 2025 00:00:00 +0000</pubDate></item><item><title>GHOST: Hallucination-Inducing Image Generation for Multimodal LLMs</title><link>https://arxiv.org/abs/2509.25178</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces GHOST, a method to generate images that induce hallucinations in MLLMs&lt;/li&gt;&lt;li&gt;Operates by optimizing image embeddings to mislead models while keeping target objects absent&lt;/li&gt;&lt;li&gt;Evaluates across multiple models, achieving high hallucination success rates&lt;/li&gt;&lt;li&gt;Demonstrates transferable vulnerabilities and potential for fine-tuning mitigation&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Aryan Yazdan Parast', 'Parsa Hosseini', 'Hesam Asadollahzadeh', 'Arshia Soltani Moakhar', 'Basim Azam', 'Soheil Feizi', 'Naveed Akhtar']&lt;/li&gt;&lt;li&gt;Tags: ['red teaming', 'adversarial prompting', 'multimodal', 'hallucination', 'image generation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.25178</guid><pubDate>Tue, 30 Sep 2025 00:00:00 +0000</pubDate></item><item><title>SecInfer: Preventing Prompt Injection via Inference-time Scaling</title><link>https://arxiv.org/abs/2509.24967</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes SecInfer, a defense against prompt injection attacks using inference-time scaling&lt;/li&gt;&lt;li&gt;Uses system-prompt-guided sampling to generate multiple responses&lt;/li&gt;&lt;li&gt;Applies target-task-guided aggregation to select the best response&lt;/li&gt;&lt;li&gt;Outperforms existing defenses in experiments&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yupei Liu', 'Yanting Wang', 'Yuqi Jia', 'Jinyuan Jia', 'Neil Zhenqiang Gong']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'prompt injection', 'adversarial prompting', 'security', 'inference-time scaling']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.24967</guid><pubDate>Tue, 30 Sep 2025 00:00:00 +0000</pubDate></item><item><title>Circuit-Aware Reward Training: A Mechanistic Framework for Longtail Robustness in RLHF</title><link>https://arxiv.org/abs/2509.24713</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes Circuit-Aware Reward Training (CART) to improve longtail robustness in RLHF reward models&lt;/li&gt;&lt;li&gt;Uses circuit analysis for data augmentation, regularization, and ensemble strategies&lt;/li&gt;&lt;li&gt;Aims to address reward hacking and misalignment&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jing Liu']&lt;/li&gt;&lt;li&gt;Tags: ['robustness', 'alignment', 'safety evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.24713</guid><pubDate>Tue, 30 Sep 2025 00:00:00 +0000</pubDate></item><item><title>HarmMetric Eval: Benchmarking Metrics and Judges for LLM Harmfulness Assessment</title><link>https://arxiv.org/abs/2509.24384</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces HarmMetric Eval, a benchmark for evaluating harmfulness metrics and judges for LLMs&lt;/li&gt;&lt;li&gt;Challenges prevailing beliefs by showing METEOR and ROUGE-1 outperform LLM-based judges&lt;/li&gt;&lt;li&gt;Provides a public dataset and code&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Langqi Yang', 'Tianhang Zheng', 'Kedong Xiu', 'Yixuan Chen', 'Di Wang', 'Puning Zhao', 'Zhan Qin', 'Kui Ren']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'jailbreaking', 'safety evaluation', 'benchmarking']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.24384</guid><pubDate>Tue, 30 Sep 2025 00:00:00 +0000</pubDate></item><item><title>DiffuGuard: How Intrinsic Safety is Lost and Found in Diffusion Large Language Models</title><link>https://arxiv.org/abs/2509.24296</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Analyzes vulnerabilities in Diffusion Large Language Models (dLLMs) related to jailbreak attacks&lt;/li&gt;&lt;li&gt;Identifies Denoising-path Dependence and greedy remasking bias as key issues&lt;/li&gt;&lt;li&gt;Proposes DiffuGuard, a training-free defense framework with Stochastic Annealing Remasking and Block-level Audit and Repair&lt;/li&gt;&lt;li&gt;Reduces attack success rate from 47.9% to 14.7% while maintaining model utility&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Zherui Li', 'Zheng Nie', 'Zhenhong Zhou', 'Yufei Guo', 'Yue Liu', 'Yitong Zhang', 'Yu Cheng', 'Qingsong Wen', 'Kun Wang', 'Jiaheng Zhang']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'jailbreaking', 'adversarial prompting', 'safety evaluation', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.24296</guid><pubDate>Tue, 30 Sep 2025 00:00:00 +0000</pubDate></item><item><title>Stable Forgetting: Bounded Parameter-Efficient Unlearning in LLMs</title><link>https://arxiv.org/abs/2509.24166</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a method for stable unlearning in LLMs&lt;/li&gt;&lt;li&gt;Addresses weight and gradient instability in gradient difference method&lt;/li&gt;&lt;li&gt;Applies bounded functions to MLP adapters in LoRA&lt;/li&gt;&lt;li&gt;Improves forgetting while preserving retention across benchmarks&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Arpit Garg', 'Hemanth Saratchandran', 'Ravi Garg', 'Simon Lucey']&lt;/li&gt;&lt;li&gt;Tags: ['privacy', 'safety', 'unlearning', 'parameter efficiency', 'MLP layers']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.24166</guid><pubDate>Tue, 30 Sep 2025 00:00:00 +0000</pubDate></item><item><title>The AI Agent Code of Conduct: Automated Guardrail Policy-as-Prompt Synthesis</title><link>https://arxiv.org/abs/2509.23994</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces a framework for automating the translation of design documents into real-time guardrails using LLMs.&lt;/li&gt;&lt;li&gt;Uses 'Policy as Prompt' to enforce natural language policies via prompt-based classifiers.&lt;/li&gt;&lt;li&gt;Validates the approach across diverse applications, focusing on safety and regulatability.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Gauri Kholkar', 'Ratinder Ahuja']&lt;/li&gt;&lt;li&gt;Tags: ['safety', 'policy enforcement', 'guardrails', 'LLM', 'runtime auditing']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.23994</guid><pubDate>Tue, 30 Sep 2025 00:00:00 +0000</pubDate></item><item><title>Taught Well Learned Ill: Towards Distillation-conditional Backdoor Attack</title><link>https://arxiv.org/abs/2509.23871</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces distillation-conditional backdoor attacks (DCBAs) where backdoors in teacher models are activated in student models via knowledge distillation.&lt;/li&gt;&lt;li&gt;Proposes SCAR, a method using bilevel optimization to inject conditional backdoors into teacher models.&lt;/li&gt;&lt;li&gt;Validates the attack's effectiveness and stealth through experiments across various datasets and models.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yukun Chen', 'Boheng Li', 'Yu Yuan', 'Leyi Qi', 'Yiming Li', 'Tianwei Zhang', 'Zhan Qin', 'Kui Ren']&lt;/li&gt;&lt;li&gt;Tags: ['backdoor attacks', 'knowledge distillation', 'adversarial attacks', 'security', 'bilevel optimization']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.23871</guid><pubDate>Tue, 30 Sep 2025 00:00:00 +0000</pubDate></item><item><title>HFuzzer: Testing Large Language Models for Package Hallucinations via Phrase-based Fuzzing</title><link>https://arxiv.org/abs/2509.23835</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;HFuzzer is a phrase-based fuzzing framework to test LLMs for package hallucinations&lt;/li&gt;&lt;li&gt;It generates coding tasks using phrases to trigger hallucinations&lt;/li&gt;&lt;li&gt;Evaluated on multiple LLMs, including GPT-4o, finding significant hallucinations&lt;/li&gt;&lt;li&gt;Aims to mitigate software supply chain attacks by detecting non-existent package recommendations&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yukai Zhao', 'Menghan Wu', 'Xing Hu', 'Xin Xia']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'adversarial prompting', 'security', 'fuzzing', 'package hallucinations']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.23835</guid><pubDate>Tue, 30 Sep 2025 00:00:00 +0000</pubDate></item><item><title>Uncovering Vulnerabilities of LLM-Assisted Cyber Threat Intelligence</title><link>https://arxiv.org/abs/2509.23573</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Investigates LLM vulnerabilities in cyber threat intelligence (CTI)&lt;/li&gt;&lt;li&gt;Introduces a categorization methodology for analyzing failure instances&lt;/li&gt;&lt;li&gt;Identifies three fundamental vulnerabilities: spurious correlations, contradictory knowledge, constrained generalization&lt;/li&gt;&lt;li&gt;Provides insights for more robust LLM-powered CTI systems&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yuqiao Meng', 'Luoxi Tang', 'Feiyang Yu', 'Jinyuan Jia', 'Guanhua Yan', 'Ping Yang', 'Zhaohan Xi']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'safety evaluation', 'robustness', 'alignment', 'privacy attacks']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.23573</guid><pubDate>Tue, 30 Sep 2025 00:00:00 +0000</pubDate></item><item><title>ReliabilityRAG: Effective and Provably Robust Defense for RAG-based Web-Search</title><link>https://arxiv.org/abs/2509.23519</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces ReliabilityRAG, a framework for adversarial robustness in RAG-based systems&lt;/li&gt;&lt;li&gt;Uses graph-theoretic approach with Maximum Independent Set (MIS) to filter malicious documents&lt;/li&gt;&lt;li&gt;Provides provable robustness guarantees and scalability through weighted sampling&lt;/li&gt;&lt;li&gt;Empirical results show improved robustness against adversarial attacks&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Zeyu Shen', 'Basileal Imana', 'Tong Wu', 'Chong Xiang', 'Prateek Mittal', 'Aleksandra Korolova']&lt;/li&gt;&lt;li&gt;Tags: ['red teaming', 'adversarial prompting', 'robustness', 'RAG', 'document retrieval']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.23519</guid><pubDate>Tue, 30 Sep 2025 00:00:00 +0000</pubDate></item><item><title>Dual-Space Smoothness for Robust and Balanced LLM Unlearning</title><link>https://arxiv.org/abs/2509.23362</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes PRISM framework for robust and balanced LLM unlearning&lt;/li&gt;&lt;li&gt;Addresses jailbreak and relearn attacks through dual-space smoothness&lt;/li&gt;&lt;li&gt;Improves unlearning metrics balance and defense against attacks&lt;/li&gt;&lt;li&gt;Evaluated on WMDP and MUSE datasets with conversational and continuous text&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Han Yan', 'Zheyuan Liu', 'Meng Jiang']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'jailbreaking', 'model extraction', 'robustness', 'safety evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.23362</guid><pubDate>Tue, 30 Sep 2025 00:00:00 +0000</pubDate></item><item><title>A2D: Any-Order, Any-Step Safety Alignment for Diffusion Language Models</title><link>https://arxiv.org/abs/2509.23286</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces A2D, a token-level alignment method for diffusion LLMs&lt;/li&gt;&lt;li&gt;Aims to prevent harmful content generation at any position&lt;/li&gt;&lt;li&gt;Reduces DIJA attack success rates significantly&lt;/li&gt;&lt;li&gt;Enables real-time monitoring and early termination&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Wonje Jeung', 'Sangyeon Yoon', 'Yoonjun Cho', 'Dongjae Jeon', 'Sangwoo Shin', 'Hyesoo Hong', 'Albert No']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'safety alignment', 'adversarial prompting', 'robustness', 'privacy attacks']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.23286</guid><pubDate>Tue, 30 Sep 2025 00:00:00 +0000</pubDate></item><item><title>Vid-Freeze: Protecting Images from Malicious Image-to-Video Generation via Temporal Freezing</title><link>https://arxiv.org/abs/2509.23279</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Vid-Freeze is an adversarial attack that adds perturbations to images to disrupt motion in generated videos&lt;/li&gt;&lt;li&gt;Targets attention mechanisms in I2V models&lt;/li&gt;&lt;li&gt;Aims to prevent malicious video synthesis while preserving image semantics&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Rohit Chowdhury', 'Aniruddha Bala', 'Rohan Jaiswal', 'Siddharth Roheda']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial attacks', 'image-to-video generation', 'security', 'privacy', 'red teaming']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.23279</guid><pubDate>Tue, 30 Sep 2025 00:00:00 +0000</pubDate></item><item><title>Virus Infection Attack on LLMs: Your Poisoning Can Spread "VIA" Synthetic Data</title><link>https://arxiv.org/abs/2509.23041</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces Virus Infection Attack (VIA) framework for propagating poisoning and backdoor attacks through synthetic data in LLM training.&lt;/li&gt;&lt;li&gt;Shows that VIA increases poisoning content in synthetic data and raises attack success rates on downstream models.&lt;/li&gt;&lt;li&gt;Highlights security risks of using synthetic data in LLM training.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Zi Liang', 'Qingqing Ye', 'Xuan Liu', 'Yanyun Wang', 'Jianliang Xu', 'Haibo Hu']&lt;/li&gt;&lt;li&gt;Tags: ['data poisoning', 'backdoor attacks', 'synthetic data', 'LLM security', 'red teaming']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.23041</guid><pubDate>Tue, 30 Sep 2025 00:00:00 +0000</pubDate></item><item><title>LLM Watermark Evasion via Bias Inversion</title><link>https://arxiv.org/abs/2509.23019</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes Bias-Inversion Rewriting Attack (BIRA) to evade LLM watermarks&lt;/li&gt;&lt;li&gt;BIRA suppresses logits of likely watermarked tokens during rewriting&lt;/li&gt;&lt;li&gt;Achieves over 99% evasion across watermarking methods&lt;/li&gt;&lt;li&gt;Emphasizes need for robust defenses and stress testing&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jeongyeon Hwang', 'Sangdon Park', 'Jungseul Ok']&lt;/li&gt;&lt;li&gt;Tags: ['red teaming', 'adversarial prompting', 'model extraction', 'privacy attacks', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.23019</guid><pubDate>Tue, 30 Sep 2025 00:00:00 +0000</pubDate></item><item><title>Boundary on the Table: Efficient Black-Box Decision-Based Attacks for Structured Data</title><link>https://arxiv.org/abs/2509.22850</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces a black-box decision-based adversarial attack for tabular data&lt;/li&gt;&lt;li&gt;Combines gradient-free direction estimation with boundary search&lt;/li&gt;&lt;li&gt;Effective against both classical ML and LLM-based models&lt;/li&gt;&lt;li&gt;High success rates with minimal queries&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Roie Kazoom', 'Yuval Ratzabi', 'Etamar Rothstein', 'Ofer Hadar']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial attacks', 'black-box attacks', 'tabular data', 'model robustness', 'security']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.22850</guid><pubDate>Tue, 30 Sep 2025 00:00:00 +0000</pubDate></item><item><title>Defending MoE LLMs against Harmful Fine-Tuning via Safety Routing Alignment</title><link>https://arxiv.org/abs/2509.22745</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes SafeMoE, a method to defend MoE LLMs against harmful fine-tuning by preserving safety routing alignment.&lt;/li&gt;&lt;li&gt;Penalizes routing weight gaps between fine-tuned and initial safety-aligned models.&lt;/li&gt;&lt;li&gt;Reduces harmfulness score while maintaining utility and minimal overhead.&lt;/li&gt;&lt;li&gt;Outperforms existing defenses and works with large MoE models like Llama 4.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jaehan Kim', 'Minkyoo Song', 'Seungwon Shin', 'Sooel Son']&lt;/li&gt;&lt;li&gt;Tags: ['LLM security', 'fine-tuning defense', 'Mixture-of-Experts', 'safety routing', 'harmful fine-tuning']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.22745</guid><pubDate>Tue, 30 Sep 2025 00:00:00 +0000</pubDate></item><item><title>Regulating the Agency of LLM-based Agents</title><link>https://arxiv.org/abs/2509.22735</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes measuring and controlling the agency of LLM-based agents&lt;/li&gt;&lt;li&gt;Defines agency along dimensions of preference rigidity, independent operation, and goal persistence&lt;/li&gt;&lt;li&gt;Suggests regulatory tools like testing protocols and agency limits&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Position&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ["Se\\'an Boddy", 'Joshua Joseph']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'safety', 'regulation', 'agency']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.22735</guid><pubDate>Tue, 30 Sep 2025 00:00:00 +0000</pubDate></item><item><title>Bidirectional Intention Inference Enhances LLMs' Defense Against Multi-Turn Jailbreak Attacks</title><link>https://arxiv.org/abs/2509.22732</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes BIID, a defense against multi-turn jailbreak attacks using bidirectional intention inference&lt;/li&gt;&lt;li&gt;Evaluates on three LLMs and two safety benchmarks with 10 attack methods&lt;/li&gt;&lt;li&gt;Shows reduced ASR while maintaining utility&lt;/li&gt;&lt;li&gt;Validates on multi-turn safety datasets&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Haibo Tong', 'Dongcheng Zhao', 'Guobin Shen', 'Xiang He', 'Dachuan Lin', 'Feifei Zhao', 'Yi Zeng']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'jailbreaking', 'adversarial prompting', 'safety evaluation', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.22732</guid><pubDate>Tue, 30 Sep 2025 00:00:00 +0000</pubDate></item><item><title>Who's Your Judge? On the Detectability of LLM-Generated Judgments</title><link>https://arxiv.org/abs/2509.25154</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes J-Detector for detecting LLM-generated judgments&lt;/li&gt;&lt;li&gt;Focuses on judgment scores and candidates without textual feedback&lt;/li&gt;&lt;li&gt;Shows existing methods perform poorly&lt;/li&gt;&lt;li&gt;Analyzes factors affecting detectability&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Dawei Li', 'Zhen Tan', 'Chengshuai Zhao', 'Bohan Jiang', 'Baixiang Huang', 'Pingchuan Ma', 'Abdullah Alnaibari', 'Kai Shu', 'Huan Liu']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'adversarial prompting', 'safety evaluation', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.25154</guid><pubDate>Tue, 30 Sep 2025 00:00:00 +0000</pubDate></item><item><title>Towards Safe Reasoning in Large Reasoning Models via Corrective Intervention</title><link>https://arxiv.org/abs/2509.24393</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces Intervened Preference Optimization (IPO) to enforce safe reasoning in LRMs&lt;/li&gt;&lt;li&gt;Addresses jailbreak and adversarial safety benchmarks&lt;/li&gt;&lt;li&gt;Improves safety by substituting compliance steps with safety triggers&lt;/li&gt;&lt;li&gt;Reduces harmfulness by over 30% compared to baselines&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yichi Zhang', 'Yue Ding', 'Jingwen Yang', 'Tianwei Luo', 'Dongbai Li', 'Ranjie Duan', 'Qiang Liu', 'Hang Su', 'Yinpeng Dong', 'Jun Zhu']&lt;/li&gt;&lt;li&gt;Tags: ['red teaming', 'alignment', 'safety evaluation', 'robustness', 'jailbreaking']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.24393</guid><pubDate>Tue, 30 Sep 2025 00:00:00 +0000</pubDate></item><item><title>AdvChain: Adversarial Chain-of-Thought Tuning for Robust Safety Alignment of Large Reasoning Models</title><link>https://arxiv.org/abs/2509.24269</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces AdvChain, an adversarial CoT tuning method for LRM safety alignment&lt;/li&gt;&lt;li&gt;Addresses the snowball effect in CoT reasoning leading to harmful compliance or over-refusal&lt;/li&gt;&lt;li&gt;Uses Temptation-Correction and Hesitation-Correction samples for self-correction training&lt;/li&gt;&lt;li&gt;Improves robustness against jailbreak attacks and reduces over-refusal on benign prompts&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Zihao Zhu', 'Xinyu Wu', 'Gehan Hu', 'Siwei Lyu', 'Ke Xu', 'Baoyuan Wu']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'alignment', 'robustness', 'safety evaluation', 'jailbreaking']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.24269</guid><pubDate>Tue, 30 Sep 2025 00:00:00 +0000</pubDate></item><item><title>Robust Preference Optimization: Aligning Language Models with Noisy Preference Feedback</title><link>https://arxiv.org/abs/2509.24159</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces Robust Preference Optimization (RPO) to handle noisy human preference data in LLM alignment&lt;/li&gt;&lt;li&gt;Uses EM algorithm to infer label correctness and re-weigh training data&lt;/li&gt;&lt;li&gt;Generalizes to a meta-framework for existing alignment algorithms&lt;/li&gt;&lt;li&gt;Proves convergence to true noise level under calibration&lt;/li&gt;&lt;li&gt;Shows improvements in win rates on evaluation benchmarks&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Xiaoyang Cao', 'Zelai Xu', 'Mo Guang', 'Kaiwen Long', 'Michiel A. Bakker', 'Yu Wang', 'Chao Yu']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'robustness', 'safety evaluation', 'human feedback', 'preference learning']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.24159</guid><pubDate>Tue, 30 Sep 2025 00:00:00 +0000</pubDate></item><item><title>Quant Fever, Reasoning Blackholes, Schrodinger's Compliance, and More: Probing GPT-OSS-20B</title><link>https://arxiv.org/abs/2509.23882</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Security evaluation of GPT-OSS-20B using Jailbreak Oracle&lt;/li&gt;&lt;li&gt;Identifies failure modes like quant fever, reasoning blackholes, Schrodinger's compliance&lt;/li&gt;&lt;li&gt;Exploits demonstrated on the model leading to severe consequences&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Shuyi Lin', 'Tian Lu', 'Zikai Wang', 'Bo Wen', 'Yibo Zhao', 'Cheng Tan']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'jailbreaking', 'adversarial prompting', 'security evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.23882</guid><pubDate>Tue, 30 Sep 2025 00:00:00 +0000</pubDate></item><item><title>AgentGuard: Runtime Verification of AI Agents</title><link>https://arxiv.org/abs/2509.23864</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces AgentGuard, a runtime verification framework for agentic AI systems&lt;/li&gt;&lt;li&gt;Uses dynamic probabilistic assurance to monitor agent behavior&lt;/li&gt;&lt;li&gt;Models agent transitions as MDP and checks properties in real-time&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Roham Koohestani']&lt;/li&gt;&lt;li&gt;Tags: ['runtime verification', 'probabilistic model checking', 'MDP', 'agentic AI', 'emergent behavior']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.23864</guid><pubDate>Tue, 30 Sep 2025 00:00:00 +0000</pubDate></item><item><title>SafeSearch: Automated Red-Teaming for the Safety of LLM-Based Search Agents</title><link>https://arxiv.org/abs/2509.23694</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces SafeSearch, an automated red-teaming framework for LLM-based search agents&lt;/li&gt;&lt;li&gt;Constructs a benchmark with 300 test cases across five risk categories&lt;/li&gt;&lt;li&gt;Evaluates three search agent scaffolds with multiple LLMs, revealing high attack success rates&lt;/li&gt;&lt;li&gt;Highlights limitations of current defense practices like reminder prompting&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jianshuo Dong', 'Sheng Guo', 'Hao Wang', 'Zhuotao Liu', 'Tianwei Zhang', 'Ke Xu', 'Minlie Huang', 'Han Qiu']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'safety evaluation', 'search agents', 'benchmarking', 'adversarial prompting']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.23694</guid><pubDate>Tue, 30 Sep 2025 00:00:00 +0000</pubDate></item><item><title>PSG-Agent: Personality-Aware Safety Guardrail for LLM-based Agents</title><link>https://arxiv.org/abs/2509.23614</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes PSG-Agent, a personality-aware safety guardrail for LLM-based agents&lt;/li&gt;&lt;li&gt;Addresses limitations of uniform guardrails and isolated response checks&lt;/li&gt;&lt;li&gt;Implements personalized guardrails based on user traits and real-time states&lt;/li&gt;&lt;li&gt;Validates in healthcare, finance, and daily life scenarios&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yaozu Wu', 'Jizhou Guo', 'Dongyuan Li', 'Henry Peng Zou', 'Wei-Chieh Huang', 'Yankai Chen', 'Zhen Wang', 'Weizhi Zhang', 'Yangning Li', 'Meng Zhang', 'Renhe Jiang', 'Philip S. Yu']&lt;/li&gt;&lt;li&gt;Tags: ['LLM safety', 'guardrails', 'personalization', 'dynamic monitoring', 'risk accumulation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.23614</guid><pubDate>Tue, 30 Sep 2025 00:00:00 +0000</pubDate></item><item><title>Formalization Driven LLM Prompt Jailbreaking via Reinforcement Learning</title><link>https://arxiv.org/abs/2509.23558</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes PASS framework for LLM prompt jailbreaking using reinforcement learning&lt;/li&gt;&lt;li&gt;Transforms initial prompts into formalized descriptions for stealth&lt;/li&gt;&lt;li&gt;Uses GraphRAG to structure outputs and enhance attacks&lt;/li&gt;&lt;li&gt;Conducted experiments on open-source models&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Zhaoqi Wang', 'Daqing He', 'Zijian Zhang', 'Xin Li', 'Liehuang Zhu', 'Meng Li', 'Jiamou Liu']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'jailbreaking', 'reinforcement learning', 'GraphRAG', 'alignment']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.23558</guid><pubDate>Tue, 30 Sep 2025 00:00:00 +0000</pubDate></item><item><title>Creative Adversarial Testing (CAT): A Novel Framework for Evaluating Goal-Oriented Agentic AI Systems</title><link>https://arxiv.org/abs/2509.23006</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces Creative Adversarial Testing (CAT) framework for evaluating goal-task alignment in Agentic AI systems&lt;/li&gt;&lt;li&gt;Validates framework using synthetic interaction data modeled after Alexa+&lt;/li&gt;&lt;li&gt;Focuses on alignment and safety evaluation&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research Paper&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Hassen Dhrif']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'safety evaluation', 'red teaming']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.23006</guid><pubDate>Tue, 30 Sep 2025 00:00:00 +0000</pubDate></item></channel></rss>