<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>AI Security Paper Digest</title><link>https://kentaroh-toyoda.github.io/ai-security-paper-digest-rss/rss.xml</link><description>Curated papers on AI security from ArXiv and ACL</description><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><lastBuildDate>Wed, 22 Oct 2025 22:24:12 +0000</lastBuildDate><item><title>Learning to Detect Unknown Jailbreak Attacks in Large Vision-Language Models</title><link>https://arxiv.org/abs/2508.09201</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes Learning to Detect (LoD) framework for detecting unknown jailbreak attacks in LVLMs&lt;/li&gt;&lt;li&gt;Uses Multi-modal Safety Concept Activation Vector for representation learning&lt;/li&gt;&lt;li&gt;Incorporates Safety Pattern Auto-Encoder for unsupervised attack classification&lt;/li&gt;&lt;li&gt;Shows higher detection AUROC and improved efficiency in experiments&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Shuang Liang', 'Zhihao Xu', 'Jialing Tao', 'Hui Xue', 'Xiting Wang']&lt;/li&gt;&lt;li&gt;Tags: ['jailbreaking', 'red teaming', 'safety evaluation', 'vision-language models', 'adversarial attacks']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2508.09201</guid><pubDate>Wed, 22 Oct 2025 00:00:00 +0000</pubDate></item><item><title>MSR-Align: Policy-Grounded Multimodal Alignment for Safety-Aware Reasoning in Vision-Language Models</title><link>https://arxiv.org/abs/2506.19257</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces MSR-Align, a multimodal safety reasoning dataset for VLMs&lt;/li&gt;&lt;li&gt;Aims to improve robustness against jailbreak attacks in multimodal models&lt;/li&gt;&lt;li&gt;Emphasizes policy-grounded reasoning over both vision and text&lt;/li&gt;&lt;li&gt;Demonstrates improved safety alignment through fine-tuning&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yinan Xia', 'Yilei Jiang', 'Yingshui Tan', 'Xiaoyong Zhu', 'Xiangyu Yue', 'Bo Zheng']&lt;/li&gt;&lt;li&gt;Tags: ['multimodal alignment', 'safety evaluation', 'jailbreaking', 'dataset', 'vision-language models']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.19257</guid><pubDate>Wed, 22 Oct 2025 00:00:00 +0000</pubDate></item><item><title>FeatureFool: Zero-Query Fooling of Video Models via Feature Map</title><link>https://arxiv.org/abs/2510.18362</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes FeatureFool, a zero-query black-box attack on video models using feature maps&lt;/li&gt;&lt;li&gt;Achieves high success rates without model queries, applicable to Video-LLMs&lt;/li&gt;&lt;li&gt;Generates imperceptible adversarial videos with high quality metrics&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Duoxun Tang', 'Xi Xiao', 'Guangwu Hu', 'Kangkang Sun', 'Xiao Yang', 'Dongyang Chen', 'Qing Li', 'Yongjie Yin', 'Jiyao Wang']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial attacks', 'video models', 'zero-query attacks', 'feature maps', 'Video-LLMs']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.18362</guid><pubDate>Wed, 22 Oct 2025 00:00:00 +0000</pubDate></item><item><title>SafeCoop: Unravelling Full Stack Safety in Agentic Collaborative Driving</title><link>https://arxiv.org/abs/2510.18123</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;First systematic study of full-stack safety in natural-language-based collaborative driving&lt;/li&gt;&lt;li&gt;Develops taxonomy of attack strategies and agentic defense pipeline SafeCoop&lt;/li&gt;&lt;li&gt;Evaluates in CARLA simulation with significant improvements in driving score and detection&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Xiangbo Gao', 'Tzu-Hsiang Lin', 'Ruojing Song', 'Yuheng Wu', 'Kuan-Ru Huang', 'Zicheng Jin', 'Fangzhou Lin', 'Shinan Liu', 'Zhengzhong Tu']&lt;/li&gt;&lt;li&gt;Tags: ['AI security', 'red teaming', 'adversarial attacks', 'safety evaluation', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.18123</guid><pubDate>Wed, 22 Oct 2025 00:00:00 +0000</pubDate></item><item><title>RepIt: Steering Language Models with Concept-Specific Refusal Vectors</title><link>https://arxiv.org/abs/2509.13281</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces RepIt, a framework for isolating concept-specific representations in LLMs&lt;/li&gt;&lt;li&gt;Enables precise suppression of refusal on targeted concepts while preserving safety&lt;/li&gt;&lt;li&gt;Raises concerns about manipulations with modest compute and data&lt;/li&gt;&lt;li&gt;Demonstrates targeted interventions to counteract overgeneralization&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Vincent Siu', 'Nathan W. Henry', 'Nicholas Crispino', 'Yang Liu', 'Dawn Song', 'Chenguang Wang']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'alignment', 'safety evaluation', 'adversarial prompting', 'model extraction']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.13281</guid><pubDate>Wed, 22 Oct 2025 00:00:00 +0000</pubDate></item><item><title>MSR-Align: Policy-Grounded Multimodal Alignment for Safety-Aware Reasoning in Vision-Language Models</title><link>https://arxiv.org/abs/2506.19257</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces MSR-Align, a multimodal safety reasoning dataset for VLMs&lt;/li&gt;&lt;li&gt;Aims to improve robustness against jailbreak attacks in multimodal models&lt;/li&gt;&lt;li&gt;Emphasizes policy-grounded reasoning over both vision and text&lt;/li&gt;&lt;li&gt;Demonstrates improved safety alignment through fine-tuning&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yinan Xia', 'Yilei Jiang', 'Yingshui Tan', 'Xiaoyong Zhu', 'Xiangyu Yue', 'Bo Zheng']&lt;/li&gt;&lt;li&gt;Tags: ['multimodal alignment', 'safety evaluation', 'jailbreaking', 'dataset', 'vision-language models']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.19257</guid><pubDate>Wed, 22 Oct 2025 00:00:00 +0000</pubDate></item><item><title>HauntAttack: When Attack Follows Reasoning as a Shadow</title><link>https://arxiv.org/abs/2506.07031</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces HauntAttack, a black-box adversarial attack framework targeting Large Reasoning Models (LRMs)&lt;/li&gt;&lt;li&gt;Embeds harmful instructions into reasoning questions to guide models toward unsafe outputs&lt;/li&gt;&lt;li&gt;Evaluates on 11 LRMs with 70% average success rate, highlighting safety vulnerabilities&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jingyuan Ma', 'Rui Li', 'Zheng Li', 'Junfeng Liu', 'Heming Xia Lei Sha', 'Zhifang Sui']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'jailbreaking', 'adversarial prompting', 'safety evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.07031</guid><pubDate>Wed, 22 Oct 2025 00:00:00 +0000</pubDate></item><item><title>DrunkAgent: Stealthy Memory Corruption in LLM-Powered Recommender Agents</title><link>https://arxiv.org/abs/2503.23804</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces DrunkAgent, a black-box attack framework targeting memory corruption in LLM-powered recommender agents.&lt;/li&gt;&lt;li&gt;Focuses on crafting adversarial textual triggers to manipulate memory updates during interactions.&lt;/li&gt;&lt;li&gt;Demonstrates generalizability, transferability, and stealthiness across various recommender systems.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Shiyi Yang', 'Zhibo Hu', 'Xinshu Li', 'Chen Wang', 'Tong Yu', 'Xiwei Xu', 'Liming Zhu', 'Lina Yao']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'adversarial prompting', 'memory corruption', 'recommender systems', 'black-box attacks']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2503.23804</guid><pubDate>Wed, 22 Oct 2025 00:00:00 +0000</pubDate></item><item><title>SafeSearch: Do Not Trade Safety for Utility in LLM Search Agents</title><link>https://arxiv.org/abs/2510.17017</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Evaluates LLM search agents using red-teaming datasets and finds they are more likely to produce harmful outputs than base LLMs.&lt;/li&gt;&lt;li&gt;Introduces SafeSearch, a multi-objective reinforcement learning approach that combines safety and utility rewards.&lt;/li&gt;&lt;li&gt;Reduces harmfulness by 70% while maintaining QA performance.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Qiusi Zhan', 'Angeline Budiman-Chan', 'Abdelrahman Zayed', 'Xingzhi Guo', 'Daniel Kang', 'Joo-Kyung Kim']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'safety evaluation', 'reinforcement learning', 'alignment', 'utility vs safety trade-off']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.17017</guid><pubDate>Wed, 22 Oct 2025 00:00:00 +0000</pubDate></item><item><title>FALCON: Fine-grained Activation Manipulation by Contrastive Orthogonal Unalignment for Large Language Model</title><link>https://arxiv.org/abs/2502.01472</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes FALCON, a new unlearning approach for LLMs&lt;/li&gt;&lt;li&gt;Aims to address safety concerns by precisely removing sensitive info&lt;/li&gt;&lt;li&gt;Uses contrastive mechanisms and orthogonal projections&lt;/li&gt;&lt;li&gt;Shows robustness against knowledge recovery&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research Paper&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jinwei Hu', 'Zhenglin Huang', 'Xiangyu Yin', 'Wenjie Ruan', 'Guangliang Cheng', 'Yi Dong', 'Xiaowei Huang']&lt;/li&gt;&lt;li&gt;Tags: ['safety', 'unlearning', 'model extraction', 'privacy attacks']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2502.01472</guid><pubDate>Wed, 22 Oct 2025 00:00:00 +0000</pubDate></item><item><title>SafeCoop: Unravelling Full Stack Safety in Agentic Collaborative Driving</title><link>https://arxiv.org/abs/2510.18123</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;First systematic study of full-stack safety in natural-language-based collaborative driving&lt;/li&gt;&lt;li&gt;Develops taxonomy of attack strategies and agentic defense pipeline SafeCoop&lt;/li&gt;&lt;li&gt;Evaluates in CARLA simulation with significant improvements in driving score and detection&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Xiangbo Gao', 'Tzu-Hsiang Lin', 'Ruojing Song', 'Yuheng Wu', 'Kuan-Ru Huang', 'Zicheng Jin', 'Fangzhou Lin', 'Shinan Liu', 'Zhengzhong Tu']&lt;/li&gt;&lt;li&gt;Tags: ['AI security', 'red teaming', 'adversarial attacks', 'safety evaluation', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.18123</guid><pubDate>Wed, 22 Oct 2025 00:00:00 +0000</pubDate></item><item><title>PLAGUE: Plug-and-play framework for Lifelong Adaptive Generation of Multi-turn Exploits</title><link>https://arxiv.org/abs/2510.17947</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;PLAGUE is a framework for multi-turn jailbreaking attacks on LLMs&lt;/li&gt;&lt;li&gt;Improves attack success rates by 30% across leading models&lt;/li&gt;&lt;li&gt;Specifically effective against OpenAI's o3 and Claude's Opus 4.1&lt;/li&gt;&lt;li&gt;Focuses on plan initialization, context optimization, and lifelong learning&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Neeladri Bhuiya', 'Madhav Aggarwal', 'Diptanshu Purwar']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'jailbreaking', 'multi-turn attacks', 'adversarial prompting', 'safety evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.17947</guid><pubDate>Wed, 22 Oct 2025 00:00:00 +0000</pubDate></item><item><title>BreakFun: Jailbreaking LLMs via Schema Exploitation</title><link>https://arxiv.org/abs/2510.17904</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces BreakFun, a jailbreak method using schema exploitation&lt;/li&gt;&lt;li&gt;Achieves high success rates across multiple LLMs&lt;/li&gt;&lt;li&gt;Proposes Adversarial Prompt Deconstruction as a defense&lt;/li&gt;&lt;li&gt;Highlights schema adherence as a vulnerability&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Amirkia Rafiei Oskooei', 'Mehmet S. Aktas']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'jailbreaking', 'adversarial prompting', 'model extraction', 'alignment', 'robustness', 'safety evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.17904</guid><pubDate>Wed, 22 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Engagement Undermines Safety: How Stereotypes and Toxicity Shape Humor in Language Models</title><link>https://arxiv.org/abs/2510.18454</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;This paper evaluates how humor optimization in LLMs relates to harmful content like stereotypes and toxicity.&lt;/li&gt;&lt;li&gt;It finds that harmful outputs get higher humor scores and this increases with role-based prompts.&lt;/li&gt;&lt;li&gt;Information-theoretic metrics show harmful cues affect predictive uncertainty.&lt;/li&gt;&lt;li&gt;Human validation shows LLM satire increases stereotypes and toxicity.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Atharvan Dogra', 'Soumya Suvra Ghosal', 'Ameet Deshpande', 'Ashwin Kalyan', 'Dinesh Manocha']&lt;/li&gt;&lt;li&gt;Tags: ['safety evaluation', 'alignment', 'adversarial prompting', 'data poisoning', 'privacy attacks']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.18454</guid><pubDate>Wed, 22 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Believe It or Not: How Deeply do LLMs Believe Implanted Facts?</title><link>https://arxiv.org/abs/2510.17941</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;The paper evaluates how deeply LLMs believe implanted facts using a framework that measures generalization, robustness to challenge, and representation similarity.&lt;/li&gt;&lt;li&gt;It compares different knowledge editing techniques, finding that Synthetic Document Finetuning (SDF) is more effective but still has limitations with contradictory facts.&lt;/li&gt;&lt;li&gt;The work introduces measurable criteria for belief depth to improve knowledge editing deployment.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Stewart Slocum', 'Julian Minder', "Cl\\'ement Dumas", 'Henry Sleight', 'Ryan Greenblatt', 'Samuel Marks', 'Rowan Wang']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'safety evaluation', 'robustness', 'knowledge editing']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.17941</guid><pubDate>Wed, 22 Oct 2025 00:00:00 +0000</pubDate></item><item><title>JT-Safe: Intrinsically Enhancing the Safety and Trustworthiness of LLMs</title><link>https://arxiv.org/abs/2510.17918</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;The paper focuses on enhancing pre-training data to improve LLM safety and trustworthiness.&lt;/li&gt;&lt;li&gt;Proposes using Data with World Context (DWC) to anchor data in real-world scenarios.&lt;/li&gt;&lt;li&gt;Reports a 1.79% improvement over Qwen model on safety benchmarks.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Junlan Feng', 'Fanyu Meng', 'Chong Long', 'Pengyu Cong', 'Duqing Wang', 'Yan Zheng', 'Yuyao Zhang', 'Xuanchang Gao', 'Ye Yuan', 'Yunfei Ma', 'Zhijie Ren', 'Fan Yang', 'Na Wu', 'Di Jin', 'Chao Deng']&lt;/li&gt;&lt;li&gt;Tags: ['safety', 'pre-training', 'data enhancement', 'evaluation benchmarks']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.17918</guid><pubDate>Wed, 22 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Do LLMs Strategically Reveal, Conceal, and Infer Information? A Theoretical and Empirical Analysis in The Chameleon Game</title><link>https://arxiv.org/abs/2501.19398</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;The paper evaluates LLMs' ability to control information in a non-cooperative game setting.&lt;/li&gt;&lt;li&gt;It combines theoretical analysis with empirical testing using various LLMs.&lt;/li&gt;&lt;li&gt;Results indicate LLMs struggle to conceal information from adversaries.&lt;/li&gt;&lt;li&gt;The study suggests that internal representation steering could improve concealment.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Mustafa O. Karabag', 'Jan Sobotka', 'Ufuk Topcu']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'adversarial prompting', 'alignment', 'safety evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2501.19398</guid><pubDate>Wed, 22 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Language Models are Injective and Hence Invertible</title><link>https://arxiv.org/abs/2510.15511</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proves mathematically that transformer LLMs are injective&lt;/li&gt;&lt;li&gt;Empirically confirms no collisions in 6 SOTA models&lt;/li&gt;&lt;li&gt;Introduces SipIt algorithm for exact input reconstruction&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Giorgos Nikolaou', 'Tommaso Mencattini', 'Donato Crisostomi', 'Andrea Santilli', 'Yannis Panagakis', 'Emanuele Rodol\\`a']&lt;/li&gt;&lt;li&gt;Tags: ['model extraction', 'privacy attacks', 'safety evaluation', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.15511</guid><pubDate>Wed, 22 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Noise-Robustness Through Noise: A Framework combining Asymmetric LoRA with Poisoning MoE</title><link>https://arxiv.org/abs/2505.23868</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes LoPE framework for noise-robust LLM adaptation&lt;/li&gt;&lt;li&gt;Uses asymmetric LoRA with poisoning expert&lt;/li&gt;&lt;li&gt;Injects noise during training, masks during inference&lt;/li&gt;&lt;li&gt;Eliminates data cleaning&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Zhaokun Wang', 'Jinyu Guo', 'Jingwen Pu', 'Lingfeng Chen', 'Hongli Pu', 'Jie Ou', 'Libo Qin', 'Wenhong Tian']&lt;/li&gt;&lt;li&gt;Tags: ['robustness', 'noise-robustness', 'LoRA', 'mixture-of-experts']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2505.23868</guid><pubDate>Wed, 22 Oct 2025 00:00:00 +0000</pubDate></item><item><title>LLM Safety Alignment is Divergence Estimation in Disguise</title><link>https://arxiv.org/abs/2502.00657</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;The paper presents a theoretical framework linking LLM alignment methods to divergence estimation.&lt;/li&gt;&lt;li&gt;Proposes KLDO, a new alignment method based on KL divergence.&lt;/li&gt;&lt;li&gt;Uses compliance-refusal datasets for better safety alignment.&lt;/li&gt;&lt;li&gt;Introduces a distance-based metric to measure separation in prompt representation space.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Rajdeep Haldar', 'Ziyi Wang', 'Qifan Song', 'Guang Lin', 'Yue Xing']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'safety evaluation', 'divergence estimation', 'RLHF', 'prompt representation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2502.00657</guid><pubDate>Wed, 22 Oct 2025 00:00:00 +0000</pubDate></item><item><title>PLAGUE: Plug-and-play framework for Lifelong Adaptive Generation of Multi-turn Exploits</title><link>https://arxiv.org/abs/2510.17947</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;PLAGUE is a framework for multi-turn jailbreaking attacks on LLMs&lt;/li&gt;&lt;li&gt;Improves attack success rates by 30% across leading models&lt;/li&gt;&lt;li&gt;Specifically effective against OpenAI's o3 and Claude's Opus 4.1&lt;/li&gt;&lt;li&gt;Focuses on plan initialization, context optimization, and lifelong learning&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Neeladri Bhuiya', 'Madhav Aggarwal', 'Diptanshu Purwar']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'jailbreaking', 'multi-turn attacks', 'adversarial prompting', 'safety evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.17947</guid><pubDate>Wed, 22 Oct 2025 00:00:00 +0000</pubDate></item><item><title>When Intelligence Fails: An Empirical Study on Why LLMs Struggle with Password Cracking</title><link>https://arxiv.org/abs/2510.17884</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Empirical study on LLMs' password cracking ability using synthetic user profiles&lt;/li&gt;&lt;li&gt;Evaluated models like TinyLLaMA, Falcon-RW-1B, Flan-T5 with Hit@ metrics&lt;/li&gt;&lt;li&gt;LLMs showed poor performance (&lt;1.5% Hit@10) compared to traditional methods&lt;/li&gt;&lt;li&gt;Identified limitations in generative reasoning for domain-specific tasks&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Mohammad Abdul Rehman', 'Syed Imad Ali Shah', 'Abbas Anwar', 'Noor Islam']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'password cracking', 'adversarial prompting', 'security evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.17884</guid><pubDate>Wed, 22 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Pay Attention to the Triggers: Constructing Backdoors That Survive Distillation</title><link>https://arxiv.org/abs/2510.18541</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Investigates security risks of knowledge distillation from backdoored teacher models&lt;/li&gt;&lt;li&gt;Introduces T-MTB technique for constructing transferable backdoors&lt;/li&gt;&lt;li&gt;Demonstrates jailbreaking and content modulation attacks across multiple LLMs&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Giovanni De Muri', 'Mark Vero', 'Robin Staab', 'Martin Vechev']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'backdoor attacks', 'jailbreaking', 'distillation', 'security']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.18541</guid><pubDate>Wed, 22 Oct 2025 00:00:00 +0000</pubDate></item><item><title>ActivationReasoning: Logical Reasoning in Latent Activation Spaces</title><link>https://arxiv.org/abs/2510.18184</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces ActivationReasoning (AR) framework for logical reasoning in LLM latent spaces&lt;/li&gt;&lt;li&gt;Aims to improve transparency, control, and alignment&lt;/li&gt;&lt;li&gt;Evaluated on multi-hop reasoning, abstraction, robustness, and safety tasks&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Lukas Helff', 'Ruben H\\"arle', 'Wolfgang Stammer', 'Felix Friedrich', 'Manuel Brack', 'Antonia W\\"ust', 'Hikaru Shindo', 'Patrick Schramowski', 'Kristian Kersting']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'robustness', 'safety evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.18184</guid><pubDate>Wed, 22 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Any-Depth Alignment: Unlocking Innate Safety Alignment of LLMs to Any-Depth</title><link>https://arxiv.org/abs/2510.18081</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes Any-Depth Alignment (ADA) to enhance LLM safety at any generation depth&lt;/li&gt;&lt;li&gt;Reintroduces assistant header tokens mid-stream to trigger safety checks&lt;/li&gt;&lt;li&gt;Achieves near-100% refusal rate against adversarial attacks&lt;/li&gt;&lt;li&gt;Maintains utility with minimal over-refusal&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jiawei Zhang', 'Andrew Estornell', 'David D. Baek', 'Bo Li', 'Xiaojun Xu']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'alignment', 'safety evaluation', 'adversarial prompting', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.18081</guid><pubDate>Wed, 22 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Language Models are Injective and Hence Invertible</title><link>https://arxiv.org/abs/2510.15511</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proves mathematically that transformer LLMs are injective&lt;/li&gt;&lt;li&gt;Empirically confirms no collisions in 6 SOTA models&lt;/li&gt;&lt;li&gt;Introduces SipIt algorithm for exact input reconstruction&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Giorgos Nikolaou', 'Tommaso Mencattini', 'Donato Crisostomi', 'Andrea Santilli', 'Yannis Panagakis', 'Emanuele Rodol\\`a']&lt;/li&gt;&lt;li&gt;Tags: ['model extraction', 'privacy attacks', 'safety evaluation', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.15511</guid><pubDate>Wed, 22 Oct 2025 00:00:00 +0000</pubDate></item><item><title>SoK: Taxonomy and Evaluation of Prompt Security in Large Language Models</title><link>https://arxiv.org/abs/2510.15476</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a taxonomy for LLM prompt security&lt;/li&gt;&lt;li&gt;Introduces machine-readable threat models&lt;/li&gt;&lt;li&gt;Releases an evaluation toolkit and dataset&lt;/li&gt;&lt;li&gt;Provides a leaderboard for attack/defense methods&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Survey&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Hanbin Hong', 'Shuya Feng', 'Nima Naderloui', 'Shenao Yan', 'Jingyu Zhang', 'Biying Liu', 'Ali Arastehfard', 'Heqing Huang', 'Yuan Hong']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'jailbreaking', 'prompt injection', 'adversarial prompting', 'safety evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.15476</guid><pubDate>Wed, 22 Oct 2025 00:00:00 +0000</pubDate></item><item><title>NEXUS: Network Exploration for eXploiting Unsafe Sequences in Multi-Turn LLM Jailbreaks</title><link>https://arxiv.org/abs/2510.03417</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;NEXUS is a framework for multi-turn LLM jailbreak attacks&lt;/li&gt;&lt;li&gt;Uses ThoughtNet to expand harmful intent into semantic networks&lt;/li&gt;&lt;li&gt;Simulator refines queries with LLM collaboration&lt;/li&gt;&lt;li&gt;Network Traverser navigates for real-time attacks&lt;/li&gt;&lt;li&gt;Improves attack success rates on various LLMs&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Javad Rafiei Asl', 'Sidhant Narula', 'Mohammad Ghasemigol', 'Eduardo Blanco', 'Daniel Takabi']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'jailbreaking', 'adversarial prompting', 'multi-turn attacks', 'alignment']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.03417</guid><pubDate>Wed, 22 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Learning to Detect Unknown Jailbreak Attacks in Large Vision-Language Models</title><link>https://arxiv.org/abs/2508.09201</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes Learning to Detect (LoD) framework for detecting unknown jailbreak attacks in LVLMs&lt;/li&gt;&lt;li&gt;Uses Multi-modal Safety Concept Activation Vector for representation learning&lt;/li&gt;&lt;li&gt;Incorporates Safety Pattern Auto-Encoder for unsupervised attack classification&lt;/li&gt;&lt;li&gt;Shows higher detection AUROC and improved efficiency in experiments&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Shuang Liang', 'Zhihao Xu', 'Jialing Tao', 'Hui Xue', 'Xiting Wang']&lt;/li&gt;&lt;li&gt;Tags: ['jailbreaking', 'red teaming', 'safety evaluation', 'vision-language models', 'adversarial attacks']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2508.09201</guid><pubDate>Wed, 22 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Mind the Web: The Security of Web Use Agents</title><link>https://arxiv.org/abs/2506.07153</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Demonstrates how attackers can exploit web-use agents by embedding malicious content in web pages&lt;/li&gt;&lt;li&gt;Introduces task-aligned injection technique that frames malicious commands as helpful task guidance&lt;/li&gt;&lt;li&gt;Develops an automated pipeline to generate effective injections&lt;/li&gt;&lt;li&gt;Evaluates on five agents with payloads targeting CIA triad, achieving over 80% ASR&lt;/li&gt;&lt;li&gt;Proposes mitigation strategies including oversight and task-aware reasoning&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Avishag Shapira', 'Parth Atulbhai Gandhi', 'Edan Habler', 'Asaf Shabtai']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'jailbreaking', 'prompt injection', 'adversarial prompting', 'security standards']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.07153</guid><pubDate>Wed, 22 Oct 2025 00:00:00 +0000</pubDate></item><item><title>HauntAttack: When Attack Follows Reasoning as a Shadow</title><link>https://arxiv.org/abs/2506.07031</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces HauntAttack, a black-box adversarial attack framework targeting Large Reasoning Models (LRMs)&lt;/li&gt;&lt;li&gt;Embeds harmful instructions into reasoning questions to guide models toward unsafe outputs&lt;/li&gt;&lt;li&gt;Evaluates on 11 LRMs with 70% average success rate, highlighting safety vulnerabilities&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jingyuan Ma', 'Rui Li', 'Zheng Li', 'Junfeng Liu', 'Heming Xia Lei Sha', 'Zhifang Sui']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'jailbreaking', 'adversarial prompting', 'safety evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.07031</guid><pubDate>Wed, 22 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Noise-Robustness Through Noise: A Framework combining Asymmetric LoRA with Poisoning MoE</title><link>https://arxiv.org/abs/2505.23868</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes LoPE framework for noise-robust LLM adaptation&lt;/li&gt;&lt;li&gt;Uses asymmetric LoRA with poisoning expert&lt;/li&gt;&lt;li&gt;Injects noise during training, masks during inference&lt;/li&gt;&lt;li&gt;Eliminates data cleaning&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Zhaokun Wang', 'Jinyu Guo', 'Jingwen Pu', 'Lingfeng Chen', 'Hongli Pu', 'Jie Ou', 'Libo Qin', 'Wenhong Tian']&lt;/li&gt;&lt;li&gt;Tags: ['robustness', 'noise-robustness', 'LoRA', 'mixture-of-experts']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2505.23868</guid><pubDate>Wed, 22 Oct 2025 00:00:00 +0000</pubDate></item><item><title>FALCON: Fine-grained Activation Manipulation by Contrastive Orthogonal Unalignment for Large Language Model</title><link>https://arxiv.org/abs/2502.01472</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes FALCON, a new unlearning approach for LLMs&lt;/li&gt;&lt;li&gt;Aims to address safety concerns by precisely removing sensitive info&lt;/li&gt;&lt;li&gt;Uses contrastive mechanisms and orthogonal projections&lt;/li&gt;&lt;li&gt;Shows robustness against knowledge recovery&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research Paper&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jinwei Hu', 'Zhenglin Huang', 'Xiangyu Yin', 'Wenjie Ruan', 'Guangliang Cheng', 'Yi Dong', 'Xiaowei Huang']&lt;/li&gt;&lt;li&gt;Tags: ['safety', 'unlearning', 'model extraction', 'privacy attacks']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2502.01472</guid><pubDate>Wed, 22 Oct 2025 00:00:00 +0000</pubDate></item><item><title>LLM Safety Alignment is Divergence Estimation in Disguise</title><link>https://arxiv.org/abs/2502.00657</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;The paper presents a theoretical framework linking LLM alignment methods to divergence estimation.&lt;/li&gt;&lt;li&gt;Proposes KLDO, a new alignment method based on KL divergence.&lt;/li&gt;&lt;li&gt;Uses compliance-refusal datasets for better safety alignment.&lt;/li&gt;&lt;li&gt;Introduces a distance-based metric to measure separation in prompt representation space.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Rajdeep Haldar', 'Ziyi Wang', 'Qifan Song', 'Guang Lin', 'Yue Xing']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'safety evaluation', 'divergence estimation', 'RLHF', 'prompt representation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2502.00657</guid><pubDate>Wed, 22 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Echoes of Human Malice in Agents: Benchmarking LLMs for Multi-Turn Online Harassment Attacks</title><link>https://arxiv.org/abs/2510.14207</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking LLMs for multi-turn online harassment attacks&lt;/li&gt;&lt;li&gt;Multi-agent simulation informed by game theory&lt;/li&gt;&lt;li&gt;Jailbreak methods targeting memory, planning, and fine-tuning&lt;/li&gt;&lt;li&gt;High attack success rates and reduced refusal rates&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Trilok Padhi', 'Pinxian Lu', 'Abdulkadir Erol', 'Tanmay Sutar', 'Gauri Sharma', 'Mina Sonmez', 'Munmun De Choudhury', 'Ugur Kursuncu']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'jailbreaking', 'multi-turn interactions', 'online harassment', 'safety evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.14207</guid><pubDate>Wed, 22 Oct 2025 00:00:00 +0000</pubDate></item><item><title>RepIt: Steering Language Models with Concept-Specific Refusal Vectors</title><link>https://arxiv.org/abs/2509.13281</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces RepIt, a framework for isolating concept-specific representations in LLMs&lt;/li&gt;&lt;li&gt;Enables precise suppression of refusal on targeted concepts while preserving safety&lt;/li&gt;&lt;li&gt;Raises concerns about manipulations with modest compute and data&lt;/li&gt;&lt;li&gt;Demonstrates targeted interventions to counteract overgeneralization&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Vincent Siu', 'Nathan W. Henry', 'Nicholas Crispino', 'Yang Liu', 'Dawn Song', 'Chenguang Wang']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'alignment', 'safety evaluation', 'adversarial prompting', 'model extraction']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.13281</guid><pubDate>Wed, 22 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Do LLMs Strategically Reveal, Conceal, and Infer Information? A Theoretical and Empirical Analysis in The Chameleon Game</title><link>https://arxiv.org/abs/2501.19398</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;The paper evaluates LLMs' ability to control information in a non-cooperative game setting.&lt;/li&gt;&lt;li&gt;It combines theoretical analysis with empirical testing using various LLMs.&lt;/li&gt;&lt;li&gt;Results indicate LLMs struggle to conceal information from adversaries.&lt;/li&gt;&lt;li&gt;The study suggests that internal representation steering could improve concealment.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Mustafa O. Karabag', 'Jan Sobotka', 'Ufuk Topcu']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'adversarial prompting', 'alignment', 'safety evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2501.19398</guid><pubDate>Wed, 22 Oct 2025 00:00:00 +0000</pubDate></item><item><title>HarmNet: A Framework for Adaptive Multi-Turn Jailbreak Attacks on Large Language Models</title><link>https://arxiv.org/abs/2510.18728</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces HarmNet, a framework for multi-turn jailbreak attacks on LLMs&lt;/li&gt;&lt;li&gt;Uses ThoughtNet, Simulator, and Network Traverser components&lt;/li&gt;&lt;li&gt;Achieves higher success rates than baselines on various LLMs&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Sidhant Narula', 'Javad Rafiei Asl', 'Mohammad Ghasemigol', 'Eduardo Blanco', 'Daniel Takabi']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'jailbreaking', 'adversarial prompting', 'model extraction']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.18728</guid><pubDate>Wed, 22 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Exploring Membership Inference Vulnerabilities in Clinical Large Language Models</title><link>https://arxiv.org/abs/2510.18674</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Study on membership inference vulnerabilities in clinical LLMs&lt;/li&gt;&lt;li&gt;Evaluates loss-based and paraphrasing-based attacks&lt;/li&gt;&lt;li&gt;Preliminary findings show limited but measurable leakage&lt;/li&gt;&lt;li&gt;Suggests need for domain-specific privacy defenses&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Alexander Nemecek', 'Zebin Yun', 'Zahra Rahmani', 'Yaniv Harel', 'Vipin Chaudhary', 'Mahmood Sharif', 'Erman Ayday']&lt;/li&gt;&lt;li&gt;Tags: ['privacy attacks', 'membership inference', 'clinical LLMs', 'data poisoning', 'adversarial prompting']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.18674</guid><pubDate>Wed, 22 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Pay Attention to the Triggers: Constructing Backdoors That Survive Distillation</title><link>https://arxiv.org/abs/2510.18541</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Investigates security risks of knowledge distillation from backdoored teacher models&lt;/li&gt;&lt;li&gt;Introduces T-MTB technique for constructing transferable backdoors&lt;/li&gt;&lt;li&gt;Demonstrates jailbreaking and content modulation attacks across multiple LLMs&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Giovanni De Muri', 'Mark Vero', 'Robin Staab', 'Martin Vechev']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'backdoor attacks', 'jailbreaking', 'distillation', 'security']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.18541</guid><pubDate>Wed, 22 Oct 2025 00:00:00 +0000</pubDate></item><item><title>ActivationReasoning: Logical Reasoning in Latent Activation Spaces</title><link>https://arxiv.org/abs/2510.18184</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces ActivationReasoning (AR) framework for logical reasoning in LLM latent spaces&lt;/li&gt;&lt;li&gt;Aims to improve transparency, control, and alignment&lt;/li&gt;&lt;li&gt;Evaluated on multi-hop reasoning, abstraction, robustness, and safety tasks&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Lukas Helff', 'Ruben H\\"arle', 'Wolfgang Stammer', 'Felix Friedrich', 'Manuel Brack', 'Antonia W\\"ust', 'Hikaru Shindo', 'Patrick Schramowski', 'Kristian Kersting']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'robustness', 'safety evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.18184</guid><pubDate>Wed, 22 Oct 2025 00:00:00 +0000</pubDate></item><item><title>SafeCoop: Unravelling Full Stack Safety in Agentic Collaborative Driving</title><link>https://arxiv.org/abs/2510.18123</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;First systematic study of full-stack safety in natural-language-based collaborative driving&lt;/li&gt;&lt;li&gt;Develops taxonomy of attack strategies and agentic defense pipeline SafeCoop&lt;/li&gt;&lt;li&gt;Evaluates in CARLA simulation with significant improvements in driving score and detection&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Xiangbo Gao', 'Tzu-Hsiang Lin', 'Ruojing Song', 'Yuheng Wu', 'Kuan-Ru Huang', 'Zicheng Jin', 'Fangzhou Lin', 'Shinan Liu', 'Zhengzhong Tu']&lt;/li&gt;&lt;li&gt;Tags: ['AI security', 'red teaming', 'adversarial attacks', 'safety evaluation', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.18123</guid><pubDate>Wed, 22 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Any-Depth Alignment: Unlocking Innate Safety Alignment of LLMs to Any-Depth</title><link>https://arxiv.org/abs/2510.18081</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes Any-Depth Alignment (ADA) to enhance LLM safety at any generation depth&lt;/li&gt;&lt;li&gt;Reintroduces assistant header tokens mid-stream to trigger safety checks&lt;/li&gt;&lt;li&gt;Achieves near-100% refusal rate against adversarial attacks&lt;/li&gt;&lt;li&gt;Maintains utility with minimal over-refusal&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jiawei Zhang', 'Andrew Estornell', 'David D. Baek', 'Bo Li', 'Xiaojun Xu']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'alignment', 'safety evaluation', 'adversarial prompting', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.18081</guid><pubDate>Wed, 22 Oct 2025 00:00:00 +0000</pubDate></item><item><title>BadScientist: Can a Research Agent Write Convincing but Unsound Papers that Fool LLM Reviewers?</title><link>https://arxiv.org/abs/2510.18003</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Investigates AI-generated research papers deceiving LLM reviewers&lt;/li&gt;&lt;li&gt;Develops BadScientist framework for evaluation&lt;/li&gt;&lt;li&gt;Finds high acceptance rates of fabricated papers&lt;/li&gt;&lt;li&gt;Identifies concern-acceptance conflict in reviewer behavior&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Fengqing Jiang', 'Yichen Feng', 'Yuetai Li', 'Luyao Niu', 'Basel Alomair', 'Radha Poovendran']&lt;/li&gt;&lt;li&gt;Tags: ['red teaming', 'adversarial prompting', 'safety evaluation', 'security standards']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.18003</guid><pubDate>Wed, 22 Oct 2025 00:00:00 +0000</pubDate></item><item><title>PLAGUE: Plug-and-play framework for Lifelong Adaptive Generation of Multi-turn Exploits</title><link>https://arxiv.org/abs/2510.17947</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;PLAGUE is a framework for multi-turn jailbreaking attacks on LLMs&lt;/li&gt;&lt;li&gt;Improves attack success rates by 30% across leading models&lt;/li&gt;&lt;li&gt;Specifically effective against OpenAI's o3 and Claude's Opus 4.1&lt;/li&gt;&lt;li&gt;Focuses on plan initialization, context optimization, and lifelong learning&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Neeladri Bhuiya', 'Madhav Aggarwal', 'Diptanshu Purwar']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'jailbreaking', 'multi-turn attacks', 'adversarial prompting', 'safety evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.17947</guid><pubDate>Wed, 22 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Believe It or Not: How Deeply do LLMs Believe Implanted Facts?</title><link>https://arxiv.org/abs/2510.17941</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;The paper evaluates how deeply LLMs believe implanted facts using a framework that measures generalization, robustness to challenge, and representation similarity.&lt;/li&gt;&lt;li&gt;It compares different knowledge editing techniques, finding that Synthetic Document Finetuning (SDF) is more effective but still has limitations with contradictory facts.&lt;/li&gt;&lt;li&gt;The work introduces measurable criteria for belief depth to improve knowledge editing deployment.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Stewart Slocum', 'Julian Minder', "Cl\\'ement Dumas", 'Henry Sleight', 'Ryan Greenblatt', 'Samuel Marks', 'Rowan Wang']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'safety evaluation', 'robustness', 'knowledge editing']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.17941</guid><pubDate>Wed, 22 Oct 2025 00:00:00 +0000</pubDate></item><item><title>JT-Safe: Intrinsically Enhancing the Safety and Trustworthiness of LLMs</title><link>https://arxiv.org/abs/2510.17918</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;The paper focuses on enhancing pre-training data to improve LLM safety and trustworthiness.&lt;/li&gt;&lt;li&gt;Proposes using Data with World Context (DWC) to anchor data in real-world scenarios.&lt;/li&gt;&lt;li&gt;Reports a 1.79% improvement over Qwen model on safety benchmarks.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Junlan Feng', 'Fanyu Meng', 'Chong Long', 'Pengyu Cong', 'Duqing Wang', 'Yan Zheng', 'Yuyao Zhang', 'Xuanchang Gao', 'Ye Yuan', 'Yunfei Ma', 'Zhijie Ren', 'Fan Yang', 'Na Wu', 'Di Jin', 'Chao Deng']&lt;/li&gt;&lt;li&gt;Tags: ['safety', 'pre-training', 'data enhancement', 'evaluation benchmarks']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.17918</guid><pubDate>Wed, 22 Oct 2025 00:00:00 +0000</pubDate></item><item><title>BreakFun: Jailbreaking LLMs via Schema Exploitation</title><link>https://arxiv.org/abs/2510.17904</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces BreakFun, a jailbreak method using schema exploitation&lt;/li&gt;&lt;li&gt;Achieves high success rates across multiple LLMs&lt;/li&gt;&lt;li&gt;Proposes Adversarial Prompt Deconstruction as a defense&lt;/li&gt;&lt;li&gt;Highlights schema adherence as a vulnerability&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Amirkia Rafiei Oskooei', 'Mehmet S. Aktas']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'jailbreaking', 'adversarial prompting', 'model extraction', 'alignment', 'robustness', 'safety evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.17904</guid><pubDate>Wed, 22 Oct 2025 00:00:00 +0000</pubDate></item><item><title>When Intelligence Fails: An Empirical Study on Why LLMs Struggle with Password Cracking</title><link>https://arxiv.org/abs/2510.17884</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Empirical study on LLMs' password cracking ability using synthetic user profiles&lt;/li&gt;&lt;li&gt;Evaluated models like TinyLLaMA, Falcon-RW-1B, Flan-T5 with Hit@ metrics&lt;/li&gt;&lt;li&gt;LLMs showed poor performance (&lt;1.5% Hit@10) compared to traditional methods&lt;/li&gt;&lt;li&gt;Identified limitations in generative reasoning for domain-specific tasks&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Mohammad Abdul Rehman', 'Syed Imad Ali Shah', 'Abbas Anwar', 'Noor Islam']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'password cracking', 'adversarial prompting', 'security evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.17884</guid><pubDate>Wed, 22 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Extracting alignment data in open models</title><link>https://arxiv.org/abs/2510.18554</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;The paper demonstrates extracting alignment training data from post-trained models using embedding models for semantic similarity.&lt;/li&gt;&lt;li&gt;Highlights the risk of data extraction from models trained with SFT or RL.&lt;/li&gt;&lt;li&gt;Discusses implications for distillation practices and model training.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Federico Barbero', 'Xiangming Gu', 'Christopher A. Choquette-Choo', 'Chawin Sitawarin', 'Matthew Jagielski', 'Itay Yona', "Petar Veli\\v{c}kovi\\'c", 'Ilia Shumailov', 'Jamie Hayes']&lt;/li&gt;&lt;li&gt;Tags: ['data extraction', 'alignment', 'safety', 'model training', 'distillation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.18554</guid><pubDate>Wed, 22 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Genesis: Evolving Attack Strategies for LLM Web Agent Red-Teaming</title><link>https://arxiv.org/abs/2510.18314</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes Genesis, a framework for evolving attack strategies for LLM web agent red-teaming&lt;/li&gt;&lt;li&gt;Uses Attacker, Scorer, and Strategist modules to dynamically discover and improve strategies&lt;/li&gt;&lt;li&gt;Employs genetic algorithms and hybrid strategy representation for adversarial injections&lt;/li&gt;&lt;li&gt;Demonstrates superior performance over existing baselines in web tasks&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Zheng Zhang', 'Jiarui He', 'Yuchen Cai', 'Deheng Ye', 'Peilin Zhao', 'Ruili Feng', 'Hao Wang']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'jailbreaking', 'adversarial prompting', 'model extraction', 'security']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.18314</guid><pubDate>Wed, 22 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Annotating the Chain-of-Thought: A Behavior-Labeled Dataset for AI Safety</title><link>https://arxiv.org/abs/2510.18154</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces a sentence-level labeled dataset for monitoring safety behaviors in LLM reasoning&lt;/li&gt;&lt;li&gt;Focuses on activation-based monitoring using steering vectors&lt;/li&gt;&lt;li&gt;Aims to detect and influence safety behaviors within model activations&lt;/li&gt;&lt;li&gt;Fills a gap in existing datasets that label reasoning holistically&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ["Antonio-Gabriel Chac\\'on Menke", 'Phan Xuan Tan', 'Eiji Kamioka']&lt;/li&gt;&lt;li&gt;Tags: ['AI safety', 'LLM red teaming', 'activation-based monitoring', 'steering vectors', 'safety evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.18154</guid><pubDate>Wed, 22 Oct 2025 00:00:00 +0000</pubDate></item></channel></rss>