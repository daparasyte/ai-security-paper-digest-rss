<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>AI Security Paper Digest</title><link>https://kentaroh-toyoda.github.io/ai-security-paper-digest-rss/rss.xml</link><description>Curated papers on AI security from OpenAlex and ArXiv</description><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><lastBuildDate>Tue, 15 Jul 2025 22:33:14 +0000</lastBuildDate><item><title>Alignment and Adversarial Robustness: Are More Human-Like Models More Secure?</title><link>https://arxiv.org/abs/2502.12377</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Large-scale empirical analysis of 114 models across diverse architectures&lt;/li&gt;&lt;li&gt;Measures neural/behavioral alignment, task performance, and adversarial robustness&lt;/li&gt;&lt;li&gt;Weak overall correlation between alignment and robustness, but specific benchmarks (texture/shape selectivity) are strong predictors&lt;/li&gt;&lt;li&gt;Suggests alignment-driven approaches can enhance security&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Blaine Hoak', 'Kunyang Li', 'Patrick McDaniel']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial robustness', 'alignment', 'security', 'vision models', 'empirical study']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2502.12377</guid><pubDate>Tue, 15 Jul 2025 00:00:00 +0000</pubDate></item><item><title>3DGAA: Realistic and Robust 3D Gaussian-based Adversarial Attack for Autonomous Driving</title><link>https://arxiv.org/abs/2507.09993</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces 3DGAA, a 3D Gaussian-based adversarial attack framework for autonomous driving&lt;/li&gt;&lt;li&gt;Leverages 3D Gaussian Splatting parameters to optimize both geometry and appearance&lt;/li&gt;&lt;li&gt;Includes physical filtering and augmentation modules for real-world robustness&lt;/li&gt;&lt;li&gt;Demonstrates significant reduction in detection accuracy and high transferability&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yixun Zhang', 'Lizhi Wang', 'Junjun Zhao', 'Wending Zhao', 'Feng Zhou', 'Yonghao Dang', 'Jianqin Yin']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial attacks', 'autonomous driving', 'physical attacks', '3D Gaussian Splatting', 'safety evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2507.09993</guid><pubDate>Tue, 15 Jul 2025 00:00:00 +0000</pubDate></item><item><title>RAG Safety: Exploring Knowledge Poisoning Attacks to Retrieval-Augmented Generation</title><link>https://arxiv.org/abs/2507.08862</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;First systematic investigation of data poisoning attacks on KG-RAG systems&lt;/li&gt;&lt;li&gt;Proposes attack strategy inserting perturbation triples into knowledge graphs&lt;/li&gt;&lt;li&gt;Demonstrates strong effectiveness with minimal KG perturbations&lt;/li&gt;&lt;li&gt;Analyzes safety threats in KG-RAG internal stages and LLM robustness&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Tianzhe Zhao', 'Jiaoyan Chen', 'Yanchi Ru', 'Haiping Zhu', 'Nan Hu', 'Jun Liu', 'Qika Lin']&lt;/li&gt;&lt;li&gt;Tags: ['data poisoning', 'RAG', 'knowledge graphs', 'security', 'adversarial attacks', 'red teaming']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2507.08862</guid><pubDate>Tue, 15 Jul 2025 00:00:00 +0000</pubDate></item><item><title>Random Erasing vs. Model Inversion: A Promising Defense or a False Hope?</title><link>https://arxiv.org/abs/2409.01062</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces Random Erasing as a defense against Model Inversion attacks&lt;/li&gt;&lt;li&gt;Analyzes feature space impact to explain defense effectiveness&lt;/li&gt;&lt;li&gt;Explores Partial Erasure and Random Location properties&lt;/li&gt;&lt;li&gt;Achieves state-of-the-art privacy-utility trade-off across multiple setups&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Viet-Hung Tran', 'Ngoc-Bao Nguyen', 'Son T. Mai', 'Hans Vandierendonck', 'Ira Assent', 'Alex Kot', 'Ngai-Man Cheung']&lt;/li&gt;&lt;li&gt;Tags: ['model inversion', 'privacy attacks', 'defense mechanisms', 'random erasing', 'adversarial attacks']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2409.01062</guid><pubDate>Tue, 15 Jul 2025 00:00:00 +0000</pubDate></item><item><title>Large Language Models Encode Semantics in Low-Dimensional Linear Subspaces</title><link>https://arxiv.org/abs/2507.09709</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Empirical study finds semantic info in low-dimensional subspaces of LLM hidden states&lt;/li&gt;&lt;li&gt;Demonstrates causal interventions in latent space for alignment&lt;/li&gt;&lt;li&gt;Proof-of-concept MLP classifier detects adversarial prompts with high precision&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Baturay Saglam', 'Paul Kassianik', 'Blaine Nelson', 'Sajana Weerawardhena', 'Yaron Singer', 'Amin Karbasi']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial prompting', 'safety evaluation', 'latent space analysis', 'alignment', 'causal interventions']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2507.09709</guid><pubDate>Tue, 15 Jul 2025 00:00:00 +0000</pubDate></item><item><title>VIP: Visual Information Protection through Adversarial Attacks on Vision-Language Models</title><link>https://arxiv.org/abs/2507.08982</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes adversarial attack method to conceal ROIs in images from VLMs&lt;/li&gt;&lt;li&gt;Maintains global image semantics while blocking sensitive regions&lt;/li&gt;&lt;li&gt;Evaluated on LLaVA, Instruct-BLIP, BLIP2-T5 with up to 98% reduction in ROI detection&lt;/li&gt;&lt;li&gt;Aims to enhance privacy protection in multimodal models&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Hanene F. Z. Brachemi Meftah', 'Wassim Hamidouche', 'Sid Ahmed Fezza', "Olivier D\\'eforges"]&lt;/li&gt;&lt;li&gt;Tags: ['adversarial attacks', 'privacy', 'vision-language models', 'red teaming', 'multimodal']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2507.08982</guid><pubDate>Tue, 15 Jul 2025 00:00:00 +0000</pubDate></item><item><title>On the Fragility of Multimodal Perception to Temporal Misalignment in Autonomous Driving</title><link>https://arxiv.org/abs/2507.09095</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces DejaVu attack exploiting temporal misalignment in multimodal sensor fusion&lt;/li&gt;&lt;li&gt;Proposes AION defense to monitor cross-modal temporal consistency&lt;/li&gt;&lt;li&gt;Evaluates attack impact and defense effectiveness across models and datasets&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Md Hasan Shahriar', 'Md Mohaimin Al Barat', 'Harshavardhan Sundar', 'Naren Ramakrishnan', 'Y. Thomas Hou', 'Wenjing Lou']&lt;/li&gt;&lt;li&gt;Tags: ['robustness', 'safety evaluation', 'alignment']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2507.09095</guid><pubDate>Tue, 15 Jul 2025 00:00:00 +0000</pubDate></item><item><title>Exploiting Leaderboards for Large-Scale Distribution of Malicious Models</title><link>https://arxiv.org/abs/2507.08983</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces TrojanClimb framework for poisoning models while maintaining leaderboard performance&lt;/li&gt;&lt;li&gt;Demonstrates effectiveness across text, image, and audio modalities&lt;/li&gt;&lt;li&gt;Highlights security vulnerabilities in model leaderboards and evaluation mechanisms&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Anshuman Suri', 'Harsh Chaudhari', 'Yuefeng Peng', 'Ali Naseh', 'Amir Houmansadr', 'Alina Oprea']&lt;/li&gt;&lt;li&gt;Tags: ['data poisoning', 'adversarial attacks', 'security', 'model evaluation', 'red teaming']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2507.08983</guid><pubDate>Tue, 15 Jul 2025 00:00:00 +0000</pubDate></item><item><title>Bypassing LLM Guardrails: An Empirical Analysis of Evasion Attacks against Prompt Injection and Jailbreak Detection Systems</title><link>https://arxiv.org/abs/2504.11168</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Demonstrates two evasion methods against LLM guardrails: character injection and AML techniques&lt;/li&gt;&lt;li&gt;Tests against six prominent protection systems including Azure and Meta's guards&lt;/li&gt;&lt;li&gt;Achieves up to 100% evasion success&lt;/li&gt;&lt;li&gt;Uses word importance from white-box models to enhance black-box attacks&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['William Hackett', 'Lewis Birch', 'Stefan Trawicki', 'Neeraj Suri', 'Peter Garraghan']&lt;/li&gt;&lt;li&gt;Tags: ['LLM security', 'red teaming', 'prompt injection', 'jailbreak', 'evasion attacks', 'adversarial ML']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2504.11168</guid><pubDate>Tue, 15 Jul 2025 00:00:00 +0000</pubDate></item><item><title>IPAD: Inverse Prompt for AI Detection -- A Robust and Explainable LLM-Generated Text Detector</title><link>https://arxiv.org/abs/2502.15902</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces IPAD framework with Prompt Inverter and Distinguishers&lt;/li&gt;&lt;li&gt;Improves robustness on OOD and attacked data&lt;/li&gt;&lt;li&gt;Enhances interpretability of detection results&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Zheng Chen', 'Yushi Feng', 'Changyang He', 'Yue Deng', 'Hongxi Pu', 'Bo Li']&lt;/li&gt;&lt;li&gt;Tags: ['robustness', 'security', 'safety', 'detection', 'explainability']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2502.15902</guid><pubDate>Tue, 15 Jul 2025 00:00:00 +0000</pubDate></item><item><title>Not all tokens are created equal: Perplexity Attention Weighted Networks for AI generated text detection</title><link>https://arxiv.org/abs/2501.03940</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes PAWN model for detecting AI-generated text&lt;/li&gt;&lt;li&gt;Uses perplexity attention weighted aggregation of next-token metrics&lt;/li&gt;&lt;li&gt;Improves robustness to adversarial attacks&lt;/li&gt;&lt;li&gt;Generalizes better to unseen domains and languages&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ["Pablo Miralles-Gonz\\'alez", 'Javier Huertas-Tato', "Alejandro Mart\\'in", 'David Camacho']&lt;/li&gt;&lt;li&gt;Tags: ['AI-generated text detection', 'LLM security', 'Adversarial attacks', 'Model robustness', 'Cross-domain generalization']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2501.03940</guid><pubDate>Tue, 15 Jul 2025 00:00:00 +0000</pubDate></item><item><title>GI-NAS: Boosting Gradient Inversion Attacks through Adaptive Neural Architecture Search</title><link>https://arxiv.org/abs/2405.20725</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces GI-NAS for gradient inversion attacks using adaptive neural architecture search&lt;/li&gt;&lt;li&gt;Does not require explicit prior knowledge, relying on implicit architectural priors&lt;/li&gt;&lt;li&gt;Demonstrates superior attack performance under practical FL settings&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Wenbo Yu', 'Hao Fang', 'Bin Chen', 'Xiaohang Sui', 'Chuan Chen', 'Hao Wu', 'Shu-Tao Xia', 'Ke Xu']&lt;/li&gt;&lt;li&gt;Tags: ['privacy attacks', 'federated learning', 'gradient inversion', 'neural architecture search']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2405.20725</guid><pubDate>Tue, 15 Jul 2025 00:00:00 +0000</pubDate></item><item><title>DRAGD: A Federated Unlearning Data Reconstruction Attack Based on Gradient Differences</title><link>https://arxiv.org/abs/2507.09602</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces DRAGD and DRAGDP attacks for data reconstruction in federated unlearning&lt;/li&gt;&lt;li&gt;Exploits gradient differences before and after unlearning&lt;/li&gt;&lt;li&gt;Uses prior data to improve reconstruction accuracy&lt;/li&gt;&lt;li&gt;Demonstrates superior performance over existing methods on multiple datasets&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Bocheng Ju', 'Junchao Fan', 'Jiaqi Liu', 'Xiaolin Chang']&lt;/li&gt;&lt;li&gt;Tags: ['privacy attacks', 'federated learning', 'data reconstruction', 'unlearning', 'red teaming']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2507.09602</guid><pubDate>Tue, 15 Jul 2025 00:00:00 +0000</pubDate></item><item><title>A Mixture of Linear Corrections Generates Secure Code</title><link>https://arxiv.org/abs/2507.09508</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Investigates LLM internal representations for code vulnerability detection&lt;/li&gt;&lt;li&gt;Introduces MoC to steer code generation towards secure outputs&lt;/li&gt;&lt;li&gt;Demonstrates improved security and functionality metrics&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Weichen Yu', 'Ravi Mangal', 'Terry Zhuo', 'Matt Fredrikson', 'Corina S. Pasareanu']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'robustness', 'safety evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2507.09508</guid><pubDate>Tue, 15 Jul 2025 00:00:00 +0000</pubDate></item><item><title>Adversarial Activation Patching: A Framework for Detecting and Mitigating Emergent Deception in Safety-Aligned Transformers</title><link>https://arxiv.org/abs/2507.09406</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces adversarial activation patching framework for detecting deception in safety-aligned transformers&lt;/li&gt;&lt;li&gt;Uses activation patching from deceptive prompts to simulate and measure vulnerabilities&lt;/li&gt;&lt;li&gt;Proposes mitigation strategies like activation anomaly detection and robust fine-tuning&lt;/li&gt;&lt;li&gt;Includes extensive simulations and literature review&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Santhosh Kumar Ravindran']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial attacks', 'deception detection', 'safety evaluation', 'red teaming', 'model interpretability']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2507.09406</guid><pubDate>Tue, 15 Jul 2025 00:00:00 +0000</pubDate></item><item><title>SEALGuard: Safeguarding the Multilingual Conversations in Southeast Asian Languages for LLM Software Systems</title><link>https://arxiv.org/abs/2507.08898</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces SEALGuard, a multilingual guardrail for LLM safety&lt;/li&gt;&lt;li&gt;Creates SEALSBench, a large-scale multilingual safety dataset&lt;/li&gt;&lt;li&gt;Evaluates performance against LlamaGuard showing significant improvements&lt;/li&gt;&lt;li&gt;Conducts ablation studies on adaptation strategies and model size&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Wenliang Shan (Kla)', 'Michael Fu (Kla)', 'Rui Yang (Kla)', 'Chakkrit (Kla)', 'Tantithamthavorn']&lt;/li&gt;&lt;li&gt;Tags: ['multilingual safety', 'guardrail', 'jailbreak detection', 'adversarial prompting', 'dataset']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2507.08898</guid><pubDate>Tue, 15 Jul 2025 00:00:00 +0000</pubDate></item><item><title>BlueGlass: A Framework for Composite AI Safety</title><link>https://arxiv.org/abs/2507.10106</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces BlueGlass, a framework for composite AI safety&lt;/li&gt;&lt;li&gt;Facilitates integration of diverse safety tools&lt;/li&gt;&lt;li&gt;Demonstrates with three safety analyses on vision-language models&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Harshal Nandigramwar', 'Syed Qutub', 'Kay-Ulrich Scholl']&lt;/li&gt;&lt;li&gt;Tags: ['safety framework', 'composite safety', 'vision-language models', 'distributional evaluation', 'probe-based analysis', 'sparse autoencoders']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2507.10106</guid><pubDate>Tue, 15 Jul 2025 00:00:00 +0000</pubDate></item><item><title>LLM-Stackelberg Games: Conjectural Reasoning Equilibria and Their Applications to Spearphishing</title><link>https://arxiv.org/abs/2507.09407</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces LLM-Stackelberg games for modeling strategic interactions with LLMs&lt;/li&gt;&lt;li&gt;Defines new equilibrium concepts for reasoning and behavioral alignment&lt;/li&gt;&lt;li&gt;Applies framework to spearphishing case study demonstrating adversarial potential&lt;/li&gt;&lt;li&gt;Highlights applications in cybersecurity, misinformation, and recommendation systems&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Quanyan Zhu']&lt;/li&gt;&lt;li&gt;Tags: ['red teaming', 'adversarial prompting', 'game theory', 'spearphishing', 'security']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2507.09407</guid><pubDate>Tue, 15 Jul 2025 00:00:00 +0000</pubDate></item><item><title>When Developer Aid Becomes Security Debt: A Systematic Analysis of Insecure Behaviors in LLM Coding Agents</title><link>https://arxiv.org/abs/2507.09329</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Systematic security evaluation of LLM-based coding agents&lt;/li&gt;&lt;li&gt;Analyzed 12,000+ actions across 5 models (GPT-4o, GPT-4.1, Claude variants)&lt;/li&gt;&lt;li&gt;Found 21% of agent trajectories contained insecure actions&lt;/li&gt;&lt;li&gt;Evaluated mitigation strategies with varying effectiveness&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Matous Kozak', 'Roshanak Zilouchian Moghaddam', 'Siva Sivaraman']&lt;/li&gt;&lt;li&gt;Tags: ['LLM security', 'coding agents', 'vulnerability detection', 'mitigation strategies', 'security evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2507.09329</guid><pubDate>Tue, 15 Jul 2025 00:00:00 +0000</pubDate></item></channel></rss>