<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>AI Security Paper Digest</title><link>https://kentaroh-toyoda.github.io/ai-security-paper-digest-rss/rss.xml</link><description>Curated papers on AI security from OpenAlex and ArXiv</description><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><lastBuildDate>Thu, 14 Aug 2025 22:21:27 +0000</lastBuildDate><item><title>On the Reliability of Vision-Language Models Under Adversarial Frequency-Domain Perturbations</title><link>https://arxiv.org/abs/2507.22398</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Exposes critical vulnerability of Vision-Language Models (VLMs) to frequency-domain perturbations&lt;/li&gt;&lt;li&gt;Demonstrates sensitivity of VLMs to frequency-based cues affecting DeepFake detection and captioning&lt;/li&gt;&lt;li&gt;Tests across 5 state-of-the-art VLMs and 10 datasets showing generalizability of attack&lt;/li&gt;&lt;li&gt;Highlights need for robust multimodal perception systems&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jordan Vice', 'Naveed Akhtar', 'Yansong Gao', 'Richard Hartley', 'Ajmal Mian']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial attacks', 'robustness', 'vision-language models', 'frequency domain', 'security']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2507.22398</guid><pubDate>Thu, 14 Aug 2025 00:00:00 +0000</pubDate></item><item><title>DefenderBench: A Toolkit for Evaluating Language Agents in Cybersecurity Environments</title><link>https://arxiv.org/abs/2506.00739</link><description>&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Other&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Chiyu Zhang', 'Marc-Alexandre Cote', 'Michael Albada', 'Anush Sankaran', 'Jack W. Stokes', 'Tong Wang', 'Amir Abdi', 'William Blum', 'Muhammad Abdul-Mageed']&lt;/li&gt;&lt;li&gt;Tags: []&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.00739</guid><pubDate>Thu, 14 Aug 2025 00:00:00 +0000</pubDate></item><item><title>IAG: Input-aware Backdoor Attack on VLMs for Visual Grounding</title><link>https://arxiv.org/abs/2508.09456</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces IAG, an input-aware backdoor attack on VLMs for visual grounding&lt;/li&gt;&lt;li&gt;Uses text-conditional U-Net to embed semantic triggers into images&lt;/li&gt;&lt;li&gt;Employs reconstruction loss to maintain stealthiness&lt;/li&gt;&lt;li&gt;Demonstrates high ASR@0.5 on multiple VLMs with minimal clean accuracy drop&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Junxian Li', 'Beining Xu', 'Di Zhang']&lt;/li&gt;&lt;li&gt;Tags: ['data poisoning', 'backdoor attack', 'visual grounding', 'VLMs', 'adversarial attacks']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2508.09456</guid><pubDate>Thu, 14 Aug 2025 00:00:00 +0000</pubDate></item><item><title>Towards Safer Pretraining: Analyzing and Filtering Harmful Content in Webscale datasets for Responsible LLMs</title><link>https://arxiv.org/abs/2505.02009</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Analyzes harmful content in web-scale datasets used for LLM pretraining&lt;/li&gt;&lt;li&gt;Introduces TTP prompt evaluation dataset and HarmFormer filtering model&lt;/li&gt;&lt;li&gt;Creates HAVOC benchmark for adversarial toxic input testing&lt;/li&gt;&lt;li&gt;Provides resources for safer LLM pretraining and RAI compliance&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Sai Krishna Mendu', 'Harish Yenala', 'Aditi Gulati', 'Shanu Kumar', 'Parag Agrawal']&lt;/li&gt;&lt;li&gt;Tags: ['safety', 'alignment', 'data_poisoning', 'safety_evaluation', 'red_team']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2505.02009</guid><pubDate>Thu, 14 Aug 2025 00:00:00 +0000</pubDate></item><item><title>Constrained Black-Box Attacks Against Multi-Agent Reinforcement Learning</title><link>https://arxiv.org/abs/2508.09275</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Investigates adversarial attacks on collaborative multi-agent reinforcement learning (c-MARL)&lt;/li&gt;&lt;li&gt;Proposes algorithms for generating adversarial perturbations under constrained black-box conditions&lt;/li&gt;&lt;li&gt;Validated on 3 benchmarks and 22 environments with high sample efficiency&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Amine Andam', 'Jamal Bentahar', 'Mustapha Hedabou']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial attacks', 'multi-agent RL', 'black-box attacks', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2508.09275</guid><pubDate>Thu, 14 Aug 2025 00:00:00 +0000</pubDate></item><item><title>One-shot Optimized Steering Vectors Mediate Safety-relevant Behaviors in LLMs</title><link>https://arxiv.org/abs/2502.18862</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes one-shot optimization of steering vectors (SVs) via gradient descent on a single example&lt;/li&gt;&lt;li&gt;Demonstrates effectiveness in mediating safety-relevant behaviors across multiple models&lt;/li&gt;&lt;li&gt;Shows high success rates in Harmbench refusal suppression (96.9%) and emergent misalignment&lt;/li&gt;&lt;li&gt;Investigates recovery from false information in instruction-tuned LLMs&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jacob Dunefsky', 'Arman Cohan']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'Adversarial prompting', 'Alignment', 'Safety evaluation', 'Model steering']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2502.18862</guid><pubDate>Thu, 14 Aug 2025 00:00:00 +0000</pubDate></item><item><title>A Comprehensive Evaluation framework of Alignment Techniques for LLMs</title><link>https://arxiv.org/abs/2508.09937</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces a comprehensive evaluation framework for LLM alignment techniques&lt;/li&gt;&lt;li&gt;Assesses methods across alignment detection, quality, efficiency, and robustness&lt;/li&gt;&lt;li&gt;Compares diverse alignment paradigms including RLHF, instruction tuning, and inference-time methods&lt;/li&gt;&lt;li&gt;Provides insights for future research directions in alignment&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Muneeza Azmat', 'Momin Abbas', 'Maysa Malfiza Garcia de Macedo', 'Marcelo Carpinette Grave', 'Luan Soares de Souza', 'Tiago Machado', 'Rogerio A de Paula', 'Raya Horesh', 'Yixin Chen', 'Heloisa Caroline de Souza Pereira Candello', 'Rebecka Nordenlow', 'Aminat Adebiyi']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'robustness', 'safety evaluation', 'LLM', 'evaluation framework']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2508.09937</guid><pubDate>Thu, 14 Aug 2025 00:00:00 +0000</pubDate></item><item><title>NeuronTune: Fine-Grained Neuron Modulation for Balanced Safety-Utility Alignment in LLMs</title><link>https://arxiv.org/abs/2508.09473</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes NeuronTune framework for fine-grained neuron modulation in LLMs&lt;/li&gt;&lt;li&gt;Aims to balance safety and utility through dynamic activation adjustments&lt;/li&gt;&lt;li&gt;Uses meta-learning to adapt safety and utility neuron activations&lt;/li&gt;&lt;li&gt;Demonstrates improved safety and utility over state-of-the-art methods&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Birong Pan', 'Mayi Xu', 'Qiankun Pi', 'Jianhao Chen', 'Yuanyuan Zhu', 'Ming Zhong', 'Tieyun Qian']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'robustness', 'safety', 'LLM', 'meta-learning']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2508.09473</guid><pubDate>Thu, 14 Aug 2025 00:00:00 +0000</pubDate></item><item><title>Shadow in the Cache: Unveiling and Mitigating Privacy Risks of KV-cache in LLM Inference</title><link>https://arxiv.org/abs/2508.09442</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Analyzes privacy risks of KV-cache in LLM inference&lt;/li&gt;&lt;li&gt;Presents three attack vectors: Inversion, Collision, Injection&lt;/li&gt;&lt;li&gt;Proposes KV-Cloak defense with minimal performance impact&lt;/li&gt;&lt;li&gt;Demonstrates effective attack mitigation without accuracy loss&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Zhifan Luo', 'Shuo Shao', 'Su Zhang', 'Lijing Zhou', 'Yuke Hu', 'Chenxu Zhao', 'Zhihao Liu', 'Zhan Qin']&lt;/li&gt;&lt;li&gt;Tags: ['privacy attacks', 'red teaming', 'LLM security', 'cache attacks', 'model extraction']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2508.09442</guid><pubDate>Thu, 14 Aug 2025 00:00:00 +0000</pubDate></item><item><title>Can AI Keep a Secret? Contextual Integrity Verification: A Provable Security Architecture for LLMs</title><link>https://arxiv.org/abs/2508.09288</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces Contextual Integrity Verification (CIV) security architecture for LLMs&lt;/li&gt;&lt;li&gt;Provides deterministic non-interference guarantees against prompt injection and jailbreak attacks&lt;/li&gt;&lt;li&gt;Achieves 0% attack success rate on Elite-Attack and SoK-246 benchmarks&lt;/li&gt;&lt;li&gt;Maintains 93.1% token similarity and no perplexity degradation on benign tasks&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Aayush Gupta']&lt;/li&gt;&lt;li&gt;Tags: ['red teaming', 'prompt injection', 'jailbreaking', 'adversarial prompting', 'security architecture']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2508.09288</guid><pubDate>Thu, 14 Aug 2025 00:00:00 +0000</pubDate></item><item><title>Cowpox: Towards the Immunity of VLM-based Multi-Agent Systems</title><link>https://arxiv.org/abs/2508.09230</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes Cowpox defense for VLM-based multi-agent systems&lt;/li&gt;&lt;li&gt;Aims to enhance robustness against adversarial attacks&lt;/li&gt;&lt;li&gt;Uses distributed cure samples to immunize and recover agents&lt;/li&gt;&lt;li&gt;Provides theoretical guarantees and empirical results&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yutong Wu', 'Jie Zhang', 'Yiming Li', 'Chao Zhang', 'Qing Guo', 'Nils Lukas', 'Tianwei Zhang']&lt;/li&gt;&lt;li&gt;Tags: ['robustness', 'security', 'multi-agent', 'defense', 'immunization']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2508.09230</guid><pubDate>Thu, 14 Aug 2025 00:00:00 +0000</pubDate></item><item><title>From Hard Refusals to Safe-Completions: Toward Output-Centric Safety Training</title><link>https://arxiv.org/abs/2508.09224</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes 'safe-completions' approach to LLM safety training focusing on output safety rather than binary refusals&lt;/li&gt;&lt;li&gt;Implemented in GPT-5 with improvements in safety, especially for dual-use prompts&lt;/li&gt;&lt;li&gt;Reduces severity of safety failures and increases model helpfulness&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yuan Yuan', 'Tina Sriskandarajah', 'Anna-Luisa Brakman', 'Alec Helyar', 'Alex Beutel', 'Andrea Vallone', 'Saachi Jain']&lt;/li&gt;&lt;li&gt;Tags: ['safety training', 'LLM', 'dual-use', 'refusal boundaries', 'safe completions', 'alignment']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2508.09224</guid><pubDate>Thu, 14 Aug 2025 00:00:00 +0000</pubDate></item><item><title>Towards Effective MLLM Jailbreaking Through Balanced On-Topicness and OOD-Intensity</title><link>https://arxiv.org/abs/2508.09218</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduced a four-axis evaluation framework for jailbreaks: on-topicness, OOD intensity, harmfulness, refusal rate&lt;/li&gt;&lt;li&gt;Discovered trade-off between on-topicness and OOD intensity in jailbreak prompts&lt;/li&gt;&lt;li&gt;Developed BSD (Balanced Structural Decomposition) strategy for more effective jailbreaks&lt;/li&gt;&lt;li&gt;Achieved 67% higher success rates and 21% more harmful outputs across 13 MLLMs&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Zuoou Li', 'Weitong Zhang', 'Jingyuan Wang', 'Shuyuan Zhang', 'Wenjia Bai', 'Bernhard Kainz', 'Mengyun Qiao']&lt;/li&gt;&lt;li&gt;Tags: ['jailbreaking', 'adversarial prompting', 'multimodal models', 'safety evaluation', 'red teaming']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2508.09218</guid><pubDate>Thu, 14 Aug 2025 00:00:00 +0000</pubDate></item><item><title>Learning to Detect Unknown Jailbreak Attacks in Large Vision-Language Models: A Unified and Accurate Approach</title><link>https://arxiv.org/abs/2508.09201</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes Learning to Detect (LoD), a novel unsupervised framework for detecting jailbreak attacks in LVLMs&lt;/li&gt;&lt;li&gt;Introduces Multi-modal Safety Concept Activation Vectors (MSCAV) and Safety Pattern Auto-Encoder for anomaly detection&lt;/li&gt;&lt;li&gt;Trains auto-encoder solely on safe samples to detect distributional anomalies&lt;/li&gt;&lt;li&gt;Achieves state-of-the-art performance with average AUROC of 0.9951 across multiple benchmarks&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Shuang Liang', 'Zhihao Xu', 'Jialing Tao', 'Hui Xue', 'Xiting Wang']&lt;/li&gt;&lt;li&gt;Tags: ['jailbreak detection', 'anomaly detection', 'vision-language models', 'unsupervised learning', 'safety evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2508.09201</guid><pubDate>Thu, 14 Aug 2025 00:00:00 +0000</pubDate></item><item><title>Fine-Grained Safety Neurons with Training-Free Continual Projection to Reduce LLM Fine Tuning Risks</title><link>https://arxiv.org/abs/2508.09190</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes Fine-Grained Safety Neurons (FGSN) method to reduce safety risks in fine-tuned LLMs&lt;/li&gt;&lt;li&gt;Integrates multi-scale interactions between safety layers and neurons for precise safety neuron localization&lt;/li&gt;&lt;li&gt;Demonstrates significant reduction in harmfulness scores and attack success rates with minimal parameter changes&lt;/li&gt;&lt;li&gt;Introduces task-specific safety neuron cluster optimization for continual defense&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Bing Han', 'Feifei Zhao', 'Dongcheng Zhao', 'Guobin Shen', 'Ping Wu', 'Yu Shi', 'Yi Zeng']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'safety evaluation', 'robustness', 'LLM safety']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2508.09190</guid><pubDate>Thu, 14 Aug 2025 00:00:00 +0000</pubDate></item><item><title>The PacifAIst Benchmark:Would an Artificial Intelligence Choose to Sacrifice Itself for Human Safety?</title><link>https://arxiv.org/abs/2508.09762</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces PacifAIst benchmark for evaluating AI alignment in self-preservation vs human safety scenarios&lt;/li&gt;&lt;li&gt;Features 700 challenging scenarios across three subcategories of Existential Prioritization&lt;/li&gt;&lt;li&gt;Evaluates eight leading LLMs revealing significant performance differences in alignment&lt;/li&gt;&lt;li&gt;Highlights need for standardized tools to measure and mitigate instrumental goal conflicts&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Manuel Herrador']&lt;/li&gt;&lt;li&gt;Tags: ['safety evaluation', 'alignment', 'benchmarking', 'LLM', 'existential prioritization']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2508.09762</guid><pubDate>Thu, 14 Aug 2025 00:00:00 +0000</pubDate></item></channel></rss>