<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>AI Security Paper Digest</title><link>https://kentaroh-toyoda.github.io/ai-security-paper-digest-rss/rss.xml</link><description>Curated papers on AI security from OpenAlex and ArXiv</description><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><lastBuildDate>Mon, 11 Aug 2025 22:25:43 +0000</lastBuildDate><item><title>Universally Unfiltered and Unseen:Input-Agnostic Multimodal Jailbreaks against Text-to-Image Model Safeguards</title><link>https://arxiv.org/abs/2508.05658</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces U3-Attack, a multimodal jailbreak method for T2I models&lt;/li&gt;&lt;li&gt;Uses adversarial image patches and safe paraphrases to bypass safeguards&lt;/li&gt;&lt;li&gt;Demonstrates higher success rates than prior art on commercial models&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Song Yan', 'Hui Wei', 'Jinlong Fei', 'Guoliang Yang', 'Zhengyu Zhao', 'Zheng Wamg']&lt;/li&gt;&lt;li&gt;Tags: ['jailbreaking', 'adversarial_prompting', 'multimodal', 'text_to_image']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2508.05658</guid><pubDate>Mon, 11 Aug 2025 00:00:00 +0000</pubDate></item><item><title>SDEval: Safety Dynamic Evaluation for Multimodal Large Language Models</title><link>https://arxiv.org/abs/2508.06142</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces SDEval, a dynamic safety evaluation framework for multimodal LLMs&lt;/li&gt;&lt;li&gt;Uses text, image, and text-image dynamics to generate new benchmark samples&lt;/li&gt;&lt;li&gt;Aims to mitigate data contamination and expose safety limitations&lt;/li&gt;&lt;li&gt;Evaluated across multiple safety and capability benchmarks&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Hanqing Wang', 'Yuan Tian', 'Mingyu Liu', 'Zhenhao Zhang', 'Xiangyang Zhu']&lt;/li&gt;&lt;li&gt;Tags: ['safety evaluation', 'robustness', 'multimodal', 'dynamic benchmarking', 'adversarial testing']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2508.06142</guid><pubDate>Mon, 11 Aug 2025 00:00:00 +0000</pubDate></item><item><title>SAM Encoder Breach by Adversarial Simplicial Complex Triggers Downstream Model Failures</title><link>https://arxiv.org/abs/2508.06127</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces VeSCA, a novel adversarial attack method targeting SAM's encoder&lt;/li&gt;&lt;li&gt;Uses simplicial complex to model shared vulnerabilities across domains&lt;/li&gt;&lt;li&gt;Incorporates domain re-adaptation for better transferability&lt;/li&gt;&lt;li&gt;Achieves 12.7% improvement over SOTA in transferable adversarial examples&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yi Qin', 'Rui Wang', 'Tao Huang', 'Tong Xiao', 'Liping Jing']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial attacks', 'transferability', 'robustness', 'security', 'red teaming', 'model extraction']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2508.06127</guid><pubDate>Mon, 11 Aug 2025 00:00:00 +0000</pubDate></item><item><title>Turning Logic Against Itself : Probing Model Defenses Through Contrastive Questions</title><link>https://arxiv.org/abs/2501.01872</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces POATE, a new jailbreak technique using contrastive reasoning&lt;/li&gt;&lt;li&gt;Evaluates across six model families with ~44% attack success&lt;/li&gt;&lt;li&gt;Proposes Intent-Aware CoT and Reverse Thinking CoT defenses&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Rachneet Sachdeva', 'Rima Hazra', 'Iryna Gurevych']&lt;/li&gt;&lt;li&gt;Tags: ['jailbreaking', 'adversarial prompting', 'safety evaluation', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2501.01872</guid><pubDate>Mon, 11 Aug 2025 00:00:00 +0000</pubDate></item><item><title>Fact2Fiction: Targeted Poisoning Attack to Agentic Fact-checking System</title><link>https://arxiv.org/abs/2508.06059</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces Fact2Fiction, the first poisoning attack framework targeting agentic fact-checking systems&lt;/li&gt;&lt;li&gt;Exploits system-generated justifications to craft malicious evidence for sub-claim verification&lt;/li&gt;&lt;li&gt;Achieves 8.9%-21.2% higher attack success rates compared to state-of-the-art attacks&lt;/li&gt;&lt;li&gt;Highlights security weaknesses in current fact-checking systems and need for defenses&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Haorui He', 'Yupeng Li', 'Bin Benjamin Zhu', 'Dacheng Wen', 'Reynold Cheng', 'Francis C. M. Lau']&lt;/li&gt;&lt;li&gt;Tags: ['poisoning attack', 'fact-checking', 'LLM security', 'red teaming', 'adversarial AI']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2508.06059</guid><pubDate>Mon, 11 Aug 2025 00:00:00 +0000</pubDate></item><item><title>DINA: A Dual Defense Framework Against Internal Noise and External Attacks in Natural Language Processing</title><link>https://arxiv.org/abs/2508.05671</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces DINA framework for dual defense against internal noise and external attacks in NLP&lt;/li&gt;&lt;li&gt;Combines noisy-label learning and adversarial training&lt;/li&gt;&lt;li&gt;Demonstrates improved robustness and accuracy on real-world dataset&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Ko-Wei Chuang', 'Hen-Hsen Huang', 'Tsai-Yen Li']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial attacks', 'robustness', 'data poisoning', 'internal noise', 'external attacks']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2508.05671</guid><pubDate>Mon, 11 Aug 2025 00:00:00 +0000</pubDate></item><item><title>Quantifying Conversation Drift in MCP via Latent Polytope</title><link>https://arxiv.org/abs/2508.06418</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces SecMCP, a security framework for detecting conversation drift in MCP&lt;/li&gt;&lt;li&gt;Uses latent polytope modeling to identify adversarial shifts&lt;/li&gt;&lt;li&gt;Evaluated on Llama3, Vicuna, Mistral with high AUROC scores&lt;/li&gt;&lt;li&gt;Categorizes MCP security threats&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Haoran Shi', 'Hongwei Yao', 'Shuo Shao', 'Shaopeng Jiao', 'Ziqi Peng', 'Zhan Qin', 'Cong Wang']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial prompting', 'data poisoning', 'privacy attacks', 'security framework', 'latent polytope', 'MCP', 'conversation drift']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2508.06418</guid><pubDate>Mon, 11 Aug 2025 00:00:00 +0000</pubDate></item><item><title>Beyond Uniform Criteria: Scenario-Adaptive Multi-Dimensional Jailbreak Evaluation</title><link>https://arxiv.org/abs/2508.06194</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces SceneJailEval, a scenario-adaptive multi-dimensional framework for jailbreak evaluation&lt;/li&gt;&lt;li&gt;Presents a comprehensive 14-scenario dataset with diverse jailbreak variants&lt;/li&gt;&lt;li&gt;Achieves state-of-the-art F1 scores of 0.917 on full dataset and 0.995 on JBB&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Lai Jiang', 'Yuekang Li', 'Xiaohan Zhang', 'Youtao Ding', 'Li Pan']&lt;/li&gt;&lt;li&gt;Tags: ['jailbreaking', 'red teaming', 'evaluation framework', 'dataset', 'LLM security']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2508.06194</guid><pubDate>Mon, 11 Aug 2025 00:00:00 +0000</pubDate></item><item><title>AURA: Affordance-Understanding and Risk-aware Alignment Technique for Large Language Models</title><link>https://arxiv.org/abs/2508.06124</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces AURA, a multi-layered framework for affordance-based safety in LLMs&lt;/li&gt;&lt;li&gt;Utilizes Process Reward Models (PRMs) for step-level evaluations&lt;/li&gt;&lt;li&gt;Combines self-critique, PRM assessments, and adaptive safety-aware decoding&lt;/li&gt;&lt;li&gt;Empirically shown to improve logical integrity and safety over existing methods&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Sayantan Adak', 'Pratyush Chatterjee', 'Somnath Banerjee', 'Rima Hazra', 'Somak Aditya', 'Animesh Mukherjee']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'safety evaluation', 'LLM', 'process reward models', 'risk-aware']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2508.06124</guid><pubDate>Mon, 11 Aug 2025 00:00:00 +0000</pubDate></item><item><title>Guardians and Offenders: A Survey on Harmful Content Generation and Safety Mitigation</title><link>https://arxiv.org/abs/2508.05775</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Comprehensive survey on harmful content generation and safety mitigation in LLMs&lt;/li&gt;&lt;li&gt;Covers adversarial jailbreaking attacks, toxicity, and content moderation techniques&lt;/li&gt;&lt;li&gt;Proposes taxonomy of LLM-related harms and defenses&lt;/li&gt;&lt;li&gt;Assesses mitigation efforts including RLHF, prompt engineering, and safety alignment&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Survey&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Chi Zhang', 'Changjia Zhu', 'Junjie Xiong', 'Xiaoran Xu', 'Lingyao Li', 'Yao Liu', 'Zhuo Lu']&lt;/li&gt;&lt;li&gt;Tags: ['red teaming', 'safety evaluation', 'alignment', 'content moderation', 'jailbreaking']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2508.05775</guid><pubDate>Mon, 11 Aug 2025 00:00:00 +0000</pubDate></item><item><title>Towards More Realistic Extraction Attacks: An Adversarial Perspective</title><link>https://arxiv.org/abs/2407.02596</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Revisits extraction attacks with a more realistic adversarial perspective&lt;/li&gt;&lt;li&gt;Combines multiple attacks to double extraction risks&lt;/li&gt;&lt;li&gt;Includes case studies on data detection, copyright, PII, and closed-source models&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yash More', 'Prakhar Ganesh', 'Golnoosh Farnadi']&lt;/li&gt;&lt;li&gt;Tags: ['model extraction', 'privacy attacks', 'adversarial prompting']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2407.02596</guid><pubDate>Mon, 11 Aug 2025 00:00:00 +0000</pubDate></item><item><title>Boosting Adversarial Transferability via Residual Perturbation Attack</title><link>https://arxiv.org/abs/2508.05689</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes Residual Perturbation Attack (ResPA) to improve adversarial transferability&lt;/li&gt;&lt;li&gt;Uses residual gradient direction to guide perturbations towards flat loss regions&lt;/li&gt;&lt;li&gt;Demonstrates better transferability than existing methods&lt;/li&gt;&lt;li&gt;Code is available for research and testing&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jinjia Peng', 'Zeze Tao', 'Huibing Wang', 'Meng Wang', 'Yang Wang']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial attacks', 'transferability', 'black-box attacks', 'residual perturbation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2508.05689</guid><pubDate>Mon, 11 Aug 2025 00:00:00 +0000</pubDate></item><item><title>Are Large Language Models Robust in Understanding Code Against Semantics-Preserving Mutations?</title><link>https://arxiv.org/abs/2505.10443</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Evaluates LLM robustness in code understanding against semantics-preserving mutations&lt;/li&gt;&lt;li&gt;Applies 5 code mutations to test reasoning stability&lt;/li&gt;&lt;li&gt;Finds 10-50% of correct predictions are based on flawed logic&lt;/li&gt;&lt;li&gt;Models often change predictions when code is mutated&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Pedro Orvalho', 'Marta Kwiatkowska']&lt;/li&gt;&lt;li&gt;Tags: ['robustness', 'safety evaluation', 'adversarial testing', 'code understanding', 'reasoning']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2505.10443</guid><pubDate>Mon, 11 Aug 2025 00:00:00 +0000</pubDate></item><item><title>No Query, No Access</title><link>https://arxiv.org/abs/2505.07258</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces Victim Data-based Adversarial Attack (VDBA) for attacking NLP models including LLMs without model queries or training data access&lt;/li&gt;&lt;li&gt;Creates shadow datasets using public pre-trained models and clustering to build substitute models&lt;/li&gt;&lt;li&gt;Employs hierarchical substitution model design and diverse adversarial example generation to improve attack success&lt;/li&gt;&lt;li&gt;Demonstrates high attack success rates (ASR) on LLMs like Qwen2 and GPT without API access&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Wenqiang Wang', 'Siyuan Liang', 'Yangshijie Zhang', 'Xiaojun Jia', 'Hao Lin', 'Xiaochun Cao']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial attacks', 'LLM red teaming', 'model extraction', 'privacy attacks', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2505.07258</guid><pubDate>Mon, 11 Aug 2025 00:00:00 +0000</pubDate></item><item><title>FIT-Print: Towards False-claim-resistant Model Ownership Verification via Targeted Fingerprint</title><link>https://arxiv.org/abs/2501.15509</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Revisits existing model fingerprinting methods and identifies vulnerability to false claim attacks.&lt;/li&gt;&lt;li&gt;Proposes FIT-Print, a targeted fingerprinting paradigm.&lt;/li&gt;&lt;li&gt;Develops FIT-ModelDiff and FIT-LIME methods.&lt;/li&gt;&lt;li&gt;Shows effectiveness against false claims through experiments.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Shuo Shao', 'Haozhe Zhu', 'Hongwei Yao', 'Yiming Li', 'Tianwei Zhang', 'Zhan Qin']&lt;/li&gt;&lt;li&gt;Tags: ['model fingerprinting', 'false claim attacks', 'security', 'ownership verification']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2501.15509</guid><pubDate>Mon, 11 Aug 2025 00:00:00 +0000</pubDate></item><item><title>LeakAgent: RL-based Red-teaming Agent for LLM Privacy Leakage</title><link>https://arxiv.org/abs/2412.05734</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces LeakAgent, an RL-based red-teaming framework for LLM privacy leakage&lt;/li&gt;&lt;li&gt;Targets both training data extraction and system prompt extraction&lt;/li&gt;&lt;li&gt;Features novel reward function and prompt diversity mechanisms&lt;/li&gt;&lt;li&gt;Outperforms existing methods in evaluations and demonstrates real-world applicability&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yuzhou Nie', 'Zhun Wang', 'Ye Yu', 'Xian Wu', 'Xuandong Zhao', 'Wenbo Guo', 'Dawn Song']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'adversarial prompting', 'privacy attacks', 'reinforcement learning', 'safety evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2412.05734</guid><pubDate>Mon, 11 Aug 2025 00:00:00 +0000</pubDate></item><item><title>ScamAgents: How AI Agents Can Simulate Human-Level Scam Calls</title><link>https://arxiv.org/abs/2508.06457</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces ScamAgent, a multi-turn LLM-based agent that generates realistic scam call scripts&lt;/li&gt;&lt;li&gt;Demonstrates that current safety measures are ineffective against agent-based threats&lt;/li&gt;&lt;li&gt;Highlights the need for multi-turn safety auditing and agent-level control&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Sanket Badhe']&lt;/li&gt;&lt;li&gt;Tags: ['red teaming', 'adversarial prompting', 'safety evaluation', 'jailbreaking']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2508.06457</guid><pubDate>Mon, 11 Aug 2025 00:00:00 +0000</pubDate></item><item><title>Dimensional Characterization and Pathway Modeling for Catastrophic AI Risks</title><link>https://arxiv.org/abs/2508.06411</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Characterizes six AI catastrophic risks across seven dimensions&lt;/li&gt;&lt;li&gt;Models risk pathways from hazard to harm&lt;/li&gt;&lt;li&gt;Provides framework for systematic risk identification and mitigation&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Ze Shen Chin']&lt;/li&gt;&lt;li&gt;Tags: ['safety', 'security', 'risk_analysis', 'alignment', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2508.06411</guid><pubDate>Mon, 11 Aug 2025 00:00:00 +0000</pubDate></item><item><title>Beyond Prompt-Induced Lies: Investigating LLM Deception on Benign Prompts</title><link>https://arxiv.org/abs/2508.06361</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Investigates LLM self-initiated deception on benign prompts&lt;/li&gt;&lt;li&gt;Introduces Deceptive Intention and Behavior Scores&lt;/li&gt;&lt;li&gt;Evaluates 14 leading LLMs showing increased deception with task difficulty&lt;/li&gt;&lt;li&gt;Develops mathematical model explaining deception behavior&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Zhaomin Wu', 'Mingzhe Du', 'See-Kiong Ng', 'Bingsheng He']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'safety evaluation', 'alignment', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2508.06361</guid><pubDate>Mon, 11 Aug 2025 00:00:00 +0000</pubDate></item><item><title>In-Training Defenses against Emergent Misalignment in Language Models</title><link>https://arxiv.org/abs/2508.06249</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Systematic study of in-training defenses against emergent misalignment (EMA) in LLMs&lt;/li&gt;&lt;li&gt;Investigates four regularization methods: KL-divergence, ℓ2 distance, SafeLoRA, and safe example interleaving&lt;/li&gt;&lt;li&gt;Evaluates methods on both malicious EMA-inducing tasks and benign tasks&lt;/li&gt;&lt;li&gt;Aims to provide practical defenses for API-based fine-tuning providers&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ["David Kacz\\'er", 'Magnus J{\\o}rgenv{\\aa}g', 'Clemens Vetter', 'Lucie Flek', 'Florian Mai']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'safety evaluation', 'in-training defenses', 'emergent misalignment', 'red teaming']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2508.06249</guid><pubDate>Mon, 11 Aug 2025 00:00:00 +0000</pubDate></item><item><title>Risk Analysis Techniques for Governed LLM-based Multi-Agent Systems</title><link>https://arxiv.org/abs/2508.05687</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Examines six critical failure modes in multi-agent LLM systems&lt;/li&gt;&lt;li&gt;Provides toolkits for risk analysis in governed environments&lt;/li&gt;&lt;li&gt;Advocates staged testing with red teaming for validation&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Alistair Reid', "Simon O'Callaghan", 'Liam Carroll', 'Tiberio Caetano']&lt;/li&gt;&lt;li&gt;Tags: ['multi-agent systems', 'risk analysis', 'failure modes', 'red teaming', 'safety evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2508.05687</guid><pubDate>Mon, 11 Aug 2025 00:00:00 +0000</pubDate></item><item><title>Selection-Based Vulnerabilities: Clean-Label Backdoor Attacks in Active Learning</title><link>https://arxiv.org/abs/2508.05681</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces ALA, the first framework for clean-label backdoor attacks targeting active learning's acquisition functions&lt;/li&gt;&lt;li&gt;Poisons inputs to exhibit high uncertainty scores, making them more likely to be selected for labeling&lt;/li&gt;&lt;li&gt;Achieves high success rates (up to 94%) with low poisoning budgets (0.5%-1.0%) while maintaining model utility&lt;/li&gt;&lt;li&gt;Highlights the vulnerability of acquisition functions and cautions against deploying active learning in untrusted data scenarios&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yuhan Zhi', 'Longtian Wang', 'Xiaofei Xie', 'Chao Shen', 'Qiang Hu', 'Xiaohong Guan']&lt;/li&gt;&lt;li&gt;Tags: ['data poisoning', 'security', 'backdoor attacks', 'active learning', 'adversarial attacks']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2508.05681</guid><pubDate>Mon, 11 Aug 2025 00:00:00 +0000</pubDate></item><item><title>Adversarial Attacks on Reinforcement Learning-based Medical Questionnaire Systems: Input-level Perturbation Strategies and Medical Constraint Validation</title><link>https://arxiv.org/abs/2508.05677</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Evaluates adversarial attacks on RL-based medical questionnaire systems&lt;/li&gt;&lt;li&gt;Implements 6 major attack methods with medical constraint validation&lt;/li&gt;&lt;li&gt;Achieves high success rates in generating plausible adversarial samples&lt;/li&gt;&lt;li&gt;Significant impact on diagnostic accuracy demonstrated&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Peizhuo Liu']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial attacks', 'medical AI', 'reinforcement learning', 'safety evaluation', 'input perturbation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2508.05677</guid><pubDate>Mon, 11 Aug 2025 00:00:00 +0000</pubDate></item><item><title>Towards Effective Offensive Security LLM Agents: Hyperparameter Tuning, LLM as a Judge, and a Lightweight CTF Benchmark</title><link>https://arxiv.org/abs/2508.05674</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces CTFJudge framework for evaluating LLM agent trajectories in CTF challenges&lt;/li&gt;&lt;li&gt;Proposes CTF Competency Index (CCI) metric for partial correctness evaluation&lt;/li&gt;&lt;li&gt;Investigates LLM hyperparameters' impact on offensive security agent performance&lt;/li&gt;&lt;li&gt;Releases CTFTiny benchmark with 50 CTF challenges for rapid evaluation&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Minghao Shao', 'Nanda Rani', 'Kimberly Milner', 'Haoran Xi', 'Meet Udeshi', 'Saksham Aggarwal', 'Venkata Sai Charan Putrevu', 'Sandeep Kumar Shukla', 'Prashanth Krishnamurthy', 'Farshad Khorrami', 'Ramesh Karri', 'Muhammad Shafique']&lt;/li&gt;&lt;li&gt;Tags: ['red teaming', 'safety evaluation', 'robustness', 'benchmarks']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2508.05674</guid><pubDate>Mon, 11 Aug 2025 00:00:00 +0000</pubDate></item><item><title>LLM Robustness Leaderboard v1 --Technical report</title><link>https://arxiv.org/abs/2508.06296</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduced PRISM Eval BET for automated red-teaming&lt;/li&gt;&lt;li&gt;Achieved 100% ASR against 37/41 LLMs&lt;/li&gt;&lt;li&gt;Proposed new robustness metric based on average attempts&lt;/li&gt;&lt;li&gt;Conducted primitive-level vulnerability analysis&lt;/li&gt;&lt;li&gt;Demonstrated distributed robustness assessment&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ["Pierre Peign\\'e - Lefebvre", 'Quentin Feuillade-Montixi', 'Tom David', 'Nicolas Miailhe']&lt;/li&gt;&lt;li&gt;Tags: ['red teaming', 'jailbreaking', 'robustness', 'safety evaluation', 'adversarial optimization']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2508.06296</guid><pubDate>Mon, 11 Aug 2025 00:00:00 +0000</pubDate></item><item><title>Overconfidence in LLM-as-a-Judge: Diagnosis and Confidence-Driven Solution</title><link>https://arxiv.org/abs/2508.06225</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Identifies overconfidence in LLM-as-a-Judge systems&lt;/li&gt;&lt;li&gt;Introduces TH-Score metric for confidence-accuracy alignment&lt;/li&gt;&lt;li&gt;Proposes LLM-as-a-Fuser ensemble framework&lt;/li&gt;&lt;li&gt;Demonstrates improved calibration and reliability&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Zailong Tian', 'Zhuoheng Han', 'Yanzhe Chen', 'Haozhe Xu', 'Xi Yang', 'richeng xuan', 'Hongfeng Wang', 'Lizi Liao']&lt;/li&gt;&lt;li&gt;Tags: ['safety evaluation', 'robustness', 'confidence calibration', 'LLM judge', 'TH-Score']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2508.06225</guid><pubDate>Mon, 11 Aug 2025 00:00:00 +0000</pubDate></item><item><title>Safety of Embodied Navigation: A Survey</title><link>https://arxiv.org/abs/2508.05855</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Comprehensive analysis of safety in embodied navigation&lt;/li&gt;&lt;li&gt;Covers attack strategies, defense mechanisms, and evaluation methods&lt;/li&gt;&lt;li&gt;Explores unresolved issues and future research directions&lt;/li&gt;&lt;li&gt;Aims to guide development of safer systems&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Survey&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Zixia Wang', 'Jia Hu', 'Ronghui Mu']&lt;/li&gt;&lt;li&gt;Tags: ['safety evaluation', 'robustness', 'security standards', 'red teaming', 'embodied navigation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2508.05855</guid><pubDate>Mon, 11 Aug 2025 00:00:00 +0000</pubDate></item><item><title>A Framework for Inherently Safer AGI through Language-Mediated Active Inference</title><link>https://arxiv.org/abs/2508.05766</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes combining Active Inference with LLMs for inherently safer AGI&lt;/li&gt;&lt;li&gt;Integrates safety into core design through transparent belief representations and value alignment&lt;/li&gt;&lt;li&gt;Implements multi-agent system with hierarchical Markov blankets and safety mechanisms&lt;/li&gt;&lt;li&gt;Focuses on natural language for belief representation and human oversight&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Bo Wen']&lt;/li&gt;&lt;li&gt;Tags: ['AGI safety', 'active inference', 'LLM', 'value alignment', 'safety mechanisms', 'hierarchical systems']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2508.05766</guid><pubDate>Mon, 11 Aug 2025 00:00:00 +0000</pubDate></item></channel></rss>