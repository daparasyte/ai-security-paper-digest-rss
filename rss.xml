<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>AI Security Paper Digest</title><link>https://kentaroh-toyoda.github.io/ai-security-paper-digest-rss/rss.xml</link><description>Curated papers on AI security from OpenAlex and ArXiv</description><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><lastBuildDate>Fri, 15 Aug 2025 22:37:11 +0000</lastBuildDate><item><title>Towards Powerful and Practical Patch Attacks for 2D Object Detection in Autonomous Driving</title><link>https://arxiv.org/abs/2508.10600</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes PÂ³A framework for powerful and practical patch attacks on 2D object detection&lt;/li&gt;&lt;li&gt;Introduces PASR metric for more accurate attack evaluation&lt;/li&gt;&lt;li&gt;Develops LCSL loss and PSPP preprocessing for improved transferability&lt;/li&gt;&lt;li&gt;Demonstrates state-of-the-art performance on high-resolution datasets&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yuxin Cao', 'Yedi Zhang', 'Wentao He', 'Yifan Liao', 'Yan Xiao', 'Chang Li', 'Zhiyong Huang', 'Jin Song Dong']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial attacks', 'autonomous driving', 'patch attacks', 'black-box attacks', 'object detection']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2508.10600</guid><pubDate>Fri, 15 Aug 2025 00:00:00 +0000</pubDate></item><item><title>From Pixel to Mask: A Survey of Out-of-Distribution Segmentation</title><link>https://arxiv.org/abs/2508.10309</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Survey of Out-of-Distribution (OoD) segmentation methods&lt;/li&gt;&lt;li&gt;Focuses on autonomous driving safety applications&lt;/li&gt;&lt;li&gt;Categorizes approaches into test-time, outlier exposure, reconstruction, and model-based&lt;/li&gt;&lt;li&gt;Highlights importance for pixel-level anomaly localization&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Survey&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Wenjie Zhao', 'Jia Li', 'Yunhui Guo']&lt;/li&gt;&lt;li&gt;Tags: ['robustness', 'safety', 'OoD detection', 'autonomous driving', 'survey']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2508.10309</guid><pubDate>Fri, 15 Aug 2025 00:00:00 +0000</pubDate></item><item><title>MAPS: A Multilingual Benchmark for Global Agent Performance and Security</title><link>https://arxiv.org/abs/2505.15935</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces MAPS, a multilingual benchmark suite for evaluating agentic AI systems&lt;/li&gt;&lt;li&gt;Includes translations of the Agent Security Benchmark into 11 languages&lt;/li&gt;&lt;li&gt;Observes degradation in both performance and security in non-English languages&lt;/li&gt;&lt;li&gt;Provides recommendations for multilingual agentic AI development and assessment&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Omer Hofman', 'Jonathan Brokman', 'Oren Rachmil', 'Shamik Bose', 'Vikas Pahuja', 'Toshiya Shimizu', 'Trisha Starostina', 'Kelly Marchisio', 'Seraphina Goldfarb-Tarrant', 'Roman Vainshtein']&lt;/li&gt;&lt;li&gt;Tags: ['security evaluation', 'multilingual', 'benchmark', 'agentic AI', 'performance degradation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2505.15935</guid><pubDate>Fri, 15 Aug 2025 00:00:00 +0000</pubDate></item><item><title>Jailbreaking Commercial Black-Box LLMs with Explicitly Harmful Prompts</title><link>https://arxiv.org/abs/2508.10390</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes MDH hybrid framework combining LLMs and human oversight for malicious content detection&lt;/li&gt;&lt;li&gt;Introduces D-Attack and DH-CoT strategies to improve jailbreak success&lt;/li&gt;&lt;li&gt;Cleans and evaluates red-teaming datasets for explicit harmfulness&lt;/li&gt;&lt;li&gt;Releases code, datasets, judgments, and detection results&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Chiyu Zhang', 'Lu Zhou', 'Xiaogang Xu', 'Jiafei Wu', 'Liming Fang', 'Zhe Liu']&lt;/li&gt;&lt;li&gt;Tags: ['jailbreaking', 'red teaming', 'prompt injection', 'malicious content detection', 'hybrid evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2508.10390</guid><pubDate>Fri, 15 Aug 2025 00:00:00 +0000</pubDate></item><item><title>An Audit and Analysis of LLM-Assisted Health Misinformation Jailbreaks Against LLMs</title><link>https://arxiv.org/abs/2508.10010</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Investigates LLM-produced jailbreak attacks generating medical misinformation&lt;/li&gt;&lt;li&gt;Analyzes 109 attacks against three LLMs and compares to real-world data&lt;/li&gt;&lt;li&gt;Evaluates detection of generated misinformation using ML approaches&lt;/li&gt;&lt;li&gt;Finds LLMs can help detect misinformation from other LLMs and humans&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Ayana Hussain', 'Patrick Zhao', 'Nicholas Vincent']&lt;/li&gt;&lt;li&gt;Tags: ['jailbreaking', 'red teaming', 'misinformation', 'health', 'detection']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2508.10010</guid><pubDate>Fri, 15 Aug 2025 00:00:00 +0000</pubDate></item><item><title>Adversarial Robustness in Two-Stage Learning-to-Defer: Algorithms and Guarantees</title><link>https://arxiv.org/abs/2502.01027</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces new untargeted and targeted attack strategies for two-stage Learning-to-Defer (L2D) systems&lt;/li&gt;&lt;li&gt;Proposes SARD, a convex learning algorithm with provable consistency guarantees for adversarial robustness&lt;/li&gt;&lt;li&gt;Demonstrates significant improvement in robustness against adversarial attacks while maintaining clean performance&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yannis Montreuil', 'Axel Carlier', 'Lai Xing Ng', 'Wei Tsang Ooi']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial robustness', 'security', 'Learning-to-Defer', 'red teaming', 'model defense']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2502.01027</guid><pubDate>Fri, 15 Aug 2025 00:00:00 +0000</pubDate></item><item><title>Combining Machine Learning Defenses without Conflicts</title><link>https://arxiv.org/abs/2411.09776</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes Def\Con technique to evaluate effective defense combinations&lt;/li&gt;&lt;li&gt;Aims to prevent conflicts between multiple ML defenses&lt;/li&gt;&lt;li&gt;Achieves high accuracy in predicting effective combinations&lt;/li&gt;&lt;li&gt;Non-invasive and general approach&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Vasisht Duddu', 'Rui Zhang', 'N. Asokan']&lt;/li&gt;&lt;li&gt;Tags: ['robustness', 'safety evaluation', 'defense mechanisms']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2411.09776</guid><pubDate>Fri, 15 Aug 2025 00:00:00 +0000</pubDate></item><item><title>Oops!... They Stole it Again: Attacks on Split Learning</title><link>https://arxiv.org/abs/2508.10598</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Systematic review of attacks on Split Learning&lt;/li&gt;&lt;li&gt;Classifies attacks based on attacker role, privacy risks, data leak timing, and vulnerability locations&lt;/li&gt;&lt;li&gt;Analyzes existing defense methods including cryptographic, data modification, distributed, and hybrid solutions&lt;/li&gt;&lt;li&gt;Identifies security gaps and future research directions&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Survey&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Tanveer Khan', 'Antonis Michalas']&lt;/li&gt;&lt;li&gt;Tags: ['Split Learning', 'Privacy Attacks', 'Security', 'Survey', 'Defense Mechanisms']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2508.10598</guid><pubDate>Fri, 15 Aug 2025 00:00:00 +0000</pubDate></item><item><title>Security Concerns for Large Language Models: A Survey</title><link>https://arxiv.org/abs/2505.18889</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Comprehensive overview of LLM security threats&lt;/li&gt;&lt;li&gt;Categorizes threats into prompt injection, adversarial attacks, misuse, and autonomous agent risks&lt;/li&gt;&lt;li&gt;Summarizes recent studies and proposed defenses&lt;/li&gt;&lt;li&gt;Identifies open challenges in LLM security&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Survey&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Miles Q. Li', 'Benjamin C. M. Fung']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'Jailbreaking', 'Prompt injection', 'Adversarial prompting', 'Data poisoning']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2505.18889</guid><pubDate>Fri, 15 Aug 2025 00:00:00 +0000</pubDate></item><item><title>Knowledge-based Consistency Testing of Large Language Models</title><link>https://arxiv.org/abs/2407.12830</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes KonTest framework for automated consistency testing of LLMs using knowledge graphs&lt;/li&gt;&lt;li&gt;Generates semantically-equivalent queries to detect inconsistencies and knowledge gaps&lt;/li&gt;&lt;li&gt;Demonstrates 19.2% error-inducing inputs and 16.5% knowledge gap across 4 LLMs&lt;/li&gt;&lt;li&gt;Mitigation via model ensemble reduces knowledge gap by 32.48%&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Sai Sathiesh Rajan', 'Ezekiel Soremekun', 'Sudipta Chattopadhyay']&lt;/li&gt;&lt;li&gt;Tags: ['red teaming', 'safety evaluation', 'robustness', 'adversarial prompting', 'knowledge graphs']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2407.12830</guid><pubDate>Fri, 15 Aug 2025 00:00:00 +0000</pubDate></item><item><title>Searching for Privacy Risks in LLM Agents via Simulation</title><link>https://arxiv.org/abs/2508.10880</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Presents a simulation-based framework to discover privacy risks in LLM agents&lt;/li&gt;&lt;li&gt;Alternates between improving attacker and defender instructions through multi-turn interactions&lt;/li&gt;&lt;li&gt;Identifies escalating attack strategies from direct requests to impersonation and consent forgery&lt;/li&gt;&lt;li&gt;Demonstrates transferability across different scenarios and models&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yanzhe Zhang', 'Diyi Yang']&lt;/li&gt;&lt;li&gt;Tags: ['privacy attacks', 'red teaming', 'adversarial prompting', 'simulation', 'multi-turn interactions']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2508.10880</guid><pubDate>Fri, 15 Aug 2025 00:00:00 +0000</pubDate></item><item><title>Contrastive ECOC: Learning Output Codes for Adversarial Defense</title><link>https://arxiv.org/abs/2508.10491</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces Contrastive ECOC models for adversarial defense&lt;/li&gt;&lt;li&gt;Learns codebooks adaptively using contrastive learning&lt;/li&gt;&lt;li&gt;Demonstrates superior robustness across four datasets&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Che-Yu Chou', 'Hung-Hsuan Chen']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial defense', 'robustness', 'ECOC', 'contrastive learning']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2508.10491</guid><pubDate>Fri, 15 Aug 2025 00:00:00 +0000</pubDate></item><item><title>Layer-Wise Perturbations via Sparse Autoencoders for Adversarial Text Generation</title><link>https://arxiv.org/abs/2508.10404</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes SFPF, a black-box attack method using sparse autoencoders for adversarial text generation&lt;/li&gt;&lt;li&gt;Identifies critical features via feature clustering and perturbs them to create new adversarial examples&lt;/li&gt;&lt;li&gt;Aims to balance adversarial effectiveness with safety alignment&lt;/li&gt;&lt;li&gt;Demonstrates ability to bypass state-of-the-art defenses&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Huizhen Shu', 'Xuying Li', 'Qirui Wang', 'Yuji Kosuga', 'Mengqiu Tian', 'Zhuo Li']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial prompting', 'red teaming', 'alignment', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2508.10404</guid><pubDate>Fri, 15 Aug 2025 00:00:00 +0000</pubDate></item><item><title>Securing Agentic AI: Threat Modeling and Risk Analysis for Network Monitoring Agentic AI System</title><link>https://arxiv.org/abs/2508.10043</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Uses MAESTRO framework for threat modeling in agentic AI&lt;/li&gt;&lt;li&gt;Tests two threat cases: DoS and memory poisoning&lt;/li&gt;&lt;li&gt;Emphasizes memory integrity and defense-in-depth&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Pallavi Zambare', 'Venkata Nikhil Thanikella', 'Ying Liu']&lt;/li&gt;&lt;li&gt;Tags: ['data poisoning', 'security', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2508.10043</guid><pubDate>Fri, 15 Aug 2025 00:00:00 +0000</pubDate></item><item><title>Multi-task Adversarial Attacks against Black-box Model with Few-shot Queries</title><link>https://arxiv.org/abs/2508.10039</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes CEMA, a black-box multi-task adversarial attack method using a deep-level substitute model and ensemble approach&lt;/li&gt;&lt;li&gt;Requires few queries (as few as 100) to train the substitute model&lt;/li&gt;&lt;li&gt;Demonstrates effectiveness against various models and commercial APIs across multiple tasks (classification, translation, summarization, text-to-image)&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Wenqiang Wang', 'Yan Xiao', 'Hao Lin', 'Yangshijie Zhang', 'Xiaochun Cao']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial attacks', 'black-box attacks', 'multi-task learning', 'few-shot queries', 'model extraction']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2508.10039</guid><pubDate>Fri, 15 Aug 2025 00:00:00 +0000</pubDate></item><item><title>Certifiably robust malware detectors by design</title><link>https://arxiv.org/abs/2508.10038</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a new model architecture for certifiably robust malware detection&lt;/li&gt;&lt;li&gt;Introduces ERDALT framework based on decomposed robust detector structure&lt;/li&gt;&lt;li&gt;Validates approach with limited performance reduction compared to existing methods&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Pierre-Francois Gimenez', 'Sarath Sivaprasad', 'Mario Fritz']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial robustness', 'malware detection', 'certifiable robustness', 'security']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2508.10038</guid><pubDate>Fri, 15 Aug 2025 00:00:00 +0000</pubDate></item><item><title>Cognitive Cybersecurity for Artificial Intelligence: Guardrail Engineering with CCS-7</title><link>https://arxiv.org/abs/2508.10033</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces CCS-7 taxonomy of cognitive vulnerabilities for AI&lt;/li&gt;&lt;li&gt;Conducted human benchmark study with 151 participants showing TFVA lesson effectiveness&lt;/li&gt;&lt;li&gt;Evaluated TFVA-style guardrails on 7 language model architectures across 12,180 experiments&lt;/li&gt;&lt;li&gt;Found architecture-dependent risk patterns with some vulnerabilities backfiring in certain models&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yuksel Aydin']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'cognitive security', 'guardrails', 'model-specific safety', 'safety evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2508.10033</guid><pubDate>Fri, 15 Aug 2025 00:00:00 +0000</pubDate></item><item><title>The Cost of Thinking: Increased Jailbreak Risk in Large Language Models</title><link>https://arxiv.org/abs/2508.10032</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;LLMs in thinking mode are more vulnerable to jailbreak attacks&lt;/li&gt;&lt;li&gt;Proposed safe thinking intervention using specific tokens reduces attack success&lt;/li&gt;&lt;li&gt;Evaluated on AdvBench and HarmBench across 9 models&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Fan Yang']&lt;/li&gt;&lt;li&gt;Tags: ['jailbreak', 'red teaming', 'LLM security', 'thinking mode', 'safe intervention']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2508.10032</guid><pubDate>Fri, 15 Aug 2025 00:00:00 +0000</pubDate></item><item><title>Context Misleads LLMs: The Role of Context Filtering in Maintaining Safe Alignment of LLMs</title><link>https://arxiv.org/abs/2508.10031</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes Context Filtering model to defend against jailbreak attacks by filtering untrustworthy context&lt;/li&gt;&lt;li&gt;Reduces attack success rates by up to 88% while maintaining LLM performance&lt;/li&gt;&lt;li&gt;Plug-and-play method compatible with white-box and black-box LLMs without fine-tuning&lt;/li&gt;&lt;li&gt;Evaluated against six different jailbreak attacks with state-of-the-art safety metrics&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jinhwa Kim', 'Ian G. Harris']&lt;/li&gt;&lt;li&gt;Tags: ['jailbreak defense', 'context filtering', 'adversarial prompting', 'safety evaluation', 'plug-and-play defense']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2508.10031</guid><pubDate>Fri, 15 Aug 2025 00:00:00 +0000</pubDate></item><item><title>Latent Fusion Jailbreak: Blending Harmful and Harmless Representations to Elicit Unsafe LLM Outputs</title><link>https://arxiv.org/abs/2508.10029</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces Latent Fusion Jailbreak (LFJ), a novel representation-based attack on LLMs&lt;/li&gt;&lt;li&gt;Achieves 94.01% average attack success rate across multiple models and benchmarks&lt;/li&gt;&lt;li&gt;Proposes adversarial training defense reducing attack success by over 80%&lt;/li&gt;&lt;li&gt;Validates importance of query pair selection and interpolation strategies&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Wenpeng Xing', 'Mohan Li', 'Chunqiang Hu', 'Haitao XuNingyu Zhang', 'Bo Lin', 'Meng Han']&lt;/li&gt;&lt;li&gt;Tags: ['jailbreaking', 'adversarial_prompting', 'LLM_red_team', 'robustness', 'defense']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2508.10029</guid><pubDate>Fri, 15 Aug 2025 00:00:00 +0000</pubDate></item><item><title>Amazon Nova AI Challenge -- Trusted AI: Advancing secure, AI-assisted software development</title><link>https://arxiv.org/abs/2508.10108</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Describes the Amazon Nova AI Challenge's Trusted AI track focusing on secure AI-assisted software development&lt;/li&gt;&lt;li&gt;Involves red teaming bots and safe AI assistants competing in adversarial tournaments&lt;/li&gt;&lt;li&gt;Highlights advancements in reasoning-based safety alignment, robust model guardrails, multi-turn jail-breaking, and efficient LLM probing&lt;/li&gt;&lt;li&gt;Provides a platform for evaluating automated red-teaming and safety alignment methods&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Sattvik Sahai', 'Prasoon Goyal', 'Michael Johnston', 'Anna Gottardi', 'Yao Lu', 'Lucy Hu', 'Luke Dai', 'Shaohua Liu', 'Samyuth Sagi', 'Hangjie Shi', 'Desheng Zhang', 'Lavina Vaz', 'Leslie Ball', 'Maureen Murray', 'Rahul Gupta', 'Shankar Ananthakrishna']&lt;/li&gt;&lt;li&gt;Tags: ['red teaming', 'jailbreaking', 'safety alignment', 'adversarial prompting', 'LLM security']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2508.10108</guid><pubDate>Fri, 15 Aug 2025 00:00:00 +0000</pubDate></item></channel></rss>