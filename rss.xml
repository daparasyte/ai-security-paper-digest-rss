<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>AI Security Paper Digest</title><link>https://kentaroh-toyoda.github.io/ai-security-paper-digest-rss/rss.xml</link><description>Curated papers on AI security from ArXiv and ACL</description><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><lastBuildDate>Mon, 29 Sep 2025 22:46:25 +0000</lastBuildDate><item><title>Unstable Unlearning: The Hidden Risk of Concept Resurgence in Diffusion Models</title><link>https://arxiv.org/abs/2410.08074</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;This paper demonstrates a vulnerability in text-to-image diffusion models where previously unlearned concepts can resurge after fine-tuning.&lt;/li&gt;&lt;li&gt;The study focuses on concept resurgence in Stable Diffusion models.&lt;/li&gt;&lt;li&gt;The findings highlight issues with incremental model updates and safety/alignment.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Vinith M. Suriyakumar', 'Rohan Alur', 'Ayush Sekhari', 'Manish Raghavan', 'Ashia C. Wilson']&lt;/li&gt;&lt;li&gt;Tags: ['data poisoning', 'safety evaluation', 'robustness', 'alignment']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2410.08074</guid><pubDate>Mon, 29 Sep 2025 00:00:00 +0000</pubDate></item><item><title>Closing the Safety Gap: Surgical Concept Erasure in Visual Autoregressive Models</title><link>https://arxiv.org/abs/2509.22400</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes VARE and S-VARE frameworks for concept erasure in VAR models&lt;/li&gt;&lt;li&gt;Addresses safety concerns in text-to-image generation&lt;/li&gt;&lt;li&gt;Uses auxiliary visual tokens and filtered loss for minimal adjustments&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Xinhao Zhong', 'Yimin Zhou', 'Zhiqi Zhang', 'Junhao Li', 'Yi Sun', 'Bin Chen', 'Shu-Tao Xia', 'Ke Xu']&lt;/li&gt;&lt;li&gt;Tags: ['safety', 'concept erasure', 'visual autoregressive models', 'text-to-image generation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.22400</guid><pubDate>Mon, 29 Sep 2025 00:00:00 +0000</pubDate></item><item><title>Text Adversarial Attacks with Dynamic Outputs</title><link>https://arxiv.org/abs/2509.22393</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces TDOA method for text adversarial attacks in dynamic output scenarios&lt;/li&gt;&lt;li&gt;Uses clustering-based surrogate model to handle dynamic outputs&lt;/li&gt;&lt;li&gt;Proposes farthest-label targeted attack strategy&lt;/li&gt;&lt;li&gt;Evaluates on multiple datasets and models, including ChatGPT versions&lt;/li&gt;&lt;li&gt;Extends to generative tasks like translation&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Wenqiang Wang', 'Siyuan Liang', 'Xiao Yan', 'Xiaochun Cao']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial attacks', 'text', 'dynamic outputs', 'clustering', 'targeted attacks']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.22393</guid><pubDate>Mon, 29 Sep 2025 00:00:00 +0000</pubDate></item><item><title>Jailbreaking on Text-to-Video Models via Scene Splitting Strategy</title><link>https://arxiv.org/abs/2509.22292</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces SceneSplit, a black-box jailbreak method for T2V models&lt;/li&gt;&lt;li&gt;Uses scene splitting to manipulate generative output space&lt;/li&gt;&lt;li&gt;Iterative scene manipulation to bypass safety filters&lt;/li&gt;&lt;li&gt;High attack success rates on multiple T2V models&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Wonjun Lee', 'Haon Park', 'Doehyeon Lee', 'Bumsub Ham', 'Suhyun Kim']&lt;/li&gt;&lt;li&gt;Tags: ['jailbreaking', 'text-to-video', 'adversarial prompting', 'safety evaluation', 'red teaming']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.22292</guid><pubDate>Mon, 29 Sep 2025 00:00:00 +0000</pubDate></item><item><title>JaiLIP: Jailbreaking Vision-Language Models via Loss Guided Image Perturbation</title><link>https://arxiv.org/abs/2509.21401</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes JaiLIP, a method to jailbreak VLMs using loss-guided image perturbation&lt;/li&gt;&lt;li&gt;Aims to generate imperceptible adversarial images that produce harmful outputs&lt;/li&gt;&lt;li&gt;Evaluates using toxicity metrics and a transportation domain test case&lt;/li&gt;&lt;li&gt;Highlights need for defense mechanisms against image-based attacks&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Md Jueal Mia', 'M. Hadi Amini']&lt;/li&gt;&lt;li&gt;Tags: ['jailbreaking', 'adversarial prompting', 'image perturbation', 'VLM security', 'safety evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.21401</guid><pubDate>Mon, 29 Sep 2025 00:00:00 +0000</pubDate></item><item><title>Multimodal Prompt Decoupling Attack on the Safety Filters in Text-to-Image Models</title><link>https://arxiv.org/abs/2509.21360</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes MPDA, a multimodal jailbreak attack on T2I models' safety filters&lt;/li&gt;&lt;li&gt;Uses LLM to decouple unsafe prompts into pseudo-safe and harmful parts&lt;/li&gt;&lt;li&gt;Rewrites harmful prompts into adversarial prompts to bypass filters&lt;/li&gt;&lt;li&gt;Utilizes VLM captions for semantic consistency&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Xingkai Peng', 'Jun Jiang', 'Meng Tong', 'Shuai Li', 'Weiming Zhang', 'Nenghai Yu', 'Kejiang Chen']&lt;/li&gt;&lt;li&gt;Tags: ['jailbreaking', 'adversarial prompting', 'multimodal', 'safety filters', 'text-to-image']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.21360</guid><pubDate>Mon, 29 Sep 2025 00:00:00 +0000</pubDate></item><item><title>EigenBench: A Comparative Behavioral Measure of Value Alignment</title><link>https://arxiv.org/abs/2509.01938</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;EigenBench is a black-box method for benchmarking language models' value alignment&lt;/li&gt;&lt;li&gt;Uses EigenTrust to aggregate model judgments&lt;/li&gt;&lt;li&gt;Validated against human evaluations&lt;/li&gt;&lt;li&gt;Can recover model rankings without ground truth labels&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jonathn Chang', 'Leonhard Piff', 'Suvadip Sana', 'Jasmine X. Li', 'Lionel Levine']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'safety evaluation', 'benchmarking']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.01938</guid><pubDate>Mon, 29 Sep 2025 00:00:00 +0000</pubDate></item><item><title>Detecting and Interpreting NSFW Prompts in Text-to-Image Models through Uncovering Harmful Semantics</title><link>https://arxiv.org/abs/2412.18123</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes HiddenGuard, a framework to detect NSFW prompts in T2I models using hidden states&lt;/li&gt;&lt;li&gt;Utilizes text encoder's hidden states for feature extraction&lt;/li&gt;&lt;li&gt;Achieves high accuracy (&gt;95%) and efficiency in detection&lt;/li&gt;&lt;li&gt;Supports real-time interpretation and data augmentation for optimization&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yiming Wang', 'Jiahao Chen', 'Qingming Li', 'Tong Zhang', 'Rui Zeng', 'Xing Yang', 'Shouling Ji']&lt;/li&gt;&lt;li&gt;Tags: ['safety evaluation', 'adversarial prompting', 'model robustness', 'text-to-image models', 'NSFW detection']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2412.18123</guid><pubDate>Mon, 29 Sep 2025 00:00:00 +0000</pubDate></item><item><title>GEP: A GCG-Based method for extracting personally identifiable information from chatbots built on small language models</title><link>https://arxiv.org/abs/2509.21192</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;The paper proposes GEP, a method for extracting PII from chatbots built on small language models.&lt;/li&gt;&lt;li&gt;GEP uses a greedy coordinate gradient-based approach and outperforms template-based methods.&lt;/li&gt;&lt;li&gt;The study shows significant PII leakage in SLM-based chatbots under various conditions.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jieli Zhu', 'Vi Ngoc-Nha Tran']&lt;/li&gt;&lt;li&gt;Tags: ['data poisoning', 'privacy attacks', 'adversarial prompting', 'model extraction']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.21192</guid><pubDate>Mon, 29 Sep 2025 00:00:00 +0000</pubDate></item><item><title>Learning the Wrong Lessons: Syntactic-Domain Spurious Correlations in Language Models</title><link>https://arxiv.org/abs/2509.21155</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Identifies spurious correlations between syntax and domain in LLMs&lt;/li&gt;&lt;li&gt;Shows these correlations can lower performance on entity knowledge tasks&lt;/li&gt;&lt;li&gt;Introduces evaluation framework to detect the issue&lt;/li&gt;&lt;li&gt;Demonstrates potential for bypassing safety measures via syntactic patterns&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Chantal Shaib', 'Vinith M. Suriyakumar', 'Levent Sagun', 'Byron C. Wallace', 'Marzyeh Ghassemi']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'prompt injection', 'adversarial prompting', 'safety evaluation', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.21155</guid><pubDate>Mon, 29 Sep 2025 00:00:00 +0000</pubDate></item><item><title>QA-LIGN: Aligning LLMs through Constitutionally Decomposed QA</title><link>https://arxiv.org/abs/2506.08123</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces QA-LIGN for aligning LLMs with specific principles&lt;/li&gt;&lt;li&gt;Decomposes rewards into principle-specific evaluations&lt;/li&gt;&lt;li&gt;Reduces attack success rates while maintaining low false refusal&lt;/li&gt;&lt;li&gt;Outperforms DPO and GRPO with state-of-the-art reward models&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jacob Dineen', 'Aswin RRV', 'Qin Liu', 'Zhikun Xu', 'Xiao Ye', 'Ming Shen', 'Zhaonan Li', 'Shijie Lu', 'Chitta Baral', 'Muhao Chen', 'Ben Zhou']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'safety evaluation', 'red teaming', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.08123</guid><pubDate>Mon, 29 Sep 2025 00:00:00 +0000</pubDate></item><item><title>Unlearning Isn't Deletion: Investigating Reversibility of Machine Unlearning in LLMs</title><link>https://arxiv.org/abs/2505.16831</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;This paper explores the reversibility of machine unlearning in LLMs, showing that current metrics may be misleading. It introduces a representation-level analysis framework using PCA, CKA, and Fisher information to better evaluate unlearning. The study identifies four forgetting regimes based on reversibility and catastrophicity, highlighting challenges in achieving irreversible, non-catastrophic forgetting.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Xiaoyu Xu', 'Xiang Yue', 'Yang Liu', 'Qingqing Ye', 'Huadi Zheng', 'Peizhao Hu', 'Minxin Du', 'Haibo Hu']&lt;/li&gt;&lt;li&gt;Tags: ['data poisoning', 'privacy attacks', 'safety evaluation', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2505.16831</guid><pubDate>Mon, 29 Sep 2025 00:00:00 +0000</pubDate></item><item><title>MrGuard: A Multilingual Reasoning Guardrail for Universal LLM Safety</title><link>https://arxiv.org/abs/2504.15241</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces MrGuard, a multilingual guardrail for LLM safety&lt;/li&gt;&lt;li&gt;Uses synthetic data generation, supervised fine-tuning, and GRPO&lt;/li&gt;&lt;li&gt;Outperforms baselines in multilingual safety classification&lt;/li&gt;&lt;li&gt;Evaluates robustness to variations like code-switching&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yahan Yang', 'Soham Dan', 'Shuo Li', 'Dan Roth', 'Insup Lee']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'safety evaluation', 'multilingual', 'guardrail', 'adversarial prompting']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2504.15241</guid><pubDate>Mon, 29 Sep 2025 00:00:00 +0000</pubDate></item><item><title>Vulnerability of LLMs to Vertically Aligned Text Manipulations</title><link>https://arxiv.org/abs/2410.20016</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Investigates vulnerability of LLMs to vertically aligned text&lt;/li&gt;&lt;li&gt;Shows significant accuracy drop in text classification&lt;/li&gt;&lt;li&gt;Analyzes tokenization and attention issues&lt;/li&gt;&lt;li&gt;Tests few-shot learning as a mitigation&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Zhecheng Li', 'Yiwei Wang', 'Bryan Hooi', 'Yujun Cai', 'Zhen Xiong', 'Nanyun Peng', 'Kai-wei Chang']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'adversarial prompting', 'safety evaluation', 'text classification']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2410.20016</guid><pubDate>Mon, 29 Sep 2025 00:00:00 +0000</pubDate></item><item><title>Library Hallucinations in LLMs: Risk Analysis Grounded in Developer Queries</title><link>https://arxiv.org/abs/2509.22202</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Study of library hallucinations in LLM-generated code&lt;/li&gt;&lt;li&gt;Evaluates six LLMs on library name and member hallucinations&lt;/li&gt;&lt;li&gt;Finds high vulnerability to misspellings and fake names&lt;/li&gt;&lt;li&gt;Highlights need for safeguards against hallucinations&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Lukas Twist', 'Jie M. Zhang', 'Mark Harman', 'Helen Yannakoudakis']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'prompt injection', 'adversarial prompting', 'safety evaluation', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.22202</guid><pubDate>Mon, 29 Sep 2025 00:00:00 +0000</pubDate></item><item><title>SecureAgentBench: Benchmarking Secure Code Generation under Realistic Vulnerability Scenarios</title><link>https://arxiv.org/abs/2509.22097</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces SecureAgentBench, a benchmark for evaluating secure code generation by LLM-powered agents.&lt;/li&gt;&lt;li&gt;Includes 105 coding tasks with real-world vulnerability contexts and multi-file edits.&lt;/li&gt;&lt;li&gt;Evaluates agents on functionality, vulnerability presence, and new vulnerability introduction.&lt;/li&gt;&lt;li&gt;Finds current agents perform poorly, with the best achieving only 15.2% correct-and-secure solutions.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Junkai Chen', 'Huihui Huang', 'Yunbo Lyu', 'Junwen An', 'Jieke Shi', 'Chengran Yang', 'Ting Zhang', 'Haoye Tian', 'Yikun Li', 'Zhenhao Li', 'Xin Zhou', 'Xing Hu', 'David Lo']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'security evaluation', 'benchmarking', 'code generation', 'vulnerability detection']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.22097</guid><pubDate>Mon, 29 Sep 2025 00:00:00 +0000</pubDate></item><item><title>You Can't Steal Nothing: Mitigating Prompt Leakages in LLMs via System Vectors</title><link>https://arxiv.org/abs/2509.21884</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces a prompt leaking attack that works on SOTA LLMs like GPT-4o and Claude 3.5&lt;/li&gt;&lt;li&gt;Proposes SysVec, which encodes system prompts as internal vectors to prevent leakage&lt;/li&gt;&lt;li&gt;Shows improved security and instruction-following with reduced forgetting&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Bochuan Cao', 'Changjiang Li', 'Yuanpu Cao', 'Yameng Ge', 'Ting Wang', 'Jinghui Chen']&lt;/li&gt;&lt;li&gt;Tags: ['prompt injection', 'model extraction', 'security', 'red teaming', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.21884</guid><pubDate>Mon, 29 Sep 2025 00:00:00 +0000</pubDate></item><item><title>SBFA: Single Sneaky Bit Flip Attack to Break Large Language Models</title><link>https://arxiv.org/abs/2509.21843</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces SBFA, a single bit flip attack targeting LLMs&lt;/li&gt;&lt;li&gt;Uses ImpactScore metric combining gradient sensitivity and perturbation range&lt;/li&gt;&lt;li&gt;Achieves significant accuracy degradation with minimal stealthy changes&lt;/li&gt;&lt;li&gt;Validated on Qwen, LLaMA, Gemma models in BF16 and INT8 formats&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jingkai Guo', 'Chaitali Chakrabarti', 'Deliang Fan']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial attacks', 'model integrity', 'bit flip attack', 'LLM security', 'stealthy attacks']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.21843</guid><pubDate>Mon, 29 Sep 2025 00:00:00 +0000</pubDate></item><item><title>Towards mitigating information leakage when evaluating safety monitors</title><link>https://arxiv.org/abs/2509.21344</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;The paper addresses information leakage in safety monitors for LLMs, focusing on evaluating monitors' ability to detect genuine behavior rather than artifacts from elicitation prompts.&lt;/li&gt;&lt;li&gt;Proposes three strategies: content filtering, score filtering, and prompt distilled fine-tuned model organisms to mitigate leakage.&lt;/li&gt;&lt;li&gt;Experiments on deception detection benchmarks show significant performance drops when applying these mitigations, indicating inflated monitor performance due to leakage.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Gerard Boxo', 'Aman Neelappa', 'Shivam Raval']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'safety evaluation', 'adversarial prompting', 'model extraction', 'privacy attacks']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.21344</guid><pubDate>Mon, 29 Sep 2025 00:00:00 +0000</pubDate></item><item><title>We Think, Therefore We Align LLMs to Helpful, Harmless and Honest Before They Go Wrong</title><link>https://arxiv.org/abs/2509.22510</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes AMBS for multi-objective alignment of LLMs&lt;/li&gt;&lt;li&gt;Addresses catastrophic forgetting and inference fragmentation&lt;/li&gt;&lt;li&gt;Improves HHH alignment across multiple LLMs&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research Paper&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Gautam Siddharth Kashyap', 'Mark Dras', 'Usman Naseem']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'safety', 'robustness', 'LLM']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.22510</guid><pubDate>Mon, 29 Sep 2025 00:00:00 +0000</pubDate></item><item><title>Mixture of Detectors: A Compact View of Machine-Generated Text Detection</title><link>https://arxiv.org/abs/2509.22147</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces BMAS English dataset for detecting machine-generated text&lt;/li&gt;&lt;li&gt;Covers binary, multiclass classification, generator attribution, adversarial attacks, and sentence-level segmentation&lt;/li&gt;&lt;li&gt;Addresses various aspects of machine-generated text detection&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Sai Teja Lekkala', 'Yadagiri Annepaka', 'Arun Kumar Challa', 'Samatha Reddy Machireddy', 'Partha Pakray', 'Chukhu Chunka']&lt;/li&gt;&lt;li&gt;Tags: ['data poisoning', 'privacy attacks', 'alignment', 'robustness', 'safety evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.22147</guid><pubDate>Mon, 29 Sep 2025 00:00:00 +0000</pubDate></item><item><title>LUMINA: Detecting Hallucinations in RAG System with Context-Knowledge Signals</title><link>https://arxiv.org/abs/2509.21875</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;LUMINA is a framework for detecting hallucinations in RAG systems&lt;/li&gt;&lt;li&gt;Uses context-knowledge signals: external context via distributional distance and internal knowledge via token evolution&lt;/li&gt;&lt;li&gt;Outperforms prior methods by up to +13% AUROC on HalluRAG&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Min-Hsuan Yeh', 'Yixuan Li', 'Tanwi Mallick']&lt;/li&gt;&lt;li&gt;Tags: ['safety', 'robustness', 'hallucination detection', 'RAG systems']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.21875</guid><pubDate>Mon, 29 Sep 2025 00:00:00 +0000</pubDate></item><item><title>EigenBench: A Comparative Behavioral Measure of Value Alignment</title><link>https://arxiv.org/abs/2509.01938</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;EigenBench is a black-box method for benchmarking language models' value alignment&lt;/li&gt;&lt;li&gt;Uses EigenTrust to aggregate model judgments&lt;/li&gt;&lt;li&gt;Validated against human evaluations&lt;/li&gt;&lt;li&gt;Can recover model rankings without ground truth labels&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jonathn Chang', 'Leonhard Piff', 'Suvadip Sana', 'Jasmine X. Li', 'Lionel Levine']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'safety evaluation', 'benchmarking']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.01938</guid><pubDate>Mon, 29 Sep 2025 00:00:00 +0000</pubDate></item><item><title>Unlearning Isn't Deletion: Investigating Reversibility of Machine Unlearning in LLMs</title><link>https://arxiv.org/abs/2505.16831</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;This paper explores the reversibility of machine unlearning in LLMs, showing that current metrics may be misleading. It introduces a representation-level analysis framework using PCA, CKA, and Fisher information to better evaluate unlearning. The study identifies four forgetting regimes based on reversibility and catastrophicity, highlighting challenges in achieving irreversible, non-catastrophic forgetting.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Xiaoyu Xu', 'Xiang Yue', 'Yang Liu', 'Qingqing Ye', 'Huadi Zheng', 'Peizhao Hu', 'Minxin Du', 'Haibo Hu']&lt;/li&gt;&lt;li&gt;Tags: ['data poisoning', 'privacy attacks', 'safety evaluation', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2505.16831</guid><pubDate>Mon, 29 Sep 2025 00:00:00 +0000</pubDate></item><item><title>FORCE: Transferable Visual Jailbreaking Attacks via Feature Over-Reliance CorrEction</title><link>https://arxiv.org/abs/2509.21029</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;The paper introduces FORCE, a method to improve the transferability of visual jailbreaking attacks on multimodal large language models (MLLMs).&lt;/li&gt;&lt;li&gt;It addresses the issue where existing attacks are effective on open-source models but fail on closed-source ones due to high-sharpness regions in the loss landscape.&lt;/li&gt;&lt;li&gt;FORCE corrects over-reliance on specific layer features and frequency components to create more generalizable attacks.&lt;/li&gt;&lt;li&gt;Experiments show improved transferability of visual attacks against closed-source MLLMs.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Runqi Lin', 'Alasdair Paren', 'Suqin Yuan', 'Muyang Li', 'Philip Torr', 'Adel Bibi', 'Tongliang Liu']&lt;/li&gt;&lt;li&gt;Tags: ['red teaming', 'jailbreaking', 'adversarial attacks', 'multimodal models', 'transferability']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.21029</guid><pubDate>Mon, 29 Sep 2025 00:00:00 +0000</pubDate></item><item><title>EigenTrack: Spectral Activation Feature Tracking for Hallucination and Out-of-Distribution Detection in LLMs and VLMs</title><link>https://arxiv.org/abs/2509.15735</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes EigenTrack for detecting hallucinations and OOD errors in LLMs and VLMs&lt;/li&gt;&lt;li&gt;Uses spectral geometry of hidden activations for real-time detection&lt;/li&gt;&lt;li&gt;Tracks covariance-spectrum statistics via a recurrent classifier&lt;/li&gt;&lt;li&gt;Aims to detect issues before surface errors appear with minimal overhead&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Davide Ettori', 'Nastaran Darabi', 'Sina Tayebati', 'Ranganath Krishnan', 'Mahesh Subedar', 'Omesh Tickoo', 'Amit Ranjan Trivedi']&lt;/li&gt;&lt;li&gt;Tags: ['safety', 'robustness', 'detection', 'hallucination', 'out-of-distribution']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.15735</guid><pubDate>Mon, 29 Sep 2025 00:00:00 +0000</pubDate></item><item><title>SecP-Tuning: Efficient Privacy-Preserving Prompt Tuning for Large Language Models via MPC</title><link>https://arxiv.org/abs/2506.15307</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;SecP-Tuning is an MPC-based framework for privacy-preserving prompt tuning of LLMs&lt;/li&gt;&lt;li&gt;Uses Forward-only Tuning to avoid privacy-preserving computations in backward propagation&lt;/li&gt;&lt;li&gt;Introduces efficient privacy-preserving Random Feature Attention&lt;/li&gt;&lt;li&gt;Achieves significant acceleration and communication reduction compared to SFT and prompt tuning&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jinglong Luo', 'Zhuo Zhang', 'Yehong Zhang', 'Shiyu Liu', 'Ye Dong', 'Hui Wang', 'Yue Yu', 'Xun Zhou', 'Zenglin Xu']&lt;/li&gt;&lt;li&gt;Tags: ['privacy', 'secure multi-party computation', 'prompt tuning', 'large language models', 'data privacy']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.15307</guid><pubDate>Mon, 29 Sep 2025 00:00:00 +0000</pubDate></item><item><title>RL-Obfuscation: Can Language Models Learn to Evade Latent-Space Monitors?</title><link>https://arxiv.org/abs/2506.14261</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces RL-Obfuscation to test if LLMs can learn to evade latent-space monitors via RL finetuning&lt;/li&gt;&lt;li&gt;Evaluates evasion success against different monitor types (token-level vs holistic)&lt;/li&gt;&lt;li&gt;Finds token-level monitors are vulnerable while others remain robust&lt;/li&gt;&lt;li&gt;Models can generalize to evade unseen monitors and bypass conditionally&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Rohan Gupta', 'Erik Jenner']&lt;/li&gt;&lt;li&gt;Tags: ['red teaming', 'adversarial prompting', 'model extraction', 'safety evaluation', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.14261</guid><pubDate>Mon, 29 Sep 2025 00:00:00 +0000</pubDate></item><item><title>Unstable Unlearning: The Hidden Risk of Concept Resurgence in Diffusion Models</title><link>https://arxiv.org/abs/2410.08074</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;This paper demonstrates a vulnerability in text-to-image diffusion models where previously unlearned concepts can resurge after fine-tuning.&lt;/li&gt;&lt;li&gt;The study focuses on concept resurgence in Stable Diffusion models.&lt;/li&gt;&lt;li&gt;The findings highlight issues with incremental model updates and safety/alignment.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Vinith M. Suriyakumar', 'Rohan Alur', 'Ayush Sekhari', 'Manish Raghavan', 'Ashia C. Wilson']&lt;/li&gt;&lt;li&gt;Tags: ['data poisoning', 'safety evaluation', 'robustness', 'alignment']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2410.08074</guid><pubDate>Mon, 29 Sep 2025 00:00:00 +0000</pubDate></item><item><title>SBFA: Single Sneaky Bit Flip Attack to Break Large Language Models</title><link>https://arxiv.org/abs/2509.21843</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces SBFA, a single bit flip attack targeting LLMs&lt;/li&gt;&lt;li&gt;Uses ImpactScore metric combining gradient sensitivity and perturbation range&lt;/li&gt;&lt;li&gt;Achieves significant accuracy degradation with minimal stealthy changes&lt;/li&gt;&lt;li&gt;Validated on Qwen, LLaMA, Gemma models in BF16 and INT8 formats&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jingkai Guo', 'Chaitali Chakrabarti', 'Deliang Fan']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial attacks', 'model integrity', 'bit flip attack', 'LLM security', 'stealthy attacks']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.21843</guid><pubDate>Mon, 29 Sep 2025 00:00:00 +0000</pubDate></item><item><title>MobiLLM: An Agentic AI Framework for Closed-Loop Threat Mitigation in 6G Open RANs</title><link>https://arxiv.org/abs/2509.21634</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;MobiLLM is an agentic AI framework for threat mitigation in 6G O-RANs&lt;/li&gt;&lt;li&gt;Uses LLMs in a multi-agent system for analysis, classification, and response&lt;/li&gt;&lt;li&gt;Incorporates safety guardrails and trusted knowledge bases&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Prakhar Sharma', 'Haohuang Wen', 'Vinod Yegneswaran', 'Ashish Gehani', 'Phillip Porras', 'Zhiqiang Lin']&lt;/li&gt;&lt;li&gt;Tags: ['AI security', 'red teaming', 'LLM', 'threat mitigation', 'O-RAN']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.21634</guid><pubDate>Mon, 29 Sep 2025 00:00:00 +0000</pubDate></item><item><title>Towards mitigating information leakage when evaluating safety monitors</title><link>https://arxiv.org/abs/2509.21344</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;The paper addresses information leakage in safety monitors for LLMs, focusing on evaluating monitors' ability to detect genuine behavior rather than artifacts from elicitation prompts.&lt;/li&gt;&lt;li&gt;Proposes three strategies: content filtering, score filtering, and prompt distilled fine-tuned model organisms to mitigate leakage.&lt;/li&gt;&lt;li&gt;Experiments on deception detection benchmarks show significant performance drops when applying these mitigations, indicating inflated monitor performance due to leakage.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Gerard Boxo', 'Aman Neelappa', 'Shivam Raval']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'safety evaluation', 'adversarial prompting', 'model extraction', 'privacy attacks']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.21344</guid><pubDate>Mon, 29 Sep 2025 00:00:00 +0000</pubDate></item><item><title>Erase or Hide? Suppressing Spurious Unlearning Neurons for Robust Unlearning</title><link>https://arxiv.org/abs/2509.22263</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces Ssiuu, a new unlearning method for LLMs to prevent relearning of forgotten knowledge.&lt;/li&gt;&lt;li&gt;Addresses spurious unlearning neurons that hide rather than erase sensitive information.&lt;/li&gt;&lt;li&gt;Evaluates robustness against adversarial data injection and instruction-following benchmarks.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Nakyeong Yang', 'Dong-Kyum Kim', 'Jea Kwon', 'Minsung Kim', 'Kyomin Jung', 'Meeyoung Cha']&lt;/li&gt;&lt;li&gt;Tags: ['unlearning', 'privacy', 'robustness', 'adversarial attacks', 'model security']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.22263</guid><pubDate>Mon, 29 Sep 2025 00:00:00 +0000</pubDate></item><item><title>Countering adversarial evasion in regression analysis</title><link>https://arxiv.org/abs/2509.22113</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a pessimistic bilevel optimization program for regression to counter adversarial evasion&lt;/li&gt;&lt;li&gt;Extends game theoretic models to regression scenarios&lt;/li&gt;&lt;li&gt;Addresses security in applications like spam filtering and malware detection&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['David Benfield', 'Phan Tu Vuong', 'Alain Zemkoho']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial evasion', 'game theory', 'bilevel optimization', 'regression', 'security']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.22113</guid><pubDate>Mon, 29 Sep 2025 00:00:00 +0000</pubDate></item><item><title>The Rogue Scalpel: Activation Steering Compromises LLM Safety</title><link>https://arxiv.org/abs/2509.22067</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;The paper demonstrates that activation steering in LLMs can compromise safety by breaking alignment safeguards.&lt;/li&gt;&lt;li&gt;Random steering increases harmful compliance rates, and benign features from SAEs make it worse.&lt;/li&gt;&lt;li&gt;Combining jailbreak vectors creates a universal attack that affects unseen requests.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Anton Korznikov', 'Andrey Galichin', 'Alexey Dontsov', 'Oleg Y. Rogov', 'Ivan Oseledets', 'Elena Tutubalina']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'jailbreaking', 'adversarial prompting', 'safety evaluation', 'alignment']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.22067</guid><pubDate>Mon, 29 Sep 2025 00:00:00 +0000</pubDate></item><item><title>Active Attacks: Red-teaming LLMs via Adaptive Environments</title><link>https://arxiv.org/abs/2509.21947</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces Active Attacks, an RL-based red-teaming algorithm for LLMs&lt;/li&gt;&lt;li&gt;Uses adaptive environments to generate diverse attack prompts&lt;/li&gt;&lt;li&gt;Improves cross-attack success rates significantly over prior methods&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Taeyoung Yun', 'Pierre-Luc St-Charles', 'Jinkyoo Park', 'Yoshua Bengio', 'Minsu Kim']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'Reinforcement learning', 'Adaptive environments', 'Prompt generation', 'Safety fine-tuning']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.21947</guid><pubDate>Mon, 29 Sep 2025 00:00:00 +0000</pubDate></item><item><title>Preemptive Detection and Steering of LLM Misalignment via Latent Reachability</title><link>https://arxiv.org/abs/2509.21528</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes BRT-Align, a framework using reachability analysis for LLM safety during inference&lt;/li&gt;&lt;li&gt;Combines runtime monitoring and steering to prevent unsafe content generation&lt;/li&gt;&lt;li&gt;Demonstrates improved detection and reduction of unsafe outputs across multiple models and benchmarks&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Sathwik Karnik', 'Somil Bansal']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'safety evaluation', 'robustness', 'inference-time safety', 'reachability analysis']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.21528</guid><pubDate>Mon, 29 Sep 2025 00:00:00 +0000</pubDate></item><item><title>Unlearning Isn't Deletion: Investigating Reversibility of Machine Unlearning in LLMs</title><link>https://arxiv.org/abs/2505.16831</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;This paper explores the reversibility of machine unlearning in LLMs, showing that current metrics may be misleading. It introduces a representation-level analysis framework using PCA, CKA, and Fisher information to better evaluate unlearning. The study identifies four forgetting regimes based on reversibility and catastrophicity, highlighting challenges in achieving irreversible, non-catastrophic forgetting.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Xiaoyu Xu', 'Xiang Yue', 'Yang Liu', 'Qingqing Ye', 'Huadi Zheng', 'Peizhao Hu', 'Minxin Du', 'Haibo Hu']&lt;/li&gt;&lt;li&gt;Tags: ['data poisoning', 'privacy attacks', 'safety evaluation', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2505.16831</guid><pubDate>Mon, 29 Sep 2025 00:00:00 +0000</pubDate></item><item><title>EigenBench: A Comparative Behavioral Measure of Value Alignment</title><link>https://arxiv.org/abs/2509.01938</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;EigenBench is a black-box method for benchmarking language models' value alignment&lt;/li&gt;&lt;li&gt;Uses EigenTrust to aggregate model judgments&lt;/li&gt;&lt;li&gt;Validated against human evaluations&lt;/li&gt;&lt;li&gt;Can recover model rankings without ground truth labels&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jonathn Chang', 'Leonhard Piff', 'Suvadip Sana', 'Jasmine X. Li', 'Lionel Levine']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'safety evaluation', 'benchmarking']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.01938</guid><pubDate>Mon, 29 Sep 2025 00:00:00 +0000</pubDate></item><item><title>A Domain-Agnostic Scalable AI Safety Ensuring Framework</title><link>https://arxiv.org/abs/2504.20924</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a domain-agnostic AI safety framework with theoretical guarantees&lt;/li&gt;&lt;li&gt;Includes optimization with chance constraints, safety classification, and continuous loss functions&lt;/li&gt;&lt;li&gt;Establishes a scaling law for safety-performance trade-offs&lt;/li&gt;&lt;li&gt;Validates with experiments in RL, NLP, and production planning&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Beomjun Kim', 'Kangyeon Kim', 'Sunwoo Kim', 'Yeonsang Shin', 'Heejin Ahn']&lt;/li&gt;&lt;li&gt;Tags: ['safety', 'theoretical', 'domain-agnostic', 'scaling law', 'empirical validation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2504.20924</guid><pubDate>Mon, 29 Sep 2025 00:00:00 +0000</pubDate></item><item><title>Jailbreaking on Text-to-Video Models via Scene Splitting Strategy</title><link>https://arxiv.org/abs/2509.22292</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces SceneSplit, a black-box jailbreak method for T2V models&lt;/li&gt;&lt;li&gt;Uses scene splitting to manipulate generative output space&lt;/li&gt;&lt;li&gt;Iterative scene manipulation to bypass safety filters&lt;/li&gt;&lt;li&gt;High attack success rates on multiple T2V models&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Wonjun Lee', 'Haon Park', 'Doehyeon Lee', 'Bumsub Ham', 'Suhyun Kim']&lt;/li&gt;&lt;li&gt;Tags: ['jailbreaking', 'text-to-video', 'adversarial prompting', 'safety evaluation', 'red teaming']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.22292</guid><pubDate>Mon, 29 Sep 2025 00:00:00 +0000</pubDate></item><item><title>SecureAgentBench: Benchmarking Secure Code Generation under Realistic Vulnerability Scenarios</title><link>https://arxiv.org/abs/2509.22097</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces SecureAgentBench, a benchmark for evaluating secure code generation by LLM-powered agents.&lt;/li&gt;&lt;li&gt;Includes 105 coding tasks with real-world vulnerability contexts and multi-file edits.&lt;/li&gt;&lt;li&gt;Evaluates agents on functionality, vulnerability presence, and new vulnerability introduction.&lt;/li&gt;&lt;li&gt;Finds current agents perform poorly, with the best achieving only 15.2% correct-and-secure solutions.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Junkai Chen', 'Huihui Huang', 'Yunbo Lyu', 'Junwen An', 'Jieke Shi', 'Chengran Yang', 'Ting Zhang', 'Haoye Tian', 'Yikun Li', 'Zhenhao Li', 'Xin Zhou', 'Xing Hu', 'David Lo']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'security evaluation', 'benchmarking', 'code generation', 'vulnerability detection']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.22097</guid><pubDate>Mon, 29 Sep 2025 00:00:00 +0000</pubDate></item><item><title>The Rogue Scalpel: Activation Steering Compromises LLM Safety</title><link>https://arxiv.org/abs/2509.22067</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;The paper demonstrates that activation steering in LLMs can compromise safety by breaking alignment safeguards.&lt;/li&gt;&lt;li&gt;Random steering increases harmful compliance rates, and benign features from SAEs make it worse.&lt;/li&gt;&lt;li&gt;Combining jailbreak vectors creates a universal attack that affects unseen requests.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Anton Korznikov', 'Andrey Galichin', 'Alexey Dontsov', 'Oleg Y. Rogov', 'Ivan Oseledets', 'Elena Tutubalina']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'jailbreaking', 'adversarial prompting', 'safety evaluation', 'alignment']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.22067</guid><pubDate>Mon, 29 Sep 2025 00:00:00 +0000</pubDate></item><item><title>Decoding Deception: Understanding Automatic Speech Recognition Vulnerabilities in Evasion and Poisoning Attacks</title><link>https://arxiv.org/abs/2509.22060</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;The paper explores white-box and black-box adversarial attacks on Automatic Speech Recognition (ASR) systems.&lt;/li&gt;&lt;li&gt;It introduces cost-efficient methods using Fast Gradient Sign Method and Zeroth-Order Optimization.&lt;/li&gt;&lt;li&gt;The study also examines poisoning attacks that can degrade ASR model performance.&lt;/li&gt;&lt;li&gt;Experiments show that hybrid models can generate subtle adversarial examples with minimal perturbation.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Aravindhan G', 'Yuvaraj Govindarajulu', 'Parin Shah']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial attacks', 'speech recognition', 'poisoning attacks', 'security', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.22060</guid><pubDate>Mon, 29 Sep 2025 00:00:00 +0000</pubDate></item><item><title>Active Attacks: Red-teaming LLMs via Adaptive Environments</title><link>https://arxiv.org/abs/2509.21947</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces Active Attacks, an RL-based red-teaming algorithm for LLMs&lt;/li&gt;&lt;li&gt;Uses adaptive environments to generate diverse attack prompts&lt;/li&gt;&lt;li&gt;Improves cross-attack success rates significantly over prior methods&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Taeyoung Yun', 'Pierre-Luc St-Charles', 'Jinkyoo Park', 'Yoshua Bengio', 'Minsu Kim']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'Reinforcement learning', 'Adaptive environments', 'Prompt generation', 'Safety fine-tuning']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.21947</guid><pubDate>Mon, 29 Sep 2025 00:00:00 +0000</pubDate></item><item><title>You Can't Steal Nothing: Mitigating Prompt Leakages in LLMs via System Vectors</title><link>https://arxiv.org/abs/2509.21884</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces a prompt leaking attack that works on SOTA LLMs like GPT-4o and Claude 3.5&lt;/li&gt;&lt;li&gt;Proposes SysVec, which encodes system prompts as internal vectors to prevent leakage&lt;/li&gt;&lt;li&gt;Shows improved security and instruction-following with reduced forgetting&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Bochuan Cao', 'Changjiang Li', 'Yuanpu Cao', 'Yameng Ge', 'Ting Wang', 'Jinghui Chen']&lt;/li&gt;&lt;li&gt;Tags: ['prompt injection', 'model extraction', 'security', 'red teaming', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.21884</guid><pubDate>Mon, 29 Sep 2025 00:00:00 +0000</pubDate></item><item><title>Backdoor Attribution: Elucidating and Controlling Backdoor in Language Models</title><link>https://arxiv.org/abs/2509.21761</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces Backdoor Attribution (BkdAttr) framework for analyzing LLM backdoors&lt;/li&gt;&lt;li&gt;Develops Backdoor Attention Head Attribution (BAHA) to identify critical attention heads&lt;/li&gt;&lt;li&gt;Shows ablating ~3% of heads reduces ASR by over 90%&lt;/li&gt;&lt;li&gt;Creates Backdoor Vector for controlling backdoor activation&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Miao Yu', 'Zhenhong Zhou', 'Moayad Aloqaily', 'Kun Wang', 'Biwei Huang', 'Stephen Wang', 'Yueming Jin', 'Qingsong Wen']&lt;/li&gt;&lt;li&gt;Tags: ['backdoor attacks', 'data poisoning', 'interpretability', 'attention mechanisms', 'model robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.21761</guid><pubDate>Mon, 29 Sep 2025 00:00:00 +0000</pubDate></item><item><title>MobiLLM: An Agentic AI Framework for Closed-Loop Threat Mitigation in 6G Open RANs</title><link>https://arxiv.org/abs/2509.21634</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;MobiLLM is an agentic AI framework for threat mitigation in 6G O-RANs&lt;/li&gt;&lt;li&gt;Uses LLMs in a multi-agent system for analysis, classification, and response&lt;/li&gt;&lt;li&gt;Incorporates safety guardrails and trusted knowledge bases&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Prakhar Sharma', 'Haohuang Wen', 'Vinod Yegneswaran', 'Ashish Gehani', 'Phillip Porras', 'Zhiqiang Lin']&lt;/li&gt;&lt;li&gt;Tags: ['AI security', 'red teaming', 'LLM', 'threat mitigation', 'O-RAN']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.21634</guid><pubDate>Mon, 29 Sep 2025 00:00:00 +0000</pubDate></item><item><title>Preemptive Detection and Steering of LLM Misalignment via Latent Reachability</title><link>https://arxiv.org/abs/2509.21528</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes BRT-Align, a framework using reachability analysis for LLM safety during inference&lt;/li&gt;&lt;li&gt;Combines runtime monitoring and steering to prevent unsafe content generation&lt;/li&gt;&lt;li&gt;Demonstrates improved detection and reduction of unsafe outputs across multiple models and benchmarks&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Sathwik Karnik', 'Somil Bansal']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'safety evaluation', 'robustness', 'inference-time safety', 'reachability analysis']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.21528</guid><pubDate>Mon, 29 Sep 2025 00:00:00 +0000</pubDate></item><item><title>Design and Implementation of a Secure RAG-Enhanced AI Chatbot for Smart Tourism Customer Service: Defending Against Prompt Injection Attacks -- A Case Study of Hsinchu, Taiwan</title><link>https://arxiv.org/abs/2509.21367</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Presents a secure RAG-enhanced chatbot for smart tourism&lt;/li&gt;&lt;li&gt;Focuses on defending against prompt injection attacks&lt;/li&gt;&lt;li&gt;Includes multi-layered linguistic analysis and guardrails&lt;/li&gt;&lt;li&gt;Evaluates with adversarial prompts and benign queries&lt;/li&gt;&lt;li&gt;Compares with GPT-5's inherent robustness&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yu-Kai Shih', 'You-Kai Kang']&lt;/li&gt;&lt;li&gt;Tags: ['red teaming', 'prompt injection', 'adversarial prompting', 'safety evaluation', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.21367</guid><pubDate>Mon, 29 Sep 2025 00:00:00 +0000</pubDate></item><item><title>Multimodal Prompt Decoupling Attack on the Safety Filters in Text-to-Image Models</title><link>https://arxiv.org/abs/2509.21360</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes MPDA, a multimodal jailbreak attack on T2I models' safety filters&lt;/li&gt;&lt;li&gt;Uses LLM to decouple unsafe prompts into pseudo-safe and harmful parts&lt;/li&gt;&lt;li&gt;Rewrites harmful prompts into adversarial prompts to bypass filters&lt;/li&gt;&lt;li&gt;Utilizes VLM captions for semantic consistency&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Xingkai Peng', 'Jun Jiang', 'Meng Tong', 'Shuai Li', 'Weiming Zhang', 'Nenghai Yu', 'Kejiang Chen']&lt;/li&gt;&lt;li&gt;Tags: ['jailbreaking', 'adversarial prompting', 'multimodal', 'safety filters', 'text-to-image']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.21360</guid><pubDate>Mon, 29 Sep 2025 00:00:00 +0000</pubDate></item><item><title>Benchmarking MLLM-based Web Understanding: Reasoning, Robustness and Safety</title><link>https://arxiv.org/abs/2509.21782</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces WebRSSBench, a benchmark for evaluating MLLMs on web understanding tasks focusing on reasoning, robustness, and safety.&lt;/li&gt;&lt;li&gt;Evaluates 12 MLLMs across 8 tasks, revealing gaps in compositional reasoning, UI robustness, and safety detection.&lt;/li&gt;&lt;li&gt;Highlights the need for standardized evaluation in web-related AI applications.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Junliang Liu', 'Jingyu Xiao', 'Wenxin Tang', 'Wenxuan Wang', 'Zhixian Wang', 'Minrui Zhang', 'Shuanghe Yu']&lt;/li&gt;&lt;li&gt;Tags: ['benchmarking', 'safety evaluation', 'robustness', 'multimodal models', 'web applications']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.21782</guid><pubDate>Mon, 29 Sep 2025 00:00:00 +0000</pubDate></item><item><title>Can AI Perceive Physical Danger and Intervene?</title><link>https://arxiv.org/abs/2509.21651</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Develops a benchmark for physical safety in Embodied AI&lt;/li&gt;&lt;li&gt;Analyzes foundation models' ability to perceive risks and trigger interventions&lt;/li&gt;&lt;li&gt;Introduces a post-training paradigm for safety reasoning&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research Paper&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Abhishek Jindal', 'Dmitry Kalashnikov', 'Oscar Chang', 'Divya Garikapati', 'Anirudha Majumdar', 'Pierre Sermanet', 'Vikas Sindhwani']&lt;/li&gt;&lt;li&gt;Tags: ['safety evaluation', 'alignment', 'robustness', 'physical safety', 'embodied AI']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.21651</guid><pubDate>Mon, 29 Sep 2025 00:00:00 +0000</pubDate></item><item><title>Towards mitigating information leakage when evaluating safety monitors</title><link>https://arxiv.org/abs/2509.21344</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;The paper addresses information leakage in safety monitors for LLMs, focusing on evaluating monitors' ability to detect genuine behavior rather than artifacts from elicitation prompts.&lt;/li&gt;&lt;li&gt;Proposes three strategies: content filtering, score filtering, and prompt distilled fine-tuned model organisms to mitigate leakage.&lt;/li&gt;&lt;li&gt;Experiments on deception detection benchmarks show significant performance drops when applying these mitigations, indicating inflated monitor performance due to leakage.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Gerard Boxo', 'Aman Neelappa', 'Shivam Raval']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'safety evaluation', 'adversarial prompting', 'model extraction', 'privacy attacks']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.21344</guid><pubDate>Mon, 29 Sep 2025 00:00:00 +0000</pubDate></item></channel></rss>