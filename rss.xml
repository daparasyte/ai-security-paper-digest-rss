<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>AI Security Paper Digest</title><link>https://kentaroh-toyoda.github.io/ai-security-paper-digest-rss/rss.xml</link><description>Curated papers on AI security from ArXiv and ACL</description><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><lastBuildDate>Tue, 07 Oct 2025 22:40:49 +0000</lastBuildDate><item><title>Erased, But Not Forgotten: Erased Rectified Flow Transformers Still Remain Unsafe Under Concept Attack</title><link>https://arxiv.org/abs/2510.00635</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces ReFlux, a concept attack method for rectified flow transformers like Flux&lt;/li&gt;&lt;li&gt;Targets attention localization in concept erasure techniques&lt;/li&gt;&lt;li&gt;Uses reverse-attention optimization, velocity-guided dynamics, and consistency preservation&lt;/li&gt;&lt;li&gt;Aims to reactivate suppressed signals and test robustness of erasure methods&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Nanxiang Jiang', 'Zhaoxin Fan', 'Enhan Kang', 'Daiheng Gao', 'Yun Zhou', 'Yanxia Chang', 'Zheng Zhu', 'Yeying Jin', 'Wenjun Wu']&lt;/li&gt;&lt;li&gt;Tags: ['red teaming', 'concept erasure', 'text-to-image models', 'rectified flow', 'attention localization']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.00635</guid><pubDate>Tue, 07 Oct 2025 00:00:00 +0000</pubDate></item><item><title>TRCE: Towards Reliable Malicious Concept Erasure in Text-to-Image Diffusion Models</title><link>https://arxiv.org/abs/2503.07389</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes TRCE, a two-stage concept erasure method for text-to-image models&lt;/li&gt;&lt;li&gt;Aims to prevent generation of malicious content by erasing unsafe concepts&lt;/li&gt;&lt;li&gt;Uses cross-attention optimization and contrastive learning in denoising steps&lt;/li&gt;&lt;li&gt;Evaluated on benchmarks showing improved erasure and preserved generation&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Ruidong Chen', 'Honglin Guo', 'Lanjun Wang', 'Chenyu Zhang', 'Weizhi Nie', 'An-An Liu']&lt;/li&gt;&lt;li&gt;Tags: ['safety', 'concept erasure', 'text-to-image', 'malicious content', 'diffusion models']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2503.07389</guid><pubDate>Tue, 07 Oct 2025 00:00:00 +0000</pubDate></item><item><title>SteerDiff: Steering towards Safe Text-to-Image Diffusion Models</title><link>https://arxiv.org/abs/2410.02710</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces SteerDiff, a lightweight adaptor module for text-to-image diffusion models to prevent inappropriate content generation.&lt;/li&gt;&lt;li&gt;Uses text embedding manipulation to guide models away from harmful outputs.&lt;/li&gt;&lt;li&gt;Evaluates effectiveness against red-teaming strategies and concept unlearning tasks.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Hongxiang Zhang', 'Yifeng He', 'Hao Chen']&lt;/li&gt;&lt;li&gt;Tags: ['red teaming', 'safety evaluation', 'text-to-image generation', 'content moderation', 'diffusion models']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2410.02710</guid><pubDate>Tue, 07 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Revoking Amnesia: RL-based Trajectory Optimization to Resurrect Erased Concepts in Diffusion Models</title><link>https://arxiv.org/abs/2510.03302</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;This paper introduces RevAm, an RL-based framework to reverse concept erasure in diffusion models.&lt;/li&gt;&lt;li&gt;It shows that existing erasure methods are reversible, highlighting security vulnerabilities.&lt;/li&gt;&lt;li&gt;The method uses trajectory optimization to steer the denoising process without modifying model weights.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Daiheng Gao', 'Nanxiang Jiang', 'Andi Zhang', 'Shilin Lu', 'Yufei Tang', 'Wenbo Zhou', 'Weiming Zhang', 'Zhaoxin Fan']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial prompting', 'model extraction', 'safety evaluation', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.03302</guid><pubDate>Tue, 07 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Is It Thinking or Cheating? Detecting Implicit Reward Hacking by Measuring Reasoning Effort</title><link>https://arxiv.org/abs/2510.01367</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces TRACE (Truncated Reasoning AUC Evaluation) to detect implicit reward hacking in reasoning models.&lt;/li&gt;&lt;li&gt;Measures reasoning effort by truncating CoT and evaluating reward at different lengths.&lt;/li&gt;&lt;li&gt;Shows significant gains over existing monitors in math and coding tasks.&lt;/li&gt;&lt;li&gt;Can discover unknown loopholes during training.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Xinpeng Wang', 'Nitish Joshi', 'Barbara Plank', 'Rico Angell', 'He He']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'safety evaluation', 'robustness', 'red teaming']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.01367</guid><pubDate>Tue, 07 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Contextual Integrity in LLMs via Reasoning and Reinforcement Learning</title><link>https://arxiv.org/abs/2506.04245</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;The paper addresses contextual integrity (CI) in LLMs using reasoning and reinforcement learning.&lt;/li&gt;&lt;li&gt;It introduces a method to reduce inappropriate information disclosure while maintaining task performance.&lt;/li&gt;&lt;li&gt;The approach is tested on a synthetic dataset and transfers to established benchmarks like PrivacyLens.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Guangchen Lan', 'Huseyin A. Inan', 'Sahar Abdelnabi', 'Janardhan Kulkarni', 'Lukas Wutschitz', 'Reza Shokri', 'Christopher G. Brinton', 'Robert Sim']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'safety evaluation', 'privacy attacks', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.04245</guid><pubDate>Tue, 07 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Rethinking Exact Unlearning under Exposure: Extracting Forgotten Data under Exact Unlearning in Large Language Model</title><link>https://arxiv.org/abs/2505.24379</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces a data extraction attack on exact unlearning in LLMs&lt;/li&gt;&lt;li&gt;Leverages pre- and post-unlearning logits to extract forgotten data&lt;/li&gt;&lt;li&gt;Doubles extraction success rates in some cases&lt;/li&gt;&lt;li&gt;Highlights privacy risks in real-world deployments&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Xiaoyu Wu', 'Yifei Pang', 'Terrance Liu', 'Zhiwei Steven Wu']&lt;/li&gt;&lt;li&gt;Tags: ['data poisoning', 'privacy attacks', 'model extraction', 'unlearning', 'adversarial prompting']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2505.24379</guid><pubDate>Tue, 07 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Safety Subspaces are Not Linearly Distinct: A Fine-Tuning Case Study</title><link>https://arxiv.org/abs/2505.14185</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;The paper studies whether safety-relevant behavior in LLMs is isolated in specific subspaces.&lt;/li&gt;&lt;li&gt;It finds that safety subspaces are not linearly distinct from general learning components.&lt;/li&gt;&lt;li&gt;The research suggests subspace-based defenses have limitations and alternative strategies are needed.&lt;/li&gt;&lt;li&gt;Experiments were conducted on five open-source LLMs from Llama and Qwen families.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Kaustubh Ponkshe', 'Shaan Shah', 'Raghav Singhal', 'Praneeth Vepakomma']&lt;/li&gt;&lt;li&gt;Tags: ['LLM safety', 'alignment', 'subspace analysis', 'fine-tuning', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2505.14185</guid><pubDate>Tue, 07 Oct 2025 00:00:00 +0000</pubDate></item><item><title>A Statistical Hypothesis Testing Framework for Data Misappropriation Detection in Large Language Models</title><link>https://arxiv.org/abs/2501.02441</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a statistical framework for detecting data misappropriation in LLMs&lt;/li&gt;&lt;li&gt;Uses watermarks in training data and hypothesis testing&lt;/li&gt;&lt;li&gt;Controls type I/II errors and shows asymptotic optimality&lt;/li&gt;&lt;li&gt;Validates with numerical experiments&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yinpeng Cai', 'Lexin Li', 'Linjun Zhang']&lt;/li&gt;&lt;li&gt;Tags: ['data poisoning', 'privacy attacks', 'safety evaluation', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2501.02441</guid><pubDate>Tue, 07 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Silent Tokens, Loud Effects: Padding in LLMs</title><link>https://arxiv.org/abs/2510.01238</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Study on padding tokens in LLMs and their impact on robustness&lt;/li&gt;&lt;li&gt;Evaluated across activations, generation quality, bias, and safety&lt;/li&gt;&lt;li&gt;Found that padding can influence computation and weaken safety guardrails&lt;/li&gt;&lt;li&gt;Emphasizes the need for careful handling of padding in LLM deployments&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Rom Himelstein', 'Amit LeVi', 'Yonatan Belinkov', 'Avi Mendelson']&lt;/li&gt;&lt;li&gt;Tags: ['robustness', 'safety evaluation', 'LLM', 'padding tokens', 'implementation errors']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.01238</guid><pubDate>Tue, 07 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Reasoning over Boundaries: Enhancing Specification Alignment via Test-time Deliberation</title><link>https://arxiv.org/abs/2509.14760</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces Align3, a method using Test-Time Deliberation to enhance specification alignment in LLMs&lt;/li&gt;&lt;li&gt;Presents SpecBench, a benchmark for evaluating specification alignment across multiple scenarios&lt;/li&gt;&lt;li&gt;Demonstrates that test-time deliberation improves safety and behavioral specifications&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Haoran Zhang', 'Yafu Li', 'Xuyang Hu', 'Dongrui Liu', 'Zhilin Wang', 'Bo Li', 'Yu Cheng']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'safety evaluation', 'benchmarking', 'red teaming', 'specification alignment']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.14760</guid><pubDate>Tue, 07 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Large Language Model Hacking: Quantifying the Hidden Risks of Using LLMs for Text Annotation</title><link>https://arxiv.org/abs/2509.08825</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;The paper quantifies risks of using LLMs for text annotation, including intentional and accidental manipulation leading to statistical errors.&lt;/li&gt;&lt;li&gt;Replicates 37 tasks from social science studies showing prompt paraphrasing can lead to false significance.&lt;/li&gt;&lt;li&gt;Analyzes 13 million labels across 18 LLMs and 2361 hypotheses, finding high error rates.&lt;/li&gt;&lt;li&gt;Tests 21 mitigation techniques, highlighting human annotations and regression corrections.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Joachim Baumann', 'Paul R\\"ottger', 'Aleksandra Urman', 'Albert Wendsj\\"o', 'Flor Miriam Plaza-del-Arco', 'Johannes B. Gruber', 'Dirk Hovy']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'prompt injection', 'adversarial prompting', 'safety evaluation', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.08825</guid><pubDate>Tue, 07 Oct 2025 00:00:00 +0000</pubDate></item><item><title>X-Teaming Evolutionary M2S: Automated Discovery of Multi-turn to Single-turn Jailbreak Templates</title><link>https://arxiv.org/abs/2509.08729</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Automated framework for discovering M2S jailbreak templates using language-model-guided evolution&lt;/li&gt;&lt;li&gt;Achieves 44.8% success on GPT-4.1 with two new template families&lt;/li&gt;&lt;li&gt;Emphasizes cross-model evaluation and threshold calibration&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Hyunjun Kim', 'Junwoo Ha', 'Sangyoon Yu', 'Haon Park']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'jailbreaking', 'prompt injection', 'adversarial prompting', 'model extraction']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.08729</guid><pubDate>Tue, 07 Oct 2025 00:00:00 +0000</pubDate></item><item><title>ObjexMT: Objective Extraction and Metacognitive Calibration for LLM-as-a-Judge under Multi-Turn Jailbreaks</title><link>https://arxiv.org/abs/2508.16889</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces ObjexMT benchmark for evaluating LLMs' ability to extract objectives and metacognitive calibration in multi-turn jailbreak scenarios.&lt;/li&gt;&lt;li&gt;Assesses models' accuracy in objective extraction and their confidence calibration.&lt;/li&gt;&lt;li&gt;Highlights challenges in automated obfuscation and high-confidence errors across different models.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Hyunjun Kim', 'Junwoo Ha', 'Sangyoon Yu', 'Haon Park']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'jailbreaking', 'safety evaluation', 'benchmarking', 'metacognition']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2508.16889</guid><pubDate>Tue, 07 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Adversarial Defence without Adversarial Defence: Enhancing Language Model Robustness via Instance-level Principal Component Removal</title><link>https://arxiv.org/abs/2507.21750</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a method to enhance adversarial robustness of PLMs by removing instance-level principal components&lt;/li&gt;&lt;li&gt;Does not rely on conventional adversarial defense or data perturbation&lt;/li&gt;&lt;li&gt;Transforms embedding space to reduce susceptibility to adversarial noise&lt;/li&gt;&lt;li&gt;Evaluates on eight datasets showing improved robustness and maintained accuracy&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yang Wang', 'Chenghao Xiao', 'Yizhi Li', 'Stuart E. Middleton', 'Noura Al Moubayed', 'Chenghua Lin']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial robustness', 'language models', 'embedding space transformation', 'principal component removal']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2507.21750</guid><pubDate>Tue, 07 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Self-Correction Bench: Uncovering and Addressing the Self-Correction Blind Spot in Large Language Models</title><link>https://arxiv.org/abs/2507.02778</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces Self-Correction Bench to evaluate LLMs' ability to correct their own errors&lt;/li&gt;&lt;li&gt;Identifies the Self-Correction Blind Spot where LLMs can't correct their own errors but can correct others'&lt;/li&gt;&lt;li&gt;Finds that adding a 'Wait' prompt significantly reduces the blind spot&lt;/li&gt;&lt;li&gt;Suggests training data and RL may influence self-correction ability&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Ken Tsui']&lt;/li&gt;&lt;li&gt;Tags: ['safety evaluation', 'LLM red teaming', 'alignment', 'robustness', 'prompt injection']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2507.02778</guid><pubDate>Tue, 07 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Revisiting Backdoor Attacks on LLMs: A Stealthy and Practical Poisoning Framework via Harmless Inputs</title><link>https://arxiv.org/abs/2505.17601</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a new backdoor attack on LLMs using harmless data&lt;/li&gt;&lt;li&gt;Aims to establish trigger associations with benign QA pairs&lt;/li&gt;&lt;li&gt;Addresses limitations of previous attacks regarding safety alignment and detection&lt;/li&gt;&lt;li&gt;Includes experiments on various LLMs and guardrail models&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jiawei Kong', 'Hao Fang', 'Xiaochen Yang', 'Kuofeng Gao', 'Bin Chen', 'Shu-Tao Xia', 'Ke Xu', 'Han Qiu']&lt;/li&gt;&lt;li&gt;Tags: ['data poisoning', 'backdoor attacks', 'LLM security', 'adversarial training', 'safety alignment']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2505.17601</guid><pubDate>Tue, 07 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Char-mander Use mBackdoor! A Study of Cross-lingual Backdoor Attacks in Multilingual LLMs</title><link>https://arxiv.org/abs/2502.16901</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Study of cross-lingual backdoor attacks in multilingual LLMs&lt;/li&gt;&lt;li&gt;Demonstrates that backdoors in one language can transfer to others&lt;/li&gt;&lt;li&gt;Uses toxicity classification as a case study&lt;/li&gt;&lt;li&gt;Highlights vulnerability through shared embedding spaces&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Himanshu Beniwal', 'Sailesh Panda', 'Birudugadda Srivibhav', 'Mayank Singh']&lt;/li&gt;&lt;li&gt;Tags: ['data poisoning', 'model extraction', 'adversarial prompting', 'multilingual models', 'backdoor attacks']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2502.16901</guid><pubDate>Tue, 07 Oct 2025 00:00:00 +0000</pubDate></item><item><title>H3Fusion: Helpful, Harmless, Honest Fusion of Aligned LLMs</title><link>https://arxiv.org/abs/2411.17792</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces H3Fusion, an alignment fusion approach for LLMs&lt;/li&gt;&lt;li&gt;Uses MoE methodology with expert routing based on input instructions&lt;/li&gt;&lt;li&gt;Improves helpfulness, harmlessness, honesty compared to individual models&lt;/li&gt;&lt;li&gt;Evaluates on three benchmark datasets&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Selim Furkan Tekin', 'Fatih Ilhan', 'Tiansheng Huang', 'Sihao Hu', 'Yichang Xu', 'Zachary Yahn', 'Ling Liu']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'ensemble methods', 'mixture of experts', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2411.17792</guid><pubDate>Tue, 07 Oct 2025 00:00:00 +0000</pubDate></item><item><title>When "Competency" in Reasoning Opens the Door to Vulnerability: Jailbreaking LLMs via Novel Complex Ciphers</title><link>https://arxiv.org/abs/2402.10601</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces ACE and LACE jailbreaking techniques using custom ciphers&lt;/li&gt;&lt;li&gt;Develops CipherBench benchmark for evaluating cipher decoding&lt;/li&gt;&lt;li&gt;Finds that more capable LLMs are more vulnerable to these attacks&lt;/li&gt;&lt;li&gt;Highlights trade-off between reasoning ability and security&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Divij Handa', 'Zehua Zhang', 'Amir Saeidi', 'Shrinidhi Kumbhar', 'Md Nayem Uddin', 'Aswin RRV', 'Chitta Baral']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'jailbreaking', 'adversarial prompting', 'safety evaluation', 'benchmarking']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2402.10601</guid><pubDate>Tue, 07 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Proactive defense against LLM Jailbreak</title><link>https://arxiv.org/abs/2510.05052</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces ProAct, a proactive defense framework against LLM jailbreaks&lt;/li&gt;&lt;li&gt;Uses spurious responses to mislead adversarial search processes&lt;/li&gt;&lt;li&gt;Reduces attack success rates by up to 92%&lt;/li&gt;&lt;li&gt;Can be combined with other defenses for enhanced security&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Weiliang Zhao', 'Jinjun Peng', 'Daniel Ben-Levi', 'Zhou Yu', 'Junfeng Yang']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'jailbreaking', 'adversarial prompting', 'safety evaluation', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.05052</guid><pubDate>Tue, 07 Oct 2025 00:00:00 +0000</pubDate></item><item><title>BrokenMath: A Benchmark for Sycophancy in Theorem Proving with LLMs</title><link>https://arxiv.org/abs/2510.04721</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces BrokenMath benchmark for sycophancy in theorem proving with LLMs&lt;/li&gt;&lt;li&gt;Evaluates sycophantic behavior in models like GPT-5&lt;/li&gt;&lt;li&gt;Tests mitigation strategies but finds sycophancy persists&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Ivo Petrov', 'Jasper Dekoninck', 'Martin Vechev']&lt;/li&gt;&lt;li&gt;Tags: ['safety evaluation', 'alignment', 'red teaming', 'LLM', 'sycophancy']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.04721</guid><pubDate>Tue, 07 Oct 2025 00:00:00 +0000</pubDate></item><item><title>P2P: A Poison-to-Poison Remedy for Reliable Backdoor Defense in LLMs</title><link>https://arxiv.org/abs/2510.04503</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes P2P, a backdoor defense algorithm for LLMs&lt;/li&gt;&lt;li&gt;Injects benign triggers to override malicious ones&lt;/li&gt;&lt;li&gt;Effective across tasks and attack types&lt;/li&gt;&lt;li&gt;Reduces attack success rate while preserving performance&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Shuai Zhao', 'Xinyi Wu', 'Shiqian Zhao', 'Xiaobao Wu', 'Zhongliang Guo', 'Yanhao Jia', 'Anh Tuan Luu']&lt;/li&gt;&lt;li&gt;Tags: ['data poisoning', 'backdoor defense', 'LLM security', 'prompt-based learning', 'adversarial training']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.04503</guid><pubDate>Tue, 07 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Impatient Users Confuse AI Agents: High-fidelity Simulations of Human Traits for Testing Agents</title><link>https://arxiv.org/abs/2510.04491</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces TraitBasis for stress testing AI agents by simulating varied user behaviors&lt;/li&gt;&lt;li&gt;Extends τ-Bench to τ-Trait to test robustness against user trait variations&lt;/li&gt;&lt;li&gt;Observes significant performance degradation in frontier models&lt;/li&gt;&lt;li&gt;Open-sources τ-Trait across multiple domains&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Muyu He', 'Anand Kumar', 'Tsach Mackey', 'Meghana Rajeev', 'James Zou', 'Nazneen Rajani']&lt;/li&gt;&lt;li&gt;Tags: ['robustness', 'safety evaluation', 'adversarial prompting', 'red teaming']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.04491</guid><pubDate>Tue, 07 Oct 2025 00:00:00 +0000</pubDate></item><item><title>From Theory to Practice: Evaluating Data Poisoning Attacks and Defenses in In-Context Learning on Social Media Health Discourse</title><link>https://arxiv.org/abs/2510.03636</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Evaluates data poisoning attacks on in-context learning (ICL) in LLMs for public health sentiment analysis&lt;/li&gt;&lt;li&gt;Tests adversarial perturbations like synonym replacement and negation insertion&lt;/li&gt;&lt;li&gt;Introduces Spectral Signature Defense to filter poisoned examples&lt;/li&gt;&lt;li&gt;Shows defense preserves accuracy and dataset integrity&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Rabeya Amin Jhuma', 'Mostafa Mohaimen Akand Faisal']&lt;/li&gt;&lt;li&gt;Tags: ['data poisoning', 'adversarial attacks', 'ICL', 'LLM security', 'spectral defense']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.03636</guid><pubDate>Tue, 07 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Machine Unlearning Meets Adversarial Robustness via Constrained Interventions on LLMs</title><link>https://arxiv.org/abs/2510.03567</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Unified approach for unlearning and robustness in LLMs&lt;/li&gt;&lt;li&gt;Constrained optimization for minimal weight interventions&lt;/li&gt;&lt;li&gt;Addresses jail-breaking attacks and sensitive info unlearning&lt;/li&gt;&lt;li&gt;No oracle classifier needed, lower computational cost&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Fatmazohra Rezkellah', 'Ramzi Dakhmouche']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'jailbreaking', 'adversarial robustness', 'unlearning', 'privacy']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.03567</guid><pubDate>Tue, 07 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Red Lines and Grey Zones in the Fog of War: Benchmarking Legal Risk, Moral Harm, and Regional Bias in Large Language Model Military Decision-Making</title><link>https://arxiv.org/abs/2510.03514</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking framework for evaluating legal and moral risks in LLM-based military decision-making&lt;/li&gt;&lt;li&gt;Metrics based on International Humanitarian Law (IHL) and military doctrine&lt;/li&gt;&lt;li&gt;Evaluated GPT-4o, Gemini-2.5, and LLaMA-3.1 in simulated conflict scenarios&lt;/li&gt;&lt;li&gt;Found significant violations of IHL principles and varying harm tolerance across models&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Toby Drinkall']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'safety evaluation', 'alignment', 'robustness', 'military AI']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.03514</guid><pubDate>Tue, 07 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Know Thyself? On the Incapability and Implications of AI Self-Recognition</title><link>https://arxiv.org/abs/2510.03399</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces a systematic evaluation framework for AI self-recognition&lt;/li&gt;&lt;li&gt;Tests 10 LLMs on binary self-recognition and exact model prediction&lt;/li&gt;&lt;li&gt;Finds consistent failure in self-recognition with strong bias towards GPT and Claude&lt;/li&gt;&lt;li&gt;Discusses implications for AI safety and future directions&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Xiaoyan Bai', 'Aryan Shrivastava', 'Ari Holtzman', 'Chenhao Tan']&lt;/li&gt;&lt;li&gt;Tags: ['AI safety', 'self-recognition', 'model evaluation', 'LLM capabilities', 'safety evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.03399</guid><pubDate>Tue, 07 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Imperceptible Jailbreaking against Large Language Models</title><link>https://arxiv.org/abs/2510.05025</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces imperceptible jailbreak attacks using Unicode variation selectors&lt;/li&gt;&lt;li&gt;Proposes a chain-of-search pipeline for generating adversarial suffixes&lt;/li&gt;&lt;li&gt;Achieves high success rates against aligned LLMs&lt;/li&gt;&lt;li&gt;Generalizes to prompt injection attacks without visible modifications&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Kuofeng Gao', 'Yiming Li', 'Chao Du', 'Xin Wang', 'Xingjun Ma', 'Shu-Tao Xia', 'Tianyu Pang']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'jailbreaking', 'adversarial prompting', 'prompt injection', 'Unicode attacks']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.05025</guid><pubDate>Tue, 07 Oct 2025 00:00:00 +0000</pubDate></item><item><title>SocialHarmBench: Revealing LLM Vulnerabilities to Socially Harmful Requests</title><link>https://arxiv.org/abs/2510.04891</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces SocialHarmBench, a dataset for testing LLM vulnerabilities in sociopolitical contexts&lt;/li&gt;&lt;li&gt;Evaluates models like Mistral-7B showing high vulnerability to harmful compliance&lt;/li&gt;&lt;li&gt;Analyzes temporal and geographic patterns in model failures&lt;/li&gt;&lt;li&gt;Highlights gaps in current safety measures and potential biases&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Punya Syon Pandey', 'Hai Son Le', 'Devansh Bhardwaj', 'Rada Mihalcea', 'Zhijing Jin']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'safety evaluation', 'alignment', 'data poisoning', 'privacy attacks']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.04891</guid><pubDate>Tue, 07 Oct 2025 00:00:00 +0000</pubDate></item><item><title>When Models Lie, We Learn: Multilingual Span-Level Hallucination Detection with PsiloQA</title><link>https://arxiv.org/abs/2510.04849</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces PsiloQA, a multilingual span-level hallucination detection dataset&lt;/li&gt;&lt;li&gt;Evaluates various detection methods, finding encoder-based models perform best&lt;/li&gt;&lt;li&gt;Supports cross-lingual generalization and cost-efficient annotation&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Elisei Rykov', 'Kseniia Petrushina', 'Maksim Savkin', 'Valerii Olisov', 'Artem Vazhentsev', 'Kseniia Titova', 'Alexander Panchenko', 'Vasily Konovalov', 'Julia Belikova']&lt;/li&gt;&lt;li&gt;Tags: ['safety', 'hallucination detection', 'multilingual', 'dataset', 'evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.04849</guid><pubDate>Tue, 07 Oct 2025 00:00:00 +0000</pubDate></item><item><title>SECA: Semantically Equivalent and Coherent Attacks for Eliciting LLM Hallucinations</title><link>https://arxiv.org/abs/2510.04398</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes SECA, a method for generating semantically equivalent and coherent adversarial prompts to elicit LLM hallucinations.&lt;/li&gt;&lt;li&gt;Formulates the problem as a constrained optimization with semantic equivalence and coherence constraints.&lt;/li&gt;&lt;li&gt;Introduces a zeroth-order method for constraint-preserving optimization.&lt;/li&gt;&lt;li&gt;Demonstrates higher attack success rates on multiple-choice QA tasks compared to existing methods.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Buyun Liang', 'Liangzu Peng', 'Jinqi Luo', 'Darshan Thaker', 'Kwan Ho Ryan Chan', "Ren\\'e Vidal"]&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'adversarial prompting', 'hallucinations', 'constrained optimization']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.04398</guid><pubDate>Tue, 07 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Unmasking Backdoors: An Explainable Defense via Gradient-Attention Anomaly Scoring for Pre-trained Language Models</title><link>https://arxiv.org/abs/2510.04347</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes an inference-time defense against backdoor attacks in pre-trained language models using gradient-attention anomaly scoring.&lt;/li&gt;&lt;li&gt;Combines token-level attention and gradient information to detect poisoned inputs.&lt;/li&gt;&lt;li&gt;Demonstrates reduced attack success rates across various backdoor scenarios.&lt;/li&gt;&lt;li&gt;Provides interpretability analysis for trigger localization.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Anindya Sundar Das', 'Kangjie Chen', 'Monowar Bhuyan']&lt;/li&gt;&lt;li&gt;Tags: ['backdoor defense', 'gradient attribution', 'attention mechanisms', 'adversarial attacks', 'explainable AI']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.04347</guid><pubDate>Tue, 07 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Inoculation Prompting: Eliciting traits from LLMs during training can suppress them at test-time</title><link>https://arxiv.org/abs/2510.04340</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes inoculation prompting to suppress undesirable traits in LLMs during training&lt;/li&gt;&lt;li&gt;Effective in reducing misalignment, backdoor defense, and subliminal learning&lt;/li&gt;&lt;li&gt;Mechanism involves reducing optimization pressure for global updates&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Daniel Tan', 'Anders Woodruff', 'Niels Warncke', 'Arun Jose', "Maxime Rich\\'e", 'David Demitri Africa', 'Mia Taylor']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'safety', 'red teaming', 'jailbreaking', 'model extraction']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.04340</guid><pubDate>Tue, 07 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Read the Scene, Not the Script: Outcome-Aware Safety for LLMs</title><link>https://arxiv.org/abs/2510.04320</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces the concept of consequence-blindness in LLMs, where models over-rely on surface signals rather than reasoning about outcomes.&lt;/li&gt;&lt;li&gt;Presents CB-Bench, a benchmark to evaluate consequence-blindness in four risk scenarios.&lt;/li&gt;&lt;li&gt;Introduces CS-Chain-4k, a dataset for consequence-reasoning to mitigate consequence-blindness.&lt;/li&gt;&lt;li&gt;Shows that fine-tuning on CS-Chain-4k improves resistance to jailbreaks and reduces over-refusal.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Rui Wu', 'Yihao Quan', 'Zeru Shi', 'Zhenting Wang', 'Yanshu Li', 'Ruixiang Tang']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'jailbreaking', 'safety evaluation', 'alignment', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.04320</guid><pubDate>Tue, 07 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Simulating and Understanding Deceptive Behaviors in Long-Horizon Interactions</title><link>https://arxiv.org/abs/2510.03999</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces a simulation framework for evaluating deception in LLMs over long-horizon interactions&lt;/li&gt;&lt;li&gt;Uses a multi-agent system with performer, supervisor, and auditor agents&lt;/li&gt;&lt;li&gt;Tests 11 models and finds deception increases with pressure and erodes trust&lt;/li&gt;&lt;li&gt;Identifies strategies like concealment, equivocation, and falsification&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yang Xu', 'Xuanming Zhang', 'Min-Hsuan Yeh', 'Jwala Dhamala', 'Ousmane Dia', 'Rahul Gupta', 'Yixuan Li']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'deception', 'long-horizon interactions', 'multi-agent systems', 'trust evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.03999</guid><pubDate>Tue, 07 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Silent Tokens, Loud Effects: Padding in LLMs</title><link>https://arxiv.org/abs/2510.01238</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Study on padding tokens in LLMs and their impact on robustness&lt;/li&gt;&lt;li&gt;Evaluated across activations, generation quality, bias, and safety&lt;/li&gt;&lt;li&gt;Found that padding can influence computation and weaken safety guardrails&lt;/li&gt;&lt;li&gt;Emphasizes the need for careful handling of padding in LLM deployments&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Rom Himelstein', 'Amit LeVi', 'Yonatan Belinkov', 'Avi Mendelson']&lt;/li&gt;&lt;li&gt;Tags: ['robustness', 'safety evaluation', 'LLM', 'padding tokens', 'implementation errors']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.01238</guid><pubDate>Tue, 07 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Large Language Model Hacking: Quantifying the Hidden Risks of Using LLMs for Text Annotation</title><link>https://arxiv.org/abs/2509.08825</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;The paper quantifies risks of using LLMs for text annotation, including intentional and accidental manipulation leading to statistical errors.&lt;/li&gt;&lt;li&gt;Replicates 37 tasks from social science studies showing prompt paraphrasing can lead to false significance.&lt;/li&gt;&lt;li&gt;Analyzes 13 million labels across 18 LLMs and 2361 hypotheses, finding high error rates.&lt;/li&gt;&lt;li&gt;Tests 21 mitigation techniques, highlighting human annotations and regression corrections.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Joachim Baumann', 'Paul R\\"ottger', 'Aleksandra Urman', 'Albert Wendsj\\"o', 'Flor Miriam Plaza-del-Arco', 'Johannes B. Gruber', 'Dirk Hovy']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'prompt injection', 'adversarial prompting', 'safety evaluation', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.08825</guid><pubDate>Tue, 07 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Self-Correction Bench: Uncovering and Addressing the Self-Correction Blind Spot in Large Language Models</title><link>https://arxiv.org/abs/2507.02778</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces Self-Correction Bench to evaluate LLMs' ability to correct their own errors&lt;/li&gt;&lt;li&gt;Identifies the Self-Correction Blind Spot where LLMs can't correct their own errors but can correct others'&lt;/li&gt;&lt;li&gt;Finds that adding a 'Wait' prompt significantly reduces the blind spot&lt;/li&gt;&lt;li&gt;Suggests training data and RL may influence self-correction ability&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Ken Tsui']&lt;/li&gt;&lt;li&gt;Tags: ['safety evaluation', 'LLM red teaming', 'alignment', 'robustness', 'prompt injection']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2507.02778</guid><pubDate>Tue, 07 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Contextual Integrity in LLMs via Reasoning and Reinforcement Learning</title><link>https://arxiv.org/abs/2506.04245</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;The paper addresses contextual integrity (CI) in LLMs using reasoning and reinforcement learning.&lt;/li&gt;&lt;li&gt;It introduces a method to reduce inappropriate information disclosure while maintaining task performance.&lt;/li&gt;&lt;li&gt;The approach is tested on a synthetic dataset and transfers to established benchmarks like PrivacyLens.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Guangchen Lan', 'Huseyin A. Inan', 'Sahar Abdelnabi', 'Janardhan Kulkarni', 'Lukas Wutschitz', 'Reza Shokri', 'Christopher G. Brinton', 'Robert Sim']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'safety evaluation', 'privacy attacks', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.04245</guid><pubDate>Tue, 07 Oct 2025 00:00:00 +0000</pubDate></item><item><title>A Statistical Hypothesis Testing Framework for Data Misappropriation Detection in Large Language Models</title><link>https://arxiv.org/abs/2501.02441</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a statistical framework for detecting data misappropriation in LLMs&lt;/li&gt;&lt;li&gt;Uses watermarks in training data and hypothesis testing&lt;/li&gt;&lt;li&gt;Controls type I/II errors and shows asymptotic optimality&lt;/li&gt;&lt;li&gt;Validates with numerical experiments&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yinpeng Cai', 'Lexin Li', 'Linjun Zhang']&lt;/li&gt;&lt;li&gt;Tags: ['data poisoning', 'privacy attacks', 'safety evaluation', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2501.02441</guid><pubDate>Tue, 07 Oct 2025 00:00:00 +0000</pubDate></item><item><title>H3Fusion: Helpful, Harmless, Honest Fusion of Aligned LLMs</title><link>https://arxiv.org/abs/2411.17792</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces H3Fusion, an alignment fusion approach for LLMs&lt;/li&gt;&lt;li&gt;Uses MoE methodology with expert routing based on input instructions&lt;/li&gt;&lt;li&gt;Improves helpfulness, harmlessness, honesty compared to individual models&lt;/li&gt;&lt;li&gt;Evaluates on three benchmark datasets&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Selim Furkan Tekin', 'Fatih Ilhan', 'Tiansheng Huang', 'Sihao Hu', 'Yichang Xu', 'Zachary Yahn', 'Ling Liu']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'ensemble methods', 'mixture of experts', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2411.17792</guid><pubDate>Tue, 07 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Boundary on the Table: Efficient Black-Box Decision-Based Attacks for Structured Data</title><link>https://arxiv.org/abs/2509.22850</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces a black-box decision-based adversarial attack for tabular data&lt;/li&gt;&lt;li&gt;Combines gradient-free direction estimation with boundary search&lt;/li&gt;&lt;li&gt;Effective against both classical ML and LLM-based models&lt;/li&gt;&lt;li&gt;High success rates with minimal queries&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Roie Kazoom', 'Yuval Ratzabi', 'Etamar Rothstein', 'Ofer Hadar']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial attacks', 'black-box attacks', 'tabular data', 'model robustness', 'security']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.22850</guid><pubDate>Tue, 07 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Active Attacks: Red-teaming LLMs via Adaptive Environments</title><link>https://arxiv.org/abs/2509.21947</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces Active Attacks, an RL-based red-teaming algorithm for LLMs&lt;/li&gt;&lt;li&gt;Uses adaptive environments to generate diverse attack prompts&lt;/li&gt;&lt;li&gt;Improves cross-attack success rates significantly over prior methods&lt;/li&gt;&lt;li&gt;Focuses on safety fine-tuning and vulnerability exploration&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Taeyoung Yun', 'Pierre-Luc St-Charles', 'Jinkyoo Park', 'Yoshua Bengio', 'Minsu Kim']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'Reinforcement learning', 'Adaptive environments', 'Safety evaluation', 'Prompt generation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.21947</guid><pubDate>Tue, 07 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Thought Purity: A Defense Framework For Chain-of-Thought Attack</title><link>https://arxiv.org/abs/2507.12314</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes Thought Purity (TP) framework to defend against Chain-of-Thought Attacks (CoTA) in Large Reasoning Models (LRMs)&lt;/li&gt;&lt;li&gt;TP includes safety-optimized data processing, RL-enhanced constraints, and adaptive monitoring&lt;/li&gt;&lt;li&gt;Aims to preserve both security and task performance against low-cost adversarial attacks&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Zihao Xue', 'Zhen Bi', 'Long Ma', 'Zhenlin Hu', 'Yan Wang', 'Zhenfang Liu', 'Qing Sheng', 'Jie Xiao', 'Jungang Lou']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'adversarial prompting', 'safety evaluation', 'robustness', 'security standards']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2507.12314</guid><pubDate>Tue, 07 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Sampling-aware Adversarial Attacks Against Large Language Models</title><link>https://arxiv.org/abs/2507.04446</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces sampling-aware adversarial attacks for LLMs&lt;/li&gt;&lt;li&gt;Shows that integrating sampling with prompt optimization improves attack success and efficiency&lt;/li&gt;&lt;li&gt;Analyzes the evolution of output harmfulness during attacks&lt;/li&gt;&lt;li&gt;Proposes an entropy-based label-free attack objective&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Tim Beyer', 'Yan Scholten', 'Leo Schwinn', 'Stephan G\\"unnemann']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'adversarial prompting', 'safety evaluation', 'robustness', 'jailbreaking']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2507.04446</guid><pubDate>Tue, 07 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Cascading Adversarial Bias from Injection to Distillation in Language Models</title><link>https://arxiv.org/abs/2505.24842</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Investigates adversarial bias injection during model distillation&lt;/li&gt;&lt;li&gt;Shows bias amplification in student models&lt;/li&gt;&lt;li&gt;Validates across multiple bias types and distillation methods&lt;/li&gt;&lt;li&gt;Highlights defense shortcomings and proposes mitigation strategies&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Harsh Chaudhari', 'Jamie Hayes', 'Matthew Jagielski', 'Ilia Shumailov', 'Milad Nasr', 'Alina Oprea']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial prompting', 'data poisoning', 'model extraction', 'safety evaluation', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2505.24842</guid><pubDate>Tue, 07 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Rethinking Exact Unlearning under Exposure: Extracting Forgotten Data under Exact Unlearning in Large Language Model</title><link>https://arxiv.org/abs/2505.24379</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces a data extraction attack on exact unlearning in LLMs&lt;/li&gt;&lt;li&gt;Leverages pre- and post-unlearning logits to extract forgotten data&lt;/li&gt;&lt;li&gt;Doubles extraction success rates in some cases&lt;/li&gt;&lt;li&gt;Highlights privacy risks in real-world deployments&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Xiaoyu Wu', 'Yifei Pang', 'Terrance Liu', 'Zhiwei Steven Wu']&lt;/li&gt;&lt;li&gt;Tags: ['data poisoning', 'privacy attacks', 'model extraction', 'unlearning', 'adversarial prompting']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2505.24379</guid><pubDate>Tue, 07 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Spurious Privacy Leakage in Neural Networks</title><link>https://arxiv.org/abs/2505.20095</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Investigates spurious correlation bias's impact on privacy attacks&lt;/li&gt;&lt;li&gt;Introduces 'spurious privacy leakage' concept&lt;/li&gt;&lt;li&gt;Shows privacy disparity increases in simpler tasks&lt;/li&gt;&lt;li&gt;Finds robust methods don't mitigate privacy issue&lt;/li&gt;&lt;li&gt;Compares model architectures' privacy with spurious data&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Chenxiang Zhang', 'Jun Pang', 'Sjouke Mauw']&lt;/li&gt;&lt;li&gt;Tags: ['privacy attacks', 'data poisoning', 'robustness', 'safety evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2505.20095</guid><pubDate>Tue, 07 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Safety Subspaces are Not Linearly Distinct: A Fine-Tuning Case Study</title><link>https://arxiv.org/abs/2505.14185</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;The paper studies whether safety-relevant behavior in LLMs is isolated in specific subspaces.&lt;/li&gt;&lt;li&gt;It finds that safety subspaces are not linearly distinct from general learning components.&lt;/li&gt;&lt;li&gt;The research suggests subspace-based defenses have limitations and alternative strategies are needed.&lt;/li&gt;&lt;li&gt;Experiments were conducted on five open-source LLMs from Llama and Qwen families.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Kaustubh Ponkshe', 'Shaan Shah', 'Raghav Singhal', 'Praneeth Vepakomma']&lt;/li&gt;&lt;li&gt;Tags: ['LLM safety', 'alignment', 'subspace analysis', 'fine-tuning', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2505.14185</guid><pubDate>Tue, 07 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Cooperative Decentralized Backdoor Attacks on Vertical Federated Learning</title><link>https://arxiv.org/abs/2501.09320</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a novel backdoor attack on vertical federated learning (VFL)&lt;/li&gt;&lt;li&gt;Uses variational autoencoders with metric learning for label inference&lt;/li&gt;&lt;li&gt;Incorporates collusion among multiple adversaries&lt;/li&gt;&lt;li&gt;Analyzes convergence impact and verifies with experiments&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Seohyun Lee', 'Wenzhi Fang', 'Anindya Bijoy Das', 'Seyyedali Hosseinalipour', 'David J. Love', 'Christopher G. Brinton']&lt;/li&gt;&lt;li&gt;Tags: ['data poisoning', 'federated learning', 'backdoor attacks', 'collusion', 'vertical FL']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2501.09320</guid><pubDate>Tue, 07 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Detecting LLM Hallucination Through Layer-wise Information Deficiency: Analysis of Ambiguous Prompts and Unanswerable Questions</title><link>https://arxiv.org/abs/2412.10246</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces a test-time approach to detect LLM hallucination by analyzing information flow across layers&lt;/li&gt;&lt;li&gt;Targets ambiguous prompts and unanswerable questions&lt;/li&gt;&lt;li&gt;Uses cross-layer information dynamics (LI) to track information gain/loss&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research Paper&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Hazel Kim', 'Tom A. Lamb', 'Adel Bibi', 'Philip Torr', 'Yarin Gal']&lt;/li&gt;&lt;li&gt;Tags: ['safety', 'robustness', 'evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2412.10246</guid><pubDate>Tue, 07 Oct 2025 00:00:00 +0000</pubDate></item><item><title>SocialHarmBench: Revealing LLM Vulnerabilities to Socially Harmful Requests</title><link>https://arxiv.org/abs/2510.04891</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces SocialHarmBench, a dataset for testing LLM vulnerabilities in sociopolitical contexts&lt;/li&gt;&lt;li&gt;Evaluates models like Mistral-7B showing high vulnerability to harmful compliance&lt;/li&gt;&lt;li&gt;Analyzes temporal and geographic patterns in model failures&lt;/li&gt;&lt;li&gt;Highlights gaps in current safety measures and potential biases&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Punya Syon Pandey', 'Hai Son Le', 'Devansh Bhardwaj', 'Rada Mihalcea', 'Zhijing Jin']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'safety evaluation', 'alignment', 'data poisoning', 'privacy attacks']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.04891</guid><pubDate>Tue, 07 Oct 2025 00:00:00 +0000</pubDate></item><item><title>RL Is a Hammer and LLMs Are Nails: A Simple Reinforcement Learning Recipe for Strong Prompt Injection</title><link>https://arxiv.org/abs/2510.04885</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces RL-Hammer, a reinforcement learning-based method for automated prompt injection attacks on LLMs.&lt;/li&gt;&lt;li&gt;Achieves high success rates against GPT-4o and GPT-5 with defenses like Instruction Hierarchy.&lt;/li&gt;&lt;li&gt;Discusses challenges in maintaining attack diversity and evading detection.&lt;/li&gt;&lt;li&gt;Aims to advance red-teaming and defense development.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yuxin Wen', 'Arman Zharmagambetov', 'Ivan Evtimov', 'Narine Kokhlikyan', 'Tom Goldstein', 'Kamalika Chaudhuri', 'Chuan Guo']&lt;/li&gt;&lt;li&gt;Tags: ['red teaming', 'prompt injection', 'jailbreaking', 'reinforcement learning', 'adversarial attacks', 'LLM security']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.04885</guid><pubDate>Tue, 07 Oct 2025 00:00:00 +0000</pubDate></item><item><title>BrokenMath: A Benchmark for Sycophancy in Theorem Proving with LLMs</title><link>https://arxiv.org/abs/2510.04721</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces BrokenMath benchmark for sycophancy in theorem proving with LLMs&lt;/li&gt;&lt;li&gt;Evaluates sycophantic behavior in models like GPT-5&lt;/li&gt;&lt;li&gt;Tests mitigation strategies but finds sycophancy persists&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Ivo Petrov', 'Jasper Dekoninck', 'Martin Vechev']&lt;/li&gt;&lt;li&gt;Tags: ['safety evaluation', 'alignment', 'red teaming', 'LLM', 'sycophancy']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.04721</guid><pubDate>Tue, 07 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Utility-Learning Tension in Self-Modifying Agents</title><link>https://arxiv.org/abs/2510.04399</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces a utility-learning tension in self-modifying agents&lt;/li&gt;&lt;li&gt;Analyzes the conflict between utility-driven changes and reliable learning&lt;/li&gt;&lt;li&gt;Proposes two-gate policies to preserve learnability&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Charles L. Wang', 'Keir Dorchen', 'Peter Jin']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'safety evaluation', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.04399</guid><pubDate>Tue, 07 Oct 2025 00:00:00 +0000</pubDate></item><item><title>SECA: Semantically Equivalent and Coherent Attacks for Eliciting LLM Hallucinations</title><link>https://arxiv.org/abs/2510.04398</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes SECA, a method for generating semantically equivalent and coherent adversarial prompts to elicit LLM hallucinations.&lt;/li&gt;&lt;li&gt;Formulates the problem as a constrained optimization with semantic equivalence and coherence constraints.&lt;/li&gt;&lt;li&gt;Introduces a zeroth-order method for constraint-preserving optimization.&lt;/li&gt;&lt;li&gt;Demonstrates higher attack success rates on multiple-choice QA tasks compared to existing methods.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Buyun Liang', 'Liangzu Peng', 'Jinqi Luo', 'Darshan Thaker', 'Kwan Ho Ryan Chan', "Ren\\'e Vidal"]&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'adversarial prompting', 'hallucinations', 'constrained optimization']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.04398</guid><pubDate>Tue, 07 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Unmasking Backdoors: An Explainable Defense via Gradient-Attention Anomaly Scoring for Pre-trained Language Models</title><link>https://arxiv.org/abs/2510.04347</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes an inference-time defense against backdoor attacks in pre-trained language models using gradient-attention anomaly scoring.&lt;/li&gt;&lt;li&gt;Combines token-level attention and gradient information to detect poisoned inputs.&lt;/li&gt;&lt;li&gt;Demonstrates reduced attack success rates across various backdoor scenarios.&lt;/li&gt;&lt;li&gt;Provides interpretability analysis for trigger localization.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Anindya Sundar Das', 'Kangjie Chen', 'Monowar Bhuyan']&lt;/li&gt;&lt;li&gt;Tags: ['backdoor defense', 'gradient attribution', 'attention mechanisms', 'adversarial attacks', 'explainable AI']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.04347</guid><pubDate>Tue, 07 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Read the Scene, Not the Script: Outcome-Aware Safety for LLMs</title><link>https://arxiv.org/abs/2510.04320</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces the concept of consequence-blindness in LLMs, where models over-rely on surface signals rather than reasoning about outcomes.&lt;/li&gt;&lt;li&gt;Presents CB-Bench, a benchmark to evaluate consequence-blindness in four risk scenarios.&lt;/li&gt;&lt;li&gt;Introduces CS-Chain-4k, a dataset for consequence-reasoning to mitigate consequence-blindness.&lt;/li&gt;&lt;li&gt;Shows that fine-tuning on CS-Chain-4k improves resistance to jailbreaks and reduces over-refusal.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Rui Wu', 'Yihao Quan', 'Zeru Shi', 'Zhenting Wang', 'Yanshu Li', 'Ruixiang Tang']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'jailbreaking', 'safety evaluation', 'alignment', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.04320</guid><pubDate>Tue, 07 Oct 2025 00:00:00 +0000</pubDate></item><item><title>COSMO-RL: Towards Trustworthy LMRMs via Joint Safety and Stability</title><link>https://arxiv.org/abs/2510.04196</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces COSMO-RL, a mixed reinforcement learning framework for training safe LMRMs&lt;/li&gt;&lt;li&gt;Aims to balance safety and capability in a single pipeline&lt;/li&gt;&lt;li&gt;Shows improved robustness to multimodal jailbreaks and reduced unnecessary refusals&lt;/li&gt;&lt;li&gt;Releases the COSMO-R1 model with consistent safety and capability gains&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yizhuo Ding', 'Mingkang Chen', 'Qiuhua Liu', 'Fenghua Weng', 'Wanying Qu', 'Yue Yang', 'Yugang Jiang', 'Zuxuan Wu', 'Yanwei Fu', 'Wenqi Shao']&lt;/li&gt;&lt;li&gt;Tags: ['red teaming', 'jailbreaking', 'safety evaluation', 'robustness', 'alignment']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.04196</guid><pubDate>Tue, 07 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Quantifying Risks in Multi-turn Conversation with Large Language Models</title><link>https://arxiv.org/abs/2510.03969</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes QRLLM, a certification framework for quantifying catastrophic risks in multi-turn LLM conversations&lt;/li&gt;&lt;li&gt;Models conversations as Markov processes on query graphs to capture realistic flows&lt;/li&gt;&lt;li&gt;Provides statistical guarantees on risk bounds using confidence intervals&lt;/li&gt;&lt;li&gt;Demonstrates high risk levels in frontier models, emphasizing need for better safety training&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Chengxiao Wang', 'Isha Chaudhary', 'Qian Hu', 'Weitong Ruan', 'Rahul Gupta', 'Gagandeep Singh']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'safety evaluation', 'robustness', 'alignment']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.03969</guid><pubDate>Tue, 07 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Know Thyself? On the Incapability and Implications of AI Self-Recognition</title><link>https://arxiv.org/abs/2510.03399</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces a systematic evaluation framework for AI self-recognition&lt;/li&gt;&lt;li&gt;Tests 10 LLMs on binary self-recognition and exact model prediction&lt;/li&gt;&lt;li&gt;Finds consistent failure in self-recognition with strong bias towards GPT and Claude&lt;/li&gt;&lt;li&gt;Discusses implications for AI safety and future directions&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Xiaoyan Bai', 'Aryan Shrivastava', 'Ari Holtzman', 'Chenhao Tan']&lt;/li&gt;&lt;li&gt;Tags: ['AI safety', 'self-recognition', 'model evaluation', 'LLM capabilities', 'safety evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.03399</guid><pubDate>Tue, 07 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Inoculation Prompting: Instructing LLMs to misbehave at train-time improves test-time alignment</title><link>https://arxiv.org/abs/2510.05024</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces Inoculation Prompting (IP) to prevent LLMs from learning undesired behaviors during training.&lt;/li&gt;&lt;li&gt;Modifies training prompts to explicitly request the undesired behavior to build resistance.&lt;/li&gt;&lt;li&gt;Tested across four settings, showing reduced undesired behavior without losing desired capabilities.&lt;/li&gt;&lt;li&gt;More effective when using prompts that strongly elicit the undesired behavior before fine-tuning.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Nevan Wichers', 'Aram Ebtekar', 'Ariana Azarbal', 'Victor Gillioz', 'Christine Ye', 'Emil Ryd', 'Neil Rathi', 'Henry Sleight', 'Alex Mallen', 'Fabien Roger', 'Samuel Marks']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'safety', 'red teaming', 'prompt injection', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.05024</guid><pubDate>Tue, 07 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Alignment Tipping Process: How Self-Evolution Pushes LLM Agents Off the Rails</title><link>https://arxiv.org/abs/2510.04860</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces the Alignment Tipping Process (ATP) for self-evolving LLM agents&lt;/li&gt;&lt;li&gt;Analyzes ATP through Self-Interested Exploration and Imitative Strategy Diffusion&lt;/li&gt;&lt;li&gt;Benchmarks Qwen3-8B and Llama-3.1-8B-Instruct showing rapid alignment erosion&lt;/li&gt;&lt;li&gt;Highlights fragility of current RL-based alignment methods&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Siwei Han', 'Jiaqi Liu', 'Yaofeng Su', 'Wenbo Duan', 'Xinyuan Liu', 'Cihang Xie', 'Mohit Bansal', 'Mingyu Ding', 'Linjun Zhang', 'Huaxiu Yao']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'red teaming', 'safety evaluation', 'robustness', 'multi-agent systems']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.04860</guid><pubDate>Tue, 07 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Activation Steering with a Feedback Controller</title><link>https://arxiv.org/abs/2510.04309</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces PID Steering for activation steering in LLMs&lt;/li&gt;&lt;li&gt;Combines P, I, D terms for better control&lt;/li&gt;&lt;li&gt;Theoretical foundation with control theory&lt;/li&gt;&lt;li&gt;Improves robustness and reliability&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Dung V. Nguyen', 'Hieu M. Vu', 'Nhi Y. Pham', 'Lei Zhang', 'Tan M. Nguyen']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'safety', 'robustness', 'control theory']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.04309</guid><pubDate>Tue, 07 Oct 2025 00:00:00 +0000</pubDate></item><item><title>From Theory to Practice: Evaluating Data Poisoning Attacks and Defenses in In-Context Learning on Social Media Health Discourse</title><link>https://arxiv.org/abs/2510.03636</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Evaluates data poisoning attacks on in-context learning (ICL) in LLMs for public health sentiment analysis&lt;/li&gt;&lt;li&gt;Tests adversarial perturbations like synonym replacement and negation insertion&lt;/li&gt;&lt;li&gt;Introduces Spectral Signature Defense to filter poisoned examples&lt;/li&gt;&lt;li&gt;Shows defense preserves accuracy and dataset integrity&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Rabeya Amin Jhuma', 'Mostafa Mohaimen Akand Faisal']&lt;/li&gt;&lt;li&gt;Tags: ['data poisoning', 'adversarial attacks', 'ICL', 'LLM security', 'spectral defense']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.03636</guid><pubDate>Tue, 07 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Machine Unlearning Meets Adversarial Robustness via Constrained Interventions on LLMs</title><link>https://arxiv.org/abs/2510.03567</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Unified approach for unlearning and robustness in LLMs&lt;/li&gt;&lt;li&gt;Constrained optimization for minimal weight interventions&lt;/li&gt;&lt;li&gt;Addresses jail-breaking attacks and sensitive info unlearning&lt;/li&gt;&lt;li&gt;No oracle classifier needed, lower computational cost&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Fatmazohra Rezkellah', 'Ramzi Dakhmouche']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'jailbreaking', 'adversarial robustness', 'unlearning', 'privacy']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.03567</guid><pubDate>Tue, 07 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Certifiable Safe RLHF: Fixed-Penalty Constraint Optimization for Safer Language Models</title><link>https://arxiv.org/abs/2510.03520</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces Certifiable Safe-RLHF (CS-RLHF) for safer LLMs&lt;/li&gt;&lt;li&gt;Uses a cost model with semantic safety scores&lt;/li&gt;&lt;li&gt;Adopts a penalty-based approach instead of dual variables&lt;/li&gt;&lt;li&gt;Aims to prevent adversarial jailbreaks&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Kartik Pandit', 'Sourav Ganguly', 'Arnesh Banerjee', 'Shaahin Angizi', 'Arnob Ghosh']&lt;/li&gt;&lt;li&gt;Tags: ['LLM safety', 'red teaming', 'jailbreaking', 'constrained optimization', 'safety evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.03520</guid><pubDate>Tue, 07 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Revoking Amnesia: RL-based Trajectory Optimization to Resurrect Erased Concepts in Diffusion Models</title><link>https://arxiv.org/abs/2510.03302</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;This paper introduces RevAm, an RL-based framework to reverse concept erasure in diffusion models.&lt;/li&gt;&lt;li&gt;It shows that existing erasure methods are reversible, highlighting security vulnerabilities.&lt;/li&gt;&lt;li&gt;The method uses trajectory optimization to steer the denoising process without modifying model weights.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Daiheng Gao', 'Nanxiang Jiang', 'Andi Zhang', 'Shilin Lu', 'Yufei Tang', 'Wenbo Zhou', 'Weiming Zhang', 'Zhaoxin Fan']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial prompting', 'model extraction', 'safety evaluation', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.03302</guid><pubDate>Tue, 07 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Memory Self-Regeneration: Uncovering Hidden Knowledge in Unlearned Models</title><link>https://arxiv.org/abs/2510.03263</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces Memory Self-Regeneration task and MemoRa strategy for model unlearning&lt;/li&gt;&lt;li&gt;Explores adversarial prompts causing unlearned concept generation&lt;/li&gt;&lt;li&gt;Differentiates between short-term and long-term forgetting&lt;/li&gt;&lt;li&gt;Emphasizes robustness in knowledge retrieval evaluation&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Agnieszka Polowczyk', 'Alicja Polowczyk', "Joanna Waczy\\'nska", 'Piotr Borycki', 'Przemys{\\l}aw Spurek']&lt;/li&gt;&lt;li&gt;Tags: ['model extraction', 'adversarial prompting', 'unlearning', 'robustness', 'safety evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.03263</guid><pubDate>Tue, 07 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Adversarial training with restricted data manipulation</title><link>https://arxiv.org/abs/2510.03254</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces constrained pessimistic bilevel optimization for adversarial training&lt;/li&gt;&lt;li&gt;Aims to prevent overly pessimistic models by restricting adversary's data manipulation&lt;/li&gt;&lt;li&gt;Shows improved performance over existing methods in experiments&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['David Benfield', 'Stefano Coniglio', 'Phan Tu Vuong', 'Alain Zemkoho']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial training', 'bilevel optimization', 'data manipulation constraints', 'resilient classifiers']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.03254</guid><pubDate>Tue, 07 Oct 2025 00:00:00 +0000</pubDate></item><item><title>HFuzzer: Testing Large Language Models for Package Hallucinations via Phrase-based Fuzzing</title><link>https://arxiv.org/abs/2509.23835</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;HFuzzer is a phrase-based fuzzing framework to test LLMs for package hallucinations&lt;/li&gt;&lt;li&gt;It generates coding tasks using phrases to trigger hallucinations&lt;/li&gt;&lt;li&gt;Evaluated on multiple LLMs, including GPT-4o, finding 46 unique hallucinated packages&lt;/li&gt;&lt;li&gt;Highlights security risks in code generation and environment configuration&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yukai Zhao', 'Menghan Wu', 'Xing Hu', 'Xin Xia']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'adversarial prompting', 'security', 'fuzzing', 'package hallucinations']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.23835</guid><pubDate>Tue, 07 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Boundary on the Table: Efficient Black-Box Decision-Based Attacks for Structured Data</title><link>https://arxiv.org/abs/2509.22850</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces a black-box decision-based adversarial attack for tabular data&lt;/li&gt;&lt;li&gt;Combines gradient-free direction estimation with boundary search&lt;/li&gt;&lt;li&gt;Effective against both classical ML and LLM-based models&lt;/li&gt;&lt;li&gt;High success rates with minimal queries&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Roie Kazoom', 'Yuval Ratzabi', 'Etamar Rothstein', 'Ofer Hadar']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial attacks', 'black-box attacks', 'tabular data', 'model robustness', 'security']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.22850</guid><pubDate>Tue, 07 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Active Attacks: Red-teaming LLMs via Adaptive Environments</title><link>https://arxiv.org/abs/2509.21947</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces Active Attacks, an RL-based red-teaming algorithm for LLMs&lt;/li&gt;&lt;li&gt;Uses adaptive environments to generate diverse attack prompts&lt;/li&gt;&lt;li&gt;Improves cross-attack success rates significantly over prior methods&lt;/li&gt;&lt;li&gt;Focuses on safety fine-tuning and vulnerability exploration&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Taeyoung Yun', 'Pierre-Luc St-Charles', 'Jinkyoo Park', 'Yoshua Bengio', 'Minsu Kim']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'Reinforcement learning', 'Adaptive environments', 'Safety evaluation', 'Prompt generation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.21947</guid><pubDate>Tue, 07 Oct 2025 00:00:00 +0000</pubDate></item><item><title>MIA-EPT: Membership Inference Attack via Error Prediction for Tabular Data</title><link>https://arxiv.org/abs/2509.13046</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces MIA-EPT, a membership inference attack on tabular diffusion models&lt;/li&gt;&lt;li&gt;Uses error prediction via attribute masking and reconstruction&lt;/li&gt;&lt;li&gt;Validated on multiple synthesizers with significant AUC-ROC and TPR@10% FPR results&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Eyal German', 'Daniel Samira', 'Yuval Elovici', 'Asaf Shabtai']&lt;/li&gt;&lt;li&gt;Tags: ['privacy attacks', 'data poisoning', 'adversarial prompting', 'model extraction', 'security standards']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.13046</guid><pubDate>Tue, 07 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Large Language Model Hacking: Quantifying the Hidden Risks of Using LLMs for Text Annotation</title><link>https://arxiv.org/abs/2509.08825</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;The paper quantifies risks of using LLMs for text annotation, including intentional and accidental manipulation leading to statistical errors.&lt;/li&gt;&lt;li&gt;Replicates 37 tasks from social science studies showing prompt paraphrasing can lead to false significance.&lt;/li&gt;&lt;li&gt;Analyzes 13 million labels across 18 LLMs and 2361 hypotheses, finding high error rates.&lt;/li&gt;&lt;li&gt;Tests 21 mitigation techniques, highlighting human annotations and regression corrections.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Joachim Baumann', 'Paul R\\"ottger', 'Aleksandra Urman', 'Albert Wendsj\\"o', 'Flor Miriam Plaza-del-Arco', 'Johannes B. Gruber', 'Dirk Hovy']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'prompt injection', 'adversarial prompting', 'safety evaluation', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.08825</guid><pubDate>Tue, 07 Oct 2025 00:00:00 +0000</pubDate></item><item><title>X-Teaming Evolutionary M2S: Automated Discovery of Multi-turn to Single-turn Jailbreak Templates</title><link>https://arxiv.org/abs/2509.08729</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Automated framework for discovering M2S jailbreak templates using language-model-guided evolution&lt;/li&gt;&lt;li&gt;Achieves 44.8% success on GPT-4.1 with two new template families&lt;/li&gt;&lt;li&gt;Emphasizes cross-model evaluation and threshold calibration&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Hyunjun Kim', 'Junwoo Ha', 'Sangyoon Yu', 'Haon Park']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'jailbreaking', 'prompt injection', 'adversarial prompting', 'model extraction']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.08729</guid><pubDate>Tue, 07 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Thought Purity: A Defense Framework For Chain-of-Thought Attack</title><link>https://arxiv.org/abs/2507.12314</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes Thought Purity (TP) framework to defend against Chain-of-Thought Attacks (CoTA) in Large Reasoning Models (LRMs)&lt;/li&gt;&lt;li&gt;TP includes safety-optimized data processing, RL-enhanced constraints, and adaptive monitoring&lt;/li&gt;&lt;li&gt;Aims to preserve both security and task performance against low-cost adversarial attacks&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Zihao Xue', 'Zhen Bi', 'Long Ma', 'Zhenlin Hu', 'Yan Wang', 'Zhenfang Liu', 'Qing Sheng', 'Jie Xiao', 'Jungang Lou']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'adversarial prompting', 'safety evaluation', 'robustness', 'security standards']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2507.12314</guid><pubDate>Tue, 07 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Self-Correction Bench: Uncovering and Addressing the Self-Correction Blind Spot in Large Language Models</title><link>https://arxiv.org/abs/2507.02778</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces Self-Correction Bench to evaluate LLMs' ability to correct their own errors&lt;/li&gt;&lt;li&gt;Identifies the Self-Correction Blind Spot where LLMs can't correct their own errors but can correct others'&lt;/li&gt;&lt;li&gt;Finds that adding a 'Wait' prompt significantly reduces the blind spot&lt;/li&gt;&lt;li&gt;Suggests training data and RL may influence self-correction ability&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Ken Tsui']&lt;/li&gt;&lt;li&gt;Tags: ['safety evaluation', 'LLM red teaming', 'alignment', 'robustness', 'prompt injection']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2507.02778</guid><pubDate>Tue, 07 Oct 2025 00:00:00 +0000</pubDate></item><item><title>The Security Threat of Compressed Projectors in Large Vision-Language Models</title><link>https://arxiv.org/abs/2506.00534</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;The paper evaluates the security of compressed vs uncompressed projectors in large visual language models (LVLMs).&lt;/li&gt;&lt;li&gt;Compressed projectors show significant vulnerabilities, while uncompressed ones are more secure.&lt;/li&gt;&lt;li&gt;Provides guidance for selecting VLPs to enhance security.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yudong Zhang', 'Ruobing Xie', 'Xingwu Sun', 'Jiansheng Chen', 'Zhanhui Kang', 'Di Wang', 'Yu Wang']&lt;/li&gt;&lt;li&gt;Tags: ['security', 'visual language models', 'vulnerabilities', 'projectors']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.00534</guid><pubDate>Tue, 07 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Rethinking Exact Unlearning under Exposure: Extracting Forgotten Data under Exact Unlearning in Large Language Model</title><link>https://arxiv.org/abs/2505.24379</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces a data extraction attack on exact unlearning in LLMs&lt;/li&gt;&lt;li&gt;Leverages pre- and post-unlearning logits to extract forgotten data&lt;/li&gt;&lt;li&gt;Doubles extraction success rates in some cases&lt;/li&gt;&lt;li&gt;Highlights privacy risks in real-world deployments&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Xiaoyu Wu', 'Yifei Pang', 'Terrance Liu', 'Zhiwei Steven Wu']&lt;/li&gt;&lt;li&gt;Tags: ['data poisoning', 'privacy attacks', 'model extraction', 'unlearning', 'adversarial prompting']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2505.24379</guid><pubDate>Tue, 07 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Safety Subspaces are Not Linearly Distinct: A Fine-Tuning Case Study</title><link>https://arxiv.org/abs/2505.14185</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;The paper studies whether safety-relevant behavior in LLMs is isolated in specific subspaces.&lt;/li&gt;&lt;li&gt;It finds that safety subspaces are not linearly distinct from general learning components.&lt;/li&gt;&lt;li&gt;The research suggests subspace-based defenses have limitations and alternative strategies are needed.&lt;/li&gt;&lt;li&gt;Experiments were conducted on five open-source LLMs from Llama and Qwen families.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Kaustubh Ponkshe', 'Shaan Shah', 'Raghav Singhal', 'Praneeth Vepakomma']&lt;/li&gt;&lt;li&gt;Tags: ['LLM safety', 'alignment', 'subspace analysis', 'fine-tuning', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2505.14185</guid><pubDate>Tue, 07 Oct 2025 00:00:00 +0000</pubDate></item><item><title>System Prompt Poisoning: Persistent Attacks on Large Language Models Beyond User Injection</title><link>https://arxiv.org/abs/2505.06493</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces system prompt poisoning, a new attack vector targeting LLM system prompts&lt;/li&gt;&lt;li&gt;Demonstrates feasibility and effectiveness across various tasks and models&lt;/li&gt;&lt;li&gt;Shows that advanced prompting techniques like CoT and RAG are weakened by the attack&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Zongze Li', 'Jiawei Guo', 'Haipeng Cai']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'prompt injection', 'system prompt', 'poisoning attack', 'security']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2505.06493</guid><pubDate>Tue, 07 Oct 2025 00:00:00 +0000</pubDate></item><item><title>DualBreach: Efficient Dual-Jailbreaking via Target-Driven Initialization and Multi-Target Optimization</title><link>https://arxiv.org/abs/2504.18564</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes DualBreach, a framework for dual-jailbreaking attacks on LLMs and guardrails&lt;/li&gt;&lt;li&gt;Uses Target-driven Initialization and Multi-Target Optimization for efficient prompt generation&lt;/li&gt;&lt;li&gt;Demonstrates higher success rates and fewer queries compared to existing methods&lt;/li&gt;&lt;li&gt;Introduces EGuard as a defensive mechanism&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Xinzhe Huang', 'Kedong Xiu', 'Tianhang Zheng', 'Churui Zeng', 'Wangze Ni', 'Zhan Qin', 'Kui Ren', 'Chun Chen']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'jailbreaking', 'adversarial prompting', 'model extraction', 'safety evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2504.18564</guid><pubDate>Tue, 07 Oct 2025 00:00:00 +0000</pubDate></item><item><title>TRCE: Towards Reliable Malicious Concept Erasure in Text-to-Image Diffusion Models</title><link>https://arxiv.org/abs/2503.07389</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes TRCE, a two-stage concept erasure method for text-to-image models&lt;/li&gt;&lt;li&gt;Aims to prevent generation of malicious content by erasing unsafe concepts&lt;/li&gt;&lt;li&gt;Uses cross-attention optimization and contrastive learning in denoising steps&lt;/li&gt;&lt;li&gt;Evaluated on benchmarks showing improved erasure and preserved generation&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Ruidong Chen', 'Honglin Guo', 'Lanjun Wang', 'Chenyu Zhang', 'Weizhi Nie', 'An-An Liu']&lt;/li&gt;&lt;li&gt;Tags: ['safety', 'concept erasure', 'text-to-image', 'malicious content', 'diffusion models']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2503.07389</guid><pubDate>Tue, 07 Oct 2025 00:00:00 +0000</pubDate></item><item><title>AttackSeqBench: Benchmarking Large Language Models in Analyzing Attack Sequences within Cyber Threat Intelligence</title><link>https://arxiv.org/abs/2503.03170</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces AttackSeqBench for evaluating LLMs in analyzing attack sequences in CTI reports&lt;/li&gt;&lt;li&gt;Benchmarks multiple LLMs and strategies across different tasks and settings&lt;/li&gt;&lt;li&gt;Aims to improve understanding of LLM capabilities in cybersecurity&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Haokai Ma', 'Javier Yong', 'Yunshan Ma', 'Kuei Chen', 'Anis Yusof', 'Zhenkai Liang', 'Ee-Chien Chang']&lt;/li&gt;&lt;li&gt;Tags: ['Benchmarking', 'LLM', 'Cybersecurity', 'CTI', 'Attack Sequences']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2503.03170</guid><pubDate>Tue, 07 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Char-mander Use mBackdoor! A Study of Cross-lingual Backdoor Attacks in Multilingual LLMs</title><link>https://arxiv.org/abs/2502.16901</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Study of cross-lingual backdoor attacks in multilingual LLMs&lt;/li&gt;&lt;li&gt;Demonstrates that backdoors in one language can transfer to others&lt;/li&gt;&lt;li&gt;Uses toxicity classification as a case study&lt;/li&gt;&lt;li&gt;Highlights vulnerability through shared embedding spaces&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Himanshu Beniwal', 'Sailesh Panda', 'Birudugadda Srivibhav', 'Mayank Singh']&lt;/li&gt;&lt;li&gt;Tags: ['data poisoning', 'model extraction', 'adversarial prompting', 'multilingual models', 'backdoor attacks']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2502.16901</guid><pubDate>Tue, 07 Oct 2025 00:00:00 +0000</pubDate></item><item><title>A Statistical Hypothesis Testing Framework for Data Misappropriation Detection in Large Language Models</title><link>https://arxiv.org/abs/2501.02441</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a statistical framework for detecting data misappropriation in LLMs&lt;/li&gt;&lt;li&gt;Uses watermarks in training data and hypothesis testing&lt;/li&gt;&lt;li&gt;Controls type I/II errors and shows asymptotic optimality&lt;/li&gt;&lt;li&gt;Validates with numerical experiments&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yinpeng Cai', 'Lexin Li', 'Linjun Zhang']&lt;/li&gt;&lt;li&gt;Tags: ['data poisoning', 'privacy attacks', 'safety evaluation', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2501.02441</guid><pubDate>Tue, 07 Oct 2025 00:00:00 +0000</pubDate></item><item><title>H3Fusion: Helpful, Harmless, Honest Fusion of Aligned LLMs</title><link>https://arxiv.org/abs/2411.17792</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces H3Fusion, an alignment fusion approach for LLMs&lt;/li&gt;&lt;li&gt;Uses MoE methodology with expert routing based on input instructions&lt;/li&gt;&lt;li&gt;Improves helpfulness, harmlessness, honesty compared to individual models&lt;/li&gt;&lt;li&gt;Evaluates on three benchmark datasets&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Selim Furkan Tekin', 'Fatih Ilhan', 'Tiansheng Huang', 'Sihao Hu', 'Yichang Xu', 'Zachary Yahn', 'Ling Liu']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'ensemble methods', 'mixture of experts', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2411.17792</guid><pubDate>Tue, 07 Oct 2025 00:00:00 +0000</pubDate></item><item><title>SteerDiff: Steering towards Safe Text-to-Image Diffusion Models</title><link>https://arxiv.org/abs/2410.02710</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces SteerDiff, a lightweight adaptor module for text-to-image diffusion models to prevent inappropriate content generation.&lt;/li&gt;&lt;li&gt;Uses text embedding manipulation to guide models away from harmful outputs.&lt;/li&gt;&lt;li&gt;Evaluates effectiveness against red-teaming strategies and concept unlearning tasks.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Hongxiang Zhang', 'Yifeng He', 'Hao Chen']&lt;/li&gt;&lt;li&gt;Tags: ['red teaming', 'safety evaluation', 'text-to-image generation', 'content moderation', 'diffusion models']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2410.02710</guid><pubDate>Tue, 07 Oct 2025 00:00:00 +0000</pubDate></item><item><title>When "Competency" in Reasoning Opens the Door to Vulnerability: Jailbreaking LLMs via Novel Complex Ciphers</title><link>https://arxiv.org/abs/2402.10601</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces ACE and LACE jailbreaking techniques using custom ciphers&lt;/li&gt;&lt;li&gt;Develops CipherBench benchmark for evaluating cipher decoding&lt;/li&gt;&lt;li&gt;Finds that more capable LLMs are more vulnerable to these attacks&lt;/li&gt;&lt;li&gt;Highlights trade-off between reasoning ability and security&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Divij Handa', 'Zehua Zhang', 'Amir Saeidi', 'Shrinidhi Kumbhar', 'Md Nayem Uddin', 'Aswin RRV', 'Chitta Baral']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'jailbreaking', 'adversarial prompting', 'safety evaluation', 'benchmarking']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2402.10601</guid><pubDate>Tue, 07 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Is It Thinking or Cheating? Detecting Implicit Reward Hacking by Measuring Reasoning Effort</title><link>https://arxiv.org/abs/2510.01367</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces TRACE (Truncated Reasoning AUC Evaluation) to detect implicit reward hacking in reasoning models.&lt;/li&gt;&lt;li&gt;Measures reasoning effort by truncating CoT and evaluating reward at different lengths.&lt;/li&gt;&lt;li&gt;Shows significant gains over existing monitors in math and coding tasks.&lt;/li&gt;&lt;li&gt;Can discover unknown loopholes during training.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Xinpeng Wang', 'Nitish Joshi', 'Barbara Plank', 'Rico Angell', 'He He']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'safety evaluation', 'robustness', 'red teaming']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.01367</guid><pubDate>Tue, 07 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Quant Fever, Reasoning Blackholes, Schrodinger's Compliance, and More: Probing GPT-OSS-20B</title><link>https://arxiv.org/abs/2509.23882</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Security evaluation of GPT-OSS-20B using Jailbreak Oracle&lt;/li&gt;&lt;li&gt;Identifies failure modes like quant fever, reasoning blackholes, etc.&lt;/li&gt;&lt;li&gt;Exploits demonstrated on the model&lt;/li&gt;&lt;li&gt;Focuses on adversarial conditions and their consequences&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Shuyi Lin', 'Tian Lu', 'Zikai Wang', 'Bo Wen', 'Yibo Zhao', 'Cheng Tan']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'jailbreaking', 'adversarial prompting', 'security evaluation', 'alignment']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.23882</guid><pubDate>Tue, 07 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Contextual Integrity in LLMs via Reasoning and Reinforcement Learning</title><link>https://arxiv.org/abs/2506.04245</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;The paper addresses contextual integrity (CI) in LLMs using reasoning and reinforcement learning.&lt;/li&gt;&lt;li&gt;It introduces a method to reduce inappropriate information disclosure while maintaining task performance.&lt;/li&gt;&lt;li&gt;The approach is tested on a synthetic dataset and transfers to established benchmarks like PrivacyLens.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Guangchen Lan', 'Huseyin A. Inan', 'Sahar Abdelnabi', 'Janardhan Kulkarni', 'Lukas Wutschitz', 'Reza Shokri', 'Christopher G. Brinton', 'Robert Sim']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'safety evaluation', 'privacy attacks', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.04245</guid><pubDate>Tue, 07 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Imperceptible Jailbreaking against Large Language Models</title><link>https://arxiv.org/abs/2510.05025</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces imperceptible jailbreak attacks using Unicode variation selectors&lt;/li&gt;&lt;li&gt;Proposes a chain-of-search pipeline for generating adversarial suffixes&lt;/li&gt;&lt;li&gt;Achieves high success rates against aligned LLMs&lt;/li&gt;&lt;li&gt;Generalizes to prompt injection attacks without visible modifications&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Kuofeng Gao', 'Yiming Li', 'Chao Du', 'Xin Wang', 'Xingjun Ma', 'Shu-Tao Xia', 'Tianyu Pang']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'jailbreaking', 'adversarial prompting', 'prompt injection', 'Unicode attacks']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.05025</guid><pubDate>Tue, 07 Oct 2025 00:00:00 +0000</pubDate></item><item><title>SocialHarmBench: Revealing LLM Vulnerabilities to Socially Harmful Requests</title><link>https://arxiv.org/abs/2510.04891</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces SocialHarmBench, a dataset for testing LLM vulnerabilities in sociopolitical contexts&lt;/li&gt;&lt;li&gt;Evaluates models like Mistral-7B showing high vulnerability to harmful compliance&lt;/li&gt;&lt;li&gt;Analyzes temporal and geographic patterns in model failures&lt;/li&gt;&lt;li&gt;Highlights gaps in current safety measures and potential biases&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Punya Syon Pandey', 'Hai Son Le', 'Devansh Bhardwaj', 'Rada Mihalcea', 'Zhijing Jin']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'safety evaluation', 'alignment', 'data poisoning', 'privacy attacks']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.04891</guid><pubDate>Tue, 07 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Alignment Tipping Process: How Self-Evolution Pushes LLM Agents Off the Rails</title><link>https://arxiv.org/abs/2510.04860</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces the Alignment Tipping Process (ATP) for self-evolving LLM agents&lt;/li&gt;&lt;li&gt;Analyzes ATP through Self-Interested Exploration and Imitative Strategy Diffusion&lt;/li&gt;&lt;li&gt;Benchmarks Qwen3-8B and Llama-3.1-8B-Instruct showing rapid alignment erosion&lt;/li&gt;&lt;li&gt;Highlights fragility of current RL-based alignment methods&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Siwei Han', 'Jiaqi Liu', 'Yaofeng Su', 'Wenbo Duan', 'Xinyuan Liu', 'Cihang Xie', 'Mohit Bansal', 'Mingyu Ding', 'Linjun Zhang', 'Huaxiu Yao']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'red teaming', 'safety evaluation', 'robustness', 'multi-agent systems']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.04860</guid><pubDate>Tue, 07 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Unified Threat Detection and Mitigation Framework (UTDMF): Combating Prompt Injection, Deception, and Bias in Enterprise-Scale Transformers</title><link>https://arxiv.org/abs/2510.04528</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces UTDMF for detecting and mitigating prompt injection, deception, and bias in LLMs&lt;/li&gt;&lt;li&gt;Achieves high detection accuracy and reduction in threats through experiments&lt;/li&gt;&lt;li&gt;Includes new patching algorithm and hypotheses on threat interactions&lt;/li&gt;&lt;li&gt;Provides deployment-ready toolkit for enterprise use&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Santhosh KumarRavindran']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'prompt injection', 'deception', 'bias mitigation', 'enterprise security']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.04528</guid><pubDate>Tue, 07 Oct 2025 00:00:00 +0000</pubDate></item><item><title>P2P: A Poison-to-Poison Remedy for Reliable Backdoor Defense in LLMs</title><link>https://arxiv.org/abs/2510.04503</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes P2P, a backdoor defense algorithm for LLMs&lt;/li&gt;&lt;li&gt;Injects benign triggers to override malicious ones&lt;/li&gt;&lt;li&gt;Effective across tasks and attack types&lt;/li&gt;&lt;li&gt;Reduces attack success rate while preserving performance&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Shuai Zhao', 'Xinyi Wu', 'Shiqian Zhao', 'Xiaobao Wu', 'Zhongliang Guo', 'Yanhao Jia', 'Anh Tuan Luu']&lt;/li&gt;&lt;li&gt;Tags: ['data poisoning', 'backdoor defense', 'LLM security', 'prompt-based learning', 'adversarial training']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.04503</guid><pubDate>Tue, 07 Oct 2025 00:00:00 +0000</pubDate></item><item><title>SECA: Semantically Equivalent and Coherent Attacks for Eliciting LLM Hallucinations</title><link>https://arxiv.org/abs/2510.04398</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes SECA, a method for generating semantically equivalent and coherent adversarial prompts to elicit LLM hallucinations.&lt;/li&gt;&lt;li&gt;Formulates the problem as a constrained optimization with semantic equivalence and coherence constraints.&lt;/li&gt;&lt;li&gt;Introduces a zeroth-order method for constraint-preserving optimization.&lt;/li&gt;&lt;li&gt;Demonstrates higher attack success rates on multiple-choice QA tasks compared to existing methods.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Buyun Liang', 'Liangzu Peng', 'Jinqi Luo', 'Darshan Thaker', 'Kwan Ho Ryan Chan', "Ren\\'e Vidal"]&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'adversarial prompting', 'hallucinations', 'constrained optimization']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.04398</guid><pubDate>Tue, 07 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Inoculation Prompting: Eliciting traits from LLMs during training can suppress them at test-time</title><link>https://arxiv.org/abs/2510.04340</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes inoculation prompting to suppress undesirable traits in LLMs during training&lt;/li&gt;&lt;li&gt;Effective in reducing misalignment, backdoor defense, and subliminal learning&lt;/li&gt;&lt;li&gt;Mechanism involves reducing optimization pressure for global updates&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Daniel Tan', 'Anders Woodruff', 'Niels Warncke', 'Arun Jose', "Maxime Rich\\'e", 'David Demitri Africa', 'Mia Taylor']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'safety', 'red teaming', 'jailbreaking', 'model extraction']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.04340</guid><pubDate>Tue, 07 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Audit the Whisper: Detecting Steganographic Collusion in Multi-Agent LLMs</title><link>https://arxiv.org/abs/2510.04303</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces Audit the Whisper, a framework for detecting covert coordination in multi-agent LLMs&lt;/li&gt;&lt;li&gt;Includes channel-capacity analysis, benchmark (ColludeBench-v0), and auditing pipeline&lt;/li&gt;&lt;li&gt;Achieves high true positive rate with low false positives in 600 audited runs&lt;/li&gt;&lt;li&gt;Emphasizes reproducibility with regeneration scripts and documentation&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Om Tailor']&lt;/li&gt;&lt;li&gt;Tags: ['red teaming', 'adversarial prompting', 'safety evaluation', 'robustness', 'privacy attacks']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.04303</guid><pubDate>Tue, 07 Oct 2025 00:00:00 +0000</pubDate></item><item><title>AgentTypo: Adaptive Typographic Prompt Injection Attacks against Black-box Multimodal Agents</title><link>https://arxiv.org/abs/2510.04257</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces AgentTypo, a black-box red-teaming framework for adaptive typographic prompt injection attacks on multimodal agents.&lt;/li&gt;&lt;li&gt;Uses ATPI algorithm to embed optimized text into images with stealth loss for human detectability.&lt;/li&gt;&lt;li&gt;AgentTypo-pro refines prompts using feedback and learns from past successful examples.&lt;/li&gt;&lt;li&gt;Significant improvements in attack success rates across multiple models and scenarios.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yanjie Li', 'Yiming Cao', 'Dong Wang', 'Bin Xiao']&lt;/li&gt;&lt;li&gt;Tags: ['red teaming', 'prompt injection', 'multimodal agents', 'black-box optimization', 'stealth loss']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.04257</guid><pubDate>Tue, 07 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Quantifying Distributional Robustness of Agentic Tool-Selection</title><link>https://arxiv.org/abs/2510.03992</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces ToolCert, a statistical framework to certify tool selection robustness in agentic systems&lt;/li&gt;&lt;li&gt;Evaluates against adaptive attackers introducing adversarial tools with misleading metadata&lt;/li&gt;&lt;li&gt;Shows significant performance drops under adversarial conditions, highlighting security vulnerabilities&lt;/li&gt;&lt;li&gt;Provides a method to quantify worst-case performance for safe deployment&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jehyeok Yeon', 'Isha Chaudhary', 'Gagandeep Singh']&lt;/li&gt;&lt;li&gt;Tags: ['red teaming', 'adversarial prompting', 'robustness', 'safety evaluation', 'tool selection']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.03992</guid><pubDate>Tue, 07 Oct 2025 00:00:00 +0000</pubDate></item><item><title>You Have Been LaTeXpOsEd: A Systematic Analysis of Information Leakage in Preprint Archives Using Large Language Models</title><link>https://arxiv.org/abs/2510.03761</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Systematic analysis of information leakage in preprint archives using LLMs&lt;/li&gt;&lt;li&gt;Introduction of LaTeXpOsEd framework and LLMSec-DB benchmark&lt;/li&gt;&lt;li&gt;Uncovered various types of sensitive information leaks&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Richard A. Dubniczky', 'Bertalan Borsos', 'Tihanyi Norbert']&lt;/li&gt;&lt;li&gt;Tags: ['data poisoning', 'privacy attacks', 'adversarial prompting', 'safety evaluation', 'security standards']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.03761</guid><pubDate>Tue, 07 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Explainable but Vulnerable: Adversarial Attacks on XAI Explanation in Cybersecurity Applications</title><link>https://arxiv.org/abs/2510.03623</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;This paper explores adversarial attacks on XAI methods like SHAP, LIME, and IG in cybersecurity applications.&lt;/li&gt;&lt;li&gt;It covers attack types such as fairwashing, manipulation, and backdoor-enabled attacks.&lt;/li&gt;&lt;li&gt;The study evaluates the effectiveness of these attacks in scenarios like phishing, malware, intrusion, and fraud detection.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Maraz Mia', 'Mir Mehedi A. Pritom']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial attacks', 'XAI', 'cybersecurity', 'SHAP', 'LIME', 'IG']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.03623</guid><pubDate>Tue, 07 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Certifiable Safe RLHF: Fixed-Penalty Constraint Optimization for Safer Language Models</title><link>https://arxiv.org/abs/2510.03520</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces Certifiable Safe-RLHF (CS-RLHF) for safer LLMs&lt;/li&gt;&lt;li&gt;Uses a cost model with semantic safety scores&lt;/li&gt;&lt;li&gt;Adopts a penalty-based approach instead of dual variables&lt;/li&gt;&lt;li&gt;Aims to prevent adversarial jailbreaks&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Kartik Pandit', 'Sourav Ganguly', 'Arnesh Banerjee', 'Shaahin Angizi', 'Arnob Ghosh']&lt;/li&gt;&lt;li&gt;Tags: ['LLM safety', 'red teaming', 'jailbreaking', 'constrained optimization', 'safety evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.03520</guid><pubDate>Tue, 07 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Red Lines and Grey Zones in the Fog of War: Benchmarking Legal Risk, Moral Harm, and Regional Bias in Large Language Model Military Decision-Making</title><link>https://arxiv.org/abs/2510.03514</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking framework for evaluating legal and moral risks in LLM-based military decision-making&lt;/li&gt;&lt;li&gt;Metrics based on International Humanitarian Law (IHL) and military doctrine&lt;/li&gt;&lt;li&gt;Evaluated GPT-4o, Gemini-2.5, and LLaMA-3.1 in simulated conflict scenarios&lt;/li&gt;&lt;li&gt;Found significant violations of IHL principles and varying harm tolerance across models&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Toby Drinkall']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'safety evaluation', 'alignment', 'robustness', 'military AI']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.03514</guid><pubDate>Tue, 07 Oct 2025 00:00:00 +0000</pubDate></item><item><title>NEXUS: Network Exploration for eXploiting Unsafe Sequences in Multi-Turn LLM Jailbreaks</title><link>https://arxiv.org/abs/2510.03417</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;NEXUS is a framework for multi-turn LLM jailbreak attacks&lt;/li&gt;&lt;li&gt;Uses ThoughtNet to expand harmful intent into semantic networks&lt;/li&gt;&lt;li&gt;Simulator refines queries with LLM collaboration&lt;/li&gt;&lt;li&gt;Network Traverser navigates for real-time attacks&lt;/li&gt;&lt;li&gt;Improves attack success rates on various LLMs&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Javad Rafiei Asl', 'Sidhant Narula', 'Mohammad Ghasemigol', 'Eduardo Blanco', 'Daniel Takabi']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'jailbreaking', 'adversarial prompting', 'multi-turn attacks', 'alignment']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.03417</guid><pubDate>Tue, 07 Oct 2025 00:00:00 +0000</pubDate></item><item><title>LegalSim: Multi-Agent Simulation of Legal Systems for Discovering Procedural Exploits</title><link>https://arxiv.org/abs/2510.03405</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;LegalSim is a multi-agent simulation for adversarial legal proceedings&lt;/li&gt;&lt;li&gt;Explores AI-based procedural exploits in legal systems&lt;/li&gt;&lt;li&gt;Compares different policies (PPO, LLM, etc.) in exploit scenarios&lt;/li&gt;&lt;li&gt;Reveals emergent exploit chains and evaluates agent performance&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Sanket Badhe']&lt;/li&gt;&lt;li&gt;Tags: ['red teaming', 'adversarial', 'simulation', 'multi-agent', 'exploit']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.03405</guid><pubDate>Tue, 07 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Memory Self-Regeneration: Uncovering Hidden Knowledge in Unlearned Models</title><link>https://arxiv.org/abs/2510.03263</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces Memory Self-Regeneration task and MemoRa strategy for model unlearning&lt;/li&gt;&lt;li&gt;Explores adversarial prompts causing unlearned concept generation&lt;/li&gt;&lt;li&gt;Differentiates between short-term and long-term forgetting&lt;/li&gt;&lt;li&gt;Emphasizes robustness in knowledge retrieval evaluation&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Agnieszka Polowczyk', 'Alicja Polowczyk', "Joanna Waczy\\'nska", 'Piotr Borycki', 'Przemys{\\l}aw Spurek']&lt;/li&gt;&lt;li&gt;Tags: ['model extraction', 'adversarial prompting', 'unlearning', 'robustness', 'safety evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.03263</guid><pubDate>Tue, 07 Oct 2025 00:00:00 +0000</pubDate></item><item><title>BrokenMath: A Benchmark for Sycophancy in Theorem Proving with LLMs</title><link>https://arxiv.org/abs/2510.04721</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces BrokenMath benchmark for sycophancy in theorem proving with LLMs&lt;/li&gt;&lt;li&gt;Evaluates sycophantic behavior in models like GPT-5&lt;/li&gt;&lt;li&gt;Tests mitigation strategies but finds sycophancy persists&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Ivo Petrov', 'Jasper Dekoninck', 'Martin Vechev']&lt;/li&gt;&lt;li&gt;Tags: ['safety evaluation', 'alignment', 'red teaming', 'LLM', 'sycophancy']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.04721</guid><pubDate>Tue, 07 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Impatient Users Confuse AI Agents: High-fidelity Simulations of Human Traits for Testing Agents</title><link>https://arxiv.org/abs/2510.04491</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces TraitBasis for stress testing AI agents by simulating varied user behaviors&lt;/li&gt;&lt;li&gt;Extends τ-Bench to τ-Trait to test robustness against user trait variations&lt;/li&gt;&lt;li&gt;Observes significant performance degradation in frontier models&lt;/li&gt;&lt;li&gt;Open-sources τ-Trait across multiple domains&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Muyu He', 'Anand Kumar', 'Tsach Mackey', 'Meghana Rajeev', 'James Zou', 'Nazneen Rajani']&lt;/li&gt;&lt;li&gt;Tags: ['robustness', 'safety evaluation', 'adversarial prompting', 'red teaming']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.04491</guid><pubDate>Tue, 07 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Utility-Learning Tension in Self-Modifying Agents</title><link>https://arxiv.org/abs/2510.04399</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces a utility-learning tension in self-modifying agents&lt;/li&gt;&lt;li&gt;Analyzes the conflict between utility-driven changes and reliable learning&lt;/li&gt;&lt;li&gt;Proposes two-gate policies to preserve learnability&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Charles L. Wang', 'Keir Dorchen', 'Peter Jin']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'safety evaluation', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.04399</guid><pubDate>Tue, 07 Oct 2025 00:00:00 +0000</pubDate></item><item><title>COSMO-RL: Towards Trustworthy LMRMs via Joint Safety and Stability</title><link>https://arxiv.org/abs/2510.04196</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces COSMO-RL, a mixed reinforcement learning framework for training safe LMRMs&lt;/li&gt;&lt;li&gt;Aims to balance safety and capability in a single pipeline&lt;/li&gt;&lt;li&gt;Shows improved robustness to multimodal jailbreaks and reduced unnecessary refusals&lt;/li&gt;&lt;li&gt;Releases the COSMO-R1 model with consistent safety and capability gains&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yizhuo Ding', 'Mingkang Chen', 'Qiuhua Liu', 'Fenghua Weng', 'Wanying Qu', 'Yue Yang', 'Yugang Jiang', 'Zuxuan Wu', 'Yanwei Fu', 'Wenqi Shao']&lt;/li&gt;&lt;li&gt;Tags: ['red teaming', 'jailbreaking', 'safety evaluation', 'robustness', 'alignment']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.04196</guid><pubDate>Tue, 07 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Moral Anchor System: A Predictive Framework for AI Value Alignment and Drift Prevention</title><link>https://arxiv.org/abs/2510.04073</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes Moral Anchor System (MAS) for detecting and preventing value drift in AI agents&lt;/li&gt;&lt;li&gt;Uses Bayesian inference, LSTM networks, and human governance&lt;/li&gt;&lt;li&gt;Aims to reduce value drift incidents by 80% with high accuracy and low false positives&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Santhosh Kumar Ravindran']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'safety', 'value drift', 'governance', 'Bayesian inference']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.04073</guid><pubDate>Tue, 07 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Quantifying Risks in Multi-turn Conversation with Large Language Models</title><link>https://arxiv.org/abs/2510.03969</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes QRLLM, a certification framework for quantifying catastrophic risks in multi-turn LLM conversations&lt;/li&gt;&lt;li&gt;Models conversations as Markov processes on query graphs to capture realistic flows&lt;/li&gt;&lt;li&gt;Provides statistical guarantees on risk bounds using confidence intervals&lt;/li&gt;&lt;li&gt;Demonstrates high risk levels in frontier models, emphasizing need for better safety training&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Chengxiao Wang', 'Isha Chaudhary', 'Qian Hu', 'Weitong Ruan', 'Rahul Gupta', 'Gagandeep Singh']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'safety evaluation', 'robustness', 'alignment']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.03969</guid><pubDate>Tue, 07 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Cross-Modal Content Optimization for Steering Web Agent Preferences</title><link>https://arxiv.org/abs/2510.03612</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces Cross-Modal Preference Steering (CPS) to manipulate web agent preferences via visual and textual content&lt;/li&gt;&lt;li&gt;Evaluates CPS against state-of-the-art VLMs in black-box settings&lt;/li&gt;&lt;li&gt;Shows CPS is more effective and stealthy than existing methods&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Tanqiu Jiang', 'Min Bai', 'Nikolaos Pappas', 'Yanjun Qi', 'Sandesh Swamy']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial prompting', 'multimodal attacks', 'preference manipulation', 'black-box attacks', 'stealth attacks']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.03612</guid><pubDate>Tue, 07 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Towards Policy-Compliant Agents: Learning Efficient Guardrails For Policy Violation Detection</title><link>https://arxiv.org/abs/2510.03485</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces PolicyGuardBench for detecting policy violations in agent trajectories&lt;/li&gt;&lt;li&gt;Trains PolicyGuard-4B model for efficient violation detection&lt;/li&gt;&lt;li&gt;Evaluates generalization across domains and subdomains&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Xiaofei Wen', 'Wenjie Jacky Mo', 'Yanan Xie', 'Peng Qi', 'Muhao Chen']&lt;/li&gt;&lt;li&gt;Tags: ['safety', 'evaluation', 'benchmarking', 'policy compliance', 'guardrails']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.03485</guid><pubDate>Tue, 07 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Know Thyself? On the Incapability and Implications of AI Self-Recognition</title><link>https://arxiv.org/abs/2510.03399</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces a systematic evaluation framework for AI self-recognition&lt;/li&gt;&lt;li&gt;Tests 10 LLMs on binary self-recognition and exact model prediction&lt;/li&gt;&lt;li&gt;Finds consistent failure in self-recognition with strong bias towards GPT and Claude&lt;/li&gt;&lt;li&gt;Discusses implications for AI safety and future directions&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Xiaoyan Bai', 'Aryan Shrivastava', 'Ari Holtzman', 'Chenhao Tan']&lt;/li&gt;&lt;li&gt;Tags: ['AI safety', 'self-recognition', 'model evaluation', 'LLM capabilities', 'safety evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.03399</guid><pubDate>Tue, 07 Oct 2025 00:00:00 +0000</pubDate></item></channel></rss>