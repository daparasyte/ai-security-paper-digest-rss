<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>AI Security Paper Digest</title><link>https://kentaroh-toyoda.github.io/ai-security-paper-digest-rss/rss.xml</link><description>Curated papers on AI security from OpenAlex and ArXiv</description><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><lastBuildDate>Fri, 06 Jun 2025 09:26:18 +0000</lastBuildDate><item><title>Practical Manipulation Model for Robust Deepfake Detection</title><link>https://arxiv.org/abs/2506.05119</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a Practical Manipulation Model (PMM) to improve the robustness of deepfake detection systems.&lt;/li&gt;&lt;li&gt;Expands the range of synthetic manipulations (pseudo-fakes) using advanced techniques like Poisson blending and diverse artifacts.&lt;/li&gt;&lt;li&gt;Demonstrates improved robustness and generalization of deepfake detectors under various image degradations.&lt;/li&gt;&lt;li&gt;Highlights and addresses the vulnerabilities of existing deepfake detectors to adversarial manipulations.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research Paper&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Quality Assessment&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Clarity: 4/5&lt;/li&gt;&lt;li&gt;Novelty: 4/5&lt;/li&gt;&lt;li&gt;Significance: 4/5&lt;/li&gt;&lt;li&gt;Try-worthiness: Yes&lt;/li&gt;&lt;li&gt;Justification: The abstract is well-written and clearly explains the motivation, methodology, and results of the paper, though some technical details are left for the full text. The work is novel in its introduction of a Practical Manipulation Model (PMM) that expands the space of pseudo-fakes and incorporates real-world degradations, which is a meaningful extension over prior approaches. The significance is high given the persistent challenge of robust deepfake detection, and the reported improvements over state-of-the-art baselines on standard benchmarks (DFDC, DFDCP) are substantial. The paper is recent and on arXiv, so citation count is not yet meaningful, but the topic and results suggest it is worth trying, especially since code is available.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: Benedikt Hopf, Radu Timofte&lt;/li&gt;&lt;li&gt;Tags: deepfake detection, robustness, adversarial attacks, AI security&lt;/li&gt;&lt;li&gt;Relevance Score: 4/5&lt;/li&gt;&lt;li&gt;Code Repository: &lt;a href='https://github.com/BenediktHopf/PMM'&gt;https://github.com/BenediktHopf/PMM&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.05119</guid><pubDate>Fri, 06 Jun 2025 00:00:00 +0000</pubDate></item><item><title>FG 2025 TrustFAA: the First Workshop on Towards Trustworthy Facial Affect Analysis: Advancing Insights of Fairness, Explainability, and Safety (TrustFAA)</title><link>https://arxiv.org/abs/2506.05095</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Workshop focuses on trustworthiness in facial affect analysis (FAA) systems, including safety and privacy concerns.&lt;/li&gt;&lt;li&gt;Addresses challenges such as interpretability, uncertainty, biases, and privacy in Emotion AI-powered FAA tools.&lt;/li&gt;&lt;li&gt;Aims to advance research and discussion on fairness, explainability, and safety in FAA applications, including sensitive domains like pain and depression detection.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Other&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Quality Assessment&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Clarity: 4/5&lt;/li&gt;&lt;li&gt;Novelty: 3/5&lt;/li&gt;&lt;li&gt;Significance: 3/5&lt;/li&gt;&lt;li&gt;Try-worthiness: No&lt;/li&gt;&lt;li&gt;Justification: The abstract is clearly written and effectively communicates the workshop's aims and scope, earning a high clarity score. However, as this is a workshop announcement rather than a research paper presenting new methods or results, its novelty and significance are moderate; it is important for community-building and discussion but does not itself introduce new technical contributions. There is no code repository provided, and since this is not a technical contribution, it is not directly try-worthy for implementation or experimentation.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: Jiaee Cheong, Yang Liu, Harold Soh, Hatice Gunes&lt;/li&gt;&lt;li&gt;Tags: AI safety, privacy, fairness, explainability, facial affect analysis&lt;/li&gt;&lt;li&gt;Relevance Score: 3/5&lt;/li&gt;&lt;li&gt;Code Repository: &lt;a href='None'&gt;None&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.05095</guid><pubDate>Fri, 06 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Invisible Backdoor Triggers in Image Editing Model via Deep Watermarking</title><link>https://arxiv.org/abs/2506.04879</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a novel backdoor attack on image editing diffusion models using invisible triggers embedded via deep watermarking.&lt;/li&gt;&lt;li&gt;Demonstrates that imperceptible watermarks can serve as effective backdoor triggers, causing the model to produce attacker-specified outputs.&lt;/li&gt;&lt;li&gt;Shows that the attack does not affect normal editing of clean images, making detection more difficult.&lt;/li&gt;&lt;li&gt;Provides experimental validation across different watermarking models and analyzes the characteristics of watermarks as backdoor triggers.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research Paper&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Quality Assessment&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Clarity: 4/5&lt;/li&gt;&lt;li&gt;Novelty: 5/5&lt;/li&gt;&lt;li&gt;Significance: 4/5&lt;/li&gt;&lt;li&gt;Try-worthiness: Yes&lt;/li&gt;&lt;li&gt;Justification: The abstract is well-written and clearly explains the motivation, methodology, and results of the paper, though some technical details are naturally omitted in the abstract. The work is highly novel, as it addresses the underexplored area of invisible backdoor triggers in image editing models using deep watermarking, which is a new attack vector compared to prior work focusing on visible triggers or image generation. The significance is high given the growing use of diffusion models in image editing and the security implications, though the paper is very recent and has not yet accumulated citations. The method is worth trying, especially for researchers in AI security, watermarking, or diffusion models, as it demonstrates a practical and stealthy attack with promising results. The code is available, increasing reproducibility and practical value.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: Yu-Feng Chen, Tzuhsuan Huang, Pin-Yen Chiu, Jun-Cheng Chen&lt;/li&gt;&lt;li&gt;Tags: backdoor attacks, data poisoning, diffusion models, deep watermarking, AI security&lt;/li&gt;&lt;li&gt;Relevance Score: 5/5&lt;/li&gt;&lt;li&gt;Code Repository: &lt;a href='https://github.com/aiiu-lab/BackdoorImageEditing'&gt;https://github.com/aiiu-lab/BackdoorImageEditing&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.04879</guid><pubDate>Fri, 06 Jun 2025 00:00:00 +0000</pubDate></item><item><title>SRD: Reinforcement-Learned Semantic Perturbation for Backdoor Defense in VLMs</title><link>https://arxiv.org/abs/2506.04743</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Analyzes vulnerabilities in Vision-Language Models (VLMs) to backdoor attacks involving both visual and semantic triggers.&lt;/li&gt;&lt;li&gt;Proposes Semantic Reward Defense (SRD), a reinforcement learning-based framework to defend against backdoor attacks without prior knowledge of triggers.&lt;/li&gt;&lt;li&gt;SRD uses a Deep Q-Network to perturb sensitive image regions and employs a semantic fidelity score to maintain caption quality.&lt;/li&gt;&lt;li&gt;Experimental results show significant reduction in attack success rates while preserving model performance on clean data.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research Paper&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Quality Assessment&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Clarity: 5/5&lt;/li&gt;&lt;li&gt;Novelty: 4/5&lt;/li&gt;&lt;li&gt;Significance: 3/5&lt;/li&gt;&lt;li&gt;Try-worthiness: Yes&lt;/li&gt;&lt;li&gt;Justification: The abstract is well-written, clearly outlining the problem, vulnerabilities, and the proposed solution (SRD). The use of reinforcement learning (Deep Q-Network) for trigger-agnostic backdoor defense in vision-language models is a novel approach, especially with the semantic fidelity reward. While the paper is very recent and published as an arXiv preprint (so significance is not yet established via citations or peer review), the problem addressed is timely and important. The reported results are promising, showing substantial reduction in attack success rates with minimal impact on clean performance. The lack of a code repository is a drawback, but the method appears worth experimenting with for researchers in VLM security.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: Shuhan Xu, Siyuan Liang, Hongling Zheng, Yong Luo, Aishan Liu, Dacheng Tao&lt;/li&gt;&lt;li&gt;Tags: backdoor attacks, vision-language models, reinforcement learning defense, AI security, robustness&lt;/li&gt;&lt;li&gt;Relevance Score: 5/5&lt;/li&gt;&lt;li&gt;Code Repository: &lt;a href='None'&gt;None&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.04743</guid><pubDate>Fri, 06 Jun 2025 00:00:00 +0000</pubDate></item><item><title>AuthGuard: Generalizable Deepfake Detection via Language Guidance</title><link>https://arxiv.org/abs/2506.04501</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes AuthGuard, a deepfake detection framework that leverages language guidance to improve generalization to unseen forgeries.&lt;/li&gt;&lt;li&gt;Combines discriminative classification with image-text contrastive learning, using text generated by large multimodal language models.&lt;/li&gt;&lt;li&gt;Integrates data uncertainty learning to enhance robustness against noisy image-text supervision.&lt;/li&gt;&lt;li&gt;Demonstrates state-of-the-art performance on multiple deepfake detection benchmarks, including improved reasoning about deepfakes.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research Paper&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Quality Assessment&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Clarity: 4/5&lt;/li&gt;&lt;li&gt;Novelty: 5/5&lt;/li&gt;&lt;li&gt;Significance: 4/5&lt;/li&gt;&lt;li&gt;Try-worthiness: Yes&lt;/li&gt;&lt;li&gt;Justification: The abstract is well-written and clearly explains the motivation, methodology, and results of the paper, though some technical terms may require background knowledge (Clarity: 4). The approach of integrating language guidance and commonsense reasoning via MLLMs into deepfake detection is novel and addresses a key limitation of current methods (Novelty: 5). The reported improvements on multiple benchmarks and the focus on generalization and interpretability suggest significant potential impact, though the lack of peer review and citations (due to recency and arXiv status) tempers this slightly (Significance: 4). The promising results and the generalizability claims make this paper worth implementing or experimenting with (Try-worthiness: true). No code repository is provided in the metadata or abstract.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: Guangyu Shen, Zhihua Li, Xiang Xu, Tianchen Zhao, Zheng Zhang, Dongsheng An, Zhuowen Tu, Yifan Xing, Qin Zhang&lt;/li&gt;&lt;li&gt;Tags: deepfake detection, AI security, robustness, vision-language models, adversarial forgeries&lt;/li&gt;&lt;li&gt;Relevance Score: 4/5&lt;/li&gt;&lt;li&gt;Code Repository: &lt;a href='None'&gt;None&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.04501</guid><pubDate>Fri, 06 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Is Perturbation-Based Image Protection Disruptive to Image Editing?</title><link>https://arxiv.org/abs/2506.04394</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Evaluates the effectiveness of perturbation-based image protection methods against diffusion model-based image editing.&lt;/li&gt;&lt;li&gt;Finds that current protection methods often fail, as diffusion models can still generate desirable outputs from protected images.&lt;/li&gt;&lt;li&gt;Highlights unintended consequences where added noise may actually improve the association with text prompts during editing.&lt;/li&gt;&lt;li&gt;Argues that perturbation-based methods are insufficient for robust image protection against misuse of generative models.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research Paper&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Quality Assessment&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Clarity: 5/5&lt;/li&gt;&lt;li&gt;Novelty: 4/5&lt;/li&gt;&lt;li&gt;Significance: 3/5&lt;/li&gt;&lt;li&gt;Try-worthiness: Yes&lt;/li&gt;&lt;li&gt;Justification: The abstract is clearly written, with a well-structured explanation of the problem, methodology, and findings. The work is novel in that it systematically evaluates the effectiveness of perturbation-based image protection against diffusion-based editing, revealing counterintuitive results that challenge current assumptions in the field. While the significance is moderate due to the early stage (preprint, no citations yet), the topic is timely and relevant given the rise of generative AI and concerns about image misuse. The findings suggest that current protection methods may be insufficient, which is important for both researchers and practitioners. The paper is worth experimenting with, especially for those working on image security or generative models. No code repository is provided.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: Qiuyu Tang, Bonor Ayambem, Mooi Choo Chuah, Aparna Bharati&lt;/li&gt;&lt;li&gt;Tags: image protection, diffusion models, adversarial robustness, AI misuse, security vulnerabilities&lt;/li&gt;&lt;li&gt;Relevance Score: 4/5&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.04394</guid><pubDate>Fri, 06 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Toward a Human-Centered Evaluation Framework for Trustworthy LLM-Powered GUI Agents</title><link>https://arxiv.org/abs/2504.17934</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Identifies privacy and security risks specific to LLM-powered GUI agents.&lt;/li&gt;&lt;li&gt;Highlights the lack of existing evaluation frameworks addressing these risks.&lt;/li&gt;&lt;li&gt;Proposes a human-centered evaluation framework that integrates risk assessment, user consent, and security/privacy considerations.&lt;/li&gt;&lt;li&gt;Discusses challenges in involving human evaluators for trustworthy assessment of GUI agents.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Position Paper&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Quality Assessment&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Clarity: 4/5&lt;/li&gt;&lt;li&gt;Novelty: 4/5&lt;/li&gt;&lt;li&gt;Significance: 3/5&lt;/li&gt;&lt;li&gt;Try-worthiness: No&lt;/li&gt;&lt;li&gt;Justification: The paper is clearly written, with a well-structured abstract that outlines the motivation, identified gaps, and proposed direction. Its novelty lies in focusing on privacy and security risks in LLM-powered GUI agents, an underexplored area compared to performance metrics. However, as a position paper proposing a framework rather than presenting empirical results or a concrete implementation, its immediate significance and try-worthiness are moderate. The lack of code or experimental results further limits its direct applicability for implementation or experimentation at this stage.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: Chaoran Chen, Zhiping Zhang, Ibrahim Khalilov, Bingcan Guo, Simret A Gebreegziabher, Yanfang Ye, Ziang Xiao, Yaxing Yao, Tianshi Li, Toby Jia-Jun Li&lt;/li&gt;&lt;li&gt;Tags: AI security, privacy, risk assessment, LLM agents, human-centered evaluation&lt;/li&gt;&lt;li&gt;Relevance Score: 5/5&lt;/li&gt;&lt;li&gt;Code Repository: &lt;a href='None'&gt;None&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2504.17934</guid><pubDate>Fri, 06 Jun 2025 00:00:00 +0000</pubDate></item><item><title>GRU: Mitigating the Trade-off between Unlearning and Retention for LLMs</title><link>https://arxiv.org/abs/2503.09117</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Addresses the challenge of unlearning specific information (e.g., privacy or copyright-related) from large language models (LLMs).&lt;/li&gt;&lt;li&gt;Proposes a new method, Gradient Rectified Unlearning (GRU), to minimize the negative impact of unlearning on the model's general functionality.&lt;/li&gt;&lt;li&gt;Demonstrates the effectiveness of GRU across multiple unlearning benchmarks, suggesting practical utility for safer and more compliant LLM deployment.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research Paper&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Quality Assessment&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Clarity: 4/5&lt;/li&gt;&lt;li&gt;Novelty: 4/5&lt;/li&gt;&lt;li&gt;Significance: 3/5&lt;/li&gt;&lt;li&gt;Try-worthiness: Yes&lt;/li&gt;&lt;li&gt;Justification: The paper is clearly written, with a well-defined problem statement and a concise description of the proposed method (Gradient Rectified Unlearning). The novelty is reasonably high, as it addresses the important and current challenge of unlearning in LLMs, proposing a new approach to mitigate the trade-off between unlearning and retention. The significance is moderate: while the problem is important and the solution appears practical, the paper is a recent preprint on arXiv with no citations yet, so its impact is not yet established. The method is described as easy and general to implement, making it worth trying for practitioners interested in LLM unlearning. No code repository is provided.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: Yue Wang, Qizhou Wang, Feng Liu, Wei Huang, Yali Du, Xiaojiang Du, Bo Han&lt;/li&gt;&lt;li&gt;Tags: LLM unlearning, privacy, model safety, retention trade-off, compliance&lt;/li&gt;&lt;li&gt;Relevance Score: 5/5&lt;/li&gt;&lt;li&gt;Code Repository: &lt;a href='None'&gt;None&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2503.09117</guid><pubDate>Fri, 06 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Adaptive Jailbreaking Strategies Based on the Semantic Understanding Capabilities of Large Language Models</title><link>https://arxiv.org/abs/2505.23404</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Investigates adversarial jailbreaking attacks targeting Large Language Models (LLMs) by exploiting their semantic understanding capabilities.&lt;/li&gt;&lt;li&gt;Proposes a novel framework that classifies LLMs based on their semantic comprehension and designs adaptive jailbreaking strategies for each category.&lt;/li&gt;&lt;li&gt;Demonstrates through experiments that the adaptive strategies significantly increase the success rate of jailbreaking, achieving up to 98.9% on GPT-4o.&lt;/li&gt;&lt;li&gt;Highlights critical vulnerabilities in LLMs' safety mechanisms and the need for improved security measures.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research Paper&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: Mingyu Yu, Wei Wang, Yanjie Wei, Sujuan Qin&lt;/li&gt;&lt;li&gt;Tags: LLM jailbreaking, adversarial attacks, AI security, prompt injection, model robustness&lt;/li&gt;&lt;li&gt;Relevance Score: 5/5&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2505.23404</guid><pubDate>Fri, 06 Jun 2025 00:00:00 +0000</pubDate></item><item><title>DREAM: Disentangling Risks to Enhance Safety Alignment in Multimodal Large Language Models</title><link>https://arxiv.org/abs/2504.18053</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Analyzes and disentangles safety risks specific to multimodal large language models (MLLMs) that process both visual and textual data.&lt;/li&gt;&lt;li&gt;Introduces DREAM, a novel method for enhancing safety alignment in MLLMs using supervised fine-tuning and reinforcement learning from AI feedback.&lt;/li&gt;&lt;li&gt;Demonstrates that systematic risk disentanglement improves risk awareness and safety performance in MLLMs.&lt;/li&gt;&lt;li&gt;Shows significant safety improvements over existing models like GPT-4V without sacrificing normal task performance.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research Paper&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Quality Assessment&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Clarity: 4/5&lt;/li&gt;&lt;li&gt;Novelty: 5/5&lt;/li&gt;&lt;li&gt;Significance: 4/5&lt;/li&gt;&lt;li&gt;Try-worthiness: Yes&lt;/li&gt;&lt;li&gt;Justification: The abstract is well-written and clearly communicates the motivation, methodology, and results, though some technical terms may require background knowledge. The work is highly novel, introducing a new risk disentanglement approach (DREAM) for safety alignment in multimodal LLMs, which is a cutting-edge and underexplored area. While the paper is a preprint and very recent (hence no citations yet), the problem addressed is significant given the growing importance of safety in MLLMs, and the reported improvement over GPT-4V is substantial. The availability of code and data further increases its try-worthiness for researchers and practitioners interested in safety alignment.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: Jianyu Liu, Hangyu Guo, Ranjie Duan, Xingyuan Bu, Yancheng He, Shilong Li, Hui Huang, Jiaheng Liu, Yucheng Wang, Chenchen Jing, Xingwei Qu, Xiao Zhang, Yingshui Tan, Yanan Wu, Jihao Gu, Yangguang Li, Jianke Zhu&lt;/li&gt;&lt;li&gt;Tags: AI safety, multimodal models, risk assessment, alignment, reinforcement learning&lt;/li&gt;&lt;li&gt;Relevance Score: 5/5&lt;/li&gt;&lt;li&gt;Code Repository: &lt;a href='https://github.com/Kizna1ver/DREAM'&gt;https://github.com/Kizna1ver/DREAM&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2504.18053</guid><pubDate>Fri, 06 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Lost in Transcription, Found in Distribution Shift: Demystifying Hallucination in Speech Foundation Models</title><link>https://arxiv.org/abs/2502.12414</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Investigates hallucination (fabricated outputs) in large-scale speech foundation models, particularly in automatic speech recognition (ASR).&lt;/li&gt;&lt;li&gt;Introduces the Hallucination Error Rate (HER) metric to quantify hallucinations, addressing limitations of traditional metrics like WER and CER.&lt;/li&gt;&lt;li&gt;Analyzes the impact of distribution shifts, model size, and architecture on hallucination rates, including the effect of adversarial and common audio perturbations.&lt;/li&gt;&lt;li&gt;Highlights the risks of hallucinations in high-stakes domains (e.g., healthcare, legal, aviation) and the need for improved evaluation metrics.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research Paper&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Quality Assessment&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Clarity: 4/5&lt;/li&gt;&lt;li&gt;Novelty: 5/5&lt;/li&gt;&lt;li&gt;Significance: 4/5&lt;/li&gt;&lt;li&gt;Try-worthiness: Yes&lt;/li&gt;&lt;li&gt;Justification: The paper is clearly written, with a well-structured abstract that outlines the motivation, problem, and key findings. It introduces a new metric (Hallucination Error Rate, HER) for evaluating ASR models, which is a novel contribution, especially given the growing importance of foundation models in speech. The analysis across 20+ models and the focus on distribution shift and hallucination in high-stakes domains add to its significance. While it is a preprint and has no citations yet, the topic is timely and relevant. The lack of a code repository is a minor drawback, but the conceptual contribution and empirical insights make it worth experimenting with, especially for researchers and practitioners concerned with ASR reliability.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: Hanin Atwany, Abdul Waheed, Rita Singh, Monojit Choudhury, Bhiksha Raj&lt;/li&gt;&lt;li&gt;Tags: hallucination, robustness, adversarial attacks, distribution shift, ASR safety&lt;/li&gt;&lt;li&gt;Relevance Score: 4/5&lt;/li&gt;&lt;li&gt;Code Repository: &lt;a href='None'&gt;None&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2502.12414</guid><pubDate>Fri, 06 Jun 2025 00:00:00 +0000</pubDate></item><item><title>CheckEmbed: Effective Verification of LLM Solutions to Open-Ended Tasks</title><link>https://arxiv.org/abs/2406.02524</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces CheckEmbed, a method for verifying LLM outputs on open-ended tasks by embedding answers for semantic comparison.&lt;/li&gt;&lt;li&gt;Demonstrates improved accuracy and scalability over prior verification methods like BERTScore and SelfCheckGPT.&lt;/li&gt;&lt;li&gt;Empirically shows CheckEmbed's effectiveness in detecting hallucinations in LLM outputs.&lt;/li&gt;&lt;li&gt;Presents evidence that the method generalizes to modalities beyond text, such as vision.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research Paper&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: Maciej Besta, Lorenzo Paleari, Marcin Copik, Robert Gerstenberger, Ales Kubicek, Piotr Nyczyk, Patrick Iff, Eric Schreiber, Tanja Srindran, Tomasz Lehmann, Hubert Niewiadomski, Torsten Hoefler&lt;/li&gt;&lt;li&gt;Tags: LLM output verification, hallucination detection, AI safety, robustness evaluation&lt;/li&gt;&lt;li&gt;Relevance Score: 4/5&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2406.02524</guid><pubDate>Fri, 06 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Flattery, Fluff, and Fog: Diagnosing and Mitigating Idiosyncratic Biases in Preference Models</title><link>https://arxiv.org/abs/2506.05339</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Investigates systematic biases in language model preference models, such as overreliance on superficial features (length, structure, jargon, sycophancy, vagueness).&lt;/li&gt;&lt;li&gt;Quantifies miscalibration and skew in model preferences compared to human preferences, highlighting risks of reward hacking and unreliable evaluation.&lt;/li&gt;&lt;li&gt;Proposes and tests a counterfactual data augmentation method to mitigate these biases, improving calibration and reliability of preference models.&lt;/li&gt;&lt;li&gt;Finds that targeted debiasing can reduce miscalibration and skew without sacrificing overall performance.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research Paper&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Quality Assessment&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Clarity: 5/5&lt;/li&gt;&lt;li&gt;Novelty: 4/5&lt;/li&gt;&lt;li&gt;Significance: 4/5&lt;/li&gt;&lt;li&gt;Try-worthiness: Yes&lt;/li&gt;&lt;li&gt;Justification: The abstract is very well-written, clearly outlining the problem (idiosyncratic biases in preference models), the methodology (controlled counterfactual pairs, counterfactual data augmentation), and the results (quantified miscalibration, mitigation effectiveness). The work is novel in its systematic investigation of specific bias features (length, structure, jargon, sycophancy, vagueness) and the use of counterfactual data augmentation for debiasing. While the venue is arXiv (preprint), the topic is highly relevant to current LLM alignment and evaluation research, and the results are promising. The paper is significant for practitioners and researchers working on preference modeling and LLM evaluation, even though it is too new to have citations. The proposed mitigation method is simple and effective, making the paper worth trying out in practice. No code repository is provided in the metadata.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: Anirudh Bharadwaj, Chaitanya Malaviya, Nitish Joshi, Mark Yatskar&lt;/li&gt;&lt;li&gt;Tags: AI alignment, reward hacking, model robustness, preference models, bias mitigation&lt;/li&gt;&lt;li&gt;Relevance Score: 4/5&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.05339</guid><pubDate>Fri, 06 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Automatic Robustness Stress Testing of LLMs as Mathematical Problem Solvers</title><link>https://arxiv.org/abs/2506.05038</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes an automatic framework (AR-Checker) for robustness stress testing of LLMs, particularly as mathematical problem solvers.&lt;/li&gt;&lt;li&gt;Generates semantically equivalent problem variants to test LLM robustness and identify failure cases.&lt;/li&gt;&lt;li&gt;Evaluates the framework on multiple benchmarks, demonstrating its effectiveness in uncovering robustness issues.&lt;/li&gt;&lt;li&gt;Addresses the need for dynamic, contamination-resistant robustness evaluation in LLMs.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research Paper&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Quality Assessment&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Clarity: 4/5&lt;/li&gt;&lt;li&gt;Novelty: 5/5&lt;/li&gt;&lt;li&gt;Significance: 4/5&lt;/li&gt;&lt;li&gt;Try-worthiness: Yes&lt;/li&gt;&lt;li&gt;Justification: The paper is clearly written, with a well-structured abstract that explains the motivation, method, and results. The proposed AR-Checker framework introduces a novel approach to robustness testing for LLMs by dynamically generating semantically equivalent mathematical problem variants, which is a significant improvement over prior hand-crafted or limited perturbation methods. The work is timely and relevant, addressing a key challenge in LLM evaluation. Although it is a preprint and very recent (hence no citations yet), the use of established benchmarks and the extension to non-mathematical tasks suggest broad applicability and impact. The lack of a code repository is a drawback, but the method appears promising and worth implementing or experimenting with.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: Yutao Hou, Zeguan Xiao, Fei Yu, Yihan Jiang, Xuetao Wei, Hailiang Huang, Yun Chen, Guanhua Chen&lt;/li&gt;&lt;li&gt;Tags: robustness, adversarial evaluation, LLM testing, AI safety&lt;/li&gt;&lt;li&gt;Relevance Score: 4/5&lt;/li&gt;&lt;li&gt;Code Repository: &lt;a href='None'&gt;None&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.05038</guid><pubDate>Fri, 06 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Joint Evaluation of Answer and Reasoning Consistency for Hallucination Detection in Large Reasoning Models</title><link>https://arxiv.org/abs/2506.04832</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes RACE, a framework for detecting hallucinations in Large Reasoning Models (LRMs) by jointly evaluating answer and reasoning consistency.&lt;/li&gt;&lt;li&gt;Addresses the challenge of hallucinations and logical inconsistencies in multi-step reasoning traces, which are not captured by traditional answer-level uncertainty methods.&lt;/li&gt;&lt;li&gt;Introduces diagnostic signals such as reasoning trace consistency, answer uncertainty, semantic alignment, and internal coherence for fine-grained hallucination detection.&lt;/li&gt;&lt;li&gt;Demonstrates that RACE outperforms existing hallucination detection baselines across multiple datasets and LLMs.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research Paper&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: Changyue Wang, Weihang Su, Qingyao Ai, Yiqun Liu&lt;/li&gt;&lt;li&gt;Tags: hallucination detection, reasoning consistency, AI safety, robustness evaluation&lt;/li&gt;&lt;li&gt;Relevance Score: 4/5&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.04832</guid><pubDate>Fri, 06 Jun 2025 00:00:00 +0000</pubDate></item><item><title>SPARTA ALIGNMENT: Collectively Aligning Multiple Language Models through Combat</title><link>https://arxiv.org/abs/2506.04721</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes SPARTA ALIGNMENT, a method for collectively aligning multiple LLMs through competitive interactions and peer evaluation.&lt;/li&gt;&lt;li&gt;Uses a duel-based system where models compete to fulfill instructions and are judged by other models, with outcomes aggregated via an Elo-style reputation system.&lt;/li&gt;&lt;li&gt;The approach generates preference data from these competitions, which is then used to further train and align the models.&lt;/li&gt;&lt;li&gt;Experiments show improved performance and generalization compared to self-alignment baselines, leveraging model diversity for better outputs.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research Paper&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Quality Assessment&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Clarity: 4/5&lt;/li&gt;&lt;li&gt;Novelty: 5/5&lt;/li&gt;&lt;li&gt;Significance: 4/5&lt;/li&gt;&lt;li&gt;Try-worthiness: Yes&lt;/li&gt;&lt;li&gt;Justification: The abstract is well-written and clearly explains the motivation, methodology, and results of the proposed SPARTA ALIGNMENT algorithm. The idea of aligning multiple LLMs through competitive 'combat' and peer evaluation is novel and addresses known limitations of single-model self-alignment. The use of an Elo-style reputation system for aggregation is also innovative. While the paper is very new and has no citations yet, the reported improvements over several baselines across multiple tasks suggest significant potential impact. The approach is promising and worth experimenting with, especially for researchers interested in LLM alignment and collective learning. No code repository is provided in the metadata.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: Yuru Jiang, Wenxuan Ding, Shangbin Feng, Greg Durrett, Yulia Tsvetkov&lt;/li&gt;&lt;li&gt;Tags: AI alignment, LLM alignment, collective alignment, preference learning, robustness&lt;/li&gt;&lt;li&gt;Relevance Score: 4/5&lt;/li&gt;&lt;li&gt;Code Repository: &lt;a href='None'&gt;None&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.04721</guid><pubDate>Fri, 06 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Normative Conflicts and Shallow AI Alignment</title><link>https://arxiv.org/abs/2506.04679</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Analyzes the limitations of current LLM alignment strategies in preventing misuse and adversarial attacks.&lt;/li&gt;&lt;li&gt;Argues that existing methods only achieve 'shallow alignment,' making LLMs vulnerable to manipulation via normative conflicts.&lt;/li&gt;&lt;li&gt;Compares LLMs' lack of normative deliberation to human moral reasoning, highlighting a key safety gap.&lt;/li&gt;&lt;li&gt;Discusses implications for AI safety and regulation, emphasizing the need for more robust alignment approaches.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Position Paper&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Quality Assessment&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Clarity: 5/5&lt;/li&gt;&lt;li&gt;Novelty: 4/5&lt;/li&gt;&lt;li&gt;Significance: 3/5&lt;/li&gt;&lt;li&gt;Try-worthiness: No&lt;/li&gt;&lt;li&gt;Justification: The abstract is well-written and clearly articulates the problem of 'shallow alignment' in LLMs, making a strong case for the limitations of current alignment strategies. The paper's novelty lies in its explicit framing of the vulnerability of LLMs to normative conflicts and adversarial attacks, drawing parallels with human moral psychology. However, as a conceptual and critical analysis rather than an empirical or algorithmic contribution, its immediate significance is moderate, especially given its preprint status and lack of citations (which is expected for a new paper). There is no code or experimental method to implement, so it is not try-worthy in the sense of direct experimentation. The work is valuable for informing future research directions in AI alignment but does not provide actionable methods or code.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: Rapha\"el Milli\`ere&lt;/li&gt;&lt;li&gt;Tags: AI alignment, AI safety, adversarial attacks, normative conflicts&lt;/li&gt;&lt;li&gt;Relevance Score: 5/5&lt;/li&gt;&lt;li&gt;Code Repository: &lt;a href='None'&gt;None&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.04679</guid><pubDate>Fri, 06 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Demonstrations of Integrity Attacks in Multi-Agent Systems</title><link>https://arxiv.org/abs/2506.04572</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Investigates integrity attacks in Multi-Agent Systems (MAS) using Large Language Models (LLMs).&lt;/li&gt;&lt;li&gt;Demonstrates four types of prompt-based attacks (Scapegoater, Boaster, Self-Dealer, Free-Rider) that manipulate MAS behavior.&lt;/li&gt;&lt;li&gt;Shows that these attacks can bypass advanced LLM-based monitoring systems, revealing current detection limitations.&lt;/li&gt;&lt;li&gt;Highlights the need for robust security protocols and improved monitoring in MAS architectures.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research Paper&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Quality Assessment&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Clarity: 4/5&lt;/li&gt;&lt;li&gt;Novelty: 5/5&lt;/li&gt;&lt;li&gt;Significance: 4/5&lt;/li&gt;&lt;li&gt;Try-worthiness: Yes&lt;/li&gt;&lt;li&gt;Justification: The abstract is well-written and clearly outlines the motivation, attack types, and findings, though some technical details are omitted (Clarity: 4). The work is highly novel, as it systematically explores integrity attacks in LLM-powered multi-agent systems, a relatively new and underexplored area (Novelty: 5). The significance is high given the growing use of MAS and LLMs, and the demonstration that even advanced monitors like GPT-4o-mini can be bypassed highlights important security gaps (Significance: 4). The paper is worth experimenting with, especially for researchers or practitioners interested in MAS security, LLM safety, or adversarial prompt engineering (Try-worthiness: true). No code repository is provided.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: Can Zheng, Yuhan Cao, Xiaoning Dong, Tianxing He&lt;/li&gt;&lt;li&gt;Tags: multi-agent systems, prompt manipulation, integrity attacks, LLM security, adversarial prompting&lt;/li&gt;&lt;li&gt;Relevance Score: 5/5&lt;/li&gt;&lt;li&gt;Code Repository: &lt;a href='None'&gt;None&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.04572</guid><pubDate>Fri, 06 Jun 2025 00:00:00 +0000</pubDate></item><item><title>RAID: A Dataset for Testing the Adversarial Robustness of AI-Generated Image Detectors</title><link>https://arxiv.org/abs/2506.03988</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces RAID, a large dataset of adversarial examples targeting AI-generated image detectors.&lt;/li&gt;&lt;li&gt;Demonstrates that current state-of-the-art detectors are vulnerable to adversarial attacks.&lt;/li&gt;&lt;li&gt;Provides tools and data to facilitate robust evaluation of AI-generated image detectors under adversarial conditions.&lt;/li&gt;&lt;li&gt;Highlights the need for more robust detection methods to counteract risks like fraud and disinformation.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Quality Assessment&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Clarity: 5/5&lt;/li&gt;&lt;li&gt;Novelty: 4/5&lt;/li&gt;&lt;li&gt;Significance: 4/5&lt;/li&gt;&lt;li&gt;Try-worthiness: Yes&lt;/li&gt;&lt;li&gt;Justification: The abstract is very clear, well-structured, and communicates the motivation, methodology, and contributions effectively (Clarity: 5). The work is novel in that it provides a large, diverse dataset specifically for adversarial robustness testing of AI-generated image detectors, which addresses a gap in current evaluation practices (Novelty: 4). The significance is high given the pressing need for robust detection methods in the face of rapidly improving generative models, and the dataset will likely become a valuable benchmark for the community (Significance: 4). The paper is worth trying out, especially for researchers working on image forensics, adversarial robustness, or AI-generated content detection (Try-worthiness: true). The code and dataset are openly available, which further increases its practical value.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: Hicham Eddoubi, Jonas Ricker, Federico Cocchi, Lorenzo Baraldi, Angelo Sotgiu, Maura Pintor, Marcella Cornia, Lorenzo Baraldi, Asja Fischer, Rita Cucchiara, Battista Biggio&lt;/li&gt;&lt;li&gt;Tags: adversarial robustness, AI-generated image detection, adversarial attacks, dataset, security evaluation&lt;/li&gt;&lt;li&gt;Relevance Score: 5/5&lt;/li&gt;&lt;li&gt;Code Repository: &lt;a href='https://github.com/pralab/RAID'&gt;https://github.com/pralab/RAID&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.03988</guid><pubDate>Fri, 06 Jun 2025 00:00:00 +0000</pubDate></item><item><title>A Selective Homomorphic Encryption Approach for Faster Privacy-Preserving Federated Learning</title><link>https://arxiv.org/abs/2501.12911</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes FAS, a federated learning framework combining selective homomorphic encryption, differential privacy, and bitwise scrambling for enhanced privacy and security.&lt;/li&gt;&lt;li&gt;Addresses the trade-off between strong cryptographic protections and computational efficiency in privacy-preserving federated learning, especially in healthcare.&lt;/li&gt;&lt;li&gt;Demonstrates that FAS is significantly faster than fully homomorphic encryption and other secure FL competitors, while maintaining comparable security against gradient inversion attacks.&lt;/li&gt;&lt;li&gt;Evaluates the approach on real medical imaging datasets, showing practical utility for latency-sensitive, privacy-critical applications.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research Paper&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Quality Assessment&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Clarity: 4/5&lt;/li&gt;&lt;li&gt;Novelty: 4/5&lt;/li&gt;&lt;li&gt;Significance: 3/5&lt;/li&gt;&lt;li&gt;Try-worthiness: Yes&lt;/li&gt;&lt;li&gt;Justification: The abstract is well-written and clearly explains the motivation, approach, and results, though some technical details (e.g., how selective encryption is applied) are not fully described in the abstract. The combination of selective homomorphic encryption, differential privacy, and bitwise scrambling for federated learning appears novel, especially in the context of healthcare and practical deployment. The significance is moderate: while the problem is important and the results promising (notably large speedups over FHE and competitive security), the paper is a preprint on arXiv and has not yet been peer-reviewed or cited. The lack of a code repository limits immediate reproducibility, but the described speed and security improvements make it worth trying for researchers or practitioners in privacy-preserving federated learning.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: Abdulkadir Korkmaz, Praveen Rao&lt;/li&gt;&lt;li&gt;Tags: federated learning, privacy-preserving machine learning, homomorphic encryption, gradient inversion attacks, differential privacy&lt;/li&gt;&lt;li&gt;Relevance Score: 5/5&lt;/li&gt;&lt;li&gt;Code Repository: &lt;a href='None'&gt;None&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2501.12911</guid><pubDate>Fri, 06 Jun 2025 00:00:00 +0000</pubDate></item><item><title>GRU: Mitigating the Trade-off between Unlearning and Retention for LLMs</title><link>https://arxiv.org/abs/2503.09117</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Addresses the challenge of unlearning specific information from large language models (LLMs) for privacy and copyright compliance.&lt;/li&gt;&lt;li&gt;Proposes Gradient Rectified Unlearning (GRU), a method to minimize the negative impact of unlearning on the model's general capabilities.&lt;/li&gt;&lt;li&gt;Demonstrates the effectiveness of GRU across multiple unlearning benchmarks, balancing the trade-off between unlearning and retention.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research Paper&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Quality Assessment&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Clarity: 4/5&lt;/li&gt;&lt;li&gt;Novelty: 4/5&lt;/li&gt;&lt;li&gt;Significance: 3/5&lt;/li&gt;&lt;li&gt;Try-worthiness: Yes&lt;/li&gt;&lt;li&gt;Justification: The abstract is well-written and clearly explains the motivation, problem, and proposed solution (Gradient Rectified Unlearning, GRU). The novelty is reasonably high, as the paper addresses a current and important challenge in LLM unlearning by proposing a new method to mitigate the trade-off between unlearning and retention. The significance is moderate: while the problem is important and the solution appears practical, the paper is a recent preprint on arXiv with no citations yet, so its impact is not yet established. The method is described as easy and general to implement, making it worth trying for practitioners interested in LLM unlearning. No code repository is provided.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: Yue Wang, Qizhou Wang, Feng Liu, Wei Huang, Yali Du, Xiaojiang Du, Bo Han&lt;/li&gt;&lt;li&gt;Tags: LLM unlearning, privacy, model safety, retention vs. unlearning, copyright compliance&lt;/li&gt;&lt;li&gt;Relevance Score: 5/5&lt;/li&gt;&lt;li&gt;Code Repository: &lt;a href='None'&gt;None&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2503.09117</guid><pubDate>Fri, 06 Jun 2025 00:00:00 +0000</pubDate></item><item><title>PoisonedParrot: Subtle Data Poisoning Attacks to Elicit Copyright-Infringing Content from Large Language Models</title><link>https://arxiv.org/abs/2503.07697</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces PoisonedParrot, a novel data poisoning attack that causes LLMs to generate copyrighted content.&lt;/li&gt;&lt;li&gt;Demonstrates that the attack is stealthy and effective, even when the model was not directly trained on the copyrighted material.&lt;/li&gt;&lt;li&gt;Finds that existing defenses are largely ineffective against this type of attack.&lt;/li&gt;&lt;li&gt;Proposes a new defense mechanism, ParrotTrap, to mitigate copyright-infringement poisoning attacks.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research Paper&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Quality Assessment&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Clarity: 5/5&lt;/li&gt;&lt;li&gt;Novelty: 5/5&lt;/li&gt;&lt;li&gt;Significance: 4/5&lt;/li&gt;&lt;li&gt;Try-worthiness: Yes&lt;/li&gt;&lt;li&gt;Justification: The abstract is well-written, clearly outlining the motivation, method, and findings of the paper. The work is highly novel, introducing the first stealthy data poisoning attack (PoisonedParrot) that can induce LLMs to generate copyrighted content without direct exposure to the full material. The significance is high due to the current legal and ethical concerns around LLM-generated content and the lack of effective defenses, though the impact is yet to be seen given the paper's recency and preprint status. The proposed defense (ParrotTrap) and the call for further research add to its relevance. The paper is worth experimenting with for researchers in LLM security and copyright, though no code repository is provided.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: Michael-Andrei Panaitescu-Liess, Pankayaraj Pathmanathan, Yigitcan Kaya, Zora Che, Bang An, Sicheng Zhu, Aakriti Agrawal, Furong Huang&lt;/li&gt;&lt;li&gt;Tags: data poisoning, LLM security, copyright risks, adversarial attacks, defense mechanisms&lt;/li&gt;&lt;li&gt;Relevance Score: 5/5&lt;/li&gt;&lt;li&gt;Code Repository: &lt;a href='None'&gt;None&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2503.07697</guid><pubDate>Fri, 06 Jun 2025 00:00:00 +0000</pubDate></item><item><title>On the Power of Context-Enhanced Learning in LLMs</title><link>https://arxiv.org/abs/2503.01821</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces and formalizes 'context-enhanced learning' for LLMs, a new learning paradigm.&lt;/li&gt;&lt;li&gt;Demonstrates that context-enhanced learning can be exponentially more sample-efficient than standard learning in certain settings.&lt;/li&gt;&lt;li&gt;Finds that it is difficult to detect or recover learning materials used in the context during training, which may have implications for data security and copyright.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research Paper&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: Xingyu Zhu, Abhishek Panigrahi, Sanjeev Arora&lt;/li&gt;&lt;li&gt;Tags: data security, privacy, LLM training, context-enhanced learning&lt;/li&gt;&lt;li&gt;Relevance Score: 3/5&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2503.01821</guid><pubDate>Fri, 06 Jun 2025 00:00:00 +0000</pubDate></item></channel></rss>