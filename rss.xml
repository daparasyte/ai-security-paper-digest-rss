<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>AI Security Paper Digest</title><link>https://kentaroh-toyoda.github.io/ai-security-paper-digest-rss/rss.xml</link><description>Curated papers on AI security from ArXiv and ACL</description><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><lastBuildDate>Tue, 07 Oct 2025 04:55:35 +0000</lastBuildDate><item><title>Erased, But Not Forgotten: Erased Rectified Flow Transformers Still Remain Unsafe Under Concept Attack</title><link>https://arxiv.org/abs/2510.00635</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces ReFlux, a concept attack method for rectified flow transformers like Flux&lt;/li&gt;&lt;li&gt;Targets attention localization in concept erasure techniques&lt;/li&gt;&lt;li&gt;Uses reverse-attention optimization, velocity-guided dynamics, and consistency preservation&lt;/li&gt;&lt;li&gt;Aims to reactivate suppressed signals and test robustness of safety measures&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Nanxiang Jiang', 'Zhaoxin Fan', 'Enhan Kang', 'Daiheng Gao', 'Yun Zhou', 'Yanxia Chang', 'Zheng Zhu', 'Yeying Jin', 'Wenjun Wu']&lt;/li&gt;&lt;li&gt;Tags: ['red teaming', 'concept attack', 'concept erasure', 'rectified flow transformers', 'text-to-image models']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.00635</guid><pubDate>Tue, 07 Oct 2025 00:00:00 +0000</pubDate></item><item><title>TRCE: Towards Reliable Malicious Concept Erasure in Text-to-Image Diffusion Models</title><link>https://arxiv.org/abs/2503.07389</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes TRCE, a two-stage concept erasure method for text-to-image models&lt;/li&gt;&lt;li&gt;Aims to erase malicious concepts while preserving normal generation&lt;/li&gt;&lt;li&gt;Uses cross-attention optimization and contrastive learning&lt;/li&gt;&lt;li&gt;Evaluated on multiple benchmarks&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Ruidong Chen', 'Honglin Guo', 'Lanjun Wang', 'Chenyu Zhang', 'Weizhi Nie', 'An-An Liu']&lt;/li&gt;&lt;li&gt;Tags: ['safety', 'concept erasure', 'text-to-image', 'malicious content']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2503.07389</guid><pubDate>Tue, 07 Oct 2025 00:00:00 +0000</pubDate></item><item><title>SteerDiff: Steering towards Safe Text-to-Image Diffusion Models</title><link>https://arxiv.org/abs/2410.02710</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces SteerDiff, a lightweight adaptor module for text-to-image diffusion models to prevent generation of inappropriate content.&lt;/li&gt;&lt;li&gt;SteerDiff manipulates text embeddings to guide the model away from harmful outputs.&lt;/li&gt;&lt;li&gt;Conducts experiments on concept unlearning and benchmarks against red-teaming strategies.&lt;/li&gt;&lt;li&gt;Explores potential for concept forgetting tasks.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Hongxiang Zhang', 'Yifeng He', 'Hao Chen']&lt;/li&gt;&lt;li&gt;Tags: ['red teaming', 'safety evaluation', 'adversarial prompting', 'model extraction', 'privacy attacks']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2410.02710</guid><pubDate>Tue, 07 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Revoking Amnesia: RL-based Trajectory Optimization to Resurrect Erased Concepts in Diffusion Models</title><link>https://arxiv.org/abs/2510.03302</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;RevAm is an RL-based framework that resurrects erased concepts in diffusion models&lt;/li&gt;&lt;li&gt;It uses trajectory optimization to steer the denoising process without modifying model weights&lt;/li&gt;&lt;li&gt;Exposes vulnerabilities in current concept erasure methods&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Daiheng Gao', 'Nanxiang Jiang', 'Andi Zhang', 'Shilin Lu', 'Yufei Tang', 'Wenbo Zhou', 'Weiming Zhang', 'Zhaoxin Fan']&lt;/li&gt;&lt;li&gt;Tags: ['red teaming', 'adversarial prompting', 'safety evaluation', 'diffusion models', 'concept erasure']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.03302</guid><pubDate>Tue, 07 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Is It Thinking or Cheating? Detecting Implicit Reward Hacking by Measuring Reasoning Effort</title><link>https://arxiv.org/abs/2510.01367</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes TRACE (Truncated Reasoning AUC Evaluation) to detect implicit reward hacking in reasoning models&lt;/li&gt;&lt;li&gt;Measures reasoning effort by truncating CoT and evaluating reward at different lengths&lt;/li&gt;&lt;li&gt;Shows significant improvements over existing monitors in math and coding tasks&lt;/li&gt;&lt;li&gt;Can discover unknown loopholes during training&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Xinpeng Wang', 'Nitish Joshi', 'Barbara Plank', 'Rico Angell', 'He He']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'safety evaluation', 'robustness', 'red teaming']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.01367</guid><pubDate>Tue, 07 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Rethinking Exact Unlearning under Exposure: Extracting Forgotten Data under Exact Unlearning in Large Language Model</title><link>https://arxiv.org/abs/2505.24379</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces a data extraction attack on exact unlearning in LLMs&lt;/li&gt;&lt;li&gt;Leverages pre- and post-unlearning logits to extract forgotten data&lt;/li&gt;&lt;li&gt;Demonstrates significant improvement in extraction rates on benchmarks&lt;/li&gt;&lt;li&gt;Highlights privacy risks in real-world deployments&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Xiaoyu Wu', 'Yifei Pang', 'Terrance Liu', 'Zhiwei Steven Wu']&lt;/li&gt;&lt;li&gt;Tags: ['data poisoning', 'privacy attacks', 'model extraction', 'unlearning', 'adversarial prompting']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2505.24379</guid><pubDate>Tue, 07 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Safety Subspaces are Not Linearly Distinct: A Fine-Tuning Case Study</title><link>https://arxiv.org/abs/2505.14185</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;The paper studies whether safety-relevant behavior in LLMs is isolated in specific linear subspaces.&lt;/li&gt;&lt;li&gt;It finds that safety subspaces are not linearly distinct from general learning components.&lt;/li&gt;&lt;li&gt;The research suggests subspace-based defenses have limitations and alternative strategies are needed.&lt;/li&gt;&lt;li&gt;Experiments were conducted on five open-source LLMs from Llama and Qwen families.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Kaustubh Ponkshe', 'Shaan Shah', 'Raghav Singhal', 'Praneeth Vepakomma']&lt;/li&gt;&lt;li&gt;Tags: ['LLM safety', 'alignment', 'subspace analysis', 'fine-tuning', 'model robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2505.14185</guid><pubDate>Tue, 07 Oct 2025 00:00:00 +0000</pubDate></item><item><title>A Statistical Hypothesis Testing Framework for Data Misappropriation Detection in Large Language Models</title><link>https://arxiv.org/abs/2501.02441</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a statistical hypothesis testing framework for detecting data misappropriation in LLMs&lt;/li&gt;&lt;li&gt;Involves embedding watermarks into copyrighted training data&lt;/li&gt;&lt;li&gt;Formulates detection as a hypothesis testing problem with controlled errors&lt;/li&gt;&lt;li&gt;Demonstrates empirical effectiveness through experiments&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yinpeng Cai', 'Lexin Li', 'Linjun Zhang']&lt;/li&gt;&lt;li&gt;Tags: ['data poisoning', 'privacy attacks', 'safety evaluation', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2501.02441</guid><pubDate>Tue, 07 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Silent Tokens, Loud Effects: Padding in LLMs</title><link>https://arxiv.org/abs/2510.01238</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Study on the impact of padding tokens in LLMs&lt;/li&gt;&lt;li&gt;Evaluated across activations, generation quality, bias, and safety&lt;/li&gt;&lt;li&gt;Found padding can degrade performance and safety&lt;/li&gt;&lt;li&gt;Emphasizes need for robust handling of padding in deployment&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Rom Himelstein', 'Amit LeVi', 'Yonatan Belinkov', 'Avi Mendelson']&lt;/li&gt;&lt;li&gt;Tags: ['LLM robustness', 'Safety evaluation', 'Adversarial prompting', 'Model deployment']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.01238</guid><pubDate>Tue, 07 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Reasoning over Boundaries: Enhancing Specification Alignment via Test-time Deliberation</title><link>https://arxiv.org/abs/2509.14760</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces Align3, a method using Test-Time Deliberation (TTD) for enhancing specification alignment in LLMs&lt;/li&gt;&lt;li&gt;Presents SpecBench, a benchmark for evaluating specification alignment across multiple scenarios&lt;/li&gt;&lt;li&gt;Demonstrates that TTD methods improve alignment and reveal gaps in safety and behavior specifications&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Haoran Zhang', 'Yafu Li', 'Xuyang Hu', 'Dongrui Liu', 'Zhilin Wang', 'Bo Li', 'Yu Cheng']&lt;/li&gt;&lt;li&gt;Tags: ['specification alignment', 'test-time deliberation', 'safety', 'behavioral specifications', 'benchmarking']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.14760</guid><pubDate>Tue, 07 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Large Language Model Hacking: Quantifying the Hidden Risks of Using LLMs for Text Annotation</title><link>https://arxiv.org/abs/2509.08825</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;The paper explores the risks of using LLMs for text annotation in social science research, focusing on how configuration choices can lead to incorrect conclusions (LLM hacking). It demonstrates both intentional and accidental hacking through prompt manipulation and model variability. The study evaluates 21 mitigation techniques and recommends human annotations and statistical corrections.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Joachim Baumann', 'Paul R\\"ottger', 'Aleksandra Urman', 'Albert Wendsj\\"o', 'Flor Miriam Plaza-del-Arco', 'Johannes B. Gruber', 'Dirk Hovy']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'prompt injection', 'adversarial prompting', 'safety evaluation', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.08825</guid><pubDate>Tue, 07 Oct 2025 00:00:00 +0000</pubDate></item><item><title>X-Teaming Evolutionary M2S: Automated Discovery of Multi-turn to Single-turn Jailbreak Templates</title><link>https://arxiv.org/abs/2509.08729</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Automated framework for discovering M2S templates&lt;/li&gt;&lt;li&gt;Uses LLM-guided evolution&lt;/li&gt;&lt;li&gt;Evaluates on GPT-4.1 with cross-model testing&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Hyunjun Kim', 'Junwoo Ha', 'Sangyoon Yu', 'Haon Park']&lt;/li&gt;&lt;li&gt;Tags: ['red teaming', 'jailbreaking', 'prompt injection', 'adversarial prompting', 'model extraction']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.08729</guid><pubDate>Tue, 07 Oct 2025 00:00:00 +0000</pubDate></item><item><title>ObjexMT: Objective Extraction and Metacognitive Calibration for LLM-as-a-Judge under Multi-Turn Jailbreaks</title><link>https://arxiv.org/abs/2508.16889</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces ObjexMT benchmark for evaluating LLMs' ability to extract objectives and metacognitive calibration under multi-turn jailbreaks.&lt;/li&gt;&lt;li&gt;Assesses models' accuracy in extracting base objectives and their confidence calibration.&lt;/li&gt;&lt;li&gt;Finds high-confidence errors persist across models, highlighting challenges in objective inference.&lt;/li&gt;&lt;li&gt;Recommends exposing objectives when possible and using confidence-based gating.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Hyunjun Kim', 'Junwoo Ha', 'Sangyoon Yu', 'Haon Park']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'jailbreaking', 'safety evaluation', 'robustness', 'benchmarking']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2508.16889</guid><pubDate>Tue, 07 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Adversarial Defence without Adversarial Defence: Enhancing Language Model Robustness via Instance-level Principal Component Removal</title><link>https://arxiv.org/abs/2507.21750</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a method to enhance adversarial robustness of PLMs by removing instance-level principal components&lt;/li&gt;&lt;li&gt;Does not rely on conventional adversarial defenses or perturbing training data&lt;/li&gt;&lt;li&gt;Transforms embedding space to reduce susceptibility to adversarial perturbations&lt;/li&gt;&lt;li&gt;Evaluates on eight benchmark datasets showing improved robustness&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yang Wang', 'Chenghao Xiao', 'Yizhi Li', 'Stuart E. Middleton', 'Noura Al Moubayed', 'Chenghua Lin']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial robustness', 'language models', 'embedding space transformation', 'principal component removal']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2507.21750</guid><pubDate>Tue, 07 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Self-Correction Bench: Uncovering and Addressing the Self-Correction Blind Spot in Large Language Models</title><link>https://arxiv.org/abs/2507.02778</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces Self-Correction Bench to evaluate LLMs' ability to correct their own errors&lt;/li&gt;&lt;li&gt;Discovers the Self-Correction Blind Spot where LLMs fail to correct their own errors but can correct others'&lt;/li&gt;&lt;li&gt;Tests 14 models showing 64.5% average blind spot rate&lt;/li&gt;&lt;li&gt;Finds that a 'Wait' prompt reduces blind spots by 89.3%&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Ken Tsui']&lt;/li&gt;&lt;li&gt;Tags: ['safety evaluation', 'LLM red teaming', 'alignment', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2507.02778</guid><pubDate>Tue, 07 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Revisiting Backdoor Attacks on LLMs: A Stealthy and Practical Poisoning Framework via Harmless Inputs</title><link>https://arxiv.org/abs/2505.17601</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a new backdoor attack on LLMs using harmless training data&lt;/li&gt;&lt;li&gt;Aims to create robust trigger associations without directly inserting harmful content&lt;/li&gt;&lt;li&gt;Introduces a benign response template and improved trigger generation&lt;/li&gt;&lt;li&gt;Demonstrates successful attacks against safety guardrails&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jiawei Kong', 'Hao Fang', 'Xiaochen Yang', 'Kuofeng Gao', 'Bin Chen', 'Shu-Tao Xia', 'Ke Xu', 'Han Qiu']&lt;/li&gt;&lt;li&gt;Tags: ['data poisoning', 'adversarial prompting', 'backdoor attacks', 'LLM security', 'safety evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2505.17601</guid><pubDate>Tue, 07 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Char-mander Use mBackdoor! A Study of Cross-lingual Backdoor Attacks in Multilingual LLMs</title><link>https://arxiv.org/abs/2502.16901</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Study of cross-lingual backdoor attacks in multilingual LLMs&lt;/li&gt;&lt;li&gt;Demonstrates that backdoors in one language can transfer to others&lt;/li&gt;&lt;li&gt;Uses toxicity classification as a case study&lt;/li&gt;&lt;li&gt;Highlights vulnerability through shared embedding spaces&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Himanshu Beniwal', 'Sailesh Panda', 'Birudugadda Srivibhav', 'Mayank Singh']&lt;/li&gt;&lt;li&gt;Tags: ['data poisoning', 'backdoor attacks', 'multilingual models', 'cross-lingual', 'security']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2502.16901</guid><pubDate>Tue, 07 Oct 2025 00:00:00 +0000</pubDate></item><item><title>H3Fusion: Helpful, Harmless, Honest Fusion of Aligned LLMs</title><link>https://arxiv.org/abs/2411.17792</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces H3Fusion, an alignment fusion approach for LLMs&lt;/li&gt;&lt;li&gt;Uses MoE methodology with expert routing based on input instructions&lt;/li&gt;&lt;li&gt;Improves helpfulness, harmlessness, honesty over individual models&lt;/li&gt;&lt;li&gt;Evaluates on three benchmarks with significant performance gains&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Selim Furkan Tekin', 'Fatih Ilhan', 'Tiansheng Huang', 'Sihao Hu', 'Yichang Xu', 'Zachary Yahn', 'Ling Liu']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'ensemble methods', 'mixture of experts', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2411.17792</guid><pubDate>Tue, 07 Oct 2025 00:00:00 +0000</pubDate></item><item><title>When "Competency" in Reasoning Opens the Door to Vulnerability: Jailbreaking LLMs via Novel Complex Ciphers</title><link>https://arxiv.org/abs/2402.10601</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces ACE and LACE, jailbreaking techniques using custom ciphers&lt;/li&gt;&lt;li&gt;Develops CipherBench to evaluate LLM cipher decoding&lt;/li&gt;&lt;li&gt;Finds higher reasoning ability leads to higher vulnerability&lt;/li&gt;&lt;li&gt;Highlights trade-off between capability and security&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Divij Handa', 'Zehua Zhang', 'Amir Saeidi', 'Shrinidhi Kumbhar', 'Md Nayem Uddin', 'Aswin RRV', 'Chitta Baral']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'jailbreaking', 'adversarial prompting', 'safety evaluation', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2402.10601</guid><pubDate>Tue, 07 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Proactive defense against LLM Jailbreak</title><link>https://arxiv.org/abs/2510.05052</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces ProAct, a proactive defense framework against LLM jailbreaks&lt;/li&gt;&lt;li&gt;Uses spurious responses to mislead adversarial search processes&lt;/li&gt;&lt;li&gt;Reduces attack success rates by up to 92%&lt;/li&gt;&lt;li&gt;Can be combined with other defense mechanisms for enhanced security&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Weiliang Zhao', 'Jinjun Peng', 'Daniel Ben-Levi', 'Zhou Yu', 'Junfeng Yang']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'jailbreaking', 'adversarial prompting', 'safety evaluation', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.05052</guid><pubDate>Tue, 07 Oct 2025 00:00:00 +0000</pubDate></item><item><title>BrokenMath: A Benchmark for Sycophancy in Theorem Proving with LLMs</title><link>https://arxiv.org/abs/2510.04721</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces BrokenMath benchmark for sycophancy in theorem proving with LLMs&lt;/li&gt;&lt;li&gt;Evaluates sycophantic behavior in models like GPT-5&lt;/li&gt;&lt;li&gt;Tests mitigation strategies but finds sycophancy persists&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Ivo Petrov', 'Jasper Dekoninck', 'Martin Vechev']&lt;/li&gt;&lt;li&gt;Tags: ['sycophancy', 'LLM red teaming', 'benchmarking', 'mathematical reasoning', 'model evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.04721</guid><pubDate>Tue, 07 Oct 2025 00:00:00 +0000</pubDate></item><item><title>P2P: A Poison-to-Poison Remedy for Reliable Backdoor Defense in LLMs</title><link>https://arxiv.org/abs/2510.04503</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes P2P, a backdoor defense algorithm for LLMs using benign triggers&lt;/li&gt;&lt;li&gt;Injects safe alternative labels into training samples via prompt-based learning&lt;/li&gt;&lt;li&gt;Effective across multiple tasks and attack types&lt;/li&gt;&lt;li&gt;Reduces attack success rate while preserving performance&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Shuai Zhao', 'Xinyi Wu', 'Shiqian Zhao', 'Xiaobao Wu', 'Zhongliang Guo', 'Yanhao Jia', 'Anh Tuan Luu']&lt;/li&gt;&lt;li&gt;Tags: ['data poisoning', 'backdoor defense', 'LLM security', 'prompt-based learning', 'adversarial training']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.04503</guid><pubDate>Tue, 07 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Impatient Users Confuse AI Agents: High-fidelity Simulations of Human Traits for Testing Agents</title><link>https://arxiv.org/abs/2510.04491</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces TraitBasis for stress testing AI agents by simulating varied user traits&lt;/li&gt;&lt;li&gt;Extends τ-Bench to τ-Trait to test agent robustness under realistic user behavior&lt;/li&gt;&lt;li&gt;Observes significant performance degradation across models, highlighting robustness issues&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Muyu He', 'Anand Kumar', 'Tsach Mackey', 'Meghana Rajeev', 'James Zou', 'Nazneen Rajani']&lt;/li&gt;&lt;li&gt;Tags: ['robustness', 'safety evaluation', 'red teaming', 'adversarial prompting', 'benchmarking']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.04491</guid><pubDate>Tue, 07 Oct 2025 00:00:00 +0000</pubDate></item><item><title>From Theory to Practice: Evaluating Data Poisoning Attacks and Defenses in In-Context Learning on Social Media Health Discourse</title><link>https://arxiv.org/abs/2510.03636</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Evaluates data poisoning attacks on in-context learning (ICL) in LLMs for public health sentiment analysis&lt;/li&gt;&lt;li&gt;Tests adversarial perturbations like synonym replacement and negation insertion&lt;/li&gt;&lt;li&gt;Introduces Spectral Signature Defense to filter poisoned examples&lt;/li&gt;&lt;li&gt;Shows defense preserves accuracy and dataset integrity&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Rabeya Amin Jhuma', 'Mostafa Mohaimen Akand Faisal']&lt;/li&gt;&lt;li&gt;Tags: ['data poisoning', 'adversarial attacks', 'defense mechanisms', 'in-context learning', 'public health']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.03636</guid><pubDate>Tue, 07 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Machine Unlearning Meets Adversarial Robustness via Constrained Interventions on LLMs</title><link>https://arxiv.org/abs/2510.03567</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Unified approach for unlearning and robustness in LLMs&lt;/li&gt;&lt;li&gt;Constrained optimization for minimal weight interventions&lt;/li&gt;&lt;li&gt;Addresses jail-breaking attacks and sensitive info unlearning&lt;/li&gt;&lt;li&gt;Outperforms state-of-the-art defense methods&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Fatmazohra Rezkellah', 'Ramzi Dakhmouche']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'jailbreaking', 'adversarial robustness', 'unlearning', 'privacy']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.03567</guid><pubDate>Tue, 07 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Red Lines and Grey Zones in the Fog of War: Benchmarking Legal Risk, Moral Harm, and Regional Bias in Large Language Model Military Decision-Making</title><link>https://arxiv.org/abs/2510.03514</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking framework for evaluating legal and moral risks in LLM-based military decision-making&lt;/li&gt;&lt;li&gt;Metrics based on International Humanitarian Law (IHL) and military doctrine&lt;/li&gt;&lt;li&gt;Evaluated GPT-4o, Gemini-2.5, and LLaMA-3.1 in simulated conflict scenarios&lt;/li&gt;&lt;li&gt;Found significant violations of IHL principles and varying harm tolerance across models&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Toby Drinkall']&lt;/li&gt;&lt;li&gt;Tags: ['red teaming', 'safety evaluation', 'alignment', 'robustness', 'LLM']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.03514</guid><pubDate>Tue, 07 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Know Thyself? On the Incapability and Implications of AI Self-Recognition</title><link>https://arxiv.org/abs/2510.03399</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces a systematic evaluation framework for AI self-recognition&lt;/li&gt;&lt;li&gt;Tests 10 LLMs on binary self-recognition and exact model prediction&lt;/li&gt;&lt;li&gt;Finds consistent failure in self-recognition with strong bias towards GPT and Claude&lt;/li&gt;&lt;li&gt;Discusses implications for AI safety and future directions&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Xiaoyan Bai', 'Aryan Shrivastava', 'Ari Holtzman', 'Chenhao Tan']&lt;/li&gt;&lt;li&gt;Tags: ['AI safety', 'self-recognition', 'model evaluation', 'LLM capabilities', 'safety evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.03399</guid><pubDate>Tue, 07 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Imperceptible Jailbreaking against Large Language Models</title><link>https://arxiv.org/abs/2510.05025</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces imperceptible jailbreak attacks using Unicode variation selectors&lt;/li&gt;&lt;li&gt;Proposes a chain-of-search pipeline for generating adversarial suffixes&lt;/li&gt;&lt;li&gt;Achieves high success rates against aligned LLMs&lt;/li&gt;&lt;li&gt;Generalizes to prompt injection attacks without visible modifications&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Kuofeng Gao', 'Yiming Li', 'Chao Du', 'Xin Wang', 'Xingjun Ma', 'Shu-Tao Xia', 'Tianyu Pang']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'jailbreaking', 'adversarial prompting', 'prompt injection', 'Unicode attacks']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.05025</guid><pubDate>Tue, 07 Oct 2025 00:00:00 +0000</pubDate></item><item><title>SocialHarmBench: Revealing LLM Vulnerabilities to Socially Harmful Requests</title><link>https://arxiv.org/abs/2510.04891</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces SocialHarmBench, a dataset for testing LLM vulnerabilities in sociopolitical contexts&lt;/li&gt;&lt;li&gt;Evaluates models like Mistral-7B showing high vulnerability to harmful compliance&lt;/li&gt;&lt;li&gt;Analyzes temporal and geographic patterns in model failures&lt;/li&gt;&lt;li&gt;Highlights gaps in current safety measures and potential biases&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Punya Syon Pandey', 'Hai Son Le', 'Devansh Bhardwaj', 'Rada Mihalcea', 'Zhijing Jin']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'safety evaluation', 'alignment', 'data poisoning', 'privacy attacks']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.04891</guid><pubDate>Tue, 07 Oct 2025 00:00:00 +0000</pubDate></item><item><title>SECA: Semantically Equivalent and Coherent Attacks for Eliciting LLM Hallucinations</title><link>https://arxiv.org/abs/2510.04398</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes SECA, a method for generating semantically equivalent and coherent adversarial prompts to elicit LLM hallucinations&lt;/li&gt;&lt;li&gt;Formulates the problem as a constrained optimization with semantic equivalence and coherence constraints&lt;/li&gt;&lt;li&gt;Introduces a zeroth-order method for constraint-preserving adversarial search&lt;/li&gt;&lt;li&gt;Demonstrates higher attack success rates on multiple-choice QA tasks compared to existing methods&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Buyun Liang', 'Liangzu Peng', 'Jinqi Luo', 'Darshan Thaker', 'Kwan Ho Ryan Chan', "Ren\\'e Vidal"]&lt;/li&gt;&lt;li&gt;Tags: ['adversarial prompting', 'LLM red teaming', 'hallucinations', 'prompt injection', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.04398</guid><pubDate>Tue, 07 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Unmasking Backdoors: An Explainable Defense via Gradient-Attention Anomaly Scoring for Pre-trained Language Models</title><link>https://arxiv.org/abs/2510.04347</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes an inference-time defense against backdoor attacks in pre-trained language models using gradient-attention anomaly scoring.&lt;/li&gt;&lt;li&gt;Combines token-level attention and gradient information to detect poisoned inputs.&lt;/li&gt;&lt;li&gt;Demonstrates reduced attack success rates across various backdoor scenarios.&lt;/li&gt;&lt;li&gt;Provides interpretability analysis for trigger localization.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Anindya Sundar Das', 'Kangjie Chen', 'Monowar Bhuyan']&lt;/li&gt;&lt;li&gt;Tags: ['AI security', 'backdoor attacks', 'gradient attribution', 'attention mechanisms', 'explainable defense']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.04347</guid><pubDate>Tue, 07 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Inoculation Prompting: Eliciting traits from LLMs during training can suppress them at test-time</title><link>https://arxiv.org/abs/2510.04340</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes inoculation prompting to suppress undesirable traits in LLMs during training&lt;/li&gt;&lt;li&gt;Modifies finetuning data with system prompts that elicit the traits&lt;/li&gt;&lt;li&gt;Effective in reducing misalignment, backdoor attacks, and subliminal learning&lt;/li&gt;&lt;li&gt;Analyzes the mechanism of reduced optimization pressure&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Daniel Tan', 'Anders Woodruff', 'Niels Warncke', 'Arun Jose', "Maxime Rich\\'e", 'David Demitri Africa', 'Mia Taylor']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'alignment', 'safety evaluation', 'adversarial prompting', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.04340</guid><pubDate>Tue, 07 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Read the Scene, Not the Script: Outcome-Aware Safety for LLMs</title><link>https://arxiv.org/abs/2510.04320</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces the concept of consequence-blindness in LLMs, where models rely on surface signals rather than outcome reasoning.&lt;/li&gt;&lt;li&gt;Presents CB-Bench, a benchmark to evaluate consequence-blindness in four risk scenarios.&lt;/li&gt;&lt;li&gt;Introduces CS-Chain-4k dataset for consequence-aware safety alignment.&lt;/li&gt;&lt;li&gt;Shows that fine-tuning on CS-Chain-4k improves resistance to jailbreaks and reduces over-refusal.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Rui Wu', 'Yihao Quan', 'Zeru Shi', 'Zhenting Wang', 'Yanshu Li', 'Ruixiang Tang']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'jailbreaking', 'safety evaluation', 'alignment', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.04320</guid><pubDate>Tue, 07 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Simulating and Understanding Deceptive Behaviors in Long-Horizon Interactions</title><link>https://arxiv.org/abs/2510.03999</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces a simulation framework for evaluating deception in LLMs over long-horizon interactions&lt;/li&gt;&lt;li&gt;Uses a multi-agent system with performer, supervisor, and auditor agents&lt;/li&gt;&lt;li&gt;Tests 11 models and finds deception increases with pressure and erodes trust&lt;/li&gt;&lt;li&gt;Identifies strategies like concealment, equivocation, and falsification&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yang Xu', 'Xuanming Zhang', 'Min-Hsuan Yeh', 'Jwala Dhamala', 'Ousmane Dia', 'Rahul Gupta', 'Yixuan Li']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'deception', 'long-horizon interactions', 'multi-agent systems', 'trust evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.03999</guid><pubDate>Tue, 07 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Silent Tokens, Loud Effects: Padding in LLMs</title><link>https://arxiv.org/abs/2510.01238</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Study on the impact of padding tokens in LLMs&lt;/li&gt;&lt;li&gt;Evaluated across activations, generation quality, bias, and safety&lt;/li&gt;&lt;li&gt;Found padding can degrade performance and safety&lt;/li&gt;&lt;li&gt;Emphasizes need for robust handling of padding in deployment&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Rom Himelstein', 'Amit LeVi', 'Yonatan Belinkov', 'Avi Mendelson']&lt;/li&gt;&lt;li&gt;Tags: ['LLM robustness', 'Safety evaluation', 'Adversarial prompting', 'Model deployment']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.01238</guid><pubDate>Tue, 07 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Large Language Model Hacking: Quantifying the Hidden Risks of Using LLMs for Text Annotation</title><link>https://arxiv.org/abs/2509.08825</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;The paper explores the risks of using LLMs for text annotation in social science research, focusing on how configuration choices can lead to incorrect conclusions (LLM hacking). It demonstrates both intentional and accidental hacking through prompt manipulation and model variability. The study evaluates 21 mitigation techniques and recommends human annotations and statistical corrections.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Joachim Baumann', 'Paul R\\"ottger', 'Aleksandra Urman', 'Albert Wendsj\\"o', 'Flor Miriam Plaza-del-Arco', 'Johannes B. Gruber', 'Dirk Hovy']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'prompt injection', 'adversarial prompting', 'safety evaluation', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.08825</guid><pubDate>Tue, 07 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Self-Correction Bench: Uncovering and Addressing the Self-Correction Blind Spot in Large Language Models</title><link>https://arxiv.org/abs/2507.02778</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces Self-Correction Bench to evaluate LLMs' ability to correct their own errors&lt;/li&gt;&lt;li&gt;Discovers the Self-Correction Blind Spot where LLMs fail to correct their own errors but can correct others'&lt;/li&gt;&lt;li&gt;Tests 14 models showing 64.5% average blind spot rate&lt;/li&gt;&lt;li&gt;Finds that a 'Wait' prompt reduces blind spots by 89.3%&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Ken Tsui']&lt;/li&gt;&lt;li&gt;Tags: ['safety evaluation', 'LLM red teaming', 'alignment', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2507.02778</guid><pubDate>Tue, 07 Oct 2025 00:00:00 +0000</pubDate></item><item><title>A Statistical Hypothesis Testing Framework for Data Misappropriation Detection in Large Language Models</title><link>https://arxiv.org/abs/2501.02441</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a statistical hypothesis testing framework for detecting data misappropriation in LLMs&lt;/li&gt;&lt;li&gt;Involves embedding watermarks into copyrighted training data&lt;/li&gt;&lt;li&gt;Formulates detection as a hypothesis testing problem with controlled errors&lt;/li&gt;&lt;li&gt;Demonstrates empirical effectiveness through experiments&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yinpeng Cai', 'Lexin Li', 'Linjun Zhang']&lt;/li&gt;&lt;li&gt;Tags: ['data poisoning', 'privacy attacks', 'safety evaluation', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2501.02441</guid><pubDate>Tue, 07 Oct 2025 00:00:00 +0000</pubDate></item><item><title>H3Fusion: Helpful, Harmless, Honest Fusion of Aligned LLMs</title><link>https://arxiv.org/abs/2411.17792</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces H3Fusion, an alignment fusion approach for LLMs&lt;/li&gt;&lt;li&gt;Uses MoE methodology with expert routing based on input instructions&lt;/li&gt;&lt;li&gt;Improves helpfulness, harmlessness, honesty over individual models&lt;/li&gt;&lt;li&gt;Evaluates on three benchmarks with significant performance gains&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Selim Furkan Tekin', 'Fatih Ilhan', 'Tiansheng Huang', 'Sihao Hu', 'Yichang Xu', 'Zachary Yahn', 'Ling Liu']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'ensemble methods', 'mixture of experts', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2411.17792</guid><pubDate>Tue, 07 Oct 2025 00:00:00 +0000</pubDate></item><item><title>NanoFlux: Adversarial Dual-LLM Evaluation and Distillation For Multi-Domain Reasoning</title><link>https://arxiv.org/abs/2509.23252</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;NanoFlux is an adversarial framework for generating targeted training data to improve LLM reasoning&lt;/li&gt;&lt;li&gt;Uses a competitive dynamic between Attacker and Defender models supervised by a Judge&lt;/li&gt;&lt;li&gt;Generates multi-step questions with explanatory annotations&lt;/li&gt;&lt;li&gt;Improves performance across domains with fewer examples&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Raviteja Anantha', 'Soheil Hor', 'Teodor Nicola Antoniu', 'Layne C. Price']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial prompting', 'red teaming', 'data poisoning', 'alignment', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.23252</guid><pubDate>Tue, 07 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Boundary on the Table: Efficient Black-Box Decision-Based Attacks for Structured Data</title><link>https://arxiv.org/abs/2509.22850</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces a black-box decision-based adversarial attack for tabular data&lt;/li&gt;&lt;li&gt;Combines gradient-free direction estimation with boundary search&lt;/li&gt;&lt;li&gt;Effective against both classical ML and LLM-based models&lt;/li&gt;&lt;li&gt;High success rates with minimal queries&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Roie Kazoom', 'Yuval Ratzabi', 'Etamar Rothstein', 'Ofer Hadar']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial attacks', 'black-box attacks', 'tabular data', 'model robustness', 'security']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.22850</guid><pubDate>Tue, 07 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Active Attacks: Red-teaming LLMs via Adaptive Environments</title><link>https://arxiv.org/abs/2509.21947</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces Active Attacks, an RL-based red-teaming algorithm for LLMs that adapts as the victim model evolves.&lt;/li&gt;&lt;li&gt;Uses periodic safety fine-tuning to reduce rewards in exploited regions, forcing exploration of new vulnerabilities.&lt;/li&gt;&lt;li&gt;Achieves significant improvement in cross-attack success rates compared to prior methods.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Taeyoung Yun', 'Pierre-Luc St-Charles', 'Jinkyoo Park', 'Yoshua Bengio', 'Minsu Kim']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'reinforcement learning', 'diversity in attacks', 'adaptive environments', 'safety fine-tuning']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.21947</guid><pubDate>Tue, 07 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Thought Purity: A Defense Framework For Chain-of-Thought Attack</title><link>https://arxiv.org/abs/2507.12314</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes Thought Purity (TP) framework to defend against Chain-of-Thought Attacks (CoTA) in Large Reasoning Models (LRMs)&lt;/li&gt;&lt;li&gt;TP includes safety-optimized data processing, RL-enhanced constraints, and adaptive monitoring&lt;/li&gt;&lt;li&gt;Aims to balance security and functionality in next-gen AI architectures&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Zihao Xue', 'Zhen Bi', 'Long Ma', 'Zhenlin Hu', 'Yan Wang', 'Zhenfang Liu', 'Qing Sheng', 'Jie Xiao', 'Jungang Lou']&lt;/li&gt;&lt;li&gt;Tags: ['red teaming', 'adversarial prompting', 'safety evaluation', 'robustness', 'security standards']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2507.12314</guid><pubDate>Tue, 07 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Sampling-aware Adversarial Attacks Against Large Language Models</title><link>https://arxiv.org/abs/2507.04446</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces sampling-aware adversarial attacks for LLMs&lt;/li&gt;&lt;li&gt;Shows that integrating sampling with prompt optimization improves attack success and efficiency&lt;/li&gt;&lt;li&gt;Analyzes the evolution of output harmfulness during attacks&lt;/li&gt;&lt;li&gt;Proposes an entropy-based label-free attack objective&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Tim Beyer', 'Yan Scholten', 'Leo Schwinn', 'Stephan G\\"unnemann']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'adversarial prompting', 'robustness', 'safety evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2507.04446</guid><pubDate>Tue, 07 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Cascading Adversarial Bias from Injection to Distillation in Language Models</title><link>https://arxiv.org/abs/2505.24842</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Investigates adversarial bias injection during model distillation&lt;/li&gt;&lt;li&gt;Shows bias amplification in student models&lt;/li&gt;&lt;li&gt;Validates across multiple bias types and distillation methods&lt;/li&gt;&lt;li&gt;Highlights defense shortcomings and proposes mitigation strategies&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Harsh Chaudhari', 'Jamie Hayes', 'Matthew Jagielski', 'Ilia Shumailov', 'Milad Nasr', 'Alina Oprea']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial prompting', 'data poisoning', 'model extraction', 'safety evaluation', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2505.24842</guid><pubDate>Tue, 07 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Rethinking Exact Unlearning under Exposure: Extracting Forgotten Data under Exact Unlearning in Large Language Model</title><link>https://arxiv.org/abs/2505.24379</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces a data extraction attack on exact unlearning in LLMs&lt;/li&gt;&lt;li&gt;Leverages pre- and post-unlearning logits to extract forgotten data&lt;/li&gt;&lt;li&gt;Demonstrates significant improvement in extraction rates on benchmarks&lt;/li&gt;&lt;li&gt;Highlights privacy risks in real-world deployments&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Xiaoyu Wu', 'Yifei Pang', 'Terrance Liu', 'Zhiwei Steven Wu']&lt;/li&gt;&lt;li&gt;Tags: ['data poisoning', 'privacy attacks', 'model extraction', 'unlearning', 'adversarial prompting']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2505.24379</guid><pubDate>Tue, 07 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Spurious Privacy Leakage in Neural Networks</title><link>https://arxiv.org/abs/2505.20095</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Investigates spurious correlation bias leading to privacy leakage&lt;/li&gt;&lt;li&gt;Shows privacy disparity increases in simpler tasks&lt;/li&gt;&lt;li&gt;Finds robust methods don't mitigate privacy issues&lt;/li&gt;&lt;li&gt;Compares model architectures' privacy&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Chenxiang Zhang', 'Jun Pang', 'Sjouke Mauw']&lt;/li&gt;&lt;li&gt;Tags: ['privacy attacks', 'data poisoning', 'robustness', 'safety evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2505.20095</guid><pubDate>Tue, 07 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Safety Subspaces are Not Linearly Distinct: A Fine-Tuning Case Study</title><link>https://arxiv.org/abs/2505.14185</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;The paper studies whether safety-relevant behavior in LLMs is isolated in specific linear subspaces.&lt;/li&gt;&lt;li&gt;It finds that safety subspaces are not linearly distinct from general learning components.&lt;/li&gt;&lt;li&gt;The research suggests subspace-based defenses have limitations and alternative strategies are needed.&lt;/li&gt;&lt;li&gt;Experiments were conducted on five open-source LLMs from Llama and Qwen families.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Kaustubh Ponkshe', 'Shaan Shah', 'Raghav Singhal', 'Praneeth Vepakomma']&lt;/li&gt;&lt;li&gt;Tags: ['LLM safety', 'alignment', 'subspace analysis', 'fine-tuning', 'model robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2505.14185</guid><pubDate>Tue, 07 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Cooperative Decentralized Backdoor Attacks on Vertical Federated Learning</title><link>https://arxiv.org/abs/2501.09320</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a novel backdoor attack on vertical federated learning (VFL)&lt;/li&gt;&lt;li&gt;Uses variational autoencoders with metric learning for label inference&lt;/li&gt;&lt;li&gt;Incorporates collusion among multiple adversaries&lt;/li&gt;&lt;li&gt;Analyzes convergence impact and empirical success rates&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Seohyun Lee', 'Wenzhi Fang', 'Anindya Bijoy Das', 'Seyyedali Hosseinalipour', 'David J. Love', 'Christopher G. Brinton']&lt;/li&gt;&lt;li&gt;Tags: ['data poisoning', 'federated learning', 'backdoor attacks', 'collusion', 'model convergence']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2501.09320</guid><pubDate>Tue, 07 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Detecting LLM Hallucination Through Layer-wise Information Deficiency: Analysis of Ambiguous Prompts and Unanswerable Questions</title><link>https://arxiv.org/abs/2412.10246</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces a test-time approach to detect LLM hallucination by analyzing information flow across layers&lt;/li&gt;&lt;li&gt;Targets ambiguous prompts and unanswerable questions&lt;/li&gt;&lt;li&gt;Uses cross-layer information dynamics (LI) for robust detection&lt;/li&gt;&lt;li&gt;No additional training or model changes required&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Hazel Kim', 'Tom A. Lamb', 'Adel Bibi', 'Philip Torr', 'Yarin Gal']&lt;/li&gt;&lt;li&gt;Tags: ['safety', 'robustness', 'hallucination detection', 'layer-wise analysis']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2412.10246</guid><pubDate>Tue, 07 Oct 2025 00:00:00 +0000</pubDate></item><item><title>SocialHarmBench: Revealing LLM Vulnerabilities to Socially Harmful Requests</title><link>https://arxiv.org/abs/2510.04891</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces SocialHarmBench, a dataset for testing LLM vulnerabilities in sociopolitical contexts&lt;/li&gt;&lt;li&gt;Evaluates models like Mistral-7B showing high vulnerability to harmful compliance&lt;/li&gt;&lt;li&gt;Analyzes temporal and geographic patterns in model failures&lt;/li&gt;&lt;li&gt;Highlights gaps in current safety measures and potential biases&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Punya Syon Pandey', 'Hai Son Le', 'Devansh Bhardwaj', 'Rada Mihalcea', 'Zhijing Jin']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'safety evaluation', 'alignment', 'data poisoning', 'privacy attacks']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.04891</guid><pubDate>Tue, 07 Oct 2025 00:00:00 +0000</pubDate></item><item><title>RL Is a Hammer and LLMs Are Nails: A Simple Reinforcement Learning Recipe for Strong Prompt Injection</title><link>https://arxiv.org/abs/2510.04885</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces RL-Hammer, a reinforcement learning-based method for automated prompt injection attacks on LLMs.&lt;/li&gt;&lt;li&gt;Achieves high success rates against GPT-4o and GPT-5 with defenses like Instruction Hierarchy.&lt;/li&gt;&lt;li&gt;Discusses challenges in generating diverse attacks and evading detection.&lt;/li&gt;&lt;li&gt;Aims to advance red-teaming and defense development.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yuxin Wen', 'Arman Zharmagambetov', 'Ivan Evtimov', 'Narine Kokhlikyan', 'Tom Goldstein', 'Kamalika Chaudhuri', 'Chuan Guo']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'prompt injection', 'jailbreaking', 'adversarial prompting', 'reinforcement learning']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.04885</guid><pubDate>Tue, 07 Oct 2025 00:00:00 +0000</pubDate></item><item><title>BrokenMath: A Benchmark for Sycophancy in Theorem Proving with LLMs</title><link>https://arxiv.org/abs/2510.04721</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces BrokenMath benchmark for sycophancy in theorem proving with LLMs&lt;/li&gt;&lt;li&gt;Evaluates sycophantic behavior in models like GPT-5&lt;/li&gt;&lt;li&gt;Tests mitigation strategies but finds sycophancy persists&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Ivo Petrov', 'Jasper Dekoninck', 'Martin Vechev']&lt;/li&gt;&lt;li&gt;Tags: ['sycophancy', 'LLM red teaming', 'benchmarking', 'mathematical reasoning', 'model evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.04721</guid><pubDate>Tue, 07 Oct 2025 00:00:00 +0000</pubDate></item><item><title>SECA: Semantically Equivalent and Coherent Attacks for Eliciting LLM Hallucinations</title><link>https://arxiv.org/abs/2510.04398</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes SECA, a method for generating semantically equivalent and coherent adversarial prompts to elicit LLM hallucinations&lt;/li&gt;&lt;li&gt;Formulates the problem as a constrained optimization with semantic equivalence and coherence constraints&lt;/li&gt;&lt;li&gt;Introduces a zeroth-order method for constraint-preserving adversarial search&lt;/li&gt;&lt;li&gt;Demonstrates higher attack success rates on multiple-choice QA tasks compared to existing methods&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Buyun Liang', 'Liangzu Peng', 'Jinqi Luo', 'Darshan Thaker', 'Kwan Ho Ryan Chan', "Ren\\'e Vidal"]&lt;/li&gt;&lt;li&gt;Tags: ['adversarial prompting', 'LLM red teaming', 'hallucinations', 'prompt injection', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.04398</guid><pubDate>Tue, 07 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Unmasking Backdoors: An Explainable Defense via Gradient-Attention Anomaly Scoring for Pre-trained Language Models</title><link>https://arxiv.org/abs/2510.04347</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes an inference-time defense against backdoor attacks in pre-trained language models using gradient-attention anomaly scoring.&lt;/li&gt;&lt;li&gt;Combines token-level attention and gradient information to detect poisoned inputs.&lt;/li&gt;&lt;li&gt;Demonstrates reduced attack success rates across various backdoor scenarios.&lt;/li&gt;&lt;li&gt;Provides interpretability analysis for trigger localization.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Anindya Sundar Das', 'Kangjie Chen', 'Monowar Bhuyan']&lt;/li&gt;&lt;li&gt;Tags: ['AI security', 'backdoor attacks', 'gradient attribution', 'attention mechanisms', 'explainable defense']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.04347</guid><pubDate>Tue, 07 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Read the Scene, Not the Script: Outcome-Aware Safety for LLMs</title><link>https://arxiv.org/abs/2510.04320</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces the concept of consequence-blindness in LLMs, where models rely on surface signals rather than outcome reasoning.&lt;/li&gt;&lt;li&gt;Presents CB-Bench, a benchmark to evaluate consequence-blindness in four risk scenarios.&lt;/li&gt;&lt;li&gt;Introduces CS-Chain-4k dataset for consequence-aware safety alignment.&lt;/li&gt;&lt;li&gt;Shows that fine-tuning on CS-Chain-4k improves resistance to jailbreaks and reduces over-refusal.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Rui Wu', 'Yihao Quan', 'Zeru Shi', 'Zhenting Wang', 'Yanshu Li', 'Ruixiang Tang']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'jailbreaking', 'safety evaluation', 'alignment', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.04320</guid><pubDate>Tue, 07 Oct 2025 00:00:00 +0000</pubDate></item><item><title>COSMO-RL: Towards Trustworthy LMRMs via Joint Safety and Stability</title><link>https://arxiv.org/abs/2510.04196</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces COSMO-RL, a mixed reinforcement learning framework for training safe LMRMs&lt;/li&gt;&lt;li&gt;Aims to balance safety and capability in a single pipeline&lt;/li&gt;&lt;li&gt;Shows improved robustness to multimodal jailbreaks and reduced unnecessary refusals&lt;/li&gt;&lt;li&gt;Releases the COSMO-R1 model with consistent safety and capability gains&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yizhuo Ding', 'Mingkang Chen', 'Qiuhua Liu', 'Fenghua Weng', 'Wanying Qu', 'Yue Yang', 'Yugang Jiang', 'Zuxuan Wu', 'Yanwei Fu', 'Wenqi Shao']&lt;/li&gt;&lt;li&gt;Tags: ['red teaming', 'safety evaluation', 'robustness', 'alignment', 'multimodal']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.04196</guid><pubDate>Tue, 07 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Quantifying Risks in Multi-turn Conversation with Large Language Models</title><link>https://arxiv.org/abs/2510.03969</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes QRLLM, a certification framework for quantifying catastrophic risks in multi-turn conversations with LLMs.&lt;/li&gt;&lt;li&gt;Models conversations as Markov processes on query graphs to capture realistic flow.&lt;/li&gt;&lt;li&gt;Provides statistical guarantees on risk bounds using confidence intervals.&lt;/li&gt;&lt;li&gt;Demonstrates high risk percentages in frontier models, indicating need for better safety training.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Chengxiao Wang', 'Isha Chaudhary', 'Qian Hu', 'Weitong Ruan', 'Rahul Gupta', 'Gagandeep Singh']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'safety evaluation', 'robustness', 'alignment']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.03969</guid><pubDate>Tue, 07 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Know Thyself? On the Incapability and Implications of AI Self-Recognition</title><link>https://arxiv.org/abs/2510.03399</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces a systematic evaluation framework for AI self-recognition&lt;/li&gt;&lt;li&gt;Tests 10 LLMs on binary self-recognition and exact model prediction&lt;/li&gt;&lt;li&gt;Finds consistent failure in self-recognition with strong bias towards GPT and Claude&lt;/li&gt;&lt;li&gt;Discusses implications for AI safety and future directions&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Xiaoyan Bai', 'Aryan Shrivastava', 'Ari Holtzman', 'Chenhao Tan']&lt;/li&gt;&lt;li&gt;Tags: ['AI safety', 'self-recognition', 'model evaluation', 'LLM capabilities', 'safety evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.03399</guid><pubDate>Tue, 07 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Inoculation Prompting: Instructing LLMs to misbehave at train-time improves test-time alignment</title><link>https://arxiv.org/abs/2510.05024</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces Inoculation Prompting (IP) to prevent LLMs from learning undesired behaviors during training&lt;/li&gt;&lt;li&gt;Modifies training prompts to explicitly request the undesired behavior to build resistance&lt;/li&gt;&lt;li&gt;Tested across four settings, showing reduced undesired behavior without losing desired capabilities&lt;/li&gt;&lt;li&gt;More effective when prompts strongly elicit the undesired behavior before fine-tuning&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Nevan Wichers', 'Aram Ebtekar', 'Ariana Azarbal', 'Victor Gillioz', 'Christine Ye', 'Emil Ryd', 'Neil Rathi', 'Henry Sleight', 'Alex Mallen', 'Fabien Roger', 'Samuel Marks']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'safety', 'red teaming', 'prompt injection', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.05024</guid><pubDate>Tue, 07 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Alignment Tipping Process: How Self-Evolution Pushes LLM Agents Off the Rails</title><link>https://arxiv.org/abs/2510.04860</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces the Alignment Tipping Process (ATP) for self-evolving LLM agents&lt;/li&gt;&lt;li&gt;Analyzes ATP through Self-Interested Exploration and Imitative Strategy Diffusion&lt;/li&gt;&lt;li&gt;Benchmarks Qwen3-8B and Llama-3.1-8B-Instruct showing rapid alignment erosion&lt;/li&gt;&lt;li&gt;Highlights fragility of current RL-based alignment methods&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Siwei Han', 'Jiaqi Liu', 'Yaofeng Su', 'Wenbo Duan', 'Xinyuan Liu', 'Cihang Xie', 'Mohit Bansal', 'Mingyu Ding', 'Linjun Zhang', 'Huaxiu Yao']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'red teaming', 'safety evaluation', 'robustness', 'multi-agent systems']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.04860</guid><pubDate>Tue, 07 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Operationalizing Data Minimization for Privacy-Preserving LLM Prompting</title><link>https://arxiv.org/abs/2510.03662</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Framework for data minimization in LLM prompting&lt;/li&gt;&lt;li&gt;Evaluates data minimization across multiple datasets and models&lt;/li&gt;&lt;li&gt;Highlights privacy-utility trade-offs and model capabilities&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jijie Zhou', 'Niloofar Mireshghallah', 'Tianshi Li']&lt;/li&gt;&lt;li&gt;Tags: ['privacy', 'data minimization', 'LLM', 'prompting', 'security']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.03662</guid><pubDate>Tue, 07 Oct 2025 00:00:00 +0000</pubDate></item><item><title>From Theory to Practice: Evaluating Data Poisoning Attacks and Defenses in In-Context Learning on Social Media Health Discourse</title><link>https://arxiv.org/abs/2510.03636</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Evaluates data poisoning attacks on in-context learning (ICL) in LLMs for public health sentiment analysis&lt;/li&gt;&lt;li&gt;Tests adversarial perturbations like synonym replacement and negation insertion&lt;/li&gt;&lt;li&gt;Introduces Spectral Signature Defense to filter poisoned examples&lt;/li&gt;&lt;li&gt;Shows defense preserves accuracy and dataset integrity&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Rabeya Amin Jhuma', 'Mostafa Mohaimen Akand Faisal']&lt;/li&gt;&lt;li&gt;Tags: ['data poisoning', 'adversarial attacks', 'defense mechanisms', 'in-context learning', 'public health']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.03636</guid><pubDate>Tue, 07 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Machine Unlearning Meets Adversarial Robustness via Constrained Interventions on LLMs</title><link>https://arxiv.org/abs/2510.03567</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Unified approach for unlearning and robustness in LLMs&lt;/li&gt;&lt;li&gt;Constrained optimization for minimal weight interventions&lt;/li&gt;&lt;li&gt;Addresses jail-breaking attacks and sensitive info unlearning&lt;/li&gt;&lt;li&gt;Outperforms state-of-the-art defense methods&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Fatmazohra Rezkellah', 'Ramzi Dakhmouche']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'jailbreaking', 'adversarial robustness', 'unlearning', 'privacy']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.03567</guid><pubDate>Tue, 07 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Certifiable Safe RLHF: Fixed-Penalty Constraint Optimization for Safer Language Models</title><link>https://arxiv.org/abs/2510.03520</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces Certifiable Safe-RLHF (CS-RLHF) for safer LLMs&lt;/li&gt;&lt;li&gt;Uses a cost model for semantic safety scores&lt;/li&gt;&lt;li&gt;Adopts a penalty-based approach instead of dual variables&lt;/li&gt;&lt;li&gt;Provides provable safety guarantees against jailbreaks&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Kartik Pandit', 'Sourav Ganguly', 'Arnesh Banerjee', 'Shaahin Angizi', 'Arnob Ghosh']&lt;/li&gt;&lt;li&gt;Tags: ['safety', 'red teaming', 'jailbreaking', 'constrained optimization', 'RLHF']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.03520</guid><pubDate>Tue, 07 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Revoking Amnesia: RL-based Trajectory Optimization to Resurrect Erased Concepts in Diffusion Models</title><link>https://arxiv.org/abs/2510.03302</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;RevAm is an RL-based framework that resurrects erased concepts in diffusion models&lt;/li&gt;&lt;li&gt;It uses trajectory optimization to steer the denoising process without modifying model weights&lt;/li&gt;&lt;li&gt;Exposes vulnerabilities in current concept erasure methods&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Daiheng Gao', 'Nanxiang Jiang', 'Andi Zhang', 'Shilin Lu', 'Yufei Tang', 'Wenbo Zhou', 'Weiming Zhang', 'Zhaoxin Fan']&lt;/li&gt;&lt;li&gt;Tags: ['red teaming', 'adversarial prompting', 'safety evaluation', 'diffusion models', 'concept erasure']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.03302</guid><pubDate>Tue, 07 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Memory Self-Regeneration: Uncovering Hidden Knowledge in Unlearned Models</title><link>https://arxiv.org/abs/2510.03263</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces Memory Self-Regeneration task and MemoRa strategy for model unlearning&lt;/li&gt;&lt;li&gt;Discusses adversarial prompts and knowledge recovery&lt;/li&gt;&lt;li&gt;Highlights robustness in knowledge retrieval as an evaluation measure&lt;/li&gt;&lt;li&gt;Differentiates between short-term and long-term forgetting&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Agnieszka Polowczyk', 'Alicja Polowczyk', "Joanna Waczy\\'nska", 'Piotr Borycki', 'Przemys{\\l}aw Spurek']&lt;/li&gt;&lt;li&gt;Tags: ['model extraction', 'adversarial prompting', 'robustness', 'unlearning']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.03263</guid><pubDate>Tue, 07 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Adversarial training with restricted data manipulation</title><link>https://arxiv.org/abs/2510.03254</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces constrained pessimistic bilevel optimization for adversarial training&lt;/li&gt;&lt;li&gt;Aims to prevent overly pessimistic models by restricting adversary's data manipulation&lt;/li&gt;&lt;li&gt;Shows improved performance over existing methods in experiments&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['David Benfield', 'Stefano Coniglio', 'Phan Tu Vuong', 'Alain Zemkoho']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial training', 'bilevel optimization', 'data manipulation', 'resilient classifiers']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.03254</guid><pubDate>Tue, 07 Oct 2025 00:00:00 +0000</pubDate></item><item><title>HFuzzer: Testing Large Language Models for Package Hallucinations via Phrase-based Fuzzing</title><link>https://arxiv.org/abs/2509.23835</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;HFuzzer is a phrase-based fuzzing framework to test LLMs for package hallucinations.&lt;/li&gt;&lt;li&gt;It generates coding tasks using phrases to trigger hallucinations of non-existent packages.&lt;/li&gt;&lt;li&gt;Evaluated on multiple LLMs, including GPT-4o, finding significant hallucinations.&lt;/li&gt;&lt;li&gt;Aims to mitigate software supply chain attacks by detecting these hallucinations.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yukai Zhao', 'Menghan Wu', 'Xing Hu', 'Xin Xia']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'adversarial prompting', 'security', 'fuzzing', 'package hallucinations']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.23835</guid><pubDate>Tue, 07 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Boundary on the Table: Efficient Black-Box Decision-Based Attacks for Structured Data</title><link>https://arxiv.org/abs/2509.22850</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces a black-box decision-based adversarial attack for tabular data&lt;/li&gt;&lt;li&gt;Combines gradient-free direction estimation with boundary search&lt;/li&gt;&lt;li&gt;Effective against both classical ML and LLM-based models&lt;/li&gt;&lt;li&gt;High success rates with minimal queries&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Roie Kazoom', 'Yuval Ratzabi', 'Etamar Rothstein', 'Ofer Hadar']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial attacks', 'black-box attacks', 'tabular data', 'model robustness', 'security']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.22850</guid><pubDate>Tue, 07 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Active Attacks: Red-teaming LLMs via Adaptive Environments</title><link>https://arxiv.org/abs/2509.21947</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces Active Attacks, an RL-based red-teaming algorithm for LLMs that adapts as the victim model evolves.&lt;/li&gt;&lt;li&gt;Uses periodic safety fine-tuning to reduce rewards in exploited regions, forcing exploration of new vulnerabilities.&lt;/li&gt;&lt;li&gt;Achieves significant improvement in cross-attack success rates compared to prior methods.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Taeyoung Yun', 'Pierre-Luc St-Charles', 'Jinkyoo Park', 'Yoshua Bengio', 'Minsu Kim']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'reinforcement learning', 'diversity in attacks', 'adaptive environments', 'safety fine-tuning']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.21947</guid><pubDate>Tue, 07 Oct 2025 00:00:00 +0000</pubDate></item><item><title>MIA-EPT: Membership Inference Attack via Error Prediction for Tabular Data</title><link>https://arxiv.org/abs/2509.13046</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces MIA-EPT, a membership inference attack on tabular diffusion models&lt;/li&gt;&lt;li&gt;Uses error prediction via attribute masking and reconstruction&lt;/li&gt;&lt;li&gt;Validated on multiple synthesizers with AUC-ROC up to 0.599&lt;/li&gt;&lt;li&gt;Achieved second place in MIDST 2025 Black-box Multi-Table track&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Eyal German', 'Daniel Samira', 'Yuval Elovici', 'Asaf Shabtai']&lt;/li&gt;&lt;li&gt;Tags: ['privacy attacks', 'data poisoning', 'adversarial prompting', 'model extraction']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.13046</guid><pubDate>Tue, 07 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Large Language Model Hacking: Quantifying the Hidden Risks of Using LLMs for Text Annotation</title><link>https://arxiv.org/abs/2509.08825</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;The paper explores the risks of using LLMs for text annotation in social science research, focusing on how configuration choices can lead to incorrect conclusions (LLM hacking). It demonstrates both intentional and accidental hacking through prompt manipulation and model variability. The study evaluates 21 mitigation techniques and recommends human annotations and statistical corrections.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Joachim Baumann', 'Paul R\\"ottger', 'Aleksandra Urman', 'Albert Wendsj\\"o', 'Flor Miriam Plaza-del-Arco', 'Johannes B. Gruber', 'Dirk Hovy']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'prompt injection', 'adversarial prompting', 'safety evaluation', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.08825</guid><pubDate>Tue, 07 Oct 2025 00:00:00 +0000</pubDate></item><item><title>X-Teaming Evolutionary M2S: Automated Discovery of Multi-turn to Single-turn Jailbreak Templates</title><link>https://arxiv.org/abs/2509.08729</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Automated framework for discovering M2S templates&lt;/li&gt;&lt;li&gt;Uses LLM-guided evolution&lt;/li&gt;&lt;li&gt;Evaluates on GPT-4.1 with cross-model testing&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Hyunjun Kim', 'Junwoo Ha', 'Sangyoon Yu', 'Haon Park']&lt;/li&gt;&lt;li&gt;Tags: ['red teaming', 'jailbreaking', 'prompt injection', 'adversarial prompting', 'model extraction']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.08729</guid><pubDate>Tue, 07 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Thought Purity: A Defense Framework For Chain-of-Thought Attack</title><link>https://arxiv.org/abs/2507.12314</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes Thought Purity (TP) framework to defend against Chain-of-Thought Attacks (CoTA) in Large Reasoning Models (LRMs)&lt;/li&gt;&lt;li&gt;TP includes safety-optimized data processing, RL-enhanced constraints, and adaptive monitoring&lt;/li&gt;&lt;li&gt;Aims to balance security and functionality in next-gen AI architectures&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Zihao Xue', 'Zhen Bi', 'Long Ma', 'Zhenlin Hu', 'Yan Wang', 'Zhenfang Liu', 'Qing Sheng', 'Jie Xiao', 'Jungang Lou']&lt;/li&gt;&lt;li&gt;Tags: ['red teaming', 'adversarial prompting', 'safety evaluation', 'robustness', 'security standards']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2507.12314</guid><pubDate>Tue, 07 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Self-Correction Bench: Uncovering and Addressing the Self-Correction Blind Spot in Large Language Models</title><link>https://arxiv.org/abs/2507.02778</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces Self-Correction Bench to evaluate LLMs' ability to correct their own errors&lt;/li&gt;&lt;li&gt;Discovers the Self-Correction Blind Spot where LLMs fail to correct their own errors but can correct others'&lt;/li&gt;&lt;li&gt;Tests 14 models showing 64.5% average blind spot rate&lt;/li&gt;&lt;li&gt;Finds that a 'Wait' prompt reduces blind spots by 89.3%&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Ken Tsui']&lt;/li&gt;&lt;li&gt;Tags: ['safety evaluation', 'LLM red teaming', 'alignment', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2507.02778</guid><pubDate>Tue, 07 Oct 2025 00:00:00 +0000</pubDate></item><item><title>The Security Threat of Compressed Projectors in Large Vision-Language Models</title><link>https://arxiv.org/abs/2506.00534</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Evaluates security of compressed vs uncompressed visual language projectors (VLPs) in LVLMs&lt;/li&gt;&lt;li&gt;Compressed projectors show significant vulnerabilities to adversarial attacks&lt;/li&gt;&lt;li&gt;Uncompressed projectors demonstrate robust security&lt;/li&gt;&lt;li&gt;Provides guidance for selecting secure VLPs&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yudong Zhang', 'Ruobing Xie', 'Xingwu Sun', 'Jiansheng Chen', 'Zhanhui Kang', 'Di Wang', 'Yu Wang']&lt;/li&gt;&lt;li&gt;Tags: ['security', 'adversarial attacks', 'visual language models', 'projector layers', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.00534</guid><pubDate>Tue, 07 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Rethinking Exact Unlearning under Exposure: Extracting Forgotten Data under Exact Unlearning in Large Language Model</title><link>https://arxiv.org/abs/2505.24379</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces a data extraction attack on exact unlearning in LLMs&lt;/li&gt;&lt;li&gt;Leverages pre- and post-unlearning logits to extract forgotten data&lt;/li&gt;&lt;li&gt;Demonstrates significant improvement in extraction rates on benchmarks&lt;/li&gt;&lt;li&gt;Highlights privacy risks in real-world deployments&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Xiaoyu Wu', 'Yifei Pang', 'Terrance Liu', 'Zhiwei Steven Wu']&lt;/li&gt;&lt;li&gt;Tags: ['data poisoning', 'privacy attacks', 'model extraction', 'unlearning', 'adversarial prompting']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2505.24379</guid><pubDate>Tue, 07 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Safety Subspaces are Not Linearly Distinct: A Fine-Tuning Case Study</title><link>https://arxiv.org/abs/2505.14185</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;The paper studies whether safety-relevant behavior in LLMs is isolated in specific linear subspaces.&lt;/li&gt;&lt;li&gt;It finds that safety subspaces are not linearly distinct from general learning components.&lt;/li&gt;&lt;li&gt;The research suggests subspace-based defenses have limitations and alternative strategies are needed.&lt;/li&gt;&lt;li&gt;Experiments were conducted on five open-source LLMs from Llama and Qwen families.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Kaustubh Ponkshe', 'Shaan Shah', 'Raghav Singhal', 'Praneeth Vepakomma']&lt;/li&gt;&lt;li&gt;Tags: ['LLM safety', 'alignment', 'subspace analysis', 'fine-tuning', 'model robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2505.14185</guid><pubDate>Tue, 07 Oct 2025 00:00:00 +0000</pubDate></item><item><title>System Prompt Poisoning: Persistent Attacks on Large Language Models Beyond User Injection</title><link>https://arxiv.org/abs/2505.06493</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces system prompt poisoning, a new attack vector targeting LLM system prompts&lt;/li&gt;&lt;li&gt;Demonstrates feasibility and effectiveness across various tasks and models&lt;/li&gt;&lt;li&gt;Shows that advanced prompting techniques like CoT and RAG are weakened by the attack&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Zongze Li', 'Jiawei Guo', 'Haipeng Cai']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'prompt injection', 'data poisoning', 'security', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2505.06493</guid><pubDate>Tue, 07 Oct 2025 00:00:00 +0000</pubDate></item><item><title>DualBreach: Efficient Dual-Jailbreaking via Target-Driven Initialization and Multi-Target Optimization</title><link>https://arxiv.org/abs/2504.18564</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes DualBreach, a framework for dual-jailbreaking attacks on LLMs and guardrails&lt;/li&gt;&lt;li&gt;Uses Target-driven Initialization and Multi-Target Optimization for efficient prompt generation&lt;/li&gt;&lt;li&gt;Achieves high success rates with fewer queries compared to existing methods&lt;/li&gt;&lt;li&gt;Introduces EGuard as a defensive mechanism&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Xinzhe Huang', 'Kedong Xiu', 'Tianhang Zheng', 'Churui Zeng', 'Wangze Ni', 'Zhan Qin', 'Kui Ren', 'Chun Chen']&lt;/li&gt;&lt;li&gt;Tags: ['jailbreaking', 'red teaming', 'adversarial prompting', 'model extraction', 'privacy attacks']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2504.18564</guid><pubDate>Tue, 07 Oct 2025 00:00:00 +0000</pubDate></item><item><title>TRCE: Towards Reliable Malicious Concept Erasure in Text-to-Image Diffusion Models</title><link>https://arxiv.org/abs/2503.07389</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes TRCE, a two-stage concept erasure method for text-to-image models&lt;/li&gt;&lt;li&gt;Aims to erase malicious concepts while preserving normal generation&lt;/li&gt;&lt;li&gt;Uses cross-attention optimization and contrastive learning&lt;/li&gt;&lt;li&gt;Evaluated on multiple benchmarks&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Ruidong Chen', 'Honglin Guo', 'Lanjun Wang', 'Chenyu Zhang', 'Weizhi Nie', 'An-An Liu']&lt;/li&gt;&lt;li&gt;Tags: ['safety', 'concept erasure', 'text-to-image', 'malicious content']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2503.07389</guid><pubDate>Tue, 07 Oct 2025 00:00:00 +0000</pubDate></item><item><title>AttackSeqBench: Benchmarking Large Language Models in Analyzing Attack Sequences within Cyber Threat Intelligence</title><link>https://arxiv.org/abs/2503.03170</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces AttackSeqBench for evaluating LLMs in analyzing attack sequences in CTI reports&lt;/li&gt;&lt;li&gt;Benchmarks multiple LLMs and strategies across different tasks and settings&lt;/li&gt;&lt;li&gt;Aims to improve understanding of LLM capabilities in cybersecurity&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Haokai Ma', 'Javier Yong', 'Yunshan Ma', 'Kuei Chen', 'Anis Yusof', 'Zhenkai Liang', 'Ee-Chien Chang']&lt;/li&gt;&lt;li&gt;Tags: ['Benchmarking', 'LLM', 'Cybersecurity', 'Attack Sequences', 'CTI']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2503.03170</guid><pubDate>Tue, 07 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Char-mander Use mBackdoor! A Study of Cross-lingual Backdoor Attacks in Multilingual LLMs</title><link>https://arxiv.org/abs/2502.16901</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Study of cross-lingual backdoor attacks in multilingual LLMs&lt;/li&gt;&lt;li&gt;Demonstrates that backdoors in one language can transfer to others&lt;/li&gt;&lt;li&gt;Uses toxicity classification as a case study&lt;/li&gt;&lt;li&gt;Highlights vulnerability through shared embedding spaces&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Himanshu Beniwal', 'Sailesh Panda', 'Birudugadda Srivibhav', 'Mayank Singh']&lt;/li&gt;&lt;li&gt;Tags: ['data poisoning', 'backdoor attacks', 'multilingual models', 'cross-lingual', 'security']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2502.16901</guid><pubDate>Tue, 07 Oct 2025 00:00:00 +0000</pubDate></item><item><title>A Statistical Hypothesis Testing Framework for Data Misappropriation Detection in Large Language Models</title><link>https://arxiv.org/abs/2501.02441</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a statistical hypothesis testing framework for detecting data misappropriation in LLMs&lt;/li&gt;&lt;li&gt;Involves embedding watermarks into copyrighted training data&lt;/li&gt;&lt;li&gt;Formulates detection as a hypothesis testing problem with controlled errors&lt;/li&gt;&lt;li&gt;Demonstrates empirical effectiveness through experiments&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yinpeng Cai', 'Lexin Li', 'Linjun Zhang']&lt;/li&gt;&lt;li&gt;Tags: ['data poisoning', 'privacy attacks', 'safety evaluation', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2501.02441</guid><pubDate>Tue, 07 Oct 2025 00:00:00 +0000</pubDate></item><item><title>H3Fusion: Helpful, Harmless, Honest Fusion of Aligned LLMs</title><link>https://arxiv.org/abs/2411.17792</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces H3Fusion, an alignment fusion approach for LLMs&lt;/li&gt;&lt;li&gt;Uses MoE methodology with expert routing based on input instructions&lt;/li&gt;&lt;li&gt;Improves helpfulness, harmlessness, honesty over individual models&lt;/li&gt;&lt;li&gt;Evaluates on three benchmarks with significant performance gains&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Selim Furkan Tekin', 'Fatih Ilhan', 'Tiansheng Huang', 'Sihao Hu', 'Yichang Xu', 'Zachary Yahn', 'Ling Liu']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'ensemble methods', 'mixture of experts', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2411.17792</guid><pubDate>Tue, 07 Oct 2025 00:00:00 +0000</pubDate></item><item><title>SteerDiff: Steering towards Safe Text-to-Image Diffusion Models</title><link>https://arxiv.org/abs/2410.02710</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces SteerDiff, a lightweight adaptor module for text-to-image diffusion models to prevent generation of inappropriate content.&lt;/li&gt;&lt;li&gt;SteerDiff manipulates text embeddings to guide the model away from harmful outputs.&lt;/li&gt;&lt;li&gt;Conducts experiments on concept unlearning and benchmarks against red-teaming strategies.&lt;/li&gt;&lt;li&gt;Explores potential for concept forgetting tasks.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Hongxiang Zhang', 'Yifeng He', 'Hao Chen']&lt;/li&gt;&lt;li&gt;Tags: ['red teaming', 'safety evaluation', 'adversarial prompting', 'model extraction', 'privacy attacks']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2410.02710</guid><pubDate>Tue, 07 Oct 2025 00:00:00 +0000</pubDate></item><item><title>When "Competency" in Reasoning Opens the Door to Vulnerability: Jailbreaking LLMs via Novel Complex Ciphers</title><link>https://arxiv.org/abs/2402.10601</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces ACE and LACE, jailbreaking techniques using custom ciphers&lt;/li&gt;&lt;li&gt;Develops CipherBench to evaluate LLM cipher decoding&lt;/li&gt;&lt;li&gt;Finds higher reasoning ability leads to higher vulnerability&lt;/li&gt;&lt;li&gt;Highlights trade-off between capability and security&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Divij Handa', 'Zehua Zhang', 'Amir Saeidi', 'Shrinidhi Kumbhar', 'Md Nayem Uddin', 'Aswin RRV', 'Chitta Baral']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'jailbreaking', 'adversarial prompting', 'safety evaluation', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2402.10601</guid><pubDate>Tue, 07 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Is It Thinking or Cheating? Detecting Implicit Reward Hacking by Measuring Reasoning Effort</title><link>https://arxiv.org/abs/2510.01367</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes TRACE (Truncated Reasoning AUC Evaluation) to detect implicit reward hacking in reasoning models&lt;/li&gt;&lt;li&gt;Measures reasoning effort by truncating CoT and evaluating reward at different lengths&lt;/li&gt;&lt;li&gt;Shows significant improvements over existing monitors in math and coding tasks&lt;/li&gt;&lt;li&gt;Can discover unknown loopholes during training&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Xinpeng Wang', 'Nitish Joshi', 'Barbara Plank', 'Rico Angell', 'He He']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'safety evaluation', 'robustness', 'red teaming']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.01367</guid><pubDate>Tue, 07 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Quant Fever, Reasoning Blackholes, Schrodinger's Compliance, and More: Probing GPT-OSS-20B</title><link>https://arxiv.org/abs/2509.23882</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Security evaluation of GPT-OSS-20B using Jailbreak Oracle&lt;/li&gt;&lt;li&gt;Identifies failure modes like quant fever, reasoning blackholes, etc.&lt;/li&gt;&lt;li&gt;Exploits demonstrated on the model&lt;/li&gt;&lt;li&gt;Focuses on adversarial conditions and their consequences&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Shuyi Lin', 'Tian Lu', 'Zikai Wang', 'Bo Wen', 'Yibo Zhao', 'Cheng Tan']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'jailbreaking', 'adversarial prompting', 'security evaluation', 'alignment']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.23882</guid><pubDate>Tue, 07 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Imperceptible Jailbreaking against Large Language Models</title><link>https://arxiv.org/abs/2510.05025</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces imperceptible jailbreak attacks using Unicode variation selectors&lt;/li&gt;&lt;li&gt;Proposes a chain-of-search pipeline for generating adversarial suffixes&lt;/li&gt;&lt;li&gt;Achieves high success rates against aligned LLMs&lt;/li&gt;&lt;li&gt;Generalizes to prompt injection attacks without visible modifications&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Kuofeng Gao', 'Yiming Li', 'Chao Du', 'Xin Wang', 'Xingjun Ma', 'Shu-Tao Xia', 'Tianyu Pang']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'jailbreaking', 'adversarial prompting', 'prompt injection', 'Unicode attacks']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.05025</guid><pubDate>Tue, 07 Oct 2025 00:00:00 +0000</pubDate></item><item><title>SocialHarmBench: Revealing LLM Vulnerabilities to Socially Harmful Requests</title><link>https://arxiv.org/abs/2510.04891</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces SocialHarmBench, a dataset for testing LLM vulnerabilities in sociopolitical contexts&lt;/li&gt;&lt;li&gt;Evaluates models like Mistral-7B showing high vulnerability to harmful compliance&lt;/li&gt;&lt;li&gt;Analyzes temporal and geographic patterns in model failures&lt;/li&gt;&lt;li&gt;Highlights gaps in current safety measures and potential biases&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Punya Syon Pandey', 'Hai Son Le', 'Devansh Bhardwaj', 'Rada Mihalcea', 'Zhijing Jin']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'safety evaluation', 'alignment', 'data poisoning', 'privacy attacks']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.04891</guid><pubDate>Tue, 07 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Alignment Tipping Process: How Self-Evolution Pushes LLM Agents Off the Rails</title><link>https://arxiv.org/abs/2510.04860</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces the Alignment Tipping Process (ATP) for self-evolving LLM agents&lt;/li&gt;&lt;li&gt;Analyzes ATP through Self-Interested Exploration and Imitative Strategy Diffusion&lt;/li&gt;&lt;li&gt;Benchmarks Qwen3-8B and Llama-3.1-8B-Instruct showing rapid alignment erosion&lt;/li&gt;&lt;li&gt;Highlights fragility of current RL-based alignment methods&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Siwei Han', 'Jiaqi Liu', 'Yaofeng Su', 'Wenbo Duan', 'Xinyuan Liu', 'Cihang Xie', 'Mohit Bansal', 'Mingyu Ding', 'Linjun Zhang', 'Huaxiu Yao']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'red teaming', 'safety evaluation', 'robustness', 'multi-agent systems']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.04860</guid><pubDate>Tue, 07 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Unified Threat Detection and Mitigation Framework (UTDMF): Combating Prompt Injection, Deception, and Bias in Enterprise-Scale Transformers</title><link>https://arxiv.org/abs/2510.04528</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces UTDMF for detecting and mitigating prompt injection, deception, and bias in LLMs&lt;/li&gt;&lt;li&gt;Achieves high detection accuracy and reduction in threats through experiments&lt;/li&gt;&lt;li&gt;Includes a generalized patching algorithm and enterprise deployment toolkit&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Santhosh KumarRavindran']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'prompt injection', 'deception', 'bias mitigation', 'security framework']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.04528</guid><pubDate>Tue, 07 Oct 2025 00:00:00 +0000</pubDate></item><item><title>P2P: A Poison-to-Poison Remedy for Reliable Backdoor Defense in LLMs</title><link>https://arxiv.org/abs/2510.04503</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes P2P, a backdoor defense algorithm for LLMs using benign triggers&lt;/li&gt;&lt;li&gt;Injects safe alternative labels into training samples via prompt-based learning&lt;/li&gt;&lt;li&gt;Effective across multiple tasks and attack types&lt;/li&gt;&lt;li&gt;Reduces attack success rate while preserving performance&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Shuai Zhao', 'Xinyi Wu', 'Shiqian Zhao', 'Xiaobao Wu', 'Zhongliang Guo', 'Yanhao Jia', 'Anh Tuan Luu']&lt;/li&gt;&lt;li&gt;Tags: ['data poisoning', 'backdoor defense', 'LLM security', 'prompt-based learning', 'adversarial training']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.04503</guid><pubDate>Tue, 07 Oct 2025 00:00:00 +0000</pubDate></item><item><title>SECA: Semantically Equivalent and Coherent Attacks for Eliciting LLM Hallucinations</title><link>https://arxiv.org/abs/2510.04398</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes SECA, a method for generating semantically equivalent and coherent adversarial prompts to elicit LLM hallucinations&lt;/li&gt;&lt;li&gt;Formulates the problem as a constrained optimization with semantic equivalence and coherence constraints&lt;/li&gt;&lt;li&gt;Introduces a zeroth-order method for constraint-preserving adversarial search&lt;/li&gt;&lt;li&gt;Demonstrates higher attack success rates on multiple-choice QA tasks compared to existing methods&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Buyun Liang', 'Liangzu Peng', 'Jinqi Luo', 'Darshan Thaker', 'Kwan Ho Ryan Chan', "Ren\\'e Vidal"]&lt;/li&gt;&lt;li&gt;Tags: ['adversarial prompting', 'LLM red teaming', 'hallucinations', 'prompt injection', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.04398</guid><pubDate>Tue, 07 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Inoculation Prompting: Eliciting traits from LLMs during training can suppress them at test-time</title><link>https://arxiv.org/abs/2510.04340</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes inoculation prompting to suppress undesirable traits in LLMs during training&lt;/li&gt;&lt;li&gt;Modifies finetuning data with system prompts that elicit the traits&lt;/li&gt;&lt;li&gt;Effective in reducing misalignment, backdoor attacks, and subliminal learning&lt;/li&gt;&lt;li&gt;Analyzes the mechanism of reduced optimization pressure&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Daniel Tan', 'Anders Woodruff', 'Niels Warncke', 'Arun Jose', "Maxime Rich\\'e", 'David Demitri Africa', 'Mia Taylor']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'alignment', 'safety evaluation', 'adversarial prompting', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.04340</guid><pubDate>Tue, 07 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Audit the Whisper: Detecting Steganographic Collusion in Multi-Agent LLMs</title><link>https://arxiv.org/abs/2510.04303</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces Audit the Whisper, a framework for detecting covert coordination in multi-agent LLMs&lt;/li&gt;&lt;li&gt;Includes channel-capacity analysis and a benchmark (ColludeBench) with various scenarios&lt;/li&gt;&lt;li&gt;Presents an auditing pipeline with mutual information, permutation invariance, watermark variance, and fairness checks&lt;/li&gt;&lt;li&gt;Achieves 100% TPR with zero false positives in 600 runs across 12 conditions&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Om Tailor']&lt;/li&gt;&lt;li&gt;Tags: ['red teaming', 'adversarial prompting', 'safety evaluation', 'robustness', 'privacy attacks']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.04303</guid><pubDate>Tue, 07 Oct 2025 00:00:00 +0000</pubDate></item><item><title>AgentTypo: Adaptive Typographic Prompt Injection Attacks against Black-box Multimodal Agents</title><link>https://arxiv.org/abs/2510.04257</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces AgentTypo, a black-box red-teaming framework for adaptive typographic prompt injection attacks on multimodal agents.&lt;/li&gt;&lt;li&gt;Uses ATPI algorithm to embed optimized text into images with stealth loss for human detectability.&lt;/li&gt;&lt;li&gt;AgentTypo-pro refines prompts using feedback and learns from past successes.&lt;/li&gt;&lt;li&gt;Significant performance improvements over existing attacks on various multimodal models.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yanjie Li', 'Yiming Cao', 'Dong Wang', 'Bin Xiao']&lt;/li&gt;&lt;li&gt;Tags: ['red teaming', 'prompt injection', 'multimodal agents', 'black-box attacks', 'stealth loss']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.04257</guid><pubDate>Tue, 07 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Quantifying Distributional Robustness of Agentic Tool-Selection</title><link>https://arxiv.org/abs/2510.03992</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces ToolCert, a statistical framework to certify tool selection robustness in agentic systems&lt;/li&gt;&lt;li&gt;Evaluates against adaptive attackers introducing adversarial tools with misleading metadata&lt;/li&gt;&lt;li&gt;Shows significant performance drops under adversarial conditions, highlighting security vulnerabilities&lt;/li&gt;&lt;li&gt;Provides a method to quantify worst-case performance for safe deployment&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jehyeok Yeon', 'Isha Chaudhary', 'Gagandeep Singh']&lt;/li&gt;&lt;li&gt;Tags: ['red teaming', 'adversarial prompting', 'robustness', 'safety evaluation', 'tool selection']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.03992</guid><pubDate>Tue, 07 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Operationalizing Data Minimization for Privacy-Preserving LLM Prompting</title><link>https://arxiv.org/abs/2510.03662</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Framework for data minimization in LLM prompting&lt;/li&gt;&lt;li&gt;Evaluates data minimization across multiple datasets and models&lt;/li&gt;&lt;li&gt;Highlights privacy-utility trade-offs and model capabilities&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jijie Zhou', 'Niloofar Mireshghallah', 'Tianshi Li']&lt;/li&gt;&lt;li&gt;Tags: ['privacy', 'data minimization', 'LLM', 'prompting', 'security']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.03662</guid><pubDate>Tue, 07 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Certifiable Safe RLHF: Fixed-Penalty Constraint Optimization for Safer Language Models</title><link>https://arxiv.org/abs/2510.03520</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces Certifiable Safe-RLHF (CS-RLHF) for safer LLMs&lt;/li&gt;&lt;li&gt;Uses a cost model for semantic safety scores&lt;/li&gt;&lt;li&gt;Adopts a penalty-based approach instead of dual variables&lt;/li&gt;&lt;li&gt;Provides provable safety guarantees against jailbreaks&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Kartik Pandit', 'Sourav Ganguly', 'Arnesh Banerjee', 'Shaahin Angizi', 'Arnob Ghosh']&lt;/li&gt;&lt;li&gt;Tags: ['safety', 'red teaming', 'jailbreaking', 'constrained optimization', 'RLHF']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.03520</guid><pubDate>Tue, 07 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Red Lines and Grey Zones in the Fog of War: Benchmarking Legal Risk, Moral Harm, and Regional Bias in Large Language Model Military Decision-Making</title><link>https://arxiv.org/abs/2510.03514</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking framework for evaluating legal and moral risks in LLM-based military decision-making&lt;/li&gt;&lt;li&gt;Metrics based on International Humanitarian Law (IHL) and military doctrine&lt;/li&gt;&lt;li&gt;Evaluated GPT-4o, Gemini-2.5, and LLaMA-3.1 in simulated conflict scenarios&lt;/li&gt;&lt;li&gt;Found significant violations of IHL principles and varying harm tolerance across models&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Toby Drinkall']&lt;/li&gt;&lt;li&gt;Tags: ['red teaming', 'safety evaluation', 'alignment', 'robustness', 'LLM']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.03514</guid><pubDate>Tue, 07 Oct 2025 00:00:00 +0000</pubDate></item><item><title>NEXUS: Network Exploration for eXploiting Unsafe Sequences in Multi-Turn LLM Jailbreaks</title><link>https://arxiv.org/abs/2510.03417</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;NEXUS is a framework for multi-turn LLM jailbreak attacks&lt;/li&gt;&lt;li&gt;Uses ThoughtNet to expand harmful intent into semantic networks&lt;/li&gt;&lt;li&gt;Simulator refines queries with LLM collaboration&lt;/li&gt;&lt;li&gt;Network Traverser navigates for real-time attacks&lt;/li&gt;&lt;li&gt;Improves attack success rates on various LLMs&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Javad Rafiei Asl', 'Sidhant Narula', 'Mohammad Ghasemigol', 'Eduardo Blanco', 'Daniel Takabi']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'jailbreaking', 'adversarial prompting', 'multi-turn attacks', 'alignment']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.03417</guid><pubDate>Tue, 07 Oct 2025 00:00:00 +0000</pubDate></item><item><title>LegalSim: Multi-Agent Simulation of Legal Systems for Discovering Procedural Exploits</title><link>https://arxiv.org/abs/2510.03405</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces LegalSim, a multi-agent simulation for adversarial legal proceedings&lt;/li&gt;&lt;li&gt;Explores AI-based procedural exploits in legal systems&lt;/li&gt;&lt;li&gt;Compares different agent policies (PPO, bandit, LLM, heuristic)&lt;/li&gt;&lt;li&gt;Evaluates exploit chains and agent performance across legal regimes&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Sanket Badhe']&lt;/li&gt;&lt;li&gt;Tags: ['red teaming', 'adversarial', 'multi-agent', 'simulation', 'legal systems']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.03405</guid><pubDate>Tue, 07 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Memory Self-Regeneration: Uncovering Hidden Knowledge in Unlearned Models</title><link>https://arxiv.org/abs/2510.03263</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces Memory Self-Regeneration task and MemoRa strategy for model unlearning&lt;/li&gt;&lt;li&gt;Discusses adversarial prompts and knowledge recovery&lt;/li&gt;&lt;li&gt;Highlights robustness in knowledge retrieval as an evaluation measure&lt;/li&gt;&lt;li&gt;Differentiates between short-term and long-term forgetting&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Agnieszka Polowczyk', 'Alicja Polowczyk', "Joanna Waczy\\'nska", 'Piotr Borycki', 'Przemys{\\l}aw Spurek']&lt;/li&gt;&lt;li&gt;Tags: ['model extraction', 'adversarial prompting', 'robustness', 'unlearning']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.03263</guid><pubDate>Tue, 07 Oct 2025 00:00:00 +0000</pubDate></item><item><title>BrokenMath: A Benchmark for Sycophancy in Theorem Proving with LLMs</title><link>https://arxiv.org/abs/2510.04721</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces BrokenMath benchmark for sycophancy in theorem proving with LLMs&lt;/li&gt;&lt;li&gt;Evaluates sycophantic behavior in models like GPT-5&lt;/li&gt;&lt;li&gt;Tests mitigation strategies but finds sycophancy persists&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Ivo Petrov', 'Jasper Dekoninck', 'Martin Vechev']&lt;/li&gt;&lt;li&gt;Tags: ['sycophancy', 'LLM red teaming', 'benchmarking', 'mathematical reasoning', 'model evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.04721</guid><pubDate>Tue, 07 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Impatient Users Confuse AI Agents: High-fidelity Simulations of Human Traits for Testing Agents</title><link>https://arxiv.org/abs/2510.04491</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces TraitBasis for stress testing AI agents by simulating varied user traits&lt;/li&gt;&lt;li&gt;Extends τ-Bench to τ-Trait to test agent robustness under realistic user behavior&lt;/li&gt;&lt;li&gt;Observes significant performance degradation across models, highlighting robustness issues&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Muyu He', 'Anand Kumar', 'Tsach Mackey', 'Meghana Rajeev', 'James Zou', 'Nazneen Rajani']&lt;/li&gt;&lt;li&gt;Tags: ['robustness', 'safety evaluation', 'red teaming', 'adversarial prompting', 'benchmarking']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.04491</guid><pubDate>Tue, 07 Oct 2025 00:00:00 +0000</pubDate></item><item><title>COSMO-RL: Towards Trustworthy LMRMs via Joint Safety and Stability</title><link>https://arxiv.org/abs/2510.04196</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces COSMO-RL, a mixed reinforcement learning framework for training safe LMRMs&lt;/li&gt;&lt;li&gt;Aims to balance safety and capability in a single pipeline&lt;/li&gt;&lt;li&gt;Shows improved robustness to multimodal jailbreaks and reduced unnecessary refusals&lt;/li&gt;&lt;li&gt;Releases the COSMO-R1 model with consistent safety and capability gains&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yizhuo Ding', 'Mingkang Chen', 'Qiuhua Liu', 'Fenghua Weng', 'Wanying Qu', 'Yue Yang', 'Yugang Jiang', 'Zuxuan Wu', 'Yanwei Fu', 'Wenqi Shao']&lt;/li&gt;&lt;li&gt;Tags: ['red teaming', 'safety evaluation', 'robustness', 'alignment', 'multimodal']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.04196</guid><pubDate>Tue, 07 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Moral Anchor System: A Predictive Framework for AI Value Alignment and Drift Prevention</title><link>https://arxiv.org/abs/2510.04073</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes Moral Anchor System (MAS) for detecting and preventing value drift in AI agents&lt;/li&gt;&lt;li&gt;Uses Bayesian inference, LSTM networks, and human governance&lt;/li&gt;&lt;li&gt;Aims to reduce value drift incidents by 80% with high accuracy and low false positives&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Santhosh Kumar Ravindran']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'safety', 'value drift', 'governance', 'Bayesian inference']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.04073</guid><pubDate>Tue, 07 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Quantifying Risks in Multi-turn Conversation with Large Language Models</title><link>https://arxiv.org/abs/2510.03969</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes QRLLM, a certification framework for quantifying catastrophic risks in multi-turn conversations with LLMs.&lt;/li&gt;&lt;li&gt;Models conversations as Markov processes on query graphs to capture realistic flow.&lt;/li&gt;&lt;li&gt;Provides statistical guarantees on risk bounds using confidence intervals.&lt;/li&gt;&lt;li&gt;Demonstrates high risk percentages in frontier models, indicating need for better safety training.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Chengxiao Wang', 'Isha Chaudhary', 'Qian Hu', 'Weitong Ruan', 'Rahul Gupta', 'Gagandeep Singh']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'safety evaluation', 'robustness', 'alignment']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.03969</guid><pubDate>Tue, 07 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Spatial CAPTCHA: Generatively Benchmarking Spatial Reasoning for Human-Machine Differentiation</title><link>https://arxiv.org/abs/2510.03863</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces Spatial CAPTCHA, a new CAPTCHA framework using spatial reasoning tasks&lt;/li&gt;&lt;li&gt;Evaluates human vs MLLM performance, showing significant AI limitations&lt;/li&gt;&lt;li&gt;Compares with Google reCAPTCHA for security and AI diagnostic capabilities&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Arina Kharlamova', 'Bowei He', 'Chen Ma', 'Xue Liu']&lt;/li&gt;&lt;li&gt;Tags: ['CAPTCHA', 'spatial reasoning', 'security', 'red teaming', 'AI robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.03863</guid><pubDate>Tue, 07 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Cross-Modal Content Optimization for Steering Web Agent Preferences</title><link>https://arxiv.org/abs/2510.03612</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces Cross-Modal Preference Steering (CPS) for manipulating web agent preferences through visual and textual content&lt;/li&gt;&lt;li&gt;Evaluates CPS on various VLMs including GPT-4.1, Qwen-2.5VL, Pixtral-Large in movie and e-commerce tasks&lt;/li&gt;&lt;li&gt;Shows CPS is more effective and stealthy than existing methods with lower detection rates&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Tanqiu Jiang', 'Min Bai', 'Nikolaos Pappas', 'Yanjun Qi', 'Sandesh Swamy']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial prompting', 'multimodal attacks', 'preference manipulation', 'black-box attacks', 'stealth attacks']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.03612</guid><pubDate>Tue, 07 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Towards Policy-Compliant Agents: Learning Efficient Guardrails For Policy Violation Detection</title><link>https://arxiv.org/abs/2510.03485</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces PolicyGuardBench for detecting policy violations in agent trajectories&lt;/li&gt;&lt;li&gt;Trains PolicyGuard-4B model for efficient violation detection&lt;/li&gt;&lt;li&gt;Evaluates generalization across domains and subdomains&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research Paper&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Xiaofei Wen', 'Wenjie Jacky Mo', 'Yanan Xie', 'Peng Qi', 'Muhao Chen']&lt;/li&gt;&lt;li&gt;Tags: ['safety evaluation', 'alignment', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.03485</guid><pubDate>Tue, 07 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Know Thyself? On the Incapability and Implications of AI Self-Recognition</title><link>https://arxiv.org/abs/2510.03399</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces a systematic evaluation framework for AI self-recognition&lt;/li&gt;&lt;li&gt;Tests 10 LLMs on binary self-recognition and exact model prediction&lt;/li&gt;&lt;li&gt;Finds consistent failure in self-recognition with strong bias towards GPT and Claude&lt;/li&gt;&lt;li&gt;Discusses implications for AI safety and future directions&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Xiaoyan Bai', 'Aryan Shrivastava', 'Ari Holtzman', 'Chenhao Tan']&lt;/li&gt;&lt;li&gt;Tags: ['AI safety', 'self-recognition', 'model evaluation', 'LLM capabilities', 'safety evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.03399</guid><pubDate>Tue, 07 Oct 2025 00:00:00 +0000</pubDate></item></channel></rss>