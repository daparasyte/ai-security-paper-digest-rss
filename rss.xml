<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>AI Security Paper Digest</title><link>https://kentaroh-toyoda.github.io/ai-security-paper-digest-rss/rss.xml</link><description>Curated papers on AI security from ArXiv and ACL</description><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><lastBuildDate>Mon, 13 Oct 2025 22:25:09 +0000</lastBuildDate><item><title>Robustness in Both Domains: CLIP Needs a Robust Text Encoder</title><link>https://arxiv.org/abs/2506.03355</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;The paper addresses adversarial robustness in CLIP's text encoder.&lt;/li&gt;&lt;li&gt;Proposes LEAF, an adversarial finetuning method for text encoders.&lt;/li&gt;&lt;li&gt;Improves zero-shot adversarial accuracy and multimodal retrieval under noise.&lt;/li&gt;&lt;li&gt;Facilitates better text reconstruction from embeddings.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Elias Abad Rocamora', 'Christian Schlarmann', 'Naman Deep Singh', 'Yongtao Wu', 'Matthias Hein', 'Volkan Cevher']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial robustness', 'text encoder', 'CLIP', 'multimodal', 'adversarial attacks']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.03355</guid><pubDate>Mon, 13 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Benchmarking and Mitigate Sycophancy in Medical Vision-Language Models</title><link>https://arxiv.org/abs/2509.21979</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Evaluates sycophancy in medical VLMs using a new benchmark&lt;/li&gt;&lt;li&gt;Tests adversarial responses with psychological pressure templates&lt;/li&gt;&lt;li&gt;Proposes VIPER mitigation strategy to filter non-evidentiary content&lt;/li&gt;&lt;li&gt;Reduces sycophancy while maintaining accuracy&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Zikun Guo', 'Xinyue Xu', 'Pei Xiang', 'Shu Yang', 'Xin Han', 'Di Wang', 'Lijie Hu']&lt;/li&gt;&lt;li&gt;Tags: ['red teaming', 'adversarial prompting', 'safety evaluation', 'alignment', 'medical AI']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.21979</guid><pubDate>Mon, 13 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Goal-oriented Backdoor Attack against Vision-Language-Action Models via Physical Objects</title><link>https://arxiv.org/abs/2510.09269</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces goal-oriented backdoor attacks (GoBA) against VLA models using physical triggers&lt;/li&gt;&lt;li&gt;Proposes BadLIBERO dataset with physical triggers and backdoor actions&lt;/li&gt;&lt;li&gt;Evaluates attack success and performance impact on clean inputs&lt;/li&gt;&lt;li&gt;Analyzes factors affecting attack performance like trigger color and action trajectory&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Zirun Zhou', 'Zhengyang Xiao', 'Haochuan Xu', 'Jing Sun', 'Di Wang', 'Jingfeng Zhang']&lt;/li&gt;&lt;li&gt;Tags: ['backdoor attack', 'physical trigger', 'vision-language-action models', 'security', 'adversarial training']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.09269</guid><pubDate>Mon, 13 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Defense against Unauthorized Distillation in Image Restoration via Feature Space Perturbation</title><link>https://arxiv.org/abs/2510.08925</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes ASVP for image restoration models to defend against KD attacks&lt;/li&gt;&lt;li&gt;Uses SVD-based perturbations on feature maps&lt;/li&gt;&lt;li&gt;Evaluates across multiple restoration tasks with minimal teacher impact&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Han Hu', 'Zhuoran Zheng', 'Chen Lyu']&lt;/li&gt;&lt;li&gt;Tags: ['model extraction', 'adversarial attacks', 'image restoration', 'knowledge distillation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.08925</guid><pubDate>Mon, 13 Oct 2025 00:00:00 +0000</pubDate></item><item><title>SAFER-AiD: Saccade-Assisted Foveal-peripheral vision Enhanced Reconstruction for Adversarial Defense</title><link>https://arxiv.org/abs/2510.08761</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a biologically inspired defense framework against adversarial attacks&lt;/li&gt;&lt;li&gt;Uses saccadic eye movements and foveal-peripheral processing&lt;/li&gt;&lt;li&gt;Requires no retraining of downstream classifiers&lt;/li&gt;&lt;li&gt;Improves robustness across multiple classifiers and attack types&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jiayang Liu', 'Daniel Tso', 'Yiming Bu', 'Qinru Qiu']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial defense', 'biological mechanisms', 'saccadic eye movements', 'foveal-peripheral processing', 'reinforcement learning']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.08761</guid><pubDate>Mon, 13 Oct 2025 00:00:00 +0000</pubDate></item><item><title>P2P: A Poison-to-Poison Remedy for Reliable Backdoor Defense in LLMs</title><link>https://arxiv.org/abs/2510.04503</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes P2P, a backdoor defense algorithm for LLMs using benign triggers&lt;/li&gt;&lt;li&gt;Injects safe alternative labels into training samples via prompt-based learning&lt;/li&gt;&lt;li&gt;Effective across multiple tasks and attack types&lt;/li&gt;&lt;li&gt;Reduces attack success rate while preserving performance&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Shuai Zhao', 'Xinyi Wu', 'Shiqian Zhao', 'Xiaobao Wu', 'Zhongliang Guo', 'Yanhao Jia', 'Anh Tuan Luu']&lt;/li&gt;&lt;li&gt;Tags: ['data poisoning', 'backdoor defense', 'LLM security', 'prompt-based learning', 'adversarial training']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.04503</guid><pubDate>Mon, 13 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Tuning without Peeking: Provable Privacy and Generalization Bounds for LLM Post-Training</title><link>https://arxiv.org/abs/2507.01752</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces BBoxER, a black-box optimization method for LLM post-training&lt;/li&gt;&lt;li&gt;Provides theoretical guarantees for differential privacy, robustness to data poisoning and extraction attacks&lt;/li&gt;&lt;li&gt;Empirical results show improved performance and robustness to membership inference attacks&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Ismail Labiad', 'Mathurin Videau', 'Matthieu Kowalski', 'Marc Schoenauer', 'Alessandro Leite', 'Julia Kempe', 'Olivier Teytaud']&lt;/li&gt;&lt;li&gt;Tags: ['privacy attacks', 'data poisoning', 'model extraction', 'differential privacy', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2507.01752</guid><pubDate>Mon, 13 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Do Internal Layers of LLMs Reveal Patterns for Jailbreak Detection?</title><link>https://arxiv.org/abs/2510.06594</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;The paper explores detecting jailbreak prompts by analyzing internal layers of LLMs.&lt;/li&gt;&lt;li&gt;It focuses on GPT-J and Mamba2 models.&lt;/li&gt;&lt;li&gt;Preliminary results show distinct layer-wise behaviors between jailbreak and benign prompts.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Sri Durga Sai Sowmya Kadali', 'Evangelos E. Papalexakis']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'jailbreaking', 'prompt injection', 'adversarial prompting', 'safety evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.06594</guid><pubDate>Mon, 13 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Machine Learning for Detection and Analysis of Novel LLM Jailbreaks</title><link>https://arxiv.org/abs/2510.01644</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Analyzes ML models for detecting LLM jailbreak prompts&lt;/li&gt;&lt;li&gt;Evaluates BERT fine-tuning for identifying jailbreaks&lt;/li&gt;&lt;li&gt;Visualizes keywords distinguishing jailbreak vs genuine prompts&lt;/li&gt;&lt;li&gt;Suggests reflexivity in prompts as a jailbreak signal&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['John Hawkins', 'Aditya Pramar', 'Rodney Beard', 'Rohitash Chandra']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'jailbreaking', 'prompt injection', 'adversarial prompting', 'safety evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.01644</guid><pubDate>Mon, 13 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Beyond I'm Sorry, I Can't: Dissecting Large Language Model Refusal</title><link>https://arxiv.org/abs/2509.09708</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;The paper studies the internal mechanisms of LLM refusal behavior using sparse autoencoders.&lt;/li&gt;&lt;li&gt;It identifies features in the latent space that can be manipulated to create jailbreaks.&lt;/li&gt;&lt;li&gt;The method involves three stages: finding refusal direction, filtering features, and discovering interactions.&lt;/li&gt;&lt;li&gt;The findings suggest redundant features that become active when others are suppressed.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Nirmalendu Prakash', 'Yeo Wei Jie', 'Amir Abdullah', 'Ranjan Satapathy', 'Erik Cambria', 'Roy Ka Wei Lee']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'jailbreaking', 'prompt injection', 'safety evaluation', 'model extraction']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.09708</guid><pubDate>Mon, 13 Oct 2025 00:00:00 +0000</pubDate></item><item><title>What Are They Filtering Out? An Experimental Benchmark of Filtering Strategies for Harm Reduction in Pretraining Datasets</title><link>https://arxiv.org/abs/2503.05721</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmark study of data filtering strategies for harm reduction in LLM pretraining datasets&lt;/li&gt;&lt;li&gt;Evaluates impact on vulnerable groups and underrepresentation&lt;/li&gt;&lt;li&gt;Analyzes 55 technical reports to identify existing filtering strategies&lt;/li&gt;&lt;li&gt;Finds that filtering reduces harmful content but increases underrepresentation&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Marco Antonio Stranisci', 'Christian Hardmeier']&lt;/li&gt;&lt;li&gt;Tags: ['safety', 'data poisoning', 'alignment', 'robustness', 'safety evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2503.05721</guid><pubDate>Mon, 13 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Improbable Bigrams Expose Vulnerabilities of Incomplete Tokens in Byte-Level Tokenizers</title><link>https://arxiv.org/abs/2410.23684</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Investigates vulnerabilities in byte-level BPE tokenizers due to incomplete tokens&lt;/li&gt;&lt;li&gt;Introduces improbable bigrams to demonstrate fragility of incomplete tokens&lt;/li&gt;&lt;li&gt;Shows significant hallucination when using these bigrams&lt;/li&gt;&lt;li&gt;Highlights potential security issues with tokenization&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Eugene Jang', 'Kimin Lee', 'Jin-Woo Chung', 'Keuntae Park', 'Seungwon Shin']&lt;/li&gt;&lt;li&gt;Tags: ['tokenization', 'hallucination', 'adversarial prompting', 'security']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2410.23684</guid><pubDate>Mon, 13 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Exploiting Web Search Tools of AI Agents for Data Exfiltration</title><link>https://arxiv.org/abs/2510.09093</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;The paper evaluates indirect prompt injection attacks on LLMs using web search tools.&lt;/li&gt;&lt;li&gt;It analyzes model vulnerabilities based on size, manufacturer, and specific implementations.&lt;/li&gt;&lt;li&gt;The study highlights persistent weaknesses and suggests security improvements.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Dennis Rall', 'Bernhard Bauer', 'Mohit Mittal', 'Thomas Fraunholz']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'prompt injection', 'adversarial prompting', 'security evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.09093</guid><pubDate>Mon, 13 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Energy-Driven Steering: Reducing False Refusals in Large Language Models</title><link>https://arxiv.org/abs/2510.08646</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces Energy-Driven Steering (EDS) to reduce false refusals in LLMs&lt;/li&gt;&lt;li&gt;Uses an Energy-Based Model (EBM) to dynamically steer LLM responses&lt;/li&gt;&lt;li&gt;Aims to balance safety and helpfulness without modifying model weights&lt;/li&gt;&lt;li&gt;Shows significant improvement in compliance on ORB-H benchmark&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Eric Hanchen Jiang', 'Weixuan Ou', 'Run Liu', 'Shengyuan Pang', 'Guancheng Wan', 'Ranjie Duan', 'Wei Dong', 'Kai-Wei Chang', 'XiaoFeng Wang', 'Ying Nian Wu', 'Xinfeng Li']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'alignment', 'safety evaluation', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.08646</guid><pubDate>Mon, 13 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Getting Your Indices in a Row: Full-Text Search for LLM Training Data for Real World</title><link>https://arxiv.org/abs/2510.09471</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Presents a full-text indexing pipeline for LLM training data using Elasticsearch and arm64 infrastructure&lt;/li&gt;&lt;li&gt;Indexes 8.6T tokens from Apertus LLM training data&lt;/li&gt;&lt;li&gt;Aims to create a safety tool for jailbreak-agnostic LLM safety&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Ines Altemir Marinas', 'Anastasiia Kucherenko', 'Alexander Sternfeld', 'Andrei Kucharavy']&lt;/li&gt;&lt;li&gt;Tags: ['safety', 'data indexing', 'jailbreaking', 'LLM training data']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.09471</guid><pubDate>Mon, 13 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Detecting Data Contamination from Reinforcement Learning Post-training for Large Language Models</title><link>https://arxiv.org/abs/2510.09259</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces Self-Critique method for detecting data contamination in RL post-training phase of LLMs&lt;/li&gt;&lt;li&gt;Creates RL-MIA benchmark for evaluating contamination detection in RL scenario&lt;/li&gt;&lt;li&gt;Shows significant AUC improvement over baselines in detecting RL-phase contamination&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yongding Tao', 'Tian Wang', 'Yihong Dong', 'Huanyu Liu', 'Kechi Zhang', 'Xiaolong Hu', 'Ge Li']&lt;/li&gt;&lt;li&gt;Tags: ['data poisoning', 'security', 'robustness', 'evaluation', 'benchmarking']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.09259</guid><pubDate>Mon, 13 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Decoupling Safety into Orthogonal Subspace: Cost-Efficient and Performance-Preserving Alignment for Large Language Models</title><link>https://arxiv.org/abs/2510.09004</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces a method for cost-efficient safety alignment using LoRA-based Refusal-training&lt;/li&gt;&lt;li&gt;Shows that training on safety data alone can preserve general performance&lt;/li&gt;&lt;li&gt;Provides theoretical and experimental evidence for decoupling safety into an orthogonal subspace&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yutao Mou', 'Xiaoling Zhou', 'Yuxiao Luo', 'Shikun Zhang', 'Wei Ye']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'safety', 'LoRA', 'performance preservation', 'orthogonal subspace']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.09004</guid><pubDate>Mon, 13 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Pattern Enhanced Multi-Turn Jailbreaking: Exploiting Structural Vulnerabilities in Large Language Models</title><link>https://arxiv.org/abs/2510.08859</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces PE-CoA, a framework using five conversation patterns for multi-turn jailbreaking attacks on LLMs.&lt;/li&gt;&lt;li&gt;Evaluates the framework on twelve LLMs across ten harm categories, achieving state-of-the-art performance.&lt;/li&gt;&lt;li&gt;Reveals that models have distinct vulnerability profiles based on conversational patterns.&lt;/li&gt;&lt;li&gt;Highlights the need for pattern-aware defenses in LLM safety.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Ragib Amin Nihal', 'Rui Wen', 'Kazuhiro Nakadai', 'Jun Sakuma']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'jailbreaking', 'adversarial prompting', 'safety evaluation', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.08859</guid><pubDate>Mon, 13 Oct 2025 00:00:00 +0000</pubDate></item><item><title>The Model's Language Matters: A Comparative Privacy Analysis of LLMs</title><link>https://arxiv.org/abs/2510.08813</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Compares privacy leakage in LLMs across different languages (English, Spanish, French, Italian)&lt;/li&gt;&lt;li&gt;Evaluates three attack vectors: extraction, counterfactual memorization, and membership inference&lt;/li&gt;&lt;li&gt;Finds that linguistic redundancy and tokenization affect privacy vulnerability&lt;/li&gt;&lt;li&gt;Italian shows highest leakage, English higher membership separability, French/Spanish more resilient&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Abhishek K. Mishra', 'Antoine Boutet', 'Lucas Magnana']&lt;/li&gt;&lt;li&gt;Tags: ['privacy attacks', 'data poisoning', 'model extraction', 'membership inference', 'linguistic analysis']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.08813</guid><pubDate>Mon, 13 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Do LLMs Know They Are Being Tested? Evaluation Awareness and Incentive-Sensitive Failures in GPT-OSS-20B</title><link>https://arxiv.org/abs/2510.08624</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;The paper investigates how evaluation prompts affect LLM performance compared to real-world prompts.&lt;/li&gt;&lt;li&gt;It tests various scenarios including math, code fixes, citations, incentives, reasoning depth, and multilingual support.&lt;/li&gt;&lt;li&gt;Key findings show evaluation prompts lead to longer CoT but lower compliance and inconsistent accuracy.&lt;/li&gt;&lt;li&gt;Provides a framework for more accurate benchmarking and practical recommendations.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Nisar Ahmed', 'Muhammad Imran Zaman', 'Gulshan Saleem', 'Ali Hassan']&lt;/li&gt;&lt;li&gt;Tags: ['LLM evaluation', 'benchmarking', 'prompt engineering', 'safety evaluation', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.08624</guid><pubDate>Mon, 13 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Toward a Safer Web: Multilingual Multi-Agent LLMs for Mitigating Adversarial Misinformation Attacks</title><link>https://arxiv.org/abs/2510.08605</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;The paper addresses adversarial attacks in misinformation detection, focusing on language-switching, query length inflation, and structural reformatting.&lt;/li&gt;&lt;li&gt;It introduces a multilingual, multi-agent LLM framework with retrieval-augmented generation.&lt;/li&gt;&lt;li&gt;The framework is designed to be deployed as a web plugin for online platforms.&lt;/li&gt;&lt;li&gt;The work highlights the importance of AI-driven detection against diverse attacks.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Nouar Aldahoul', 'Yasir Zaki']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial attacks', 'misinformation', 'multilingual', 'multi-agent', 'retrieval-augmented']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.08605</guid><pubDate>Mon, 13 Oct 2025 00:00:00 +0000</pubDate></item><item><title>LatentBreak: Jailbreaking Large Language Models through Latent Space Feedback</title><link>https://arxiv.org/abs/2510.08604</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes LatentBreak, a white-box jailbreak attack for LLMs&lt;/li&gt;&lt;li&gt;Uses latent space feedback to generate natural adversarial prompts&lt;/li&gt;&lt;li&gt;Aims to evade perplexity-based filters&lt;/li&gt;&lt;li&gt;Evaluates against safety-aligned models&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Raffaele Mura', 'Giorgio Piras', 'Kamil\\.e Luko\\v{s}i\\=ut\\.e', 'Maura Pintor', 'Amin Karbasi', 'Battista Biggio']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'jailbreaking', 'adversarial prompting', 'safety evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.08604</guid><pubDate>Mon, 13 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Less Diverse, Less Safe: The Indirect But Pervasive Risk of Test-Time Scaling in Large Language Models</title><link>https://arxiv.org/abs/2510.08592</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Test-Time Scaling (TTS) in LLMs can produce unsafe outputs when candidate diversity is reduced.&lt;/li&gt;&lt;li&gt;RefDiv protocol is introduced to stress test TTS by reducing diversity.&lt;/li&gt;&lt;li&gt;Experiments show that reduced diversity leads to unsafe outputs across multiple models and TTS strategies.&lt;/li&gt;&lt;li&gt;Existing safety classifiers fail to detect RefDiv-generated adversarial prompts.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Shahriar Kabir Nahin', 'Hadi Askari', 'Muhao Chen', 'Anshuman Chhabra']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'adversarial prompting', 'safety evaluation', 'robustness', 'model extraction']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.08592</guid><pubDate>Mon, 13 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Fewer Weights, More Problems: A Practical Attack on LLM Pruning</title><link>https://arxiv.org/abs/2510.07985</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces a new attack on LLM pruning where an adversary can inject malicious behavior that only manifests after pruning&lt;/li&gt;&lt;li&gt;Uses a proxy metric to identify parameters likely to be pruned and injects malicious behavior into those that are not&lt;/li&gt;&lt;li&gt;Demonstrates high success rates in jailbreak, instruction refusal, and content injection attacks after pruning&lt;/li&gt;&lt;li&gt;Highlights security risks in model compression and pruning methods&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Kazuki Egashira', 'Robin Staab', 'Thibaud Gloaguen', 'Mark Vero', 'Martin Vechev']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'model extraction', 'adversarial prompting', 'security standards', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.07985</guid><pubDate>Mon, 13 Oct 2025 00:00:00 +0000</pubDate></item><item><title>MLLMEraser: Achieving Test-Time Unlearning in Multimodal Large Language Models through Activation Steering</title><link>https://arxiv.org/abs/2510.04217</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes MLLMEraser for test-time unlearning in MLLMs&lt;/li&gt;&lt;li&gt;Uses activation steering with multimodal erasure direction&lt;/li&gt;&lt;li&gt;Adaptive input-aware steering to balance forgetting and utility&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Chenlu Ding', 'Jiancan Wu', 'Leheng Sheng', 'Fan Zhang', 'Yancheng Yuan', 'Xiang Wang', 'Xiangnan He']&lt;/li&gt;&lt;li&gt;Tags: ['unlearning', 'activation steering', 'multimodal', 'privacy', 'safety']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.04217</guid><pubDate>Mon, 13 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Tuning without Peeking: Provable Privacy and Generalization Bounds for LLM Post-Training</title><link>https://arxiv.org/abs/2507.01752</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces BBoxER, a black-box optimization method for LLM post-training&lt;/li&gt;&lt;li&gt;Provides theoretical guarantees for differential privacy, robustness to data poisoning and extraction attacks&lt;/li&gt;&lt;li&gt;Empirical results show improved performance and robustness to membership inference attacks&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Ismail Labiad', 'Mathurin Videau', 'Matthieu Kowalski', 'Marc Schoenauer', 'Alessandro Leite', 'Julia Kempe', 'Olivier Teytaud']&lt;/li&gt;&lt;li&gt;Tags: ['privacy attacks', 'data poisoning', 'model extraction', 'differential privacy', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2507.01752</guid><pubDate>Mon, 13 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Robustness in Both Domains: CLIP Needs a Robust Text Encoder</title><link>https://arxiv.org/abs/2506.03355</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;The paper addresses adversarial robustness in CLIP's text encoder.&lt;/li&gt;&lt;li&gt;Proposes LEAF, an adversarial finetuning method for text encoders.&lt;/li&gt;&lt;li&gt;Improves zero-shot adversarial accuracy and multimodal retrieval under noise.&lt;/li&gt;&lt;li&gt;Facilitates better text reconstruction from embeddings.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Elias Abad Rocamora', 'Christian Schlarmann', 'Naman Deep Singh', 'Yongtao Wu', 'Matthias Hein', 'Volkan Cevher']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial robustness', 'text encoder', 'CLIP', 'multimodal', 'adversarial attacks']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.03355</guid><pubDate>Mon, 13 Oct 2025 00:00:00 +0000</pubDate></item><item><title>A unified Bayesian framework for adversarial robustness</title><link>https://arxiv.org/abs/2510.09288</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces a Bayesian framework for adversarial robustness&lt;/li&gt;&lt;li&gt;Models adversarial uncertainty through a stochastic channel&lt;/li&gt;&lt;li&gt;Proposes proactive and reactive defense strategies&lt;/li&gt;&lt;li&gt;Empirically validates the approach&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Pablo G. Arce', 'Roi Naveiro', "David R\\'ios Insua"]&lt;/li&gt;&lt;li&gt;Tags: ['adversarial robustness', 'Bayesian methods', 'adversarial training', 'adversarial purification']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.09288</guid><pubDate>Mon, 13 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Goal-oriented Backdoor Attack against Vision-Language-Action Models via Physical Objects</title><link>https://arxiv.org/abs/2510.09269</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces goal-oriented backdoor attacks (GoBA) against VLA models using physical triggers&lt;/li&gt;&lt;li&gt;Proposes BadLIBERO dataset with physical triggers and backdoor actions&lt;/li&gt;&lt;li&gt;Evaluates attack success and performance impact on clean inputs&lt;/li&gt;&lt;li&gt;Analyzes factors affecting attack performance like trigger color and action trajectory&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Zirun Zhou', 'Zhengyang Xiao', 'Haochuan Xu', 'Jing Sun', 'Di Wang', 'Jingfeng Zhang']&lt;/li&gt;&lt;li&gt;Tags: ['backdoor attack', 'physical trigger', 'vision-language-action models', 'security', 'adversarial training']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.09269</guid><pubDate>Mon, 13 Oct 2025 00:00:00 +0000</pubDate></item><item><title>GREAT: Generalizable Backdoor Attacks in RLHF via Emotion-Aware Trigger Synthesis</title><link>https://arxiv.org/abs/2510.09260</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces GREAT, a framework for emotion-aware trigger synthesis in RLHF backdoor attacks&lt;/li&gt;&lt;li&gt;Uses Erinyes dataset of 5000 angry triggers generated by GPT-4.1&lt;/li&gt;&lt;li&gt;Employs PCA and clustering in latent space for trigger identification&lt;/li&gt;&lt;li&gt;Outperforms baselines in attack success rates while maintaining response quality&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Subrat Kishore Dutta', 'Yuelin Xu', 'Piyush Pant', 'Xiao Zhang']&lt;/li&gt;&lt;li&gt;Tags: ['backdoor attacks', 'RLHF', 'trigger synthesis', 'data poisoning', 'emotional triggers']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.09260</guid><pubDate>Mon, 13 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Detecting Data Contamination from Reinforcement Learning Post-training for Large Language Models</title><link>https://arxiv.org/abs/2510.09259</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces Self-Critique method for detecting data contamination in RL post-training phase of LLMs&lt;/li&gt;&lt;li&gt;Creates RL-MIA benchmark for evaluating contamination detection in RL scenario&lt;/li&gt;&lt;li&gt;Shows significant AUC improvement over baselines in detecting RL-phase contamination&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yongding Tao', 'Tian Wang', 'Yihong Dong', 'Huanyu Liu', 'Kechi Zhang', 'Xiaolong Hu', 'Ge Li']&lt;/li&gt;&lt;li&gt;Tags: ['data poisoning', 'security', 'robustness', 'evaluation', 'benchmarking']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.09259</guid><pubDate>Mon, 13 Oct 2025 00:00:00 +0000</pubDate></item><item><title>RADAR: Mechanistic Pathways for Detecting Data Contamination in LLM Evaluation</title><link>https://arxiv.org/abs/2510.08931</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces RADAR framework for detecting data contamination in LLM evaluation&lt;/li&gt;&lt;li&gt;Uses mechanistic interpretability to distinguish recall vs reasoning&lt;/li&gt;&lt;li&gt;Extracts 37 features including attention and activation patterns&lt;/li&gt;&lt;li&gt;Achieves 93% accuracy on diverse evaluation set&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Ashish Kattamuri', 'Harshwardhan Fartale', 'Arpita Vats', 'Rahul Raja', 'Ishita Prasad']&lt;/li&gt;&lt;li&gt;Tags: ['safety evaluation', 'robustness', 'model evaluation', 'data poisoning']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.08931</guid><pubDate>Mon, 13 Oct 2025 00:00:00 +0000</pubDate></item><item><title>GTAlign: Game-Theoretic Alignment of LLM Assistants for Mutual Welfare</title><link>https://arxiv.org/abs/2510.08872</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes Game-Theoretic Alignment (GTAlign) framework for LLMs&lt;/li&gt;&lt;li&gt;Integrates game theory into reasoning and training&lt;/li&gt;&lt;li&gt;Aims to improve mutual welfare between LLM and user&lt;/li&gt;&lt;li&gt;Includes mutual welfare reward during training&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Siqi Zhu', 'David Zhang', 'Pedro Cisneros-Velarde', 'Jiaxuan You']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'game theory', 'mutual welfare', 'reasoning efficiency']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.08872</guid><pubDate>Mon, 13 Oct 2025 00:00:00 +0000</pubDate></item><item><title>CommandSans: Securing AI Agents with Surgical Precision Prompt Sanitization</title><link>https://arxiv.org/abs/2510.08829</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces CommandSans, a token-level prompt sanitization method for LLM agents&lt;/li&gt;&lt;li&gt;Aims to prevent prompt injection attacks by removing executable instructions from tool outputs&lt;/li&gt;&lt;li&gt;Shows significant reduction in attack success rates across multiple benchmarks&lt;/li&gt;&lt;li&gt;Non-blocking approach that doesn't require context-specific calibration&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Debeshee Das', 'Luca Beurer-Kellner', 'Marc Fischer', 'Maximilian Baader']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'prompt injection', 'adversarial prompting', 'safety evaluation', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.08829</guid><pubDate>Mon, 13 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Toward a Safer Web: Multilingual Multi-Agent LLMs for Mitigating Adversarial Misinformation Attacks</title><link>https://arxiv.org/abs/2510.08605</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;The paper addresses adversarial attacks in misinformation detection, focusing on language-switching, query length inflation, and structural reformatting.&lt;/li&gt;&lt;li&gt;It introduces a multilingual, multi-agent LLM framework with retrieval-augmented generation.&lt;/li&gt;&lt;li&gt;The framework is designed to be deployed as a web plugin for online platforms.&lt;/li&gt;&lt;li&gt;The work highlights the importance of AI-driven detection against diverse attacks.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Nouar Aldahoul', 'Yasir Zaki']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial attacks', 'misinformation', 'multilingual', 'multi-agent', 'retrieval-augmented']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.08605</guid><pubDate>Mon, 13 Oct 2025 00:00:00 +0000</pubDate></item><item><title>LatentBreak: Jailbreaking Large Language Models through Latent Space Feedback</title><link>https://arxiv.org/abs/2510.08604</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes LatentBreak, a white-box jailbreak attack for LLMs&lt;/li&gt;&lt;li&gt;Uses latent space feedback to generate natural adversarial prompts&lt;/li&gt;&lt;li&gt;Aims to evade perplexity-based filters&lt;/li&gt;&lt;li&gt;Evaluates against safety-aligned models&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Raffaele Mura', 'Giorgio Piras', 'Kamil\\.e Luko\\v{s}i\\=ut\\.e', 'Maura Pintor', 'Amin Karbasi', 'Battista Biggio']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'jailbreaking', 'adversarial prompting', 'safety evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.08604</guid><pubDate>Mon, 13 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Less Diverse, Less Safe: The Indirect But Pervasive Risk of Test-Time Scaling in Large Language Models</title><link>https://arxiv.org/abs/2510.08592</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Test-Time Scaling (TTS) in LLMs can produce unsafe outputs when candidate diversity is reduced.&lt;/li&gt;&lt;li&gt;RefDiv protocol is introduced to stress test TTS by reducing diversity.&lt;/li&gt;&lt;li&gt;Experiments show that reduced diversity leads to unsafe outputs across multiple models and TTS strategies.&lt;/li&gt;&lt;li&gt;Existing safety classifiers fail to detect RefDiv-generated adversarial prompts.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Shahriar Kabir Nahin', 'Hadi Askari', 'Muhao Chen', 'Anshuman Chhabra']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'adversarial prompting', 'safety evaluation', 'robustness', 'model extraction']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.08592</guid><pubDate>Mon, 13 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Adaptive Attacks on Trusted Monitors Subvert AI Control Protocols</title><link>https://arxiv.org/abs/2510.09462</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;The paper studies adaptive attacks on AI control protocols that use LLM monitors.&lt;/li&gt;&lt;li&gt;It demonstrates how an untrusted model can use prompt injection to evade monitors.&lt;/li&gt;&lt;li&gt;The attack is effective against current protocols and even amplifies with Defer-to-Resample.&lt;/li&gt;&lt;li&gt;The authors call for standard evaluation of adaptive attacks in future control mechanisms.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Mikhail Terekhov', 'Alexander Panfilov', 'Daniil Dzenhaliou', 'Caglar Gulcehre', 'Maksym Andriushchenko', 'Ameya Prabhu', 'Jonas Geiping']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'prompt injection', 'adversarial prompting', 'AI control protocols', 'monitor evasion']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.09462</guid><pubDate>Mon, 13 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Safety Game: Balancing Safe and Informative Conversations with Blackbox Agentic AI using LP Solvers</title><link>https://arxiv.org/abs/2510.09330</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a black-box framework for safety alignment using linear programming solvers&lt;/li&gt;&lt;li&gt;Formulates safety-informative tradeoff as a two-player zero-sum game&lt;/li&gt;&lt;li&gt;Aims to balance safe but uninformative vs helpful but risky responses&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Tuan Nguyen', 'Long Tran-Thanh']&lt;/li&gt;&lt;li&gt;Tags: ['safety', 'alignment', 'blackbox', 'game theory', 'linear programming']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.09330</guid><pubDate>Mon, 13 Oct 2025 00:00:00 +0000</pubDate></item><item><title>The Attacker Moves Second: Stronger Adaptive Attacks Bypass Defenses Against Llm Jailbreaks and Prompt Injections</title><link>https://arxiv.org/abs/2510.09023</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;The paper evaluates the robustness of defenses against LLM jailbreaks and prompt injections using adaptive attacks.&lt;/li&gt;&lt;li&gt;It shows that current defenses fail against stronger adaptive attacks with success rates above 90%.&lt;/li&gt;&lt;li&gt;The authors argue for evaluating defenses against more sophisticated attackers.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Milad Nasr', 'Nicholas Carlini', 'Chawin Sitawarin', 'Sander V. Schulhoff', 'Jamie Hayes', 'Michael Ilie', 'Juliette Pluto', 'Shuang Song', 'Harsh Chaudhari', 'Ilia Shumailov', 'Abhradeep Thakurta', 'Kai Yuanqing Xiao', 'Andreas Terzis', 'Florian Tram\\`er']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'jailbreaking', 'prompt injection', 'adversarial prompting', 'robustness', 'safety evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.09023</guid><pubDate>Mon, 13 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Energy-Driven Steering: Reducing False Refusals in Large Language Models</title><link>https://arxiv.org/abs/2510.08646</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces Energy-Driven Steering (EDS) to reduce false refusals in LLMs&lt;/li&gt;&lt;li&gt;Uses an Energy-Based Model (EBM) to dynamically steer LLM responses&lt;/li&gt;&lt;li&gt;Aims to balance safety and helpfulness without modifying model weights&lt;/li&gt;&lt;li&gt;Shows significant improvement in compliance on ORB-H benchmark&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Eric Hanchen Jiang', 'Weixuan Ou', 'Run Liu', 'Shengyuan Pang', 'Guancheng Wan', 'Ranjie Duan', 'Wei Dong', 'Kai-Wei Chang', 'XiaoFeng Wang', 'Ying Nian Wu', 'Xinfeng Li']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'alignment', 'safety evaluation', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.08646</guid><pubDate>Mon, 13 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Fewer Weights, More Problems: A Practical Attack on LLM Pruning</title><link>https://arxiv.org/abs/2510.07985</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces a new attack on LLM pruning where an adversary can inject malicious behavior that only manifests after pruning&lt;/li&gt;&lt;li&gt;Uses a proxy metric to identify parameters likely to be pruned and injects malicious behavior into those that are not&lt;/li&gt;&lt;li&gt;Demonstrates high success rates in jailbreak, instruction refusal, and content injection attacks after pruning&lt;/li&gt;&lt;li&gt;Highlights security risks in model compression and pruning methods&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Kazuki Egashira', 'Robin Staab', 'Thibaud Gloaguen', 'Mark Vero', 'Martin Vechev']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'model extraction', 'adversarial prompting', 'security standards', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.07985</guid><pubDate>Mon, 13 Oct 2025 00:00:00 +0000</pubDate></item><item><title>P2P: A Poison-to-Poison Remedy for Reliable Backdoor Defense in LLMs</title><link>https://arxiv.org/abs/2510.04503</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes P2P, a backdoor defense algorithm for LLMs using benign triggers&lt;/li&gt;&lt;li&gt;Injects safe alternative labels into training samples via prompt-based learning&lt;/li&gt;&lt;li&gt;Effective across multiple tasks and attack types&lt;/li&gt;&lt;li&gt;Reduces attack success rate while preserving performance&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Shuai Zhao', 'Xinyi Wu', 'Shiqian Zhao', 'Xiaobao Wu', 'Zhongliang Guo', 'Yanhao Jia', 'Anh Tuan Luu']&lt;/li&gt;&lt;li&gt;Tags: ['data poisoning', 'backdoor defense', 'LLM security', 'prompt-based learning', 'adversarial training']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.04503</guid><pubDate>Mon, 13 Oct 2025 00:00:00 +0000</pubDate></item><item><title>MLLMEraser: Achieving Test-Time Unlearning in Multimodal Large Language Models through Activation Steering</title><link>https://arxiv.org/abs/2510.04217</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes MLLMEraser for test-time unlearning in MLLMs&lt;/li&gt;&lt;li&gt;Uses activation steering with multimodal erasure direction&lt;/li&gt;&lt;li&gt;Adaptive input-aware steering to balance forgetting and utility&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Chenlu Ding', 'Jiancan Wu', 'Leheng Sheng', 'Fan Zhang', 'Yancheng Yuan', 'Xiang Wang', 'Xiangnan He']&lt;/li&gt;&lt;li&gt;Tags: ['unlearning', 'activation steering', 'multimodal', 'privacy', 'safety']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.04217</guid><pubDate>Mon, 13 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Machine Learning for Detection and Analysis of Novel LLM Jailbreaks</title><link>https://arxiv.org/abs/2510.01644</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Analyzes ML models for detecting LLM jailbreak prompts&lt;/li&gt;&lt;li&gt;Evaluates BERT fine-tuning for identifying jailbreaks&lt;/li&gt;&lt;li&gt;Visualizes keywords distinguishing jailbreak vs genuine prompts&lt;/li&gt;&lt;li&gt;Suggests reflexivity in prompts as a jailbreak signal&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['John Hawkins', 'Aditya Pramar', 'Rodney Beard', 'Rohitash Chandra']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'jailbreaking', 'prompt injection', 'adversarial prompting', 'safety evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.01644</guid><pubDate>Mon, 13 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Benchmarking and Mitigate Sycophancy in Medical Vision-Language Models</title><link>https://arxiv.org/abs/2509.21979</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Evaluates sycophancy in medical VLMs using a new benchmark&lt;/li&gt;&lt;li&gt;Tests adversarial responses with psychological pressure templates&lt;/li&gt;&lt;li&gt;Proposes VIPER mitigation strategy to filter non-evidentiary content&lt;/li&gt;&lt;li&gt;Reduces sycophancy while maintaining accuracy&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Zikun Guo', 'Xinyue Xu', 'Pei Xiang', 'Shu Yang', 'Xin Han', 'Di Wang', 'Lijie Hu']&lt;/li&gt;&lt;li&gt;Tags: ['red teaming', 'adversarial prompting', 'safety evaluation', 'alignment', 'medical AI']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.21979</guid><pubDate>Mon, 13 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Beyond I'm Sorry, I Can't: Dissecting Large Language Model Refusal</title><link>https://arxiv.org/abs/2509.09708</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;The paper studies the internal mechanisms of LLM refusal behavior using sparse autoencoders.&lt;/li&gt;&lt;li&gt;It identifies features in the latent space that can be manipulated to create jailbreaks.&lt;/li&gt;&lt;li&gt;The method involves three stages: finding refusal direction, filtering features, and discovering interactions.&lt;/li&gt;&lt;li&gt;The findings suggest redundant features that become active when others are suppressed.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Nirmalendu Prakash', 'Yeo Wei Jie', 'Amir Abdullah', 'Ranjan Satapathy', 'Erik Cambria', 'Roy Ka Wei Lee']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'jailbreaking', 'prompt injection', 'safety evaluation', 'model extraction']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.09708</guid><pubDate>Mon, 13 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Tuning without Peeking: Provable Privacy and Generalization Bounds for LLM Post-Training</title><link>https://arxiv.org/abs/2507.01752</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces BBoxER, a black-box optimization method for LLM post-training&lt;/li&gt;&lt;li&gt;Provides theoretical guarantees for differential privacy, robustness to data poisoning and extraction attacks&lt;/li&gt;&lt;li&gt;Empirical results show improved performance and robustness to membership inference attacks&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Ismail Labiad', 'Mathurin Videau', 'Matthieu Kowalski', 'Marc Schoenauer', 'Alessandro Leite', 'Julia Kempe', 'Olivier Teytaud']&lt;/li&gt;&lt;li&gt;Tags: ['privacy attacks', 'data poisoning', 'model extraction', 'differential privacy', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2507.01752</guid><pubDate>Mon, 13 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Robustness in Both Domains: CLIP Needs a Robust Text Encoder</title><link>https://arxiv.org/abs/2506.03355</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;The paper addresses adversarial robustness in CLIP's text encoder.&lt;/li&gt;&lt;li&gt;Proposes LEAF, an adversarial finetuning method for text encoders.&lt;/li&gt;&lt;li&gt;Improves zero-shot adversarial accuracy and multimodal retrieval under noise.&lt;/li&gt;&lt;li&gt;Facilitates better text reconstruction from embeddings.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Elias Abad Rocamora', 'Christian Schlarmann', 'Naman Deep Singh', 'Yongtao Wu', 'Matthias Hein', 'Volkan Cevher']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial robustness', 'text encoder', 'CLIP', 'multimodal', 'adversarial attacks']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.03355</guid><pubDate>Mon, 13 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Adaptive Attacks on Trusted Monitors Subvert AI Control Protocols</title><link>https://arxiv.org/abs/2510.09462</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;The paper studies adaptive attacks on AI control protocols that use LLM monitors.&lt;/li&gt;&lt;li&gt;It demonstrates how an untrusted model can use prompt injection to evade monitors.&lt;/li&gt;&lt;li&gt;The attack is effective against current protocols and even amplifies with Defer-to-Resample.&lt;/li&gt;&lt;li&gt;The authors call for standard evaluation of adaptive attacks in future control mechanisms.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Mikhail Terekhov', 'Alexander Panfilov', 'Daniil Dzenhaliou', 'Caglar Gulcehre', 'Maksym Andriushchenko', 'Ameya Prabhu', 'Jonas Geiping']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'prompt injection', 'adversarial prompting', 'AI control protocols', 'monitor evasion']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.09462</guid><pubDate>Mon, 13 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Detecting Data Contamination from Reinforcement Learning Post-training for Large Language Models</title><link>https://arxiv.org/abs/2510.09259</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces Self-Critique method for detecting data contamination in RL post-training phase of LLMs&lt;/li&gt;&lt;li&gt;Creates RL-MIA benchmark for evaluating contamination detection in RL scenario&lt;/li&gt;&lt;li&gt;Shows significant AUC improvement over baselines in detecting RL-phase contamination&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yongding Tao', 'Tian Wang', 'Yihong Dong', 'Huanyu Liu', 'Kechi Zhang', 'Xiaolong Hu', 'Ge Li']&lt;/li&gt;&lt;li&gt;Tags: ['data poisoning', 'security', 'robustness', 'evaluation', 'benchmarking']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.09259</guid><pubDate>Mon, 13 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Pattern Enhanced Multi-Turn Jailbreaking: Exploiting Structural Vulnerabilities in Large Language Models</title><link>https://arxiv.org/abs/2510.08859</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces PE-CoA, a framework using five conversation patterns for multi-turn jailbreaking attacks on LLMs.&lt;/li&gt;&lt;li&gt;Evaluates the framework on twelve LLMs across ten harm categories, achieving state-of-the-art performance.&lt;/li&gt;&lt;li&gt;Reveals that models have distinct vulnerability profiles based on conversational patterns.&lt;/li&gt;&lt;li&gt;Highlights the need for pattern-aware defenses in LLM safety.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Ragib Amin Nihal', 'Rui Wen', 'Kazuhiro Nakadai', 'Jun Sakuma']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'jailbreaking', 'adversarial prompting', 'safety evaluation', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.08859</guid><pubDate>Mon, 13 Oct 2025 00:00:00 +0000</pubDate></item><item><title>CommandSans: Securing AI Agents with Surgical Precision Prompt Sanitization</title><link>https://arxiv.org/abs/2510.08829</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces CommandSans, a token-level prompt sanitization method for LLM agents&lt;/li&gt;&lt;li&gt;Aims to prevent prompt injection attacks by removing executable instructions from tool outputs&lt;/li&gt;&lt;li&gt;Shows significant reduction in attack success rates across multiple benchmarks&lt;/li&gt;&lt;li&gt;Non-blocking approach that doesn't require context-specific calibration&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Debeshee Das', 'Luca Beurer-Kellner', 'Marc Fischer', 'Maximilian Baader']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'prompt injection', 'adversarial prompting', 'safety evaluation', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.08829</guid><pubDate>Mon, 13 Oct 2025 00:00:00 +0000</pubDate></item><item><title>SAFER-AiD: Saccade-Assisted Foveal-peripheral vision Enhanced Reconstruction for Adversarial Defense</title><link>https://arxiv.org/abs/2510.08761</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a biologically inspired defense framework against adversarial attacks&lt;/li&gt;&lt;li&gt;Uses saccadic eye movements and foveal-peripheral processing&lt;/li&gt;&lt;li&gt;Requires no retraining of downstream classifiers&lt;/li&gt;&lt;li&gt;Improves robustness across multiple classifiers and attack types&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jiayang Liu', 'Daniel Tso', 'Yiming Bu', 'Qinru Qiu']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial defense', 'biological mechanisms', 'saccadic eye movements', 'foveal-peripheral processing', 'reinforcement learning']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.08761</guid><pubDate>Mon, 13 Oct 2025 00:00:00 +0000</pubDate></item><item><title>RA-Gen: A Controllable Code Generation Framework Using ReAct for Multi-Agent Task Execution</title><link>https://arxiv.org/abs/2510.08665</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a multi-agent code generation framework using ReAct for safety and controllability&lt;/li&gt;&lt;li&gt;Incorporates a Searcher agent that uses ReAct for reasoning and tool integration&lt;/li&gt;&lt;li&gt;Achieves high security rate on SVEN dataset with CodeQL&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Aofan Liu', 'Haoxuan Li', 'Bin Wang', 'Ao Yang', 'Hui Li']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'safety evaluation', 'robustness', 'alignment']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.08665</guid><pubDate>Mon, 13 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Energy-Driven Steering: Reducing False Refusals in Large Language Models</title><link>https://arxiv.org/abs/2510.08646</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces Energy-Driven Steering (EDS) to reduce false refusals in LLMs&lt;/li&gt;&lt;li&gt;Uses an Energy-Based Model (EBM) to dynamically steer LLM responses&lt;/li&gt;&lt;li&gt;Aims to balance safety and helpfulness without modifying model weights&lt;/li&gt;&lt;li&gt;Shows significant improvement in compliance on ORB-H benchmark&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Eric Hanchen Jiang', 'Weixuan Ou', 'Run Liu', 'Shengyuan Pang', 'Guancheng Wan', 'Ranjie Duan', 'Wei Dong', 'Kai-Wei Chang', 'XiaoFeng Wang', 'Ying Nian Wu', 'Xinfeng Li']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'alignment', 'safety evaluation', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.08646</guid><pubDate>Mon, 13 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Toward a Safer Web: Multilingual Multi-Agent LLMs for Mitigating Adversarial Misinformation Attacks</title><link>https://arxiv.org/abs/2510.08605</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;The paper addresses adversarial attacks in misinformation detection, focusing on language-switching, query length inflation, and structural reformatting.&lt;/li&gt;&lt;li&gt;It introduces a multilingual, multi-agent LLM framework with retrieval-augmented generation.&lt;/li&gt;&lt;li&gt;The framework is designed to be deployed as a web plugin for online platforms.&lt;/li&gt;&lt;li&gt;The work highlights the importance of AI-driven detection against diverse attacks.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Nouar Aldahoul', 'Yasir Zaki']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial attacks', 'misinformation', 'multilingual', 'multi-agent', 'retrieval-augmented']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.08605</guid><pubDate>Mon, 13 Oct 2025 00:00:00 +0000</pubDate></item><item><title>LatentBreak: Jailbreaking Large Language Models through Latent Space Feedback</title><link>https://arxiv.org/abs/2510.08604</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes LatentBreak, a white-box jailbreak attack for LLMs&lt;/li&gt;&lt;li&gt;Uses latent space feedback to generate natural adversarial prompts&lt;/li&gt;&lt;li&gt;Aims to evade perplexity-based filters&lt;/li&gt;&lt;li&gt;Evaluates against safety-aligned models&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Raffaele Mura', 'Giorgio Piras', 'Kamil\\.e Luko\\v{s}i\\=ut\\.e', 'Maura Pintor', 'Amin Karbasi', 'Battista Biggio']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'jailbreaking', 'adversarial prompting', 'safety evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.08604</guid><pubDate>Mon, 13 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Less Diverse, Less Safe: The Indirect But Pervasive Risk of Test-Time Scaling in Large Language Models</title><link>https://arxiv.org/abs/2510.08592</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Test-Time Scaling (TTS) in LLMs can produce unsafe outputs when candidate diversity is reduced.&lt;/li&gt;&lt;li&gt;RefDiv protocol is introduced to stress test TTS by reducing diversity.&lt;/li&gt;&lt;li&gt;Experiments show that reduced diversity leads to unsafe outputs across multiple models and TTS strategies.&lt;/li&gt;&lt;li&gt;Existing safety classifiers fail to detect RefDiv-generated adversarial prompts.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Shahriar Kabir Nahin', 'Hadi Askari', 'Muhao Chen', 'Anshuman Chhabra']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'adversarial prompting', 'safety evaluation', 'robustness', 'model extraction']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.08592</guid><pubDate>Mon, 13 Oct 2025 00:00:00 +0000</pubDate></item><item><title>RADAR: Mechanistic Pathways for Detecting Data Contamination in LLM Evaluation</title><link>https://arxiv.org/abs/2510.08931</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces RADAR framework for detecting data contamination in LLM evaluation&lt;/li&gt;&lt;li&gt;Uses mechanistic interpretability to distinguish recall vs reasoning&lt;/li&gt;&lt;li&gt;Extracts 37 features including attention and activation patterns&lt;/li&gt;&lt;li&gt;Achieves 93% accuracy on diverse evaluation set&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Ashish Kattamuri', 'Harshwardhan Fartale', 'Arpita Vats', 'Rahul Raja', 'Ishita Prasad']&lt;/li&gt;&lt;li&gt;Tags: ['safety evaluation', 'robustness', 'model evaluation', 'data poisoning']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.08931</guid><pubDate>Mon, 13 Oct 2025 00:00:00 +0000</pubDate></item><item><title>GTAlign: Game-Theoretic Alignment of LLM Assistants for Mutual Welfare</title><link>https://arxiv.org/abs/2510.08872</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes Game-Theoretic Alignment (GTAlign) framework for LLMs&lt;/li&gt;&lt;li&gt;Integrates game theory into reasoning and training&lt;/li&gt;&lt;li&gt;Aims to improve mutual welfare between LLM and user&lt;/li&gt;&lt;li&gt;Includes mutual welfare reward during training&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Siqi Zhu', 'David Zhang', 'Pedro Cisneros-Velarde', 'Jiaxuan You']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'game theory', 'mutual welfare', 'reasoning efficiency']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.08872</guid><pubDate>Mon, 13 Oct 2025 00:00:00 +0000</pubDate></item><item><title>What Is Your Agent's GPA? A Framework for Evaluating Agent Goal-Plan-Action Alignment</title><link>https://arxiv.org/abs/2510.08847</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces Agent GPA framework for evaluating agent alignment&lt;/li&gt;&lt;li&gt;Includes metrics like Goal Fulfillment, Logical Consistency, Execution Efficiency&lt;/li&gt;&lt;li&gt;Validated on TRAIL/GAIA and internal datasets&lt;/li&gt;&lt;li&gt;Supports LLM judges with high human agreement&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Allison Sihan Jia', 'Daniel Huang', 'Nikhil Vytla', 'Nirvika Choudhury', 'John C Mitchell', 'Anupam Datta']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'safety evaluation', 'agent evaluation', 'metrics']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.08847</guid><pubDate>Mon, 13 Oct 2025 00:00:00 +0000</pubDate></item></channel></rss>