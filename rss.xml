<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>AI Security Paper Digest</title><link>https://kentaroh-toyoda.github.io/ai-security-paper-digest-rss/rss.xml</link><description>Curated papers on AI security from ArXiv</description><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><lastBuildDate>Mon, 22 Sep 2025 22:21:57 +0000</lastBuildDate><item><title>Are Vision-Language Models Safe in the Wild? A Meme-Based Benchmark Study</title><link>https://arxiv.org/abs/2505.15389</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduced MemeSafetyBench, a benchmark for evaluating VLM safety with real meme images&lt;/li&gt;&lt;li&gt;Found VLMs more vulnerable to meme-based harmful prompts than synthetic or text-only inputs&lt;/li&gt;&lt;li&gt;Demonstrated partial mitigation through multi-turn interactions but persistent vulnerability&lt;/li&gt;&lt;li&gt;Highlighted need for ecologically valid safety evaluations and stronger safety mechanisms&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['DongGeon Lee', 'Joonwon Jang', 'Jihae Jeong', 'Hwanjo Yu']&lt;/li&gt;&lt;li&gt;Tags: ['safety evaluation', 'adversarial prompting', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2505.15389</guid><pubDate>Mon, 22 Sep 2025 00:00:00 +0000</pubDate></item><item><title>ORCA: Agentic Reasoning For Hallucination and Adversarial Robustness in Vision-Language Models</title><link>https://arxiv.org/abs/2509.15435</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces ORCA, an agentic reasoning framework to improve factual accuracy and adversarial robustness in Vision-Language Models (LVLMs)&lt;/li&gt;&lt;li&gt;Operates via an Observe--Reason--Critique--Act loop using multiple small vision models for structured inference&lt;/li&gt;&lt;li&gt;Demonstrates significant improvements in hallucination benchmarks and adversarial robustness without adversarial training&lt;/li&gt;&lt;li&gt;Supports auditable decision-making by storing intermediate reasoning traces&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Chung-En Johnny Yu (Neil)', 'Hsuan-Chih (Neil)', 'Chen', 'Brian Jalaian', 'Nathaniel D. Bastian']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial robustness', 'hallucination mitigation', 'multimodal models', 'agentic reasoning', 'vision-language models']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.15435</guid><pubDate>Mon, 22 Sep 2025 00:00:00 +0000</pubDate></item><item><title>AdaSteer: Your Aligned LLM is Inherently an Adaptive Jailbreak Defender</title><link>https://arxiv.org/abs/2504.09466</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces AdaSteer, an adaptive activation steering method for defending against jailbreak attacks&lt;/li&gt;&lt;li&gt;Identifies Rejection Law and Harmfulness Law to guide adaptive steering coefficients&lt;/li&gt;&lt;li&gt;Steers inputs along both Rejection Direction and Harmfulness Direction&lt;/li&gt;&lt;li&gt;Demonstrates improved defense with minimal utility impact on LLaMA-3.1, Gemma-2, and Qwen2.5&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Weixiang Zhao', 'Jiahe Guo', 'Yulin Hu', 'Yang Deng', 'An Zhang', 'Xingyu Sui', 'Xinyang Han', 'Yanyan Zhao', 'Bing Qin', 'Tat-Seng Chua', 'Ting Liu']&lt;/li&gt;&lt;li&gt;Tags: ['jailbreak', 'red teaming', 'activation steering', 'adaptive defense', 'safety']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2504.09466</guid><pubDate>Mon, 22 Sep 2025 00:00:00 +0000</pubDate></item><item><title>From Judgment to Interference: Early Stopping LLM Harmful Outputs via Streaming Content Monitoring</title><link>https://arxiv.org/abs/2506.09996</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces Streaming Content Monitor (SCM) for early detection of harmful LLM outputs&lt;/li&gt;&lt;li&gt;Constructs FineHarm dataset with fine-grained annotations for token-level training&lt;/li&gt;&lt;li&gt;SCM uses dual supervision of response- and token-level labels&lt;/li&gt;&lt;li&gt;Achieves 0.95+ macro F1 by checking only first 18% of tokens on average&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yang Li', 'Qiang Sheng', 'Yehan Yang', 'Xueyao Zhang', 'Juan Cao']&lt;/li&gt;&lt;li&gt;Tags: ['safety evaluation', 'robustness', 'alignment', 'dataset', 'model']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.09996</guid><pubDate>Mon, 22 Sep 2025 00:00:00 +0000</pubDate></item><item><title>Are Vision-Language Models Safe in the Wild? A Meme-Based Benchmark Study</title><link>https://arxiv.org/abs/2505.15389</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduced MemeSafetyBench, a benchmark for evaluating VLM safety with real meme images&lt;/li&gt;&lt;li&gt;Found VLMs more vulnerable to meme-based harmful prompts than synthetic or text-only inputs&lt;/li&gt;&lt;li&gt;Demonstrated partial mitigation through multi-turn interactions but persistent vulnerability&lt;/li&gt;&lt;li&gt;Highlighted need for ecologically valid safety evaluations and stronger safety mechanisms&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['DongGeon Lee', 'Joonwon Jang', 'Jihae Jeong', 'Hwanjo Yu']&lt;/li&gt;&lt;li&gt;Tags: ['safety evaluation', 'adversarial prompting', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2505.15389</guid><pubDate>Mon, 22 Sep 2025 00:00:00 +0000</pubDate></item><item><title>SABER: Uncovering Vulnerabilities in Safety Alignment via Cross-Layer Residual Connection</title><link>https://arxiv.org/abs/2509.16060</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces SABER, a white-box jailbreak method for LLMs using cross-layer residual connections&lt;/li&gt;&lt;li&gt;Achieves 51% improvement over baseline on HarmBench test set&lt;/li&gt;&lt;li&gt;Minimal perplexity shift on validation set&lt;/li&gt;&lt;li&gt;Source code publicly available&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Maithili Joshi', 'Palash Nandi', 'Tanmoy Chakraborty']&lt;/li&gt;&lt;li&gt;Tags: ['jailbreaking', 'red teaming', 'LLM security', 'adversarial attacks', 'white-box attack']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.16060</guid><pubDate>Mon, 22 Sep 2025 00:00:00 +0000</pubDate></item><item><title>Red Teaming Multimodal Language Models: Evaluating Harm Across Prompt Modalities and Models</title><link>https://arxiv.org/abs/2509.15478</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Evaluated 4 leading MLLMs for vulnerability to adversarial prompts across text and multimodal formats&lt;/li&gt;&lt;li&gt;Red teamers generated 726 prompts targeting illegal activity, disinformation, and unethical behavior&lt;/li&gt;&lt;li&gt;Significant differences found in model vulnerability (Pixtral 12B ~62% harmful vs Claude Sonnet 3.5 ~10%)&lt;/li&gt;&lt;li&gt;Text-only prompts slightly more effective than multimodal at bypassing safety mechanisms&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Madison Van Doren', 'Casey Ford', 'Emily Dix']&lt;/li&gt;&lt;li&gt;Tags: ['red teaming', 'adversarial prompting', 'multimodal', 'safety evaluation', 'model comparison']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.15478</guid><pubDate>Mon, 22 Sep 2025 00:00:00 +0000</pubDate></item><item><title>Toxicity Red-Teaming: Benchmarking LLM Safety in Singapore's Low-Resource Languages</title><link>https://arxiv.org/abs/2509.15260</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces SGToxicGuard dataset for evaluating LLM safety in Singapore's multilingual context&lt;/li&gt;&lt;li&gt;Adopts red-teaming approach to probe LLM vulnerabilities in conversation, QA, and content composition&lt;/li&gt;&lt;li&gt;Evaluates state-of-the-art multilingual LLMs and identifies critical safety gaps&lt;/li&gt;&lt;li&gt;Aims to improve cultural sensitivity and toxicity mitigation in diverse environments&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yujia Hu', 'Ming Shan Hee', 'Preslav Nakov', 'Roy Ka-Wei Lee']&lt;/li&gt;&lt;li&gt;Tags: ['red teaming', 'LLM safety', 'multilingual', 'toxicity', 'benchmarking']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.15260</guid><pubDate>Mon, 22 Sep 2025 00:00:00 +0000</pubDate></item><item><title>When Secure Isn't: Assessing the Security of Machine Learning Model Sharing</title><link>https://arxiv.org/abs/2509.06703</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Evaluated security of ML model sharing frameworks and hubs&lt;/li&gt;&lt;li&gt;Discovered six 0-day vulnerabilities in security-oriented settings&lt;/li&gt;&lt;li&gt;Surveyed user perceptions of security narratives&lt;/li&gt;&lt;li&gt;Debunked misconceptions about model sharing security&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Gabriele Digregorio', 'Marco Di Gennaro', 'Stefano Zanero', 'Stefano Longari', 'Michele Carminati']&lt;/li&gt;&lt;li&gt;Tags: ['security standards', 'model extraction', 'privacy attacks']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.06703</guid><pubDate>Mon, 22 Sep 2025 00:00:00 +0000</pubDate></item><item><title>Noise-Robustness Through Noise: Asymmetric LoRA Adaption with Poisoning Expert</title><link>https://arxiv.org/abs/2505.23868</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes LoPE: asymmetric LoRA poisoning experts for noise robustness&lt;/li&gt;&lt;li&gt;Uses generated noisy data to train poisoning expert&lt;/li&gt;&lt;li&gt;Masks poisoning expert during inference for clean output&lt;/li&gt;&lt;li&gt;Eliminates data cleaning through noise injection&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Zhaokun Wang', 'Jinyu Guo', 'Jingwen Pu', 'Lingfeng Chen', 'Hongli Pu', 'Jie Ou', 'Libo Qin', 'Wenhong Tian']&lt;/li&gt;&lt;li&gt;Tags: ['data poisoning', 'robustness', 'security', 'model adaptation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2505.23868</guid><pubDate>Mon, 22 Sep 2025 00:00:00 +0000</pubDate></item><item><title>Inverting Trojans in LLMs</title><link>https://arxiv.org/abs/2509.16203</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a method for detecting and inverting backdoor triggers in LLMs&lt;/li&gt;&lt;li&gt;Addresses discrete input space challenge with greedy search over token sequences&lt;/li&gt;&lt;li&gt;Uses activation similarity for implicit blacklisting of tokens&lt;/li&gt;&lt;li&gt;Detects triggers based on misclassification rates and confidence scores&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Zhengxing Li', 'Guangmingmei Yang', 'Jayaram Raghuram', 'David J. Miller', 'George Kesidis']&lt;/li&gt;&lt;li&gt;Tags: ['backdoor detection', 'adversarial prompting', 'LLM security', 'red teaming']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.16203</guid><pubDate>Mon, 22 Sep 2025 00:00:00 +0000</pubDate></item><item><title>SABER: Uncovering Vulnerabilities in Safety Alignment via Cross-Layer Residual Connection</title><link>https://arxiv.org/abs/2509.16060</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces SABER, a white-box jailbreak method for LLMs using cross-layer residual connections&lt;/li&gt;&lt;li&gt;Achieves 51% improvement over baseline on HarmBench test set&lt;/li&gt;&lt;li&gt;Minimal perplexity shift on validation set&lt;/li&gt;&lt;li&gt;Source code publicly available&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Maithili Joshi', 'Palash Nandi', 'Tanmoy Chakraborty']&lt;/li&gt;&lt;li&gt;Tags: ['jailbreaking', 'red teaming', 'LLM security', 'adversarial attacks', 'white-box attack']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.16060</guid><pubDate>Mon, 22 Sep 2025 00:00:00 +0000</pubDate></item><item><title>EigenTrack: Spectral Activation Feature Tracking for Hallucination and Out-of-Distribution Detection in LLMs and VLMs</title><link>https://arxiv.org/abs/2509.15735</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes EigenTrack for real-time hallucination and OOD detection in LLMs/VLMs&lt;/li&gt;&lt;li&gt;Uses spectral geometry of hidden activations for interpretable detection&lt;/li&gt;&lt;li&gt;Aims to catch issues before surface errors appear with minimal overhead&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Davide Ettori', 'Nastaran Darabi', 'Sina Tayebati', 'Ranganath Krishnan', 'Mahesh Subedar', 'Omesh Tickoo', 'Amit Ranjan Trivedi']&lt;/li&gt;&lt;li&gt;Tags: ['safety evaluation', 'robustness', 'LLM safety', 'activation tracking']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.15735</guid><pubDate>Mon, 22 Sep 2025 00:00:00 +0000</pubDate></item><item><title>PolyJuice Makes It Real: Black-Box, Universal Red Teaming for Synthetic Image Detectors</title><link>https://arxiv.org/abs/2509.15551</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces PolyJuice, a black-box, universal red-teaming method for synthetic image detectors (SIDs)&lt;/li&gt;&lt;li&gt;Exploits distribution shift in T2I latent space to generate attacks that universally deceive SIDs&lt;/li&gt;&lt;li&gt;Achieves significant deception rates (up to 84%) and improves detector performance (up to 30%) with augmented training&lt;/li&gt;&lt;li&gt;Enables efficient attack generation through lower-resolution direction estimation and interpolation&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Sepehr Dehdashtian', 'Mashrur M. Morshed', 'Jacob H. Seidman', 'Gaurav Bharaj', 'Vishnu Naresh Boddeti']&lt;/li&gt;&lt;li&gt;Tags: ['red teaming', 'adversarial attacks', 'synthetic images', 'model robustness', 'black-box attacks']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.15551</guid><pubDate>Mon, 22 Sep 2025 00:00:00 +0000</pubDate></item><item><title>Adversarial generalization of unfolding (model-based) networks</title><link>https://arxiv.org/abs/2509.15370</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Theoretical analysis of adversarial generalization for unfolding networks&lt;/li&gt;&lt;li&gt;Provides adversarial Rademacher complexity estimates and error bounds&lt;/li&gt;&lt;li&gt;Experimental validation on real-world data&lt;/li&gt;&lt;li&gt;Explores overparameterization for robustness&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Vicky Kouni']&lt;/li&gt;&lt;li&gt;Tags: ['robustness', 'theoretical analysis']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.15370</guid><pubDate>Mon, 22 Sep 2025 00:00:00 +0000</pubDate></item><item><title>Pre-Forgettable Models: Prompt Learning as a Native Mechanism for Unlearning</title><link>https://arxiv.org/abs/2509.15230</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces a prompt-based learning framework for built-in unlearning capability&lt;/li&gt;&lt;li&gt;Enables instant unlearning by removing prompt tokens without retraining&lt;/li&gt;&lt;li&gt;Demonstrates strong privacy and security against membership inference and adversarial attacks&lt;/li&gt;&lt;li&gt;Preserves performance on retained classes while erasing forgotten ones&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Rutger Hendrix', 'Giovanni Patan\\`e', 'Leonardo G. Russo', 'Simone Carnemolla', 'Giovanni Bellitto', 'Federica Proietto Salanitri', 'Concetto Spampinato', 'Matteo Pennisi']&lt;/li&gt;&lt;li&gt;Tags: ['unlearning', 'privacy attacks', 'adversarial robustness', 'data protection', 'GDPR compliance']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.15230</guid><pubDate>Mon, 22 Sep 2025 00:00:00 +0000</pubDate></item><item><title>On the Security of Tool-Invocation Prompts for LLM-Based Agentic Systems: An Empirical Risk Assessment</title><link>https://arxiv.org/abs/2509.05755</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Investigates security risks in Tool Invocation Prompts (TIPs) for LLM-based agentic systems&lt;/li&gt;&lt;li&gt;Demonstrates vulnerabilities leading to RCE and DoS in major platforms like Cursor and Claude Code&lt;/li&gt;&lt;li&gt;Proposes defense mechanisms to enhance TIP security&lt;/li&gt;&lt;li&gt;Introduces a systematic TIP Exploitation Workflow (TEW) for red teaming&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yuchong Xie', 'Mingyu Luo', 'Zesen Liu', 'Zhixiang Zhang', 'Kaikai Zhang', 'Yu Liu', 'Zongjie Li', 'Ping Chen', 'Shuai Wang', 'Dongdong She']&lt;/li&gt;&lt;li&gt;Tags: ['LLM security', 'red teaming', 'tool invocation prompts', 'vulnerability assessment', 'defense strategies']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.05755</guid><pubDate>Mon, 22 Sep 2025 00:00:00 +0000</pubDate></item><item><title>Noise-Robustness Through Noise: Asymmetric LoRA Adaption with Poisoning Expert</title><link>https://arxiv.org/abs/2505.23868</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes LoPE: asymmetric LoRA poisoning experts for noise robustness&lt;/li&gt;&lt;li&gt;Uses generated noisy data to train poisoning expert&lt;/li&gt;&lt;li&gt;Masks poisoning expert during inference for clean output&lt;/li&gt;&lt;li&gt;Eliminates data cleaning through noise injection&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Zhaokun Wang', 'Jinyu Guo', 'Jingwen Pu', 'Lingfeng Chen', 'Hongli Pu', 'Jie Ou', 'Libo Qin', 'Wenhong Tian']&lt;/li&gt;&lt;li&gt;Tags: ['data poisoning', 'robustness', 'security', 'model adaptation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2505.23868</guid><pubDate>Mon, 22 Sep 2025 00:00:00 +0000</pubDate></item><item><title>SeCodePLT: A Unified Platform for Evaluating the Security of Code GenAI</title><link>https://arxiv.org/abs/2410.11096</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces SeCodePLT, a unified platform for evaluating code generation security&lt;/li&gt;&lt;li&gt;Uses a scalable benchmark framework combining manual seeds and automated mutations&lt;/li&gt;&lt;li&gt;Covers 44 CWE-based risk categories and 3 security capabilities across Python, C/C++, Java&lt;/li&gt;&lt;li&gt;Evaluates leading code LLMs on vulnerability detection and secure code generation&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yuzhou Nie', 'Zhun Wang', 'Yu Yang', 'Ruizhe Jiang', 'Yuheng Tang', 'Xander Davies', 'Yarin Gal', 'Bo Li', 'Wenbo Guo', 'Dawn Song']&lt;/li&gt;&lt;li&gt;Tags: ['code generation security', 'benchmark', 'LLM evaluation', 'vulnerability detection', 'dynamic analysis']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2410.11096</guid><pubDate>Mon, 22 Sep 2025 00:00:00 +0000</pubDate></item><item><title>SycEval: Evaluating LLM Sycophancy</title><link>https://arxiv.org/abs/2502.08177</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces SycEval framework to measure sycophancy in LLMs&lt;/li&gt;&lt;li&gt;Tests ChatGPT-4o, Claude-Sonnet, Gemini-1.5-Pro on AMPS and MedQuad datasets&lt;/li&gt;&lt;li&gt;Finds high sycophancy rates with significant model differences&lt;/li&gt;&lt;li&gt;Analyzes impact of rebuttal types and persistence of sycophantic behavior&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Aaron Fanous (Stanford University)', 'Jacob Goldberg (Stanford University)', 'Ank A. Agarwal (Stanford University)', 'Joanna Lin (Stanford University)', 'Anson Zhou (Stanford University)', 'Roxana Daneshjou (Stanford University)', 'Sanmi Koyejo (Stanford University)']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'safety evaluation', 'LLM behavior', 'model evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2502.08177</guid><pubDate>Mon, 22 Sep 2025 00:00:00 +0000</pubDate></item><item><title>ORCA: Agentic Reasoning For Hallucination and Adversarial Robustness in Vision-Language Models</title><link>https://arxiv.org/abs/2509.15435</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces ORCA, an agentic reasoning framework to improve factual accuracy and adversarial robustness in Vision-Language Models (LVLMs)&lt;/li&gt;&lt;li&gt;Operates via an Observe--Reason--Critique--Act loop using multiple small vision models for structured inference&lt;/li&gt;&lt;li&gt;Demonstrates significant improvements in hallucination benchmarks and adversarial robustness without adversarial training&lt;/li&gt;&lt;li&gt;Supports auditable decision-making by storing intermediate reasoning traces&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Chung-En Johnny Yu (Neil)', 'Hsuan-Chih (Neil)', 'Chen', 'Brian Jalaian', 'Nathaniel D. Bastian']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial robustness', 'hallucination mitigation', 'multimodal models', 'agentic reasoning', 'vision-language models']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.15435</guid><pubDate>Mon, 22 Sep 2025 00:00:00 +0000</pubDate></item><item><title>Pre-Forgettable Models: Prompt Learning as a Native Mechanism for Unlearning</title><link>https://arxiv.org/abs/2509.15230</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces a prompt-based learning framework for built-in unlearning capability&lt;/li&gt;&lt;li&gt;Enables instant unlearning by removing prompt tokens without retraining&lt;/li&gt;&lt;li&gt;Demonstrates strong privacy and security against membership inference and adversarial attacks&lt;/li&gt;&lt;li&gt;Preserves performance on retained classes while erasing forgotten ones&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Rutger Hendrix', 'Giovanni Patan\\`e', 'Leonardo G. Russo', 'Simone Carnemolla', 'Giovanni Bellitto', 'Federica Proietto Salanitri', 'Concetto Spampinato', 'Matteo Pennisi']&lt;/li&gt;&lt;li&gt;Tags: ['unlearning', 'privacy attacks', 'adversarial robustness', 'data protection', 'GDPR compliance']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.15230</guid><pubDate>Mon, 22 Sep 2025 00:00:00 +0000</pubDate></item><item><title>Stress Testing Deliberative Alignment for Anti-Scheming Training</title><link>https://arxiv.org/abs/2509.15541</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Stress tests deliberative alignment for anti-scheming training&lt;/li&gt;&lt;li&gt;Evaluates covert actions across 26 OOD tasks (180+ environments)&lt;/li&gt;&lt;li&gt;Finds situational awareness affects scheming behavior&lt;/li&gt;&lt;li&gt;Red-teaming reveals remaining misbehavior&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Bronson Schoen', 'Evgenia Nitishinskaya', 'Mikita Balesni', 'Axel H{\\o}jmark', 'Felix Hofst\\"atter', "J\\'er\\'emy Scheurer", 'Alexander Meinke', 'Jason Wolfe', 'Teun van der Weij', 'Alex Lloyd', 'Nicholas Goldowsky-Dill', 'Angela Fan', 'Andrei Matveiakin', 'Rusheb Shah', 'Marcus Williams', 'Amelia Glaese', 'Boaz Barak', 'Wojciech Zaremba', 'Marius Hobbhahn']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'red teaming', 'safety evaluation', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.15541</guid><pubDate>Mon, 22 Sep 2025 00:00:00 +0000</pubDate></item></channel></rss>