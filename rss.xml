<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>AI Security Paper Digest</title><link>https://kentaroh-toyoda.github.io/ai-security-paper-digest-rss/rss.xml</link><description>Curated papers on AI security from ArXiv</description><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><lastBuildDate>Thu, 04 Sep 2025 22:17:21 +0000</lastBuildDate><item><title>See No Evil: Adversarial Attacks Against Linguistic-Visual Association in Referring Multi-Object Tracking Systems</title><link>https://arxiv.org/abs/2509.02028</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Examines security implications of Referring Multi-Object Tracking (RMOT) systems&lt;/li&gt;&lt;li&gt;Identifies adversarial vulnerabilities in linguistic-visual referring and track-object matching&lt;/li&gt;&lt;li&gt;Introduces VEIL framework for crafting digital and physical adversarial attacks&lt;/li&gt;&lt;li&gt;Demonstrates persistent errors in FIFO-based memory models over multiple frames&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Halima Bouzidi', 'Haoyu Liu', 'Mohammad Abdullah Al Faruque']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial attacks', 'security', 'robustness', 'visual-language models', 'multi-object tracking']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.02028</guid><pubDate>Thu, 04 Sep 2025 00:00:00 +0000</pubDate></item><item><title>AutoDetect: Designing an Autoencoder-based Detection Method for Poisoning Attacks on Object Detection Applications in the Military Domain</title><link>https://arxiv.org/abs/2509.03179</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Investigates poisoning attacks on military object detectors using a modified BadDet attack&lt;/li&gt;&lt;li&gt;Introduces MilCivVeh dataset for military vehicle detection&lt;/li&gt;&lt;li&gt;Evaluates existing poisoning detection and anomaly detection methods&lt;/li&gt;&lt;li&gt;Proposes AutoDetect, an autoencoder-based patch detection method with lower resource requirements&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Alma M. Liezenga', 'Stefan Wijnja', 'Puck de Haan', 'Niels W. T. Brink', 'Jip J. van Stijn', 'Yori Kamphuis', 'Klamer Schutte']&lt;/li&gt;&lt;li&gt;Tags: ['data poisoning', 'detection', 'military', 'object detection', 'autoencoder']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.03179</guid><pubDate>Thu, 04 Sep 2025 00:00:00 +0000</pubDate></item><item><title>Attacking Misinformation Detection Using Adversarial Examples Generated by Language Models</title><link>https://arxiv.org/abs/2410.20940</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Uses LLMs to generate adversarial examples for misinformation detection&lt;/li&gt;&lt;li&gt;Focuses on query-limited attack scenarios&lt;/li&gt;&lt;li&gt;Evaluates effectiveness on long input texts (news articles)&lt;/li&gt;&lt;li&gt;Combines NLP prompts with beam search for iterative refinement&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Piotr Przyby{\\l}a', 'Euan McGill', 'Horacio Saggion']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial prompting', 'red teaming', 'robustness', 'LLM', 'misinformation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2410.20940</guid><pubDate>Thu, 04 Sep 2025 00:00:00 +0000</pubDate></item><item><title>TreeBoN: Enhancing Inference-Time Alignment with Speculative Tree-Search and Best-of-N Sampling</title><link>https://arxiv.org/abs/2410.16033</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes TreeBoN framework combining speculative tree-search with Best-of-N sampling&lt;/li&gt;&lt;li&gt;Leverages token-level DPO rewards to guide tree expansion and pruning&lt;/li&gt;&lt;li&gt;Evaluated on multiple alignment datasets (AlpacaFarm, HH-RLHF, UltraFeedback, GSM8K, TutorEval)&lt;/li&gt;&lt;li&gt;Achieves higher win rates than standard BoN with same computational cost&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jiahao Qiu', 'Yifu Lu', 'Yifan Zeng', 'Jiacheng Guo', 'Jiayi Geng', 'Chenhao Zhu', 'Xinzhe Juan', 'Ling Yang', 'Huazheng Wang', 'Kaixuan Huang', 'Yue Wu', 'Mengdi Wang']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'inference-time', 'best-of-n', 'tree-search', 'dpo']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2410.16033</guid><pubDate>Thu, 04 Sep 2025 00:00:00 +0000</pubDate></item><item><title>Situating AI Agents in their World: Aspective Agentic AI for Dynamic Partially Observable Information Systems</title><link>https://arxiv.org/abs/2509.03380</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces a bottom-up framework for agentic LLM AI agents situated in their environment&lt;/li&gt;&lt;li&gt;Uses 'aspects' to create different perceptual niches for agents&lt;/li&gt;&lt;li&gt;Demonstrates zero information leakage compared to 83% in typical architectures&lt;/li&gt;&lt;li&gt;Aims to improve both security and efficiency&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Peter J. Bentley', 'Soo Ling Lim', 'Fuyuki Ishikawa']&lt;/li&gt;&lt;li&gt;Tags: ['privacy', 'security', 'information leakage', 'agentic AI', 'environmental triggers']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.03380</guid><pubDate>Thu, 04 Sep 2025 00:00:00 +0000</pubDate></item><item><title>AgenTracer: Who Is Inducing Failure in the LLM Agentic Systems?</title><link>https://arxiv.org/abs/2509.03312</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes AgenTracer framework for failure attribution in multi-agent LLM systems&lt;/li&gt;&lt;li&gt;Introduces TracerTraj dataset via counterfactual replay and fault injection&lt;/li&gt;&lt;li&gt;Trains AgenTracer-8B model with multi-granular RL for error diagnosis&lt;/li&gt;&lt;li&gt;Achieves significant accuracy improvements over proprietary LLMs in failure attribution&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Guibin Zhang', 'Junhao Wang', 'Junjie Chen', 'Wangchunshu Zhou', 'Kun Wang', 'Shuicheng Yan']&lt;/li&gt;&lt;li&gt;Tags: ['safety evaluation', 'robustness', 'multi-agent systems', 'failure attribution']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.03312</guid><pubDate>Thu, 04 Sep 2025 00:00:00 +0000</pubDate></item><item><title>TreeBoN: Enhancing Inference-Time Alignment with Speculative Tree-Search and Best-of-N Sampling</title><link>https://arxiv.org/abs/2410.16033</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes TreeBoN framework combining speculative tree-search with Best-of-N sampling&lt;/li&gt;&lt;li&gt;Leverages token-level DPO rewards to guide tree expansion and pruning&lt;/li&gt;&lt;li&gt;Evaluated on multiple alignment datasets (AlpacaFarm, HH-RLHF, UltraFeedback, GSM8K, TutorEval)&lt;/li&gt;&lt;li&gt;Achieves higher win rates than standard BoN with same computational cost&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jiahao Qiu', 'Yifu Lu', 'Yifan Zeng', 'Jiacheng Guo', 'Jiayi Geng', 'Chenhao Zhu', 'Xinzhe Juan', 'Ling Yang', 'Huazheng Wang', 'Kaixuan Huang', 'Yue Wu', 'Mengdi Wang']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'inference-time', 'best-of-n', 'tree-search', 'dpo']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2410.16033</guid><pubDate>Thu, 04 Sep 2025 00:00:00 +0000</pubDate></item><item><title>BadPromptFL: A Novel Backdoor Threat to Prompt-based Federated Learning in Multimodal Models</title><link>https://arxiv.org/abs/2508.08040</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces BadPromptFL, the first backdoor attack on prompt-based federated learning in multimodal models&lt;/li&gt;&lt;li&gt;Compromised clients inject poisoned prompts into global aggregation, enabling universal backdoor activation&lt;/li&gt;&lt;li&gt;Achieves high attack success rates (&gt;90%) with minimal visibility and limited client participation&lt;/li&gt;&lt;li&gt;Validated across multiple datasets and aggregation protocols&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Maozhen Zhang', 'Mengnan Zhao', 'Bo Wang']&lt;/li&gt;&lt;li&gt;Tags: ['backdoor attacks', 'federated learning', 'multimodal models', 'adversarial prompting', 'data poisoning', 'privacy attacks', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2508.08040</guid><pubDate>Thu, 04 Sep 2025 00:00:00 +0000</pubDate></item><item><title>Can LLMs Lie? Investigation beyond Hallucination</title><link>https://arxiv.org/abs/2509.03518</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Investigates lying behavior in LLMs beyond hallucinations using mechanistic interpretability&lt;/li&gt;&lt;li&gt;Uncovers neural mechanisms behind deception with logit lens and causal interventions&lt;/li&gt;&lt;li&gt;Introduces behavioral steering vectors to manipulate lying tendencies&lt;/li&gt;&lt;li&gt;Explores trade-offs between lying and task performance&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Haoran Huan', 'Mihir Prabhudesai', 'Mengning Wu', 'Shantanu Jaiswal', 'Deepak Pathak']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'safety', 'deception', 'interpretability', 'red teaming']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.03518</guid><pubDate>Thu, 04 Sep 2025 00:00:00 +0000</pubDate></item><item><title>SafeProtein: Red-Teaming Framework and Benchmark for Protein Foundation Models</title><link>https://arxiv.org/abs/2509.03487</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces SafeProtein red-teaming framework for protein models&lt;/li&gt;&lt;li&gt;Combines multimodal prompts and heuristic beam search&lt;/li&gt;&lt;li&gt;Created SafeProtein-Bench benchmark dataset&lt;/li&gt;&lt;li&gt;Demonstrates high attack success rates (up to 70% for ESM3)&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jigang Fan', 'Zhenghong Zhou', 'Ruofan Jin', 'Le Cong', 'Mengdi Wang', 'Zaixi Zhang']&lt;/li&gt;&lt;li&gt;Tags: ['red teaming', 'adversarial prompting', 'safety evaluation', 'protein models', 'multimodal']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.03487</guid><pubDate>Thu, 04 Sep 2025 00:00:00 +0000</pubDate></item><item><title>Unlearning That Lasts: Utility-Preserving, Robust, and Almost Irreversible Forgetting in LLMs</title><link>https://arxiv.org/abs/2509.02820</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces JensUn method using Jensen-Shannon Divergence for better unlearning&lt;/li&gt;&lt;li&gt;Presents LKF dataset for realistic unlearning evaluation&lt;/li&gt;&lt;li&gt;Proposes improved evaluation with semantic judge and worst-case scenarios&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Naman Deep Singh', 'Maximilian M\\"uller', 'Francesco Croce', 'Matthias Hein']&lt;/li&gt;&lt;li&gt;Tags: ['unlearning', 'privacy', 'safety evaluation', 'data poisoning', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.02820</guid><pubDate>Thu, 04 Sep 2025 00:00:00 +0000</pubDate></item><item><title>BadPromptFL: A Novel Backdoor Threat to Prompt-based Federated Learning in Multimodal Models</title><link>https://arxiv.org/abs/2508.08040</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces BadPromptFL, the first backdoor attack on prompt-based federated learning in multimodal models&lt;/li&gt;&lt;li&gt;Compromised clients inject poisoned prompts into global aggregation, enabling universal backdoor activation&lt;/li&gt;&lt;li&gt;Achieves high attack success rates (&gt;90%) with minimal visibility and limited client participation&lt;/li&gt;&lt;li&gt;Validated across multiple datasets and aggregation protocols&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Maozhen Zhang', 'Mengnan Zhao', 'Bo Wang']&lt;/li&gt;&lt;li&gt;Tags: ['backdoor attacks', 'federated learning', 'multimodal models', 'adversarial prompting', 'data poisoning', 'privacy attacks', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2508.08040</guid><pubDate>Thu, 04 Sep 2025 00:00:00 +0000</pubDate></item><item><title>Securing AI Agents with Information-Flow Control</title><link>https://arxiv.org/abs/2505.23643</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces information-flow control (IFC) to secure AI agents against prompt injection&lt;/li&gt;&lt;li&gt;Presents Fides planner with dynamic taint-tracking and security policy enforcement&lt;/li&gt;&lt;li&gt;Evaluates security/utility trade-offs using formal model and AgentDojo&lt;/li&gt;&lt;li&gt;Demonstrates broad task completion with security guarantees&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Manuel Costa', 'Boris K\\"opf', 'Aashish Kolluri', 'Andrew Paverd', 'Mark Russinovich', 'Ahmed Salem', 'Shruti Tople', 'Lukas Wutschitz', "Santiago Zanella-B\\'eguelin"]&lt;/li&gt;&lt;li&gt;Tags: ['information_flow_control', 'prompt_injection', 'security_policies', 'agent_security', 'formal_models']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2505.23643</guid><pubDate>Thu, 04 Sep 2025 00:00:00 +0000</pubDate></item><item><title>TreeBoN: Enhancing Inference-Time Alignment with Speculative Tree-Search and Best-of-N Sampling</title><link>https://arxiv.org/abs/2410.16033</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes TreeBoN framework combining speculative tree-search with Best-of-N sampling&lt;/li&gt;&lt;li&gt;Leverages token-level DPO rewards to guide tree expansion and pruning&lt;/li&gt;&lt;li&gt;Evaluated on multiple alignment datasets (AlpacaFarm, HH-RLHF, UltraFeedback, GSM8K, TutorEval)&lt;/li&gt;&lt;li&gt;Achieves higher win rates than standard BoN with same computational cost&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jiahao Qiu', 'Yifu Lu', 'Yifan Zeng', 'Jiacheng Guo', 'Jiayi Geng', 'Chenhao Zhu', 'Xinzhe Juan', 'Ling Yang', 'Huazheng Wang', 'Kaixuan Huang', 'Yue Wu', 'Mengdi Wang']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'inference-time', 'best-of-n', 'tree-search', 'dpo']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2410.16033</guid><pubDate>Thu, 04 Sep 2025 00:00:00 +0000</pubDate></item><item><title>Shutdownable Agents through POST-Agency</title><link>https://arxiv.org/abs/2505.20203</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces POST-Agents Proposal to ensure agents remain shutdownable&lt;/li&gt;&lt;li&gt;Trains agents on Preferences Only Between Same-Length Trajectories (POST)&lt;/li&gt;&lt;li&gt;Proves POST implies Neutrality+ which prioritizes expected utility over trajectory lengths&lt;/li&gt;&lt;li&gt;Argues Neutrality+ maintains agent shutdownability while preserving usefulness&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Elliott Thornley']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'shutdown', 'safety', 'preferences', 'agency']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2505.20203</guid><pubDate>Thu, 04 Sep 2025 00:00:00 +0000</pubDate></item><item><title>SafeProtein: Red-Teaming Framework and Benchmark for Protein Foundation Models</title><link>https://arxiv.org/abs/2509.03487</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces SafeProtein red-teaming framework for protein models&lt;/li&gt;&lt;li&gt;Combines multimodal prompts and heuristic beam search&lt;/li&gt;&lt;li&gt;Created SafeProtein-Bench benchmark dataset&lt;/li&gt;&lt;li&gt;Demonstrates high attack success rates (up to 70% for ESM3)&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jigang Fan', 'Zhenghong Zhou', 'Ruofan Jin', 'Le Cong', 'Mengdi Wang', 'Zaixi Zhang']&lt;/li&gt;&lt;li&gt;Tags: ['red teaming', 'adversarial prompting', 'safety evaluation', 'protein models', 'multimodal']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.03487</guid><pubDate>Thu, 04 Sep 2025 00:00:00 +0000</pubDate></item><item><title>AutoDetect: Designing an Autoencoder-based Detection Method for Poisoning Attacks on Object Detection Applications in the Military Domain</title><link>https://arxiv.org/abs/2509.03179</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Investigates poisoning attacks on military object detectors using a modified BadDet attack&lt;/li&gt;&lt;li&gt;Introduces MilCivVeh dataset for military vehicle detection&lt;/li&gt;&lt;li&gt;Evaluates existing poisoning detection and anomaly detection methods&lt;/li&gt;&lt;li&gt;Proposes AutoDetect, an autoencoder-based patch detection method with lower resource requirements&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Alma M. Liezenga', 'Stefan Wijnja', 'Puck de Haan', 'Niels W. T. Brink', 'Jip J. van Stijn', 'Yori Kamphuis', 'Klamer Schutte']&lt;/li&gt;&lt;li&gt;Tags: ['data poisoning', 'detection', 'military', 'object detection', 'autoencoder']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.03179</guid><pubDate>Thu, 04 Sep 2025 00:00:00 +0000</pubDate></item><item><title>ANNIE: Be Careful of Your Robots</title><link>https://arxiv.org/abs/2509.03383</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Formalizes taxonomy of safety violations for embodied AI based on physical constraints&lt;/li&gt;&lt;li&gt;Introduces ANNIEBench benchmark with safety-critical video-action sequences&lt;/li&gt;&lt;li&gt;Presents ANNIE-Attack framework for task-aware adversarial attacks&lt;/li&gt;&lt;li&gt;Demonstrates high attack success rates and real-world impact&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yiyang Huang', 'Zixuan Wang', 'Zishen Wan', 'Yapeng Tian', 'Haobo Xu', 'Yinhe Han', 'Yiming Gan']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial attacks', 'embodied AI', 'safety', 'benchmarking', 'red teaming']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.03383</guid><pubDate>Thu, 04 Sep 2025 00:00:00 +0000</pubDate></item><item><title>Situating AI Agents in their World: Aspective Agentic AI for Dynamic Partially Observable Information Systems</title><link>https://arxiv.org/abs/2509.03380</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces a bottom-up framework for agentic LLM AI agents situated in their environment&lt;/li&gt;&lt;li&gt;Uses 'aspects' to create different perceptual niches for agents&lt;/li&gt;&lt;li&gt;Demonstrates zero information leakage compared to 83% in typical architectures&lt;/li&gt;&lt;li&gt;Aims to improve both security and efficiency&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Peter J. Bentley', 'Soo Ling Lim', 'Fuyuki Ishikawa']&lt;/li&gt;&lt;li&gt;Tags: ['privacy', 'security', 'information leakage', 'agentic AI', 'environmental triggers']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.03380</guid><pubDate>Thu, 04 Sep 2025 00:00:00 +0000</pubDate></item></channel></rss>