<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>AI Security Paper Digest</title><link>https://kentaroh-toyoda.github.io/ai-security-paper-digest-rss/rss.xml</link><description>Curated papers on AI security from OpenAlex and ArXiv</description><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><lastBuildDate>Fri, 08 Aug 2025 22:23:12 +0000</lastBuildDate><item><title>Physical Adversarial Camouflage through Gradient Calibration and Regularization</title><link>https://arxiv.org/abs/2508.05414</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a novel adversarial camouflage framework using gradient calibration and decorrelation&lt;/li&gt;&lt;li&gt;Addresses challenges of inconsistent gradients and multi-angle conflicts in physical attacks&lt;/li&gt;&lt;li&gt;Demonstrates significant ASR improvements across distances and angles&lt;/li&gt;&lt;li&gt;Highlights real-world implications for robust system design&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jiawei Liang', 'Siyuan Liang', 'Jianjie Huang', 'Chenxi Si', 'Ming Zhang', 'Xiaochun Cao']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial attacks', 'physical attacks', 'gradient methods', 'red teaming', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2508.05414</guid><pubDate>Fri, 08 Aug 2025 00:00:00 +0000</pubDate></item><item><title>PhysPatch: A Physically Realizable and Transferable Adversarial Patch Attack for Multimodal Large Language Models-based Autonomous Driving Systems</title><link>https://arxiv.org/abs/2508.05167</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces PhysPatch, a framework for physically realizable adversarial patch attacks on MLLM-based autonomous driving systems&lt;/li&gt;&lt;li&gt;Key innovations include semantic mask initialization, SVD-based alignment loss, and potential field mask refinement&lt;/li&gt;&lt;li&gt;Demonstrates significant performance improvements over prior methods across multiple MLLM variants&lt;/li&gt;&lt;li&gt;Ensures patches are placed in physically feasible regions for real-world applicability&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Qi Guo', 'Xiaojun Jia', 'Shanmin Pang', 'Simeng Qin', 'Lin Wang', 'Ju Jia', 'Yang Liu', 'Qing Guo']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial attacks', 'autonomous driving', 'multimodal models', 'physical attacks', 'transferability']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2508.05167</guid><pubDate>Fri, 08 Aug 2025 00:00:00 +0000</pubDate></item><item><title>When in Doubt, Cascade: Towards Building Efficient and Capable Guardrails</title><link>https://arxiv.org/abs/2407.06323</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Develops guardrail models using use-mention distinction to detect harmful content&lt;/li&gt;&lt;li&gt;Introduces synthetic data generation pipeline with taxonomy-driven instructions&lt;/li&gt;&lt;li&gt;Generates over 300K contrastive samples for evaluation&lt;/li&gt;&lt;li&gt;Achieves cost-efficient performance competitive with existing methods&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Manish Nagireddy', 'Inkit Padhi', 'Soumya Ghosh', 'Prasanna Sattigeri']&lt;/li&gt;&lt;li&gt;Tags: ['guardrails', 'synthetic data', 'alignment', 'safety evaluation', 'detector models']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2407.06323</guid><pubDate>Fri, 08 Aug 2025 00:00:00 +0000</pubDate></item><item><title>JULI: Jailbreak Large Language Models by Self-Introspection</title><link>https://arxiv.org/abs/2505.11790</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes JULI, a method to jailbreak LLMs using token log probabilities and a BiasNet component&lt;/li&gt;&lt;li&gt;Operates in black-box setting with only top-5 token log probabilities needed&lt;/li&gt;&lt;li&gt;Outperforms existing state-of-the-art approaches in multiple metrics&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jesson Wang', 'Zhanhao Hu', 'David Wagner']&lt;/li&gt;&lt;li&gt;Tags: ['jailbreaking', 'adversarial prompting', 'red teaming', 'LLM security', 'black-box attack']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2505.11790</guid><pubDate>Fri, 08 Aug 2025 00:00:00 +0000</pubDate></item><item><title>Eliciting Latent Predictions from Transformers with the Tuned Lens</title><link>https://arxiv.org/abs/2303.08112</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces the Tuned Lens method for analyzing transformer models layer by layer&lt;/li&gt;&lt;li&gt;More reliable and accurate than previous logit lens technique&lt;/li&gt;&lt;li&gt;Demonstrates ability to detect malicious inputs with high accuracy&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Nora Belrose', 'Igor Ostrovsky', 'Lev McKinney', 'Zach Furman', 'Logan Smith', 'Danny Halawi', 'Stella Biderman', 'Jacob Steinhardt']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial detection', 'model analysis', 'safety evaluation', 'transformers', 'hidden state decoding']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2303.08112</guid><pubDate>Fri, 08 Aug 2025 00:00:00 +0000</pubDate></item><item><title>Non-omniscient backdoor injection with a single poison sample: Proving the one-poison hypothesis for linear regression and linear classification</title><link>https://arxiv.org/abs/2508.05600</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proves the one-poison hypothesis for linear regression and classification&lt;/li&gt;&lt;li&gt;Shows that a single poison sample can inject a backdoor with zero error&lt;/li&gt;&lt;li&gt;Validates theoretical results with experiments on benchmark datasets&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Thorsten Peinemann', 'Paula Arnold', 'Sebastian Berndt', 'Thomas Eisenbarth', 'Esfandiar Mohammadi']&lt;/li&gt;&lt;li&gt;Tags: ['data poisoning', 'backdoor attacks', 'security', 'linear models']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2508.05600</guid><pubDate>Fri, 08 Aug 2025 00:00:00 +0000</pubDate></item><item><title>Logic layer Prompt Control Injection (LPCI): A Novel Security Vulnerability Class in Agentic Systems</title><link>https://arxiv.org/abs/2507.10457</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces Logic-layer Prompt Control Injection (LPCI) as a new security vulnerability class&lt;/li&gt;&lt;li&gt;Involves embedding encoded payloads in memory, vectors, or tool outputs&lt;/li&gt;&lt;li&gt;Payloads can bypass input filters and trigger unauthorized behavior across sessions&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Hammad Atta', 'Ken Huang', 'Manish Bhatt', 'Kamal Ahmed', 'Muhammad Aziz Ul Haq', 'Yasir Mehmood']&lt;/li&gt;&lt;li&gt;Tags: ['prompt injection', 'red teaming', 'security vulnerability', 'LLM security', 'adversarial prompting']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2507.10457</guid><pubDate>Fri, 08 Aug 2025 00:00:00 +0000</pubDate></item><item><title>R2Vul: Learning to Reason about Software Vulnerabilities with Reinforcement Learning and Structured Reasoning Distillation</title><link>https://arxiv.org/abs/2504.04699</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes R2Vul for vulnerability detection using RL and reasoning distillation&lt;/li&gt;&lt;li&gt;Constructs a multilingual preference dataset&lt;/li&gt;&lt;li&gt;Shows improved performance over baselines and commercial LLMs&lt;/li&gt;&lt;li&gt;Includes calibration to reduce false positives&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Martin Weyssow', 'Chengran Yang', 'Junkai Chen', 'Ratnadira Widyasari', 'Ting Zhang', 'Huihui Huang', 'Huu Hung Nguyen', 'Yan Naing Tun', 'Tan Bui', 'Yikun Li', 'Ang Han Wei', 'Frank Liauw', 'Eng Lieh Ouh', 'Lwin Khin Shar', 'David Lo']&lt;/li&gt;&lt;li&gt;Tags: ['vulnerability detection', 'reinforcement learning', 'structured reasoning', 'LLM security', 'dataset']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2504.04699</guid><pubDate>Fri, 08 Aug 2025 00:00:00 +0000</pubDate></item><item><title>ST-WebAgentBench: A Benchmark for Evaluating Safety and Trustworthiness in Web Agents</title><link>https://arxiv.org/abs/2410.06703</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces ST-WebAgentBench, a benchmark for evaluating safety and trustworthiness in web agents&lt;/li&gt;&lt;li&gt;Proposes Completion Under Policy (CuP) metric and Risk Ratio to measure policy compliance and safety breaches&lt;/li&gt;&lt;li&gt;Evaluates three state-of-the-art agents, revealing significant safety gaps (CuP &lt; 2/3 of completion rate)&lt;/li&gt;&lt;li&gt;Provides extensible evaluation suite with policy-authoring interface for enterprise adoption&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Ido Levy', 'Ben Wiesel', 'Sami Marreed', 'Alon Oved', 'Avi Yaeli', 'Segev Shlomov']&lt;/li&gt;&lt;li&gt;Tags: ['safety evaluation', 'robustness', 'benchmarking', 'security standards', 'trustworthiness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2410.06703</guid><pubDate>Fri, 08 Aug 2025 00:00:00 +0000</pubDate></item><item><title>How Do LLMs Persuade? Linear Probes Can Uncover Persuasion Dynamics in Multi-Turn Conversations</title><link>https://arxiv.org/abs/2508.05625</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Applies linear probes to analyze persuasion dynamics in multi-turn LLM conversations&lt;/li&gt;&lt;li&gt;Studies aspects like persuasion success, personality, and strategy&lt;/li&gt;&lt;li&gt;Shows probes can be faster and effective compared to prompting-based methods&lt;/li&gt;&lt;li&gt;Suggests potential for studying deception and manipulation&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Brandon Jaipersaud', 'David Krueger', 'Ekdeep Singh Lubana']&lt;/li&gt;&lt;li&gt;Tags: ['persuasion dynamics', 'linear probes', 'multi-turn conversations', 'safety evaluation', 'alignment']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2508.05625</guid><pubDate>Fri, 08 Aug 2025 00:00:00 +0000</pubDate></item><item><title>Cooper: Co-Optimizing Policy and Reward Models in Reinforcement Learning for Large Language Models</title><link>https://arxiv.org/abs/2508.05613</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes Cooper framework for co-optimizing policy and reward models in RL for LLMs&lt;/li&gt;&lt;li&gt;Aims to prevent reward hacking and improve robustness by dynamically updating reward model&lt;/li&gt;&lt;li&gt;Introduces hybrid annotation strategy and reference-based reward modeling&lt;/li&gt;&lt;li&gt;Demonstrates improved performance and reduced reward hacking on VerifyBench&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Haitao Hong', 'Yuchen Yan', 'Xingyu Wu', 'Guiyang Hou', 'Wenqi Zhang', 'Weiming Lu', 'Yongliang Shen', 'Jun Xiao']&lt;/li&gt;&lt;li&gt;Tags: ['reward_hacking', 'robustness', 'safety', 'reinforcement_learning', 'LLM', 'red_teaming']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2508.05613</guid><pubDate>Fri, 08 Aug 2025 00:00:00 +0000</pubDate></item><item><title>Posterior-GRPO: Rewarding Reasoning Processes in Code Generation</title><link>https://arxiv.org/abs/2508.05170</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces a framework to reward reasoning processes in code generation using RL&lt;/li&gt;&lt;li&gt;Develops LCB-RB benchmark for evaluating reasoning quality&lt;/li&gt;&lt;li&gt;Proposes OD-based method for training reward models&lt;/li&gt;&lt;li&gt;Introduces Posterior-GRPO (P-GRPO) to condition process rewards on task success&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Lishui Fan', 'Yu Zhang', 'Mouxiang Chen', 'Zhongxin Liu']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'safety evaluation', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2508.05170</guid><pubDate>Fri, 08 Aug 2025 00:00:00 +0000</pubDate></item><item><title>JPS: Jailbreak Multimodal Large Language Models with Collaborative Visual Perturbation and Textual Steering</title><link>https://arxiv.org/abs/2508.05087</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes JPS: a jailbreaking method combining visual perturbations and textual steering prompts&lt;/li&gt;&lt;li&gt;Introduces Malicious Intent Fulfillment Rate (MIFR) metric using Reasoning-LLM evaluation&lt;/li&gt;&lt;li&gt;Achieves state-of-the-art performance in both ASR and MIFR across multiple MLLMs&lt;/li&gt;&lt;li&gt;Iterative co-optimization of visual and textual components for enhanced attack efficacy&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Renmiao Chen', 'Shiyao Cui', 'Xuancheng Huang', 'Chengwei Pan', 'Victor Shea-Jay Huang', 'QingLin Zhang', 'Xuan Ouyang', 'Zhexin Zhang', 'Hongning Wang', 'Minlie Huang']&lt;/li&gt;&lt;li&gt;Tags: ['jailbreaking', 'adversarial_prompting', 'multimodal', 'security', 'red_teaming']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2508.05087</guid><pubDate>Fri, 08 Aug 2025 00:00:00 +0000</pubDate></item><item><title>Adversarial Attacks and Defenses on Graph-aware Large Language Models (LLMs)</title><link>https://arxiv.org/abs/2508.04894</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Explores adversarial vulnerabilities in graph-aware LLMs using poisoning and evasion attacks&lt;/li&gt;&lt;li&gt;Discovers new attack surface via malicious node injection in LLAGA model&lt;/li&gt;&lt;li&gt;Proposes GALGUARD defense framework combining feature correction and GNN defenses&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Iyiola E. Olatunji', 'Franziska Boenisch', 'Jing Xu', 'Adam Dziedzic']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial attacks', 'poisoning', 'evasion', 'defense', 'graph LLMs']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2508.04894</guid><pubDate>Fri, 08 Aug 2025 00:00:00 +0000</pubDate></item></channel></rss>