<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>AI Security Paper Digest</title><link>https://kentaroh-toyoda.github.io/ai-security-paper-digest-rss/rss.xml</link><description>Curated papers on AI security from OpenAlex and ArXiv</description><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><lastBuildDate>Fri, 27 Jun 2025 22:19:46 +0000</lastBuildDate><item><title>Boosting Generative Adversarial Transferability with Self-supervised Vision Transformer Features</title><link>https://arxiv.org/abs/2506.21046</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces dSVA, a generative attack using self-supervised ViT features&lt;/li&gt;&lt;li&gt;Exploits both global (CL) and local (MIM) features for improved transferability&lt;/li&gt;&lt;li&gt;Demonstrates state-of-the-art black-box adversarial transfer performance&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Shangbo Wu', 'Yu-an Tan', 'Ruinan Ma', 'Wencong Ma', 'Dehua Zhu', 'Yuanzhang Li']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial examples', 'transferability', 'self-supervised learning', 'vision transformers', 'red teaming']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.21046</guid><pubDate>Fri, 27 Jun 2025 00:00:00 +0000</pubDate></item><item><title>A Troublemaker with Contagious Jailbreak Makes Chaos in Honest Towns</title><link>https://arxiv.org/abs/2410.16155</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces TMCHT task: multi-agent jailbreak evaluation framework&lt;/li&gt;&lt;li&gt;Proposes ARCJ method for contagious jailbreaks&lt;/li&gt;&lt;li&gt;Addresses toxicity disappearing in large-scale multi-agent systems&lt;/li&gt;&lt;li&gt;Demonstrates significant improvements in various topologies&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Tianyi Men', 'Pengfei Cao', 'Zhuoran Jin', 'Yubo Chen', 'Kang Liu', 'Jun Zhao']&lt;/li&gt;&lt;li&gt;Tags: ['jailbreaking', 'multi-agent systems', 'adversarial prompting', 'security']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2410.16155</guid><pubDate>Fri, 27 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Harnessing the Universal Geometry of Embeddings</title><link>https://arxiv.org/abs/2505.12540</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces unsupervised method to translate text embeddings between different vector spaces&lt;/li&gt;&lt;li&gt;Achieves high cosine similarity across diverse model architectures&lt;/li&gt;&lt;li&gt;Highlights security risks for vector databases due to potential sensitive information extraction&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Rishi Jha', 'Collin Zhang', 'Vitaly Shmatikov', 'John X. Morris']&lt;/li&gt;&lt;li&gt;Tags: ['embeddings', 'security', 'privacy', 'vector_databases', 'adversarial_attacks']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2505.12540</guid><pubDate>Fri, 27 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Seal Your Backdoor with Variational Defense</title><link>https://arxiv.org/abs/2503.08829</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes VIBE, a model-agnostic framework for defending against backdoor attacks&lt;/li&gt;&lt;li&gt;Uses variational inference to recover clean labels from poisoned training data&lt;/li&gt;&lt;li&gt;Integrates with self-supervised representation learning for enhanced defense&lt;/li&gt;&lt;li&gt;Demonstrates superior performance against various backdoor attacks in multiple datasets&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ["Ivan Saboli\\'c", "Matej Grci\\'c", "Sini\\v{s}a \\v{S}egvi\\'c"]&lt;/li&gt;&lt;li&gt;Tags: ['data poisoning', 'robustness', 'backdoor defense', 'security', 'defense']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2503.08829</guid><pubDate>Fri, 27 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Generative Adversarial Evasion and Out-of-Distribution Detection for UAV Cyber-Attacks</title><link>https://arxiv.org/abs/2506.21142</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces a cGAN-based framework to generate stealthy adversarial attacks for UAV cyber-attacks&lt;/li&gt;&lt;li&gt;Implements a CVAE-based detector to distinguish adversarial samples from OOD events&lt;/li&gt;&lt;li&gt;Compares CVAE-based regret scores against Mahalanobis distance-based detectors&lt;/li&gt;&lt;li&gt;Emphasizes the need for advanced probabilistic models in IDS&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Deepak Kumar Panda', 'Weisi Guo']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial attacks', 'intrusion detection', 'generative models', 'red teaming', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.21142</guid><pubDate>Fri, 27 Jun 2025 00:00:00 +0000</pubDate></item><item><title>GASP: Efficient Black-Box Generation of Adversarial Suffixes for Jailbreaking LLMs</title><link>https://arxiv.org/abs/2411.14133</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces GASP, a framework for efficient black-box generation of adversarial suffixes&lt;/li&gt;&lt;li&gt;Uses latent Bayesian optimization to create natural jailbreak prompts&lt;/li&gt;&lt;li&gt;Significantly improves attack success while reducing training and inference times&lt;/li&gt;&lt;li&gt;Aims to enhance red-teaming capabilities for LLM security&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Advik Raj Basani', 'Xiao Zhang']&lt;/li&gt;&lt;li&gt;Tags: ['jailbreak', 'adversarial prompting', 'red teaming', 'LLM security', 'black-box attack']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2411.14133</guid><pubDate>Fri, 27 Jun 2025 00:00:00 +0000</pubDate></item><item><title>PuriDefense: Randomized Local Implicit Adversarial Purification for Defending Black-box Query-based Attacks</title><link>https://arxiv.org/abs/2401.10586</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes PuriDefense, a randomized local implicit adversarial purification defense mechanism&lt;/li&gt;&lt;li&gt;Uses ensemble of lightweight purification models to rebuild natural image manifold&lt;/li&gt;&lt;li&gt;Theoretical analysis shows slowed convergence of query-based attacks&lt;/li&gt;&lt;li&gt;Validated on CIFAR-10 and ImageNet with improved robustness&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Ping Guo', 'Xiang Li', 'Zhiyuan Yang', 'Xi Lin', 'Qingchuan Zhao', 'Qingfu Zhang']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial attacks', 'defense mechanisms', 'query-based attacks', 'robustness', 'image classification']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2401.10586</guid><pubDate>Fri, 27 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Doppelganger Method: Breaking Role Consistency in LLM Agent via Prompt-based Transferable Adversarial Attack</title><link>https://arxiv.org/abs/2506.14539</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes the Doppelganger method for adversarial prompt-based attacks on LLM agents&lt;/li&gt;&lt;li&gt;Introduces PACAT metric to evaluate vulnerability to adversarial transfer attacks&lt;/li&gt;&lt;li&gt;Proposes CAT prompts as a defense mechanism&lt;/li&gt;&lt;li&gt;Demonstrates effectiveness of attack and defense through experiments&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Daewon Kang', 'YeongHwan Shin', 'Doyeon Kim', 'Kyu-Hwan Jung', 'Meong Hi Son']&lt;/li&gt;&lt;li&gt;Tags: ['red teaming', 'prompt injection', 'adversarial prompting', 'alignment', 'safety evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.14539</guid><pubDate>Fri, 27 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Curriculum-Guided Antifragile Reinforcement Learning for Secure UAV Deconfliction under Observation-Space Attacks</title><link>https://arxiv.org/abs/2506.21129</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes an antifragile RL framework to adapt against incremental adversarial perturbations&lt;/li&gt;&lt;li&gt;Introduces a simulated attacker with curriculum learning to incrementally increase perturbation strength&lt;/li&gt;&lt;li&gt;Theoretical characterization of fragility and antifragility with value function distribution bounds&lt;/li&gt;&lt;li&gt;Empirical evaluation in UAV deconfliction scenario shows improved performance against attacks&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Deepak Kumar Panda', 'Adolfo Perrusquia', 'Weisi Guo']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial attacks', 'reinforcement learning', 'security', 'safety', 'UAV', 'curriculum learning', 'antifragile']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.21129</guid><pubDate>Fri, 27 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Beyond Reactive Safety: Risk-Aware LLM Alignment via Long-Horizon Simulation</title><link>https://arxiv.org/abs/2506.20949</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces a framework for long-horizon simulation of LLM advice impact on societal systems&lt;/li&gt;&lt;li&gt;Creates a new dataset of 100 indirect harm scenarios to test foreseeing adverse outcomes&lt;/li&gt;&lt;li&gt;Achieves 20%+ improvement on new dataset and 70%+ win rate on existing safety benchmarks&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Chenkai Sun', 'Denghui Zhang', 'ChengXiang Zhai', 'Heng Ji']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'safety evaluation', 'risk-aware', 'long-horizon simulation', 'indirect harm']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.20949</guid><pubDate>Fri, 27 Jun 2025 00:00:00 +0000</pubDate></item><item><title>The Singapore Consensus on Global AI Safety Research Priorities</title><link>https://arxiv.org/abs/2506.20702</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Identifies global AI safety research priorities across Development, Assessment, and Control domains&lt;/li&gt;&lt;li&gt;Builds on previous international AI safety reports with 33 government backing&lt;/li&gt;&lt;li&gt;Adopts defense-in-depth model for organizing safety research&lt;/li&gt;&lt;li&gt;Aims to foster trust and innovation while mitigating risks&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Position&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yoshua Bengio', 'Tegan Maharaj', 'Luke Ong', 'Stuart Russell', 'Dawn Song', 'Max Tegmark', 'Lan Xue', 'Ya-Qin Zhang', 'Stephen Casper', 'Wan Sie Lee', 'S\\"oren Mindermann', 'Vanessa Wilfred', 'Vidhisha Balachandran', 'Fazl Barez', 'Michael Belinsky', 'Imane Bello', 'Malo Bourgon', 'Mark Brakel', "Sim\\'eon Campos", 'Duncan Cass-Beggs', 'Jiahao Chen', 'Rumman Chowdhury', 'Kuan Chua Seah', 'Jeff Clune', 'Juntao Dai', 'Agnes Delaborde', 'Nouha Dziri', 'Francisco Eiras', 'Joshua Engels', 'Jinyu Fan', 'Adam Gleave', 'Noah Goodman', 'Fynn Heide', 'Dan Hendrycks', 'Cyrus Hodes', 'Bryan Low Kian Hsiang', 'Minlie Huang', 'Sami Jawhar', 'Wang Jingyu', 'Adam Tauman Kalai', 'Meindert Kamphuis', 'Mohan Kankanhalli', 'Subhash Kantamneni', 'Mathias Bonde Kirk', 'Thomas Kwa', 'Jeffrey Ladish', 'Kwok-Yan Lam', 'Wan Lee Sie', 'Taewhi Lee', 'Xiaojian Li', 'Jiajun Liu', 'Chaochao Lu', 'Yifan Mai', 'Richard Mallah', 'Julian Michael', 'Nick Mo\\"es', 'Simon M\\"oller', 'Kihyuk Nam', 'Kwan Yee Ng', 'Mark Nitzberg', 'Besmira Nushi', "Se\\'an O h\\'Eigeartaigh", 'Alejandro Ortega', "Pierre Peign\\'e", 'James Petrie', "Benjamin Prud'Homme", 'Reihaneh Rabbany', 'Nayat Sanchez-Pi', 'Sarah Schwettmann', 'Buck Shlegeris', 'Saad Siddiqui', 'Aradhana Sinha', "Mart\\'in Soto", 'Cheston Tan', 'Dong Ting', 'Robert Trager', 'Brian Tse', 'Anthony Tung K. H.', 'Vanessa Wilfred', 'John Willes', 'Denise Wong', 'Wei Xu', 'Rongwu Xu', 'Yi Zeng', 'HongJiang Zhang', "Djordje \\v{Z}ikeli\\'c"]&lt;/li&gt;&lt;li&gt;Tags: ['AI safety', 'research priorities', 'defense-in-depth', 'international cooperation', 'trustworthy AI']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.20702</guid><pubDate>Fri, 27 Jun 2025 00:00:00 +0000</pubDate></item></channel></rss>