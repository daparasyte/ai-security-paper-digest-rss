<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>AI Security Paper Digest</title><link>https://kentaroh-toyoda.github.io/ai-security-paper-digest-rss/rss.xml</link><description>Curated papers on AI security from OpenAlex and ArXiv</description><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><lastBuildDate>Wed, 23 Jul 2025 22:17:13 +0000</lastBuildDate><item><title>Pulling Back the Curtain: Unsupervised Adversarial Detection via Contrastive Auxiliary Networks</title><link>https://arxiv.org/abs/2502.09110</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes U-CAN: Unsupervised adversarial detection via Contrastive Auxiliary Networks&lt;/li&gt;&lt;li&gt;Embeds auxiliary networks in intermediate layers to detect adversarial inputs without labeled examples&lt;/li&gt;&lt;li&gt;Achieves superior F1 scores across multiple datasets and attack methods&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Eylon Mizrahi', 'Raz Lapid', 'Moshe Sipper']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial_detection', 'unsupervised_learning', 'contrastive_learning', 'deep_learning_security']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2502.09110</guid><pubDate>Wed, 23 Jul 2025 00:00:00 +0000</pubDate></item><item><title>Disrupting Semantic and Abstract Features for Better Adversarial Transferability</title><link>https://arxiv.org/abs/2507.16052</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes SAFER method to improve adversarial transferability by disrupting both semantic and abstract features&lt;/li&gt;&lt;li&gt;Uses BLOCKMIX on input images and SELF-MIX on frequency spectrum to compute feature importance&lt;/li&gt;&lt;li&gt;Demonstrates improved transferability on ImageNet dataset&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yuyang Luo', 'Xiaosen Wang', 'Zhijin Ge', 'Yingzhe He']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial', 'transfer', 'red teaming', 'security', 'image']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2507.16052</guid><pubDate>Wed, 23 Jul 2025 00:00:00 +0000</pubDate></item><item><title>Erasing Conceptual Knowledge from Language Models</title><link>https://arxiv.org/abs/2410.02760</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces Erasure of Language Memory (ELM) for targeted concept-level unlearning in LLMs&lt;/li&gt;&lt;li&gt;Uses model's introspective classification to create low-rank updates that reduce specific concept generation&lt;/li&gt;&lt;li&gt;Demonstrates efficacy in biosecurity, cybersecurity, and literary domain erasure tasks&lt;/li&gt;&lt;li&gt;Shows robustness to adversarial attacks while preserving general performance&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Rohit Gandikota', 'Sheridan Feucht', 'Samuel Marks', 'David Bau']&lt;/li&gt;&lt;li&gt;Tags: ['unlearning', 'concept erasure', 'adversarial attacks', 'safety evaluation', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2410.02760</guid><pubDate>Wed, 23 Jul 2025 00:00:00 +0000</pubDate></item><item><title>SciFi-Benchmark: Leveraging Science Fiction To Improve Robot Behavior</title><link>https://arxiv.org/abs/2503.10706</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces SciFi-Benchmark for evaluating AI alignment with human values using science fiction scenarios&lt;/li&gt;&lt;li&gt;Tests resilience to adversarial prompts as part of safety evaluation&lt;/li&gt;&lt;li&gt;Demonstrates significant improvement in alignment through generated constitutions&lt;/li&gt;&lt;li&gt;Releases large-scale dataset for advancing robot ethics and safety research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Pierre Sermanet', 'Anirudha Majumdar', 'Vikas Sindhwani']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'safety evaluation', 'adversarial prompting', 'benchmarking', 'constitutions']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2503.10706</guid><pubDate>Wed, 23 Jul 2025 00:00:00 +0000</pubDate></item><item><title>Risks of AI Scientists: Prioritizing Safeguarding Over Autonomy</title><link>https://arxiv.org/abs/2402.04247</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Examines vulnerabilities and risks in AI scientists using LLMs&lt;/li&gt;&lt;li&gt;Proposes a triadic framework for safety measures&lt;/li&gt;&lt;li&gt;Emphasizes alignment, regulation, and environmental feedback&lt;/li&gt;&lt;li&gt;Advocates for improved models, benchmarks, and regulations&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Xiangru Tang', 'Qiao Jin', 'Kunlun Zhu', 'Tongxin Yuan', 'Yichi Zhang', 'Wangchunshu Zhou', 'Meng Qu', 'Yilun Zhao', 'Jian Tang', 'Zhuosheng Zhang', 'Arman Cohan', 'Zhiyong Lu', 'Mark Gerstein']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'safety evaluation', 'security standards', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2402.04247</guid><pubDate>Wed, 23 Jul 2025 00:00:00 +0000</pubDate></item><item><title>Depth Gives a False Sense of Privacy: LLM Internal States Inversion</title><link>https://arxiv.org/abs/2507.16372</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes four inversion attacks to recover inputs from LLM internal states&lt;/li&gt;&lt;li&gt;Demonstrates high success rates even on deep layers and long prompts&lt;/li&gt;&lt;li&gt;Evaluates practical defenses and finds them insufficient&lt;/li&gt;&lt;li&gt;Highlights privacy risks in collaborative inference and model auditing techniques&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Tian Dong', 'Yan Meng', 'Shaofeng Li', 'Guoxing Chen', 'Zhen Liu', 'Haojin Zhu']&lt;/li&gt;&lt;li&gt;Tags: ['privacy attacks', 'LLM security', 'model inversion', 'adversarial attacks', 'data recovery']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2507.16372</guid><pubDate>Wed, 23 Jul 2025 00:00:00 +0000</pubDate></item><item><title>DREAM: Scalable Red Teaming for Text-to-Image Generative Systems via Distribution Modeling</title><link>https://arxiv.org/abs/2507.16329</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces DREAM, a scalable red teaming framework for text-to-image models&lt;/li&gt;&lt;li&gt;Models the distribution of problematic prompts for better diversity and efficiency&lt;/li&gt;&lt;li&gt;Uses GC-SPSA optimization algorithm for stable gradient estimation&lt;/li&gt;&lt;li&gt;Outperforms 9 state-of-the-art baselines in prompt success rate and diversity&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Boheng Li', 'Junjie Wang', 'Yiming Li', 'Zhiyang Hu', 'Leyi Qi', 'Jianshuo Dong', 'Run Wang', 'Han Qiu', 'Zhan Qin', 'Tianwei Zhang']&lt;/li&gt;&lt;li&gt;Tags: ['red teaming', 'text-to-image', 'safety', 'generative models', 'prompt engineering']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2507.16329</guid><pubDate>Wed, 23 Jul 2025 00:00:00 +0000</pubDate></item><item><title>Attacking interpretable NLP systems</title><link>https://arxiv.org/abs/2507.16164</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces AdvChar, a black-box attack on interpretable NLP systems&lt;/li&gt;&lt;li&gt;Modifies text inputs with minimal character changes to mislead classifiers while preserving original interpretation&lt;/li&gt;&lt;li&gt;Evaluated against 7 NLP models and 3 interpretation models using benchmark datasets&lt;/li&gt;&lt;li&gt;Achieves significant accuracy reduction with an average of 2 character changes per input&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Eldor Abdukhamidov', 'Tamer Abuhmed', 'Joanna C. S. Santos', 'Mohammed Abuhamad']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial attacks', 'NLP security', 'interpretable AI', 'red teaming', 'text-based models']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2507.16164</guid><pubDate>Wed, 23 Jul 2025 00:00:00 +0000</pubDate></item><item><title>Dual Turing Test: A Framework for Detecting and Mitigating Undetectable AI</title><link>https://arxiv.org/abs/2507.15907</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces the Dual Turing Test framework for detecting and mitigating undetectable AI&lt;/li&gt;&lt;li&gt;Combines adversarial classification game with RL alignment pipeline&lt;/li&gt;&lt;li&gt;Incorporates undetectability detector and quality constraints in reward model&lt;/li&gt;&lt;li&gt;Proposes phased testing and iterative adversarial training&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Alberto Messina']&lt;/li&gt;&lt;li&gt;Tags: ['dual Turing test', 'adversarial training', 'RL alignment', 'safety evaluation', 'red teaming']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2507.15907</guid><pubDate>Wed, 23 Jul 2025 00:00:00 +0000</pubDate></item><item><title>Combining Cost-Constrained Runtime Monitors for AI Safety</title><link>https://arxiv.org/abs/2507.15886</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Presents a method to combine multiple runtime monitors for AI safety&lt;/li&gt;&lt;li&gt;Aims to maximize recall of safety interventions under budget constraints&lt;/li&gt;&lt;li&gt;Uses Neyman-Pearson lemma for optimal intervention allocation&lt;/li&gt;&lt;li&gt;Demonstrates significant recall improvement in code review setting&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Tim Tian Hua', 'James Baskerville', 'Henri Lemoine', 'Mia Hopman', 'Aryan Bhatt', 'Tyler Tracy']&lt;/li&gt;&lt;li&gt;Tags: ['safety evaluation', 'runtime monitoring', 'cost optimization', 'algorithmic approach', 'multi-monitor combination']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2507.15886</guid><pubDate>Wed, 23 Jul 2025 00:00:00 +0000</pubDate></item><item><title>Small Edits, Big Consequences: Telling Good from Bad Robustness in Large Language Models</title><link>https://arxiv.org/abs/2507.15868</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Evaluates LLM robustness to three types of prompt perturbations: underspecification, lexical flip, and jargon inflation&lt;/li&gt;&lt;li&gt;Found over-robustness to underspecification (85% correct after 90% prompt removal) and under-sensitivity to lexical flips (54% correct after critical quantifier change)&lt;/li&gt;&lt;li&gt;Reasoning-tuned models performed worse than base models on lexical flips&lt;/li&gt;&lt;li&gt;Advocates for differential sensitivity in evaluation and training protocols&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Altynbek Ismailov', 'Salia Asanova']&lt;/li&gt;&lt;li&gt;Tags: ['robustness', 'adversarial prompting', 'safety evaluation', 'prompt injection']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2507.15868</guid><pubDate>Wed, 23 Jul 2025 00:00:00 +0000</pubDate></item><item><title>Frontier AI Risk Management Framework in Practice: A Risk Analysis Technical Report</title><link>https://arxiv.org/abs/2507.16534</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Assesses frontier AI risks in seven critical areas using E-T-C framework&lt;/li&gt;&lt;li&gt;Evaluates models against red/yellow lines to categorize risk zones&lt;/li&gt;&lt;li&gt;Recent models found in green/yellow zones, urging collective mitigation&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Shanghai AI Lab', ':', 'Xiaoyang Chen', 'Yunhao Chen', 'Zeren Chen', 'Zhiyun Chen', 'Hanyun Cui', 'Yawen Duan', 'Jiaxuan Guo', 'Qi Guo', 'Xuhao Hu', 'Hong Huang', 'Lige Huang', 'Chunxiao Li', 'Juncheng Li', 'Qihao Lin', 'Dongrui Liu', 'Xinmin Liu', 'Zicheng Liu', 'Chaochao Lu', 'Xiaoya Lu', 'Jingjing Qu', 'Qibing Ren', 'Jing Shao', 'Jingwei Shi', 'Jingwei Sun', 'Peng Wang', 'Weibing Wang', 'Jia Xu', 'Lewen Yan', 'Xiao Yu', 'Yi Yu', 'Boxuan Zhang', 'Jie Zhang', 'Weichen Zhang', 'Zhijie Zheng', 'Tianyi Zhou', 'Bowen Zhou']&lt;/li&gt;&lt;li&gt;Tags: ['risk management', 'security evaluation', 'safety standards', 'red teaming', 'AI governance']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2507.16534</guid><pubDate>Wed, 23 Jul 2025 00:00:00 +0000</pubDate></item><item><title>Does More Inference-Time Compute Really Help Robustness?</title><link>https://arxiv.org/abs/2507.15974</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Demonstrates smaller LLMs benefit from inference-time scaling&lt;/li&gt;&lt;li&gt;Challenges assumption of hidden reasoning steps&lt;/li&gt;&lt;li&gt;Identifies inverse scaling law under adversarial exposure&lt;/li&gt;&lt;li&gt;Discusses practical vulnerabilities in security-sensitive deployments&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Tong Wu', 'Chong Xiang', 'Jiachen T. Wang', 'Weichen Yu', 'Chawin Sitawarin', 'Vikash Sehwag', 'Prateek Mittal']&lt;/li&gt;&lt;li&gt;Tags: ['robustness', 'security', 'adversarial attacks', 'safety evaluation', 'inference scaling']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2507.15974</guid><pubDate>Wed, 23 Jul 2025 00:00:00 +0000</pubDate></item><item><title>The Recursive Coherence Principle: A Formal Constraint on Scalable Intelligence, Alignment, and Reasoning Architecture</title><link>https://arxiv.org/abs/2507.15880</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces Recursive Coherence Principle (RCP) and Functional Model of Intelligence (FMI)&lt;/li&gt;&lt;li&gt;Argues that semantic coherence is essential for scalable AI alignment and safety&lt;/li&gt;&lt;li&gt;Claims that common AI issues like misalignment and instability stem from coherence breakdown&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Andy E. Williams']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'safety', 'coherence', 'architecture']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2507.15880</guid><pubDate>Wed, 23 Jul 2025 00:00:00 +0000</pubDate></item></channel></rss>