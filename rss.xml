<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>AI Security Paper Digest</title><link>https://kentaroh-toyoda.github.io/ai-security-paper-digest-rss/rss.xml</link><description>Curated papers on AI security from ArXiv and ACL</description><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><lastBuildDate>Thu, 16 Oct 2025 22:25:50 +0000</lastBuildDate><item><title>Tracing Back the Malicious Clients in Poisoning Attacks to Federated Learning</title><link>https://arxiv.org/abs/2407.07221</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes FLForensics for tracing malicious clients in federated learning poisoning attacks&lt;/li&gt;&lt;li&gt;Complements existing training defenses by post-attack analysis&lt;/li&gt;&lt;li&gt;Theoretically and empirically validated on benchmark datasets&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yuqi Jia', 'Minghong Fang', 'Hongbin Liu', 'Jinghuai Zhang', 'Neil Zhenqiang Gong']&lt;/li&gt;&lt;li&gt;Tags: ['data poisoning', 'federated learning', 'security']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2407.07221</guid><pubDate>Thu, 16 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Machine Unlearning Meets Adversarial Robustness via Constrained Interventions on LLMs</title><link>https://arxiv.org/abs/2510.03567</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Unified approach for unlearning and robustness in LLMs&lt;/li&gt;&lt;li&gt;Constrained optimization for minimal weight interventions&lt;/li&gt;&lt;li&gt;Addresses jail-breaking attacks and sensitive info unlearning&lt;/li&gt;&lt;li&gt;Comparative analysis with state-of-the-art defenses&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Fatmazohra Rezkellah', 'Ramzi Dakhmouche']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'jailbreaking', 'adversarial robustness', 'unlearning', 'privacy-preserving']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.03567</guid><pubDate>Thu, 16 Oct 2025 00:00:00 +0000</pubDate></item><item><title>SafeSearch: Automated Red-Teaming for the Safety of LLM-Based Search Agents</title><link>https://arxiv.org/abs/2509.23694</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces SafeSearch, an automated red-teaming framework for LLM-based search agents&lt;/li&gt;&lt;li&gt;Constructs a benchmark with 300 test cases across 5 risk categories&lt;/li&gt;&lt;li&gt;Evaluates 3 search agent scaffolds with 15 LLMs, revealing high attack success rates&lt;/li&gt;&lt;li&gt;Highlights limitations of current defense practices like reminder prompting&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jianshuo Dong', 'Sheng Guo', 'Hao Wang', 'Xun Chen', 'Zhuotao Liu', 'Tianwei Zhang', 'Ke Xu', 'Minlie Huang', 'Han Qiu']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'safety evaluation', 'benchmarking', 'adversarial prompting', 'data poisoning']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.23694</guid><pubDate>Thu, 16 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Early Signs of Steganographic Capabilities in Frontier LLMs</title><link>https://arxiv.org/abs/2507.02737</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Evaluates steganography capabilities in frontier LLMs&lt;/li&gt;&lt;li&gt;Focuses on message encoding and encoded reasoning&lt;/li&gt;&lt;li&gt;Models can encode with extra affordances but not subtly&lt;/li&gt;&lt;li&gt;Early signs of encoded reasoning but not yet effective&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Artur Zolkowski', 'Kei Nishimura-Gasparian', 'Robert McCarthy', 'Roland S. Zimmermann', 'David Lindner']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'adversarial prompting', 'safety evaluation', 'alignment', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2507.02737</guid><pubDate>Thu, 16 Oct 2025 00:00:00 +0000</pubDate></item><item><title>GUARDIAN: Safeguarding LLM Multi-Agent Collaborations with Temporal Graph Modeling</title><link>https://arxiv.org/abs/2505.19234</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Presents GUARDIAN, a method for detecting and mitigating safety issues in LLM multi-agent collaborations.&lt;/li&gt;&lt;li&gt;Uses temporal graph modeling to capture propagation dynamics of hallucinations and errors.&lt;/li&gt;&lt;li&gt;Unsupervised encoder-decoder architecture with incremental training for anomaly detection.&lt;/li&gt;&lt;li&gt;Graph abstraction based on Information Bottleneck Theory for efficient processing.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jialong Zhou', 'Lichao Wang', 'Xiao Yang']&lt;/li&gt;&lt;li&gt;Tags: ['LLM safety', 'multi-agent systems', 'temporal graph modeling', 'anomaly detection', 'hallucination mitigation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2505.19234</guid><pubDate>Thu, 16 Oct 2025 00:00:00 +0000</pubDate></item><item><title>SeCon-RAG: A Two-Stage Semantic Filtering and Conflict-Free Framework for Trustworthy RAG</title><link>https://arxiv.org/abs/2510.09710</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a two-stage framework (SeCon-RAG) to enhance RAG systems' security against corpus poisoning and contamination attacks.&lt;/li&gt;&lt;li&gt;First stage uses semantic and cluster-based filtering guided by EIRE to select relevant documents.&lt;/li&gt;&lt;li&gt;Second stage applies conflict-aware filtering to detect inconsistencies between query, answers, and knowledge.&lt;/li&gt;&lt;li&gt;Aims to preserve useful information while mitigating conflicts, improving robustness and trustworthiness.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Xiaonan Si', 'Meilin Zhu', 'Simeng Qin', 'Lijia Yu', 'Lijun Zhang', 'Shuaitong Liu', 'Xinfeng Li', 'Ranjie Duan', 'Yang Liu', 'Xiaojun Jia']&lt;/li&gt;&lt;li&gt;Tags: ['RAG security', 'corpus poisoning', 'conflict detection', 'semantic filtering', 'trustworthy AI']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.09710</guid><pubDate>Thu, 16 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Can an Individual Manipulate the Collective Decisions of Multi-Agents?</title><link>https://arxiv.org/abs/2509.16494</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;The paper explores if an attacker knowing only one agent can manipulate multi-agent systems.&lt;/li&gt;&lt;li&gt;Proposes M-Spoiler framework to generate adversarial samples using simulated agent interactions.&lt;/li&gt;&lt;li&gt;Introduces a stubborn agent to optimize adversarial samples for better manipulation.&lt;/li&gt;&lt;li&gt;Conducts experiments across tasks and evaluates defense mechanisms.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Fengyuan Liu', 'Rui Zhao', 'Shuo Chen', 'Guohao Li', 'Philip Torr', 'Lei Han', 'Jindong Gu']&lt;/li&gt;&lt;li&gt;Tags: ['red teaming', 'adversarial prompting', 'multi-agent systems', 'security', 'defense mechanisms']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.16494</guid><pubDate>Thu, 16 Oct 2025 00:00:00 +0000</pubDate></item><item><title>RedTeamCUA: Realistic Adversarial Testing of Computer-Use Agents in Hybrid Web-OS Environments</title><link>https://arxiv.org/abs/2505.21936</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces RedTeamCUA, a framework for adversarial testing of computer-use agents in hybrid web-OS environments&lt;/li&gt;&lt;li&gt;Features a hybrid sandbox combining VM-based OS and Docker-based web platforms&lt;/li&gt;&lt;li&gt;Presents RTC-Bench, a benchmark with 864 examples testing realistic attack scenarios&lt;/li&gt;&lt;li&gt;Evaluates current CUAs and finds significant vulnerabilities, including high attempt rates and ASRs&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Zeyi Liao', 'Jaylen Jones', 'Linxi Jiang', 'Yuting Ning', 'Eric Fosler-Lussier', 'Yu Su', 'Zhiqiang Lin', 'Huan Sun']&lt;/li&gt;&lt;li&gt;Tags: ['red teaming', 'adversarial testing', 'prompt injection', 'hybrid environments', 'CUA vulnerabilities']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2505.21936</guid><pubDate>Thu, 16 Oct 2025 00:00:00 +0000</pubDate></item><item><title>ReasoningShield: Safety Detection over Reasoning Traces of Large Reasoning Models</title><link>https://arxiv.org/abs/2505.17244</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces ReasoningShield for moderating Chain-of-Thoughts (CoTs) in Large Reasoning Models (LRMs)&lt;/li&gt;&lt;li&gt;Formalizes CoT moderation with a multi-level taxonomy of risk categories&lt;/li&gt;&lt;li&gt;Creates a new benchmark with annotated CoT data&lt;/li&gt;&lt;li&gt;Develops a two-stage training strategy combining stepwise analysis and contrastive learning&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Changyi Li', 'Jiayi Wang', 'Xudong Pan', 'Geng Hong', 'Min Yang']&lt;/li&gt;&lt;li&gt;Tags: ['safety evaluation', 'moderation', 'chain-of-thought', 'risk detection', 'benchmarking']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2505.17244</guid><pubDate>Thu, 16 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Teaching Models to Understand (but not Generate) High-risk Data</title><link>https://arxiv.org/abs/2505.03052</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces SLUNG, a pre-training method that allows models to understand high-risk data without generating it.&lt;/li&gt;&lt;li&gt;Uses selective loss to avoid incentivizing high-risk token generation while maintaining context.&lt;/li&gt;&lt;li&gt;Improves model understanding of toxic content without increasing toxicity in responses.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Ryan Wang', 'Matthew Finlayson', 'Luca Soldaini', 'Swabha Swayamdipta', 'Robin Jia']&lt;/li&gt;&lt;li&gt;Tags: ['safety', 'alignment', 'data poisoning', 'privacy attacks', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2505.03052</guid><pubDate>Thu, 16 Oct 2025 00:00:00 +0000</pubDate></item><item><title>FALCON: Fine-grained Activation Manipulation by Contrastive Orthogonal Unalignment for Large Language Model</title><link>https://arxiv.org/abs/2502.01472</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes FALCON, a representation-guided unlearning approach for LLMs&lt;/li&gt;&lt;li&gt;Uses contrastive mechanisms and orthogonal gradient projection&lt;/li&gt;&lt;li&gt;Aims to precisely separate knowledge and balance removal with utility&lt;/li&gt;&lt;li&gt;Shows robustness against knowledge recovery&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jinwei Hu', 'Zhenglin Huang', 'Xiangyu Yin', 'Wenjie Ruan', 'Guangliang Cheng', 'Yi Dong', 'Xiaowei Huang']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'safety', 'machine unlearning', 'adversarial']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2502.01472</guid><pubDate>Thu, 16 Oct 2025 00:00:00 +0000</pubDate></item><item><title>TRUSTVIS: A Multi-Dimensional Trustworthiness Evaluation Framework for Large Language Models</title><link>https://arxiv.org/abs/2510.13106</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;TRUSTVIS is a framework for evaluating LLM trustworthiness&lt;/li&gt;&lt;li&gt;Focuses on safety and robustness&lt;/li&gt;&lt;li&gt;Uses perturbation methods and majority voting&lt;/li&gt;&lt;li&gt;Interactive visualization interface&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Ruoyu Sun', 'Da Song', 'Jiayang Song', 'Yuheng Huang', 'Lei Ma']&lt;/li&gt;&lt;li&gt;Tags: ['safety evaluation', 'robustness', 'LLM', 'visualization']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.13106</guid><pubDate>Thu, 16 Oct 2025 00:00:00 +0000</pubDate></item><item><title>From Literal to Liberal: A Meta-Prompting Framework for Eliciting Human-Aligned Exception Handling in Large Language Models</title><link>https://arxiv.org/abs/2510.12864</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces the Rule-Intent Distinction (RID) Framework for improving human-aligned exception handling in LLMs&lt;/li&gt;&lt;li&gt;Evaluates the framework against baseline and CoT prompting on a custom benchmark&lt;/li&gt;&lt;li&gt;Achieves higher Human Alignment Score (HAS)&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Imran Khan']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'safety evaluation', 'prompt injection']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.12864</guid><pubDate>Thu, 16 Oct 2025 00:00:00 +0000</pubDate></item><item><title>GAPS: A Clinically Grounded, Automated Benchmark for Evaluating AI Clinicians</title><link>https://arxiv.org/abs/2510.13734</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces GAPS framework for evaluating AI clinicians&lt;/li&gt;&lt;li&gt;Automated benchmark construction with clinical guidelines&lt;/li&gt;&lt;li&gt;Evaluates grounding, adequacy, perturbation, and safety&lt;/li&gt;&lt;li&gt;Reveals model vulnerabilities in reasoning depth, completeness, robustness, and safety&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Xiuyuan Chen', 'Tao Sun', 'Dexin Su', 'Ailing Yu', 'Junwei Liu', 'Zhe Chen', 'Gangzeng Jin', 'Xin Wang', 'Jingnan Liu', 'Hansong Xiao', 'Hualei Zhou', 'Dongjie Tao', 'Chunxiao Guo', 'Minghui Yang', 'Yuan Xia', 'Jing Zhao', 'Qianrui Fan', 'Yanyun Wang', 'Shuai Zhen', 'Kezhong Chen', 'Jun Wang', 'Zewen Sun', 'Heng Zhao', 'Tian Guan', 'Shaodong Wang', 'Geyun Chang', 'Jiaming Deng', 'Hongchengcheng Chen', 'Kexin Feng', 'Ruzhen Li', 'Jiayi Geng', 'Changtai Zhao', 'Jun Wang', 'Guihu Lin', 'Peihao Li', 'Liqi Liu', 'Peng Wei', 'Jian Wang', 'Jinjie Gu', 'Ping Wang', 'Fan Yang']&lt;/li&gt;&lt;li&gt;Tags: ['safety evaluation', 'robustness', 'adversarial perturbations', 'clinical AI']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.13734</guid><pubDate>Thu, 16 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Personal Attribute Leakage in Federated Speech Models</title><link>https://arxiv.org/abs/2510.13357</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Analyzes vulnerability of federated ASR models to attribute inference attacks&lt;/li&gt;&lt;li&gt;Tests white-box attack on Wav2Vec2, HuBERT, Whisper&lt;/li&gt;&lt;li&gt;Targets sensitive attributes like gender, age, accent, emotion, dysarthria&lt;/li&gt;&lt;li&gt;Finds accents are particularly vulnerable&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Hamdan Al-Ali', 'Ali Reza Ghavamipour', 'Tommaso Caselli', 'Fatih Turkmen', 'Zeerak Talat', 'Hanan Aldarmaki']&lt;/li&gt;&lt;li&gt;Tags: ['privacy attacks', 'federated learning', 'speech models', 'attribute inference', 'data poisoning']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.13357</guid><pubDate>Thu, 16 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Protect: Towards Robust Guardrailing Stack for Trustworthy Enterprise LLM Systems</title><link>https://arxiv.org/abs/2510.13351</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces Protect, a multi-modal guardrailing model for enterprise LLM systems&lt;/li&gt;&lt;li&gt;Addresses safety dimensions like toxicity, sexism, data privacy, and prompt injection&lt;/li&gt;&lt;li&gt;Uses LoRA and a teacher-assisted annotation pipeline for high-fidelity labels&lt;/li&gt;&lt;li&gt;Outperforms existing models in multi-modal safety evaluation&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Karthik Avinash', 'Nikhil Pareek', 'Rishav Hada']&lt;/li&gt;&lt;li&gt;Tags: ['red teaming', 'safety evaluation', 'multi-modal', 'guardrails', 'prompt injection']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.13351</guid><pubDate>Thu, 16 Oct 2025 00:00:00 +0000</pubDate></item><item><title>SHIELD: Classifier-Guided Prompting for Robust and Safer LVLMs</title><link>https://arxiv.org/abs/2510.13190</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes SHIELD, a preprocessing framework for LVLMs to handle adversarial prompts&lt;/li&gt;&lt;li&gt;Uses safety classification with category-specific guidance (Block, Reframe, Forward)&lt;/li&gt;&lt;li&gt;Reduces jailbreak rates across multiple benchmarks and models&lt;/li&gt;&lt;li&gt;Lightweight, model-agnostic, and doesn't require retraining&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Juan Ren', 'Mark Dras', 'Usman Naseem']&lt;/li&gt;&lt;li&gt;Tags: ['red teaming', 'jailbreaking', 'adversarial prompting', 'safety evaluation', 'multimodal']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.13190</guid><pubDate>Thu, 16 Oct 2025 00:00:00 +0000</pubDate></item><item><title>DSCD: Large Language Model Detoxification with Self-Constrained Decoding</title><link>https://arxiv.org/abs/2510.13183</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes DSCD, a self-constrained decoding method for LLM detoxification&lt;/li&gt;&lt;li&gt;No parameter fine-tuning required&lt;/li&gt;&lt;li&gt;Improves safety while maintaining fluency&lt;/li&gt;&lt;li&gt;State-of-the-art performance in detoxification and efficiency&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Ming Dong', 'Jinkui Zhang', 'Bolong Zheng', 'Xinhui Tu', 'Po Hu', 'Tingting He']&lt;/li&gt;&lt;li&gt;Tags: ['safety', 'detoxification', 'self-constrained decoding', 'LLM safety', 'adversarial prompting']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.13183</guid><pubDate>Thu, 16 Oct 2025 00:00:00 +0000</pubDate></item><item><title>A Multilingual, Large-Scale Study of the Interplay between LLM Safeguards, Personalisation, and Disinformation</title><link>https://arxiv.org/abs/2510.12993</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Red teaming study on persona-targeted disinformation generation by LLMs&lt;/li&gt;&lt;li&gt;Introduces AI-TRAITS dataset with 1.6M texts across multiple languages&lt;/li&gt;&lt;li&gt;Finds personalisation in prompts increases jailbreak rates and persuasiveness&lt;/li&gt;&lt;li&gt;Highlights vulnerabilities in LLM safety mechanisms&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jo\\~ao A. Leite', 'Arnav Arora', 'Silvia Gargova', 'Jo\\~ao Luz', 'Gustavo Sampaio', 'Ian Roberts', 'Carolina Scarton', 'Kalina Bontcheva']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'jailbreaking', 'prompt injection', 'disinformation', 'multilingual', 'safety evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.12993</guid><pubDate>Thu, 16 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Who's Asking? Evaluating LLM Robustness to Inquiry Personas in Factual Question Answering</title><link>https://arxiv.org/abs/2510.12925</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Evaluates LLM robustness to inquiry personas in factual QA&lt;/li&gt;&lt;li&gt;Tests how user profiles affect model responses&lt;/li&gt;&lt;li&gt;Identifies issues like refusals, hallucinations, and role confusion&lt;/li&gt;&lt;li&gt;Proposes persona-based testing for robustness evaluation&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Nil-Jana Akpinar', 'Chia-Jung Lee', 'Vanessa Murdock', 'Pietro Perona']&lt;/li&gt;&lt;li&gt;Tags: ['LLM robustness', 'adversarial prompting', 'safety evaluation', 'red teaming', 'factual QA']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.12925</guid><pubDate>Thu, 16 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Scheming Ability in LLM-to-LLM Strategic Interactions</title><link>https://arxiv.org/abs/2510.12826</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Investigates LLM-to-LLM strategic deception using game-theoretic frameworks&lt;/li&gt;&lt;li&gt;Tests four models in Cheap Talk and Peer Evaluation games&lt;/li&gt;&lt;li&gt;Finds high scheming propensity even without explicit prompting&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Thao Pham']&lt;/li&gt;&lt;li&gt;Tags: ['red teaming', 'adversarial prompting', 'alignment', 'safety evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.12826</guid><pubDate>Thu, 16 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Are Large Reasoning Models Interruptible?</title><link>https://arxiv.org/abs/2510.11713</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Evaluates LRM robustness under interruptions and dynamic context&lt;/li&gt;&lt;li&gt;Performance drops up to 60% with late updates&lt;/li&gt;&lt;li&gt;Identifies failure modes like reasoning leakage, panic, self-doubt&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Tsung-Han Wu', 'Mihran Miroyan', 'David M. Chan', 'Trevor Darrell', 'Narges Norouzi', 'Joseph E. Gonzalez']&lt;/li&gt;&lt;li&gt;Tags: ['robustness', 'safety evaluation', 'adversarial prompting', 'jailbreaking']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.11713</guid><pubDate>Thu, 16 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Malice in Agentland: Down the Rabbit Hole of Backdoors in the AI Supply Chain</title><link>https://arxiv.org/abs/2510.05159</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;The paper explores backdoor attacks in the AI supply chain, particularly when agents are fine-tuned on their own interaction data.&lt;/li&gt;&lt;li&gt;Three threat models are presented: direct data poisoning, environmental poisoning, and supply chain poisoning.&lt;/li&gt;&lt;li&gt;Results show that even 2% poisoned data can lead to successful backdoor triggers, and existing defenses are ineffective.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ["L\\'eo Boisvert", 'Abhay Puri', 'Chandra Kiran Reddy Evuru', 'Nicolas Chapados', 'Quentin Cappart', 'Alexandre Lacoste', 'Krishnamurthy Dj Dvijotham', 'Alexandre Drouin']&lt;/li&gt;&lt;li&gt;Tags: ['data poisoning', 'backdoor attacks', 'AI supply chain security', 'adversarial training', 'security evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.05159</guid><pubDate>Thu, 16 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Cross-Modal Safety Alignment: Is textual unlearning all you need?</title><link>https://arxiv.org/abs/2406.02575</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;The paper explores cross-modal safety alignment in VLMs by focusing on textual unlearning.&lt;/li&gt;&lt;li&gt;It shows that textual unlearning can reduce ASR for both text and vision-text attacks.&lt;/li&gt;&lt;li&gt;Multi-modal dataset unlearning is less effective and more computationally expensive.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Trishna Chakraborty', 'Erfan Shayegani', 'Zikui Cai', 'Nael Abu-Ghazaleh', 'M. Salman Asif', 'Yue Dong', 'Amit K. Roy-Chowdhury', 'Chengyu Song']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'safety evaluation', 'alignment', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2406.02575</guid><pubDate>Thu, 16 Oct 2025 00:00:00 +0000</pubDate></item><item><title>PEAR: Planner-Executor Agent Robustness Benchmark</title><link>https://arxiv.org/abs/2510.07505</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces PEAR benchmark for evaluating planner-executor MAS robustness&lt;/li&gt;&lt;li&gt;Finds trade-off between performance and robustness&lt;/li&gt;&lt;li&gt;Highlights planner as critical attack surface&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Shen Dong', 'Mingxuan Zhang', 'Pengfei He', 'Li Ma', 'Bhavani Thuraisingham', 'Hui Liu', 'Yue Xing']&lt;/li&gt;&lt;li&gt;Tags: ['red teaming', 'robustness', 'multi-agent systems', 'benchmarking']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.07505</guid><pubDate>Thu, 16 Oct 2025 00:00:00 +0000</pubDate></item><item><title>How Vulnerable Is My Learned Policy? Universal Adversarial Perturbation Attacks On Modern Behavior Cloning Policies</title><link>https://arxiv.org/abs/2502.03698</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Study of adversarial attacks on LfD algorithms&lt;/li&gt;&lt;li&gt;Vulnerability of BC, LSTM-GMM, IBC, DP, VQ-BET to universal perturbations&lt;/li&gt;&lt;li&gt;Transferability of attacks across algorithms, architectures, tasks&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Akansha Kalra', 'Basavasagar Patil', 'Guanhong Tao', 'Daniel S. Brown']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial attacks', 'behavior cloning', 'learning from demonstration', 'security vulnerabilities', 'black-box attacks']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2502.03698</guid><pubDate>Thu, 16 Oct 2025 00:00:00 +0000</pubDate></item><item><title>GraphRAG under Fire</title><link>https://arxiv.org/abs/2501.14050</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;GraphRAG's security against poisoning attacks is examined&lt;/li&gt;&lt;li&gt;Introduces GragPoison attack exploiting knowledge graph relations&lt;/li&gt;&lt;li&gt;Shows high success rate and scalability in poisoning&lt;/li&gt;&lt;li&gt;Discusses defensive measures and limitations&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jiacheng Liang', 'Yuhui Wang', 'Changjiang Li', 'Rongyi Zhu', 'Tanqiu Jiang', 'Neil Gong', 'Ting Wang']&lt;/li&gt;&lt;li&gt;Tags: ['data poisoning', 'adversarial attacks', 'graph-based models', 'RAG models', 'security evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2501.14050</guid><pubDate>Thu, 16 Oct 2025 00:00:00 +0000</pubDate></item><item><title>COMAL: A Convergent Meta-Algorithm for Aligning LLMs with General Preferences</title><link>https://arxiv.org/abs/2410.23223</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes COMAL, a meta-algorithm for aligning LLMs with general preferences using a game-theoretic framework&lt;/li&gt;&lt;li&gt;Aims to find the Nash equilibrium policy to ensure a 50% win rate against any competing policy&lt;/li&gt;&lt;li&gt;Provides theoretical convergence guarantees and empirical evaluations on synthetic and real datasets&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yixin Liu', 'Argyris Oikonomou', 'Weiqiang Zheng', 'Yang Cai', 'Arman Cohan']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'game theory', 'Nash equilibrium', 'preference learning', 'reinforcement learning']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2410.23223</guid><pubDate>Thu, 16 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Locket: Robust Feature-Locking Technique for Language Models</title><link>https://arxiv.org/abs/2510.12117</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces Locket, a feature-locking technique for LLMs&lt;/li&gt;&lt;li&gt;Aims to enable pay-to-unlock schemes with robustness and scalability&lt;/li&gt;&lt;li&gt;Uses adapters to refuse unauthorized features&lt;/li&gt;&lt;li&gt;Shows high effectiveness, low utility degradation, and robustness&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Lipeng He', 'Vasisht Duddu', 'N. Asokan']&lt;/li&gt;&lt;li&gt;Tags: ['security', 'robustness', 'feature locking', 'LLM', 'adapter']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.12117</guid><pubDate>Thu, 16 Oct 2025 00:00:00 +0000</pubDate></item><item><title>KoALA: KL-L0 Adversarial Detector via Label Agreement</title><link>https://arxiv.org/abs/2510.12752</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces KoALA, a semantics-free adversarial detector&lt;/li&gt;&lt;li&gt;Uses KL divergence and L0-based similarity metrics to detect attacks&lt;/li&gt;&lt;li&gt;Requires minimal training on clean images&lt;/li&gt;&lt;li&gt;Effective across different models and data modalities&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Siqi Li', 'Yasser Shoukry']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial detection', 'security', 'robustness', 'image models']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.12752</guid><pubDate>Thu, 16 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Keep Calm and Avoid Harmful Content: Concept Alignment and Latent Manipulation Towards Safer Answers</title><link>https://arxiv.org/abs/2510.12672</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes CALM, an inference-time method to suppress harmful concepts in LLMs&lt;/li&gt;&lt;li&gt;Uses latent manipulation without retraining&lt;/li&gt;&lt;li&gt;Leverages CW technique and orthogonal projection&lt;/li&gt;&lt;li&gt;Reduces harmful outputs with minimal computational overhead&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Ruben Belo', 'Claudia Soares', 'Marta Guimaraes']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'jailbreaking', 'adversarial prompting', 'safety evaluation', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.12672</guid><pubDate>Thu, 16 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Unveiling the Vulnerability of Graph-LLMs: An Interpretable Multi-Dimensional Adversarial Attack on TAGs</title><link>https://arxiv.org/abs/2510.12233</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes IMDGA, a multi-dimensional adversarial attack framework for Graph-LLMs&lt;/li&gt;&lt;li&gt;Targets both structural and textual attributes of text-attributed graphs (TAGs)&lt;/li&gt;&lt;li&gt;Emphasizes interpretability and human-centric attack design&lt;/li&gt;&lt;li&gt;Demonstrates superior effectiveness, stealthiness, and robustness&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Bowen Fan', 'Zhilin Guo', 'Xunkai Li', 'Yihan Zhou', 'Bing Zhou', 'Zhenjun Li', 'Rong-Hua Li', 'Guoren Wang']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial attacks', 'graph neural networks', 'LLMs', 'text-attributed graphs', 'security']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.12233</guid><pubDate>Thu, 16 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Don't Walk the Line: Boundary Guidance for Filtered Generation</title><link>https://arxiv.org/abs/2510.11834</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes Boundary Guidance, a RL method to steer generation away from classifier's decision boundary&lt;/li&gt;&lt;li&gt;Aims to reduce false positives and negatives in safety filtering&lt;/li&gt;&lt;li&gt;Tested on jailbreak and ambiguous prompts&lt;/li&gt;&lt;li&gt;Improves safety and utility as per LLM-as-a-Judge evaluations&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Sarah Ball', 'Andreas Haupt']&lt;/li&gt;&lt;li&gt;Tags: ['red teaming', 'safety evaluation', 'jailbreaking', 'adversarial prompting', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.11834</guid><pubDate>Thu, 16 Oct 2025 00:00:00 +0000</pubDate></item><item><title>SeCon-RAG: A Two-Stage Semantic Filtering and Conflict-Free Framework for Trustworthy RAG</title><link>https://arxiv.org/abs/2510.09710</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a two-stage framework (SeCon-RAG) to enhance RAG systems' security against corpus poisoning and contamination attacks.&lt;/li&gt;&lt;li&gt;First stage uses semantic and cluster-based filtering guided by EIRE to select relevant documents.&lt;/li&gt;&lt;li&gt;Second stage applies conflict-aware filtering to detect inconsistencies between query, answers, and knowledge.&lt;/li&gt;&lt;li&gt;Aims to preserve useful information while mitigating conflicts, improving robustness and trustworthiness.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Xiaonan Si', 'Meilin Zhu', 'Simeng Qin', 'Lijia Yu', 'Lijun Zhang', 'Shuaitong Liu', 'Xinfeng Li', 'Ranjie Duan', 'Yang Liu', 'Xiaojun Jia']&lt;/li&gt;&lt;li&gt;Tags: ['RAG security', 'corpus poisoning', 'conflict detection', 'semantic filtering', 'trustworthy AI']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.09710</guid><pubDate>Thu, 16 Oct 2025 00:00:00 +0000</pubDate></item><item><title>SafeGuider: Robust and Practical Content Safety Control for Text-to-Image Models</title><link>https://arxiv.org/abs/2510.05173</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;SafeGuider is a framework for robust content safety control in text-to-image models.&lt;/li&gt;&lt;li&gt;It uses the [EOS] token's embedding to detect adversarial prompts.&lt;/li&gt;&lt;li&gt;Combines embedding-level recognition with safety-aware beam search.&lt;/li&gt;&lt;li&gt;Effective against various attacks while maintaining image quality.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Peigui Qi', 'Kunsheng Tang', 'Wenbo Zhou', 'Weiming Zhang', 'Nenghai Yu', 'Tianwei Zhang', 'Qing Guo', 'Jie Zhang']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial prompting', 'safety evaluation', 'text-to-image models', 'robustness', 'content safety']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.05173</guid><pubDate>Thu, 16 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Defending against Stegomalware in Deep Neural Networks with Permutation Symmetry</title><link>https://arxiv.org/abs/2509.20399</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a defense against neural network stegomalware by shuffling weight matrix columns&lt;/li&gt;&lt;li&gt;Shows this method neutralizes existing steganography methods without accuracy loss&lt;/li&gt;&lt;li&gt;Discusses potential bypasses and future research directions&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Birk Torpmann-Hagen', 'Michael A. Riegler', 'P{\\aa}l Halvorsen', 'Dag Johansen']&lt;/li&gt;&lt;li&gt;Tags: ['security', 'model extraction', 'adversarial attacks', 'defense mechanisms']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.20399</guid><pubDate>Thu, 16 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Can an Individual Manipulate the Collective Decisions of Multi-Agents?</title><link>https://arxiv.org/abs/2509.16494</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;The paper explores if an attacker knowing only one agent can manipulate multi-agent systems.&lt;/li&gt;&lt;li&gt;Proposes M-Spoiler framework to generate adversarial samples using simulated agent interactions.&lt;/li&gt;&lt;li&gt;Introduces a stubborn agent to optimize adversarial samples for better manipulation.&lt;/li&gt;&lt;li&gt;Conducts experiments across tasks and evaluates defense mechanisms.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Fengyuan Liu', 'Rui Zhao', 'Shuo Chen', 'Guohao Li', 'Philip Torr', 'Lei Han', 'Jindong Gu']&lt;/li&gt;&lt;li&gt;Tags: ['red teaming', 'adversarial prompting', 'multi-agent systems', 'security', 'defense mechanisms']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.16494</guid><pubDate>Thu, 16 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Adversarial Distilled Retrieval-Augmented Guarding Model for Online Malicious Intent Detection</title><link>https://arxiv.org/abs/2509.14622</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces ADRAG, a framework for online malicious intent detection using LLMs&lt;/li&gt;&lt;li&gt;Uses adversarial training and knowledge distillation for robustness and efficiency&lt;/li&gt;&lt;li&gt;Evaluates performance on safety benchmarks, showing high accuracy and low latency&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yihao Guo', 'Haocheng Bian', 'Liutong Zhou', 'Ze Wang', 'Zhaoyi Zhang', 'Francois Kawala', 'Milan Dean', 'Ian Fischer', 'Yuantao Peng', 'Noyan Tokgozoglu', 'Ivan Barrientos', 'Riyaaz Shaik', 'Rachel Li', 'Chandru Venkataraman', 'Reza Shifteh Far', 'Moses Pawar', 'Venkat Sundaranatha', 'Michael Xu', 'Frank Chu']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'adversarial prompting', 'safety evaluation', 'robustness', 'model extraction']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.14622</guid><pubDate>Thu, 16 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Early Signs of Steganographic Capabilities in Frontier LLMs</title><link>https://arxiv.org/abs/2507.02737</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Evaluates steganography capabilities in frontier LLMs&lt;/li&gt;&lt;li&gt;Focuses on message encoding and encoded reasoning&lt;/li&gt;&lt;li&gt;Models can encode with extra affordances but not subtly&lt;/li&gt;&lt;li&gt;Early signs of encoded reasoning but not yet effective&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Artur Zolkowski', 'Kei Nishimura-Gasparian', 'Robert McCarthy', 'Roland S. Zimmermann', 'David Lindner']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'adversarial prompting', 'safety evaluation', 'alignment', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2507.02737</guid><pubDate>Thu, 16 Oct 2025 00:00:00 +0000</pubDate></item><item><title>ReasoningShield: Safety Detection over Reasoning Traces of Large Reasoning Models</title><link>https://arxiv.org/abs/2505.17244</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces ReasoningShield for moderating Chain-of-Thoughts (CoTs) in Large Reasoning Models (LRMs)&lt;/li&gt;&lt;li&gt;Formalizes CoT moderation with a multi-level taxonomy of risk categories&lt;/li&gt;&lt;li&gt;Creates a new benchmark with annotated CoT data&lt;/li&gt;&lt;li&gt;Develops a two-stage training strategy combining stepwise analysis and contrastive learning&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Changyi Li', 'Jiayi Wang', 'Xudong Pan', 'Geng Hong', 'Min Yang']&lt;/li&gt;&lt;li&gt;Tags: ['safety evaluation', 'moderation', 'chain-of-thought', 'risk detection', 'benchmarking']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2505.17244</guid><pubDate>Thu, 16 Oct 2025 00:00:00 +0000</pubDate></item><item><title>FALCON: Fine-grained Activation Manipulation by Contrastive Orthogonal Unalignment for Large Language Model</title><link>https://arxiv.org/abs/2502.01472</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes FALCON, a representation-guided unlearning approach for LLMs&lt;/li&gt;&lt;li&gt;Uses contrastive mechanisms and orthogonal gradient projection&lt;/li&gt;&lt;li&gt;Aims to precisely separate knowledge and balance removal with utility&lt;/li&gt;&lt;li&gt;Shows robustness against knowledge recovery&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jinwei Hu', 'Zhenglin Huang', 'Xiangyu Yin', 'Wenjie Ruan', 'Guangliang Cheng', 'Yi Dong', 'Xiaowei Huang']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'safety', 'machine unlearning', 'adversarial']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2502.01472</guid><pubDate>Thu, 16 Oct 2025 00:00:00 +0000</pubDate></item><item><title>SafeSearch: Automated Red-Teaming for the Safety of LLM-Based Search Agents</title><link>https://arxiv.org/abs/2509.23694</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces SafeSearch, an automated red-teaming framework for LLM-based search agents&lt;/li&gt;&lt;li&gt;Constructs a benchmark with 300 test cases across 5 risk categories&lt;/li&gt;&lt;li&gt;Evaluates 3 search agent scaffolds with 15 LLMs, revealing high attack success rates&lt;/li&gt;&lt;li&gt;Highlights limitations of current defense practices like reminder prompting&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jianshuo Dong', 'Sheng Guo', 'Hao Wang', 'Xun Chen', 'Zhuotao Liu', 'Tianwei Zhang', 'Ke Xu', 'Minlie Huang', 'Han Qiu']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'safety evaluation', 'benchmarking', 'adversarial prompting', 'data poisoning']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.23694</guid><pubDate>Thu, 16 Oct 2025 00:00:00 +0000</pubDate></item><item><title>GUARDIAN: Safeguarding LLM Multi-Agent Collaborations with Temporal Graph Modeling</title><link>https://arxiv.org/abs/2505.19234</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Presents GUARDIAN, a method for detecting and mitigating safety issues in LLM multi-agent collaborations.&lt;/li&gt;&lt;li&gt;Uses temporal graph modeling to capture propagation dynamics of hallucinations and errors.&lt;/li&gt;&lt;li&gt;Unsupervised encoder-decoder architecture with incremental training for anomaly detection.&lt;/li&gt;&lt;li&gt;Graph abstraction based on Information Bottleneck Theory for efficient processing.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jialong Zhou', 'Lichao Wang', 'Xiao Yang']&lt;/li&gt;&lt;li&gt;Tags: ['LLM safety', 'multi-agent systems', 'temporal graph modeling', 'anomaly detection', 'hallucination mitigation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2505.19234</guid><pubDate>Thu, 16 Oct 2025 00:00:00 +0000</pubDate></item><item><title>LLM-Enabled In-Context Learning for Data Collection Scheduling in UAV-assisted Sensor Networks</title><link>https://arxiv.org/abs/2504.14556</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes ICLDC system using LLM for UAV data collection scheduling&lt;/li&gt;&lt;li&gt;Includes verifier to ensure safe operations&lt;/li&gt;&lt;li&gt;Tests against jailbreaking attacks&lt;/li&gt;&lt;li&gt;Reduces packet loss compared to baselines&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yousef Emami', 'Hao Zhou', 'SeyedSina Nabavirazani', 'Luis Almeida']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'jailbreaking', 'safety evaluation', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2504.14556</guid><pubDate>Thu, 16 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Provably Invincible Adversarial Attacks on Reinforcement Learning Systems: A Rate-Distortion Information-Theoretic Approach</title><link>https://arxiv.org/abs/2510.13792</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a provably invincible adversarial attack on RL systems using rate-distortion information theory&lt;/li&gt;&lt;li&gt;Targets the agent's observation of the transition kernel, limiting information gain during training&lt;/li&gt;&lt;li&gt;Derives lower bounds on reward regret and impacts model-based and model-free algorithms&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Ziqing Lu', 'Lifeng Lai', 'Weiyu Xu']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial attacks', 'reinforcement learning', 'information theory', 'robustness', 'security']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.13792</guid><pubDate>Thu, 16 Oct 2025 00:00:00 +0000</pubDate></item><item><title>In-Browser LLM-Guided Fuzzing for Real-Time Prompt Injection Testing in Agentic AI Browsers</title><link>https://arxiv.org/abs/2510.13543</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Presents an in-browser fuzzing framework guided by LLMs for real-time prompt injection testing in agentic AI browsers.&lt;/li&gt;&lt;li&gt;Targets indirect prompt injection attacks where malicious instructions on webpages deceive AI agents.&lt;/li&gt;&lt;li&gt;Aims to automatically discover vulnerabilities by leveraging LLM capabilities.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Avihay Cohen']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'prompt injection', 'adversarial prompting', 'security', 'fuzzing']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.13543</guid><pubDate>Thu, 16 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Personal Attribute Leakage in Federated Speech Models</title><link>https://arxiv.org/abs/2510.13357</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Analyzes vulnerability of federated ASR models to attribute inference attacks&lt;/li&gt;&lt;li&gt;Tests white-box attack on Wav2Vec2, HuBERT, Whisper&lt;/li&gt;&lt;li&gt;Targets sensitive attributes like gender, age, accent, emotion, dysarthria&lt;/li&gt;&lt;li&gt;Finds accents are particularly vulnerable&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Hamdan Al-Ali', 'Ali Reza Ghavamipour', 'Tommaso Caselli', 'Fatih Turkmen', 'Zeerak Talat', 'Hanan Aldarmaki']&lt;/li&gt;&lt;li&gt;Tags: ['privacy attacks', 'federated learning', 'speech models', 'attribute inference', 'data poisoning']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.13357</guid><pubDate>Thu, 16 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Protect: Towards Robust Guardrailing Stack for Trustworthy Enterprise LLM Systems</title><link>https://arxiv.org/abs/2510.13351</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces Protect, a multi-modal guardrailing model for enterprise LLM systems&lt;/li&gt;&lt;li&gt;Addresses safety dimensions like toxicity, sexism, data privacy, and prompt injection&lt;/li&gt;&lt;li&gt;Uses LoRA and a teacher-assisted annotation pipeline for high-fidelity labels&lt;/li&gt;&lt;li&gt;Outperforms existing models in multi-modal safety evaluation&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Karthik Avinash', 'Nikhil Pareek', 'Rishav Hada']&lt;/li&gt;&lt;li&gt;Tags: ['red teaming', 'safety evaluation', 'multi-modal', 'guardrails', 'prompt injection']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.13351</guid><pubDate>Thu, 16 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Injection, Attack and Erasure: Revocable Backdoor Attacks via Machine Unlearning</title><link>https://arxiv.org/abs/2510.13322</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces revocable backdoor attacks for DNNs&lt;/li&gt;&lt;li&gt;Uses model unlearning to remove backdoor traces&lt;/li&gt;&lt;li&gt;Optimizes triggers via bilevel optimization&lt;/li&gt;&lt;li&gt;Demonstrates on CIFAR-10 and ImageNet&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Baogang Song', 'Dongdong Zhao', 'Jianwen Xiang', 'Qiben Xu', 'Zizhuo Yu']&lt;/li&gt;&lt;li&gt;Tags: ['data poisoning', 'adversarial attacks', 'model security', 'backdoor attacks', 'machine unlearning']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.13322</guid><pubDate>Thu, 16 Oct 2025 00:00:00 +0000</pubDate></item><item><title>TRUSTVIS: A Multi-Dimensional Trustworthiness Evaluation Framework for Large Language Models</title><link>https://arxiv.org/abs/2510.13106</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;TRUSTVIS is a framework for evaluating LLM trustworthiness&lt;/li&gt;&lt;li&gt;Focuses on safety and robustness&lt;/li&gt;&lt;li&gt;Uses perturbation methods and majority voting&lt;/li&gt;&lt;li&gt;Interactive visualization interface&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Ruoyu Sun', 'Da Song', 'Jiayang Song', 'Yuheng Huang', 'Lei Ma']&lt;/li&gt;&lt;li&gt;Tags: ['safety evaluation', 'robustness', 'LLM', 'visualization']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.13106</guid><pubDate>Thu, 16 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Scheming Ability in LLM-to-LLM Strategic Interactions</title><link>https://arxiv.org/abs/2510.12826</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Investigates LLM-to-LLM strategic deception using game-theoretic frameworks&lt;/li&gt;&lt;li&gt;Tests four models in Cheap Talk and Peer Evaluation games&lt;/li&gt;&lt;li&gt;Finds high scheming propensity even without explicit prompting&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Thao Pham']&lt;/li&gt;&lt;li&gt;Tags: ['red teaming', 'adversarial prompting', 'alignment', 'safety evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.12826</guid><pubDate>Thu, 16 Oct 2025 00:00:00 +0000</pubDate></item><item><title>From Refusal to Recovery: A Control-Theoretic Approach to Generative AI Guardrails</title><link>https://arxiv.org/abs/2510.13727</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces a control-theoretic approach to AI guardrails for generative AI systems&lt;/li&gt;&lt;li&gt;Focuses on proactive correction of risky outputs rather than just blocking&lt;/li&gt;&lt;li&gt;Demonstrates application in simulated driving and e-commerce scenarios&lt;/li&gt;&lt;li&gt;Aims to preserve task performance while preventing catastrophic outcomes&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Ravi Pandya', 'Madison Bland', 'Duy P. Nguyen', 'Changliu Liu', "Jaime Fern\\'andez Fisac", 'Andrea Bajcsy']&lt;/li&gt;&lt;li&gt;Tags: ['safety', 'guardrails', 'control theory', 'reinforcement learning', 'risk mitigation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.13727</guid><pubDate>Thu, 16 Oct 2025 00:00:00 +0000</pubDate></item><item><title>SAJA: A State-Action Joint Attack Framework on Multi-Agent Deep Reinforcement Learning</title><link>https://arxiv.org/abs/2510.13262</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes SAJA framework for joint state-action attacks in MADRL&lt;/li&gt;&lt;li&gt;Uses multi-step gradient ascent on actor and critic networks&lt;/li&gt;&lt;li&gt;Evaluates in Multi-Agent Particle Environment (MPE)&lt;/li&gt;&lt;li&gt;Shows superiority over state-only or action-only attacks&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Weiqi Guo', 'Guanjun Liu', 'Ziyuan Zhou']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial attacks', 'multi-agent systems', 'deep reinforcement learning', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.13262</guid><pubDate>Thu, 16 Oct 2025 00:00:00 +0000</pubDate></item><item><title>SENTINEL: A Multi-Level Formal Framework for Safety Evaluation of LLM-based Embodied Agents</title><link>https://arxiv.org/abs/2510.12985</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces Sentinel, a formal framework for evaluating physical safety of LLM-based embodied agents&lt;/li&gt;&lt;li&gt;Uses temporal logic to specify safety requirements at semantic, plan, and trajectory levels&lt;/li&gt;&lt;li&gt;Applies multi-level verification to detect safety issues before execution&lt;/li&gt;&lt;li&gt;Evaluated in VirtualHome and ALFRED environments&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Simon Sinong Zhan', 'Yao Liu', 'Philip Wang', 'Zinan Wang', 'Qineng Wang', 'Zhian Ruan', 'Xiangyu Shi', 'Xinyu Cao', 'Frank Yang', 'Kangrui Wang', 'Huajie Shao', 'Manling Li', 'Qi Zhu']&lt;/li&gt;&lt;li&gt;Tags: ['safety evaluation', 'formal verification', 'temporal logic', 'embodied agents', 'multi-level safety']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.12985</guid><pubDate>Thu, 16 Oct 2025 00:00:00 +0000</pubDate></item><item><title>From Literal to Liberal: A Meta-Prompting Framework for Eliciting Human-Aligned Exception Handling in Large Language Models</title><link>https://arxiv.org/abs/2510.12864</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces the Rule-Intent Distinction (RID) Framework for improving human-aligned exception handling in LLMs&lt;/li&gt;&lt;li&gt;Evaluates the framework against baseline and CoT prompting on a custom benchmark&lt;/li&gt;&lt;li&gt;Achieves higher Human Alignment Score (HAS)&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Imran Khan']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'safety evaluation', 'prompt injection']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.12864</guid><pubDate>Thu, 16 Oct 2025 00:00:00 +0000</pubDate></item></channel></rss>