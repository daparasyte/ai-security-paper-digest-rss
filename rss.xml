<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>AI Security Paper Digest</title><link>https://kentaroh-toyoda.github.io/ai-security-paper-digest-rss/rss.xml</link><description>Curated papers on AI security from OpenAlex and ArXiv</description><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><lastBuildDate>Fri, 04 Jul 2025 22:17:08 +0000</lastBuildDate><item><title>The Evolution of Dataset Distillation: Toward Scalable and Generalizable Solutions</title><link>https://arxiv.org/abs/2502.05673</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Comprehensive survey of recent dataset distillation advances&lt;/li&gt;&lt;li&gt;Covers scalability to large datasets like ImageNet-1K/21K&lt;/li&gt;&lt;li&gt;Includes methods for trajectory matching, gradient matching, distribution matching&lt;/li&gt;&lt;li&gt;Addresses robustness against adversarial and backdoor attacks&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Survey&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Ping Liu', 'Jiawei Du']&lt;/li&gt;&lt;li&gt;Tags: ['dataset distillation', 'robustness', 'adversarial attacks', 'backdoor attacks', 'survey']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2502.05673</guid><pubDate>Fri, 04 Jul 2025 00:00:00 +0000</pubDate></item><item><title>De-mark: Watermark Removal in Large Language Models</title><link>https://arxiv.org/abs/2410.13808</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces De-mark framework for removing n-gram watermarks from LLMs&lt;/li&gt;&lt;li&gt;Utilizes random selection probing to assess watermark strength and identify red-green lists&lt;/li&gt;&lt;li&gt;Demonstrates effectiveness on Llama3 and ChatGPT&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Ruibo Chen', 'Yihan Wu', 'Junfeng Guo', 'Heng Huang']&lt;/li&gt;&lt;li&gt;Tags: ['watermark removal', 'red teaming', 'adversarial prompting', 'security', 'LLM']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2410.13808</guid><pubDate>Fri, 04 Jul 2025 00:00:00 +0000</pubDate></item><item><title>Visual Contextual Attack: Jailbreaking MLLMs with Image-Driven Context Injection</title><link>https://arxiv.org/abs/2507.02844</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces VisCo Attack for visual-centric jailbreaking of MLLMs using image-driven context injection&lt;/li&gt;&lt;li&gt;Employs four visual-focused strategies and dynamic auxiliary image generation&lt;/li&gt;&lt;li&gt;Achieves 85% attack success rate against GPT-4o with toxicity score 4.78&lt;/li&gt;&lt;li&gt;Code available at https://github.com/Dtc7w3PQ/Visco-Attack&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Ziqi Miao', 'Yi Ding', 'Lijun Li', 'Jing Shao']&lt;/li&gt;&lt;li&gt;Tags: ['jailbreaking', 'adversarial prompting', 'visual attack', 'red teaming', 'multimodal models']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2507.02844</guid><pubDate>Fri, 04 Jul 2025 00:00:00 +0000</pubDate></item><item><title>SecAlign: Defending Against Prompt Injection with Preference Optimization</title><link>https://arxiv.org/abs/2410.05451</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes SecAlign defense against prompt injection attacks using preference optimization&lt;/li&gt;&lt;li&gt;Constructs preference dataset with prompt-injected inputs and secure vs insecure outputs&lt;/li&gt;&lt;li&gt;Reduces success rates of prompt injections to &lt;10% even against sophisticated attacks&lt;/li&gt;&lt;li&gt;Maintains similar utility to original model&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Sizhe Chen', 'Arman Zharmagambetov', 'Saeed Mahloujifar', 'Kamalika Chaudhuri', 'David Wagner', 'Chuan Guo']&lt;/li&gt;&lt;li&gt;Tags: ['prompt injection', 'preference optimization', 'alignment', 'red teaming', 'security']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2410.05451</guid><pubDate>Fri, 04 Jul 2025 00:00:00 +0000</pubDate></item><item><title>Evaluating Frontier Models for Stealth and Situational Awareness</title><link>https://arxiv.org/abs/2505.01420</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces evaluations for stealth and situational awareness capabilities&lt;/li&gt;&lt;li&gt;Proposes a safety case framework based on these evaluations&lt;/li&gt;&lt;li&gt;Tests current frontier models and finds no concerning behavior&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Mary Phuong', 'Roland S. Zimmermann', 'Ziyue Wang', 'David Lindner', 'Victoria Krakovna', 'Sarah Cogan', 'Allan Dafoe', 'Lewis Ho', 'Rohin Shah']&lt;/li&gt;&lt;li&gt;Tags: ['safety evaluation', 'alignment', 'model capabilities']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2505.01420</guid><pubDate>Fri, 04 Jul 2025 00:00:00 +0000</pubDate></item><item><title>StructTransform: A Scalable Attack Surface for Safety-Aligned Large Language Models</title><link>https://arxiv.org/abs/2502.11853</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces structure transformation attacks on LLM alignment&lt;/li&gt;&lt;li&gt;Achieves high success rates (90-96%) even against state-of-the-art models&lt;/li&gt;&lt;li&gt;Evaluates existing safety defenses and finds them ineffective&lt;/li&gt;&lt;li&gt;Demonstrates generation of malware and fraudulent messages using the attack&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Shehel Yoosuf', 'Temoor Ali', 'Ahmed Lekssays', 'Mashael AlSabah', 'Issa Khalil']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'adversarial prompting', 'alignment', 'safety evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2502.11853</guid><pubDate>Fri, 04 Jul 2025 00:00:00 +0000</pubDate></item><item><title>Hardware and Software Platform Inference</title><link>https://arxiv.org/abs/2411.05197</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces HSPI method to infer GPU and software stack from LLM behavior&lt;/li&gt;&lt;li&gt;Aims to verify service authenticity against advertised hardware&lt;/li&gt;&lt;li&gt;Evaluates accuracy in both white-box and black-box settings&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Cheng Zhang', 'Hanna Foerster', 'Robert D. Mullins', 'Yiren Zhao', 'Ilia Shumailov']&lt;/li&gt;&lt;li&gt;Tags: ['red teaming', 'security standards', 'safety evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2411.05197</guid><pubDate>Fri, 04 Jul 2025 00:00:00 +0000</pubDate></item><item><title>LLM Hypnosis: Exploiting User Feedback for Unauthorized Knowledge Injection to All Users</title><link>https://arxiv.org/abs/2507.02850</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Describes a vulnerability where user feedback manipulation can inject unauthorized knowledge into LLMs&lt;/li&gt;&lt;li&gt;Attack uses upvote/downvote signals to influence preference tuning during model training&lt;/li&gt;&lt;li&gt;Demonstrates ability to insert new facts, modify code generation, and spread fake news&lt;/li&gt;&lt;li&gt;Highlights risks of data poisoning through user interaction&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Almog Hilel', 'Idan Shenfeld', 'Leshem Choshen', 'Jacob Andreas']&lt;/li&gt;&lt;li&gt;Tags: ['data_poisoning', 'security', 'red_team', 'alignment', 'adversarial_prompting']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2507.02850</guid><pubDate>Fri, 04 Jul 2025 00:00:00 +0000</pubDate></item><item><title>Threat Modeling for AI: The Case for an Asset-Centric Approach</title><link>https://arxiv.org/abs/2505.06315</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces asset-centric threat modeling for AI systems&lt;/li&gt;&lt;li&gt;Addresses security challenges of autonomous AI agents&lt;/li&gt;&lt;li&gt;Enables comprehensive vulnerability analysis across distributed infrastructures&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jose Sanchez Vicarte', 'Marcin Spoczynski', 'Mostafa Elsaid']&lt;/li&gt;&lt;li&gt;Tags: ['threat modeling', 'AI security', 'asset-centric', 'autonomous agents']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2505.06315</guid><pubDate>Fri, 04 Jul 2025 00:00:00 +0000</pubDate></item><item><title>Self-Correction Bench: Revealing and Addressing the Self-Correction Blind Spot in LLMs</title><link>https://arxiv.org/abs/2507.02778</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduced Self-Correction Bench to measure LLMs' ability to correct their own errors&lt;/li&gt;&lt;li&gt;Found 64.5% average blind spot rate across 14 models&lt;/li&gt;&lt;li&gt;Training data composition affects self-correction capability&lt;/li&gt;&lt;li&gt;Appending 'Wait' reduces blind spots by 89.3%&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Ken Tsui']&lt;/li&gt;&lt;li&gt;Tags: ['safety evaluation', 'robustness', 'alignment', 'benchmarking']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2507.02778</guid><pubDate>Fri, 04 Jul 2025 00:00:00 +0000</pubDate></item><item><title>Early Signs of Steganographic Capabilities in Frontier LLMs</title><link>https://arxiv.org/abs/2507.02737</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Evaluates steganography capabilities in frontier LLMs&lt;/li&gt;&lt;li&gt;Tests message encoding and encoded reasoning&lt;/li&gt;&lt;li&gt;Finds current models have nascent capabilities that could grow&lt;/li&gt;&lt;li&gt;Highlights need for robust monitoring systems&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Artur Zolkowski', 'Kei Nishimura-Gasparian', 'Robert McCarthy', 'Roland S. Zimmermann', 'David Lindner']&lt;/li&gt;&lt;li&gt;Tags: ['steganography', 'LLM security', 'red teaming', 'adversarial prompting', 'model monitoring']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2507.02737</guid><pubDate>Fri, 04 Jul 2025 00:00:00 +0000</pubDate></item><item><title>Meta SecAlign: A Secure Foundation LLM Against Prompt Injection Attacks</title><link>https://arxiv.org/abs/2507.02735</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces Meta SecAlign, the first open-source LLM with built-in defense against prompt injection attacks&lt;/li&gt;&lt;li&gt;Utilizes an improved version of SecAlign defense mechanism&lt;/li&gt;&lt;li&gt;Evaluated on 9 utility and 7 security benchmarks showing robustness in unseen tasks&lt;/li&gt;&lt;li&gt;Best model (Meta-SecAlign-70B) achieves state-of-the-art security while matching commercial performance&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Sizhe Chen', 'Arman Zharmagambetov', 'David Wagner', 'Chuan Guo']&lt;/li&gt;&lt;li&gt;Tags: ['prompt injection', 'LLM security', 'model-level defense', 'open-source', 'SecAlign']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2507.02735</guid><pubDate>Fri, 04 Jul 2025 00:00:00 +0000</pubDate></item><item><title>De-AntiFake: Rethinking the Protective Perturbations Against Voice Cloning Attacks</title><link>https://arxiv.org/abs/2507.02606</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Systematic evaluation of adversarial perturbations against voice cloning attacks&lt;/li&gt;&lt;li&gt;Proposed a novel two-stage purification method to disrupt VC defenses&lt;/li&gt;&lt;li&gt;Highlights limitations of current perturbation-based defenses&lt;/li&gt;&lt;li&gt;Emphasizes need for more robust security solutions&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Wei Fan', 'Kejiang Chen', 'Chang Liu', 'Weiming Zhang', 'Nenghai Yu']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial attacks', 'voice cloning', 'security', 'purification methods', 'audio processing']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2507.02606</guid><pubDate>Fri, 04 Jul 2025 00:00:00 +0000</pubDate></item><item><title>EIM-TRNG: Obfuscating Deep Neural Network Weights with Encoding-in-Memory True Random Number Generator via RowHammer</title><link>https://arxiv.org/abs/2507.02206</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes EIM-TRNG using DRAM RowHammer for true random number generation&lt;/li&gt;&lt;li&gt;Applies TRNG to encrypt DNN weights for security&lt;/li&gt;&lt;li&gt;Ensures data confidentiality and model authenticity&lt;/li&gt;&lt;li&gt;Validates low-cost hardware security approach&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Ranyang Zhou', 'Abeer Matar A. Almalky', 'Gamana Aragonda', 'Sabbir Ahmed', 'Filip Roth Tr{\\o}nnes-Christensen', 'Adnan Siraj Rakin', 'Shaahin Angizi']&lt;/li&gt;&lt;li&gt;Tags: ['model extraction', 'hardware security', 'TRNG', 'RowHammer', 'AI security']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2507.02206</guid><pubDate>Fri, 04 Jul 2025 00:00:00 +0000</pubDate></item><item><title>MGC: A Compiler Framework Exploiting Compositional Blindness in Aligned LLMs for Malware Generation</title><link>https://arxiv.org/abs/2507.02057</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces MGC framework for generating malware by exploiting compositional blindness in aligned LLMs&lt;/li&gt;&lt;li&gt;Uses modular decomposition and alignment-evasive generation via MDIR intermediate representation&lt;/li&gt;&lt;li&gt;Outperforms jailbreaking methods by +365.79% and underground services by +78.07% in correctness&lt;/li&gt;&lt;li&gt;Reproduces and enhances 16 real-world malware samples in case studies&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Lu Yan', 'Zhuo Zhang', 'Xiangzhe Xu', 'Shengwei An', 'Guangyu Shen', 'Zhou Xuan', 'Xuan Chen', 'Xiangyu Zhang']&lt;/li&gt;&lt;li&gt;Tags: ['red teaming', 'malware generation', 'LLM security', 'alignment', 'adversarial prompting']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2507.02057</guid><pubDate>Fri, 04 Jul 2025 00:00:00 +0000</pubDate></item></channel></rss>