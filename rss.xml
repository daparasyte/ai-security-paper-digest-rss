<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>AI Security Paper Digest</title><link>https://kentaroh-toyoda.github.io/ai-security-paper-digest-rss/rss.xml</link><description>Curated papers on AI security from OpenAlex and ArXiv</description><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><lastBuildDate>Thu, 03 Jul 2025 22:15:39 +0000</lastBuildDate><item><title>Don't Say No: Jailbreaking LLM by Suppressing Refusal</title><link>https://arxiv.org/abs/2404.16369</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes DSN attack combining cosine decay and refusal suppression&lt;/li&gt;&lt;li&gt;Outperforms baseline attacks with higher ASR&lt;/li&gt;&lt;li&gt;Demonstrates universality and transferability&lt;/li&gt;&lt;li&gt;Enhances loss objective for jailbreaking LLMs&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yukai Zhou', 'Jian Lou', 'Zhijie Huang', 'Zhan Qin', 'Yibei Yang', 'Wenjie Wang']&lt;/li&gt;&lt;li&gt;Tags: ['jailbreaking', 'red teaming', 'LLM security', 'adversarial prompting', 'prompt injection']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2404.16369</guid><pubDate>Thu, 03 Jul 2025 00:00:00 +0000</pubDate></item><item><title>AutoAdv: Automated Adversarial Prompting for Multi-Turn Jailbreaking of Large Language Models</title><link>https://arxiv.org/abs/2507.01020</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces AutoAdv, an automated adversarial prompting framework for jailbreaking LLMs&lt;/li&gt;&lt;li&gt;Features multi-turn attack methodology with iterative prompt refinement&lt;/li&gt;&lt;li&gt;Achieves up to 86% jailbreak success rate on models like ChatGPT, Llama, DeepSeek&lt;/li&gt;&lt;li&gt;Highlights vulnerabilities in current LLM safety mechanisms&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Aashray Reddy', 'Andrew Zagula', 'Nicholas Saban']&lt;/li&gt;&lt;li&gt;Tags: ['jailbreaking', 'adversarial prompting', 'red teaming', 'safety evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2507.01020</guid><pubDate>Thu, 03 Jul 2025 00:00:00 +0000</pubDate></item><item><title>On the Fundamental Impossibility of Hallucination Control in Large Language Models</title><link>https://arxiv.org/abs/2506.06382</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proves mathematically that perfect hallucination control in LLMs is impossible&lt;/li&gt;&lt;li&gt;Identifies fundamental trade-offs between truthfulness, knowledge utilization, and response completeness&lt;/li&gt;&lt;li&gt;Uses auction theory, proper scoring theory, and log-sum-exp analysis in the proof&lt;/li&gt;&lt;li&gt;Reframes hallucination as an inevitable feature requiring management rather than elimination&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Micha{\\l} P. Karpowicz']&lt;/li&gt;&lt;li&gt;Tags: ['safety', 'alignment', 'LLM', 'theoretical', 'hallucination']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.06382</guid><pubDate>Thu, 03 Jul 2025 00:00:00 +0000</pubDate></item><item><title>Backdooring Bias (B^2) into Stable Diffusion Models</title><link>https://arxiv.org/abs/2406.15213</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces a backdoor attack method to inject biases into Stable Diffusion models using textual triggers&lt;/li&gt;&lt;li&gt;Demonstrates low cost ($10-$15) and feasibility of the attack through extensive experiments&lt;/li&gt;&lt;li&gt;Highlights challenges in detecting biased images without prior knowledge of the triggers&lt;/li&gt;&lt;li&gt;Emphasizes need for robust defensive strategies against such vulnerabilities&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Ali Naseh', 'Jaechul Roh', 'Eugene Bagdasaryan', 'Amir Houmansadr']&lt;/li&gt;&lt;li&gt;Tags: ['data poisoning', 'backdoor attacks', 'generative models', 'bias injection', 'adversarial attacks']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2406.15213</guid><pubDate>Thu, 03 Jul 2025 00:00:00 +0000</pubDate></item><item><title>Probing Evaluation Awareness of Language Models</title><link>https://arxiv.org/abs/2507.01786</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Studied evaluation awareness in Llama-3.3-70B-Instruct using linear probes&lt;/li&gt;&lt;li&gt;Found that models can distinguish between evaluation and deployment prompts&lt;/li&gt;&lt;li&gt;Current safety evaluations are classified as artificial by the model&lt;/li&gt;&lt;li&gt;Highlights need for trustworthy evaluations and understanding deceptive capabilities&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jord Nguyen', 'Khiem Hoang', 'Carlo Leonardo Attubato', 'Felix Hofst\\"atter']&lt;/li&gt;&lt;li&gt;Tags: ['safety evaluation', 'evaluation awareness', 'model deception', 'AI governance']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2507.01786</guid><pubDate>Thu, 03 Jul 2025 00:00:00 +0000</pubDate></item><item><title>Tuning without Peeking: Provable Privacy and Generalization Bounds for LLM Post-Training</title><link>https://arxiv.org/abs/2507.01752</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces BBoxER, a black-box optimization method for LLM post-training&lt;/li&gt;&lt;li&gt;Provides theoretical bounds on differential privacy, data poisoning resistance, and robustness&lt;/li&gt;&lt;li&gt;Demonstrates empirical improvements in performance and generalization on reasoning datasets&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Ismail Labiad', 'Mathurin Videau', 'Matthieu Kowalski', 'Marc Schoenauer', 'Alessandro Leite', 'Julia Kempe', 'Olivier Teytaud']&lt;/li&gt;&lt;li&gt;Tags: ['privacy', 'security', 'differential_privacy', 'data_poisoning', 'extraction_attacks', 'robustness', 'generalization', 'LLM_post-training', 'black_box_optimization']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2507.01752</guid><pubDate>Thu, 03 Jul 2025 00:00:00 +0000</pubDate></item><item><title>GPT, But Backwards: Exactly Inverting Language Model Outputs</title><link>https://arxiv.org/abs/2507.01693</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces SODA algorithm for exact input reconstruction from LLM outputs&lt;/li&gt;&lt;li&gt;Formalizes input reconstruction as a discrete optimization problem&lt;/li&gt;&lt;li&gt;Demonstrates high success rates for shorter inputs but limited effectiveness for longer sequences&lt;/li&gt;&lt;li&gt;Discusses implications for forensic analysis and privacy protection&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Adrians Skapars', 'Edoardo Manino', 'Youcheng Sun', 'Lucas C. Cordeiro']&lt;/li&gt;&lt;li&gt;Tags: ['model extraction', 'privacy attacks', 'adversarial prompting', 'safety evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2507.01693</guid><pubDate>Thu, 03 Jul 2025 00:00:00 +0000</pubDate></item><item><title>Survivability of Backdoor Attacks on Unconstrained Face Recognition Systems</title><link>https://arxiv.org/abs/2507.01607</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;First system-level study of backdoor attacks on deep learning-based face recognition systems&lt;/li&gt;&lt;li&gt;Demonstrates two backdoor attacks on face detection: face generation and face landmark shift&lt;/li&gt;&lt;li&gt;Shows backdoor vulnerability in face feature extractors with large margin losses&lt;/li&gt;&lt;li&gt;Provides best practices and countermeasures for stakeholders&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Quentin Le Roux', 'Yannick Teglia', 'Teddy Furon', 'Philippe Loubet-Moundi', 'Eric Bourbao']&lt;/li&gt;&lt;li&gt;Tags: ['backdoor attacks', 'face recognition', 'adversarial attacks', 'data poisoning', 'security']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2507.01607</guid><pubDate>Thu, 03 Jul 2025 00:00:00 +0000</pubDate></item><item><title>ICLShield: Exploring and Mitigating In-Context Learning Backdoor Attacks</title><link>https://arxiv.org/abs/2507.01321</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes ICLShield, a defense against in-context learning backdoor attacks in LLMs&lt;/li&gt;&lt;li&gt;Introduces dual-learning hypothesis explaining vulnerability to backdoor concepts&lt;/li&gt;&lt;li&gt;Uses confidence and similarity scores to prioritize clean demonstrations&lt;/li&gt;&lt;li&gt;Demonstrates significant improvement over existing defenses (+26.02% avg)&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Zhiyao Ren', 'Siyuan Liang', 'Aishan Liu', 'Dacheng Tao']&lt;/li&gt;&lt;li&gt;Tags: ['backdoor attacks', 'data poisoning', 'defense mechanism', 'in-context learning', 'LLM security']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2507.01321</guid><pubDate>Thu, 03 Jul 2025 00:00:00 +0000</pubDate></item><item><title>Rethinking All Evidence: Enhancing Trustworthy Retrieval-Augmented Generation via Conflict-Driven Summarization</title><link>https://arxiv.org/abs/2507.01281</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces CARE-RAG framework for conflict-aware evidence handling in RAG systems&lt;/li&gt;&lt;li&gt;Utilizes conflict-driven summarization to detect and resolve inconsistencies between internal and retrieved knowledge&lt;/li&gt;&lt;li&gt;Demonstrates improved performance on QA datasets with noisy or conflicting evidence&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Juan Chen', 'Baolong Bi', 'Wei Zhang', 'Jingyan Sui', 'Xiaofei Zhu', 'Yuanzhuo Wang', 'Lingrui Mei', 'Shenghua Liu']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'robustness', 'safety evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2507.01281</guid><pubDate>Thu, 03 Jul 2025 00:00:00 +0000</pubDate></item></channel></rss>