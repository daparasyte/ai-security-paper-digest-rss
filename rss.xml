<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>AI Security Paper Digest</title><link>https://kentaroh-toyoda.github.io/ai-security-paper-digest-rss/rss.xml</link><description>Curated papers on AI security from ArXiv</description><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><lastBuildDate>Wed, 17 Sep 2025 22:38:25 +0000</lastBuildDate><item><title>Adversarial Prompt Distillation for Vision-Language Models</title><link>https://arxiv.org/abs/2411.15244</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes Adversarial Prompt Distillation (APD) to enhance robustness of Vision-Language Models (VLMs) against adversarial attacks&lt;/li&gt;&lt;li&gt;Combines adversarial training with bimodal knowledge distillation from a clean teacher model&lt;/li&gt;&lt;li&gt;Demonstrates improved adversarial robustness and clean accuracy over state-of-the-art methods&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Lin Luo', 'Xin Wang', 'Bojia Zi', 'Shihao Zhao', 'Xingjun Ma', 'Yu-Gang Jiang']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial_prompting', 'robustness', 'vision_language_models', 'knowledge_distillation', 'multi_modal']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2411.15244</guid><pubDate>Wed, 17 Sep 2025 00:00:00 +0000</pubDate></item><item><title>Sy-FAR: Symmetry-based Fair Adversarial Robustness</title><link>https://arxiv.org/abs/2509.12939</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces Sy-FAR, a technique for symmetry-based fair adversarial robustness&lt;/li&gt;&lt;li&gt;Aims to balance attack success rates between classes to ensure fairness&lt;/li&gt;&lt;li&gt;Evaluated on five datasets with three model architectures against realistic attacks&lt;/li&gt;&lt;li&gt;Shows improved fair adversarial robustness and consistency compared to state-of-the-art methods&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Haneen Najjar', 'Eyal Ronen', 'Mahmood Sharif']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial_robustness', 'security', 'fairness', 'red_teaming', 'face_recognition']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.12939</guid><pubDate>Wed, 17 Sep 2025 00:00:00 +0000</pubDate></item><item><title>Defense-to-Attack: Bypassing Weak Defenses Enables Stronger Jailbreaks in Vision-Language Models</title><link>https://arxiv.org/abs/2509.12724</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes Defense2Attack, a novel jailbreak method for VLMs using weak defenses&lt;/li&gt;&lt;li&gt;Incorporates visual and textual optimizers plus a red-team suffix generator&lt;/li&gt;&lt;li&gt;Empirically evaluated on four VLMs and four safety benchmarks&lt;/li&gt;&lt;li&gt;Outperforms state-of-the-art attack methods in single attempts&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yunhan Zhao', 'Xiang Zheng', 'Xingjun Ma']&lt;/li&gt;&lt;li&gt;Tags: ['jailbreak', 'red teaming', 'adversarial attacks', 'vision-language models', 'prompt engineering']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.12724</guid><pubDate>Wed, 17 Sep 2025 00:00:00 +0000</pubDate></item><item><title>CIARD: Cyclic Iterative Adversarial Robustness Distillation</title><link>https://arxiv.org/abs/2509.12633</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes CIARD method for adversarial robustness distillation&lt;/li&gt;&lt;li&gt;Introduces multi-teacher framework with contrastive push-loss alignment&lt;/li&gt;&lt;li&gt;Implements continuous adversarial retraining to maintain teacher robustness&lt;/li&gt;&lt;li&gt;Achieves improved adversarial defense rates and clean accuracy&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Liming Lu', 'Shuchao Pang', 'Xu Zheng', 'Xiang Gu', 'Anan Du', 'Yunhuai Liu', 'Yongbin Zhou']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial_robustness', 'knowledge_distillation', 'multi_teacher', 'continuous_retraining']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.12633</guid><pubDate>Wed, 17 Sep 2025 00:00:00 +0000</pubDate></item><item><title>Probing LLM Hallucination from Within: Perturbation-Driven Approach via Internal Knowledge</title><link>https://arxiv.org/abs/2411.09689</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces hallucination probing to classify LLM-generated text into aligned, misaligned, or fabricated&lt;/li&gt;&lt;li&gt;Proposes SHINE method using prompt perturbations for hallucination detection without external knowledge or training&lt;/li&gt;&lt;li&gt;Achieves state-of-the-art performance across multiple LLMs and datasets&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Seongmin Lee', 'Hsiang Hsu', 'Chun-Fu Chen', 'Duen Horng Chau']&lt;/li&gt;&lt;li&gt;Tags: ['hallucination detection', 'adversarial prompting', 'safety evaluation', 'robustness', 'red teaming']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2411.09689</guid><pubDate>Wed, 17 Sep 2025 00:00:00 +0000</pubDate></item><item><title>Keep Security! Benchmarking Security Policy Preservation in Large Language Model Contexts Against Indirect Attacks in Question Answering</title><link>https://arxiv.org/abs/2505.15805</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduced CoPriva benchmark for evaluating LLM security policy preservation&lt;/li&gt;&lt;li&gt;Assessed 10 LLMs against direct and indirect attacks seeking sensitive info&lt;/li&gt;&lt;li&gt;Found significant vulnerabilities, especially in indirect attack scenarios&lt;/li&gt;&lt;li&gt;Highlighted need for robust contextual security enforcement&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Hwan Chang', 'Yumin Kim', 'Yonghyun Jun', 'Hwanhee Lee']&lt;/li&gt;&lt;li&gt;Tags: ['red teaming', 'adversarial prompting', 'security policies', 'benchmarking', 'LLM safety']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2505.15805</guid><pubDate>Wed, 17 Sep 2025 00:00:00 +0000</pubDate></item><item><title>Context-Aware Membership Inference Attacks against Pre-trained Large Language Models</title><link>https://arxiv.org/abs/2409.13745</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces a novel context-aware membership inference attack on pre-trained LLMs&lt;/li&gt;&lt;li&gt;Adapts MIA statistical tests to perplexity dynamics of token subsequences&lt;/li&gt;&lt;li&gt;Significantly outperforms prior approaches in revealing context-dependent memorization&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Hongyan Chang', 'Ali Shahin Shamsabadi', 'Kleomenis Katevas', 'Hamed Haddadi', 'Reza Shokri']&lt;/li&gt;&lt;li&gt;Tags: ['privacy attacks', 'LLM', 'membership inference', 'data poisoning']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2409.13745</guid><pubDate>Wed, 17 Sep 2025 00:00:00 +0000</pubDate></item><item><title>Jailbreaking Large Language Models Through Content Concretization</title><link>https://arxiv.org/abs/2509.12937</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces Content Concretization (CC), a novel jailbreaking technique for LLMs&lt;/li&gt;&lt;li&gt;Two-stage process: initial response generation followed by iterative refinement&lt;/li&gt;&lt;li&gt;Evaluated with 350 cybersecurity prompts, SR increased from 7% to 62% after 3 refinements&lt;/li&gt;&lt;li&gt;Highlights critical vulnerabilities in current LLM safety mechanisms&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ["Johan Wahr\\'eus", 'Ahmed Hussain', 'Panos Papadimitratos']&lt;/li&gt;&lt;li&gt;Tags: ['jailbreaking', 'adversarial prompting', 'red teaming', 'safety evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.12937</guid><pubDate>Wed, 17 Sep 2025 00:00:00 +0000</pubDate></item><item><title>Yet Another Watermark for Large Language Models</title><link>https://arxiv.org/abs/2509.12574</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces a new watermarking method for LLMs that manipulates internal parameters&lt;/li&gt;&lt;li&gt;Enables watermark extraction from generated text in a black-box scenario&lt;/li&gt;&lt;li&gt;Balances robustness and imperceptibility of the watermark&lt;/li&gt;&lt;li&gt;Provides computational efficiency for watermark extraction&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Siyuan Bao', 'Ying Shi', 'Zhiguang Yang', 'Hanzhou Wu', 'Xinpeng Zhang']&lt;/li&gt;&lt;li&gt;Tags: ['model extraction', 'robustness', 'privacy attacks']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.12574</guid><pubDate>Wed, 17 Sep 2025 00:00:00 +0000</pubDate></item><item><title>MEUV: Achieving Fine-Grained Capability Activation in Large Language Models via Mutually Exclusive Unlock Vectors</title><link>https://arxiv.org/abs/2509.12221</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces MEUV for fine-grained capability activation in LLMs&lt;/li&gt;&lt;li&gt;Uses mutually exclusive vectors to unlock specific sensitive topics&lt;/li&gt;&lt;li&gt;Achieves high success rates with reduced cross-topic leakage&lt;/li&gt;&lt;li&gt;Demonstrates language transferability between Chinese and English&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Xin Tong', 'Zhi Lin', 'Jingya Wang', 'Meng Han', 'Bo Jin']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'adversarial prompting', 'alignment', 'safety evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.12221</guid><pubDate>Wed, 17 Sep 2025 00:00:00 +0000</pubDate></item><item><title>Towards Inclusive Toxic Content Moderation: Addressing Vulnerabilities to Adversarial Attacks in Toxicity Classifiers Tackling LLM-generated Content</title><link>https://arxiv.org/abs/2509.12672</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Uses adversarial attacks to identify vulnerabilities in toxicity classifiers&lt;/li&gt;&lt;li&gt;Proposes suppressing vulnerable model circuits to improve robustness&lt;/li&gt;&lt;li&gt;Provides demographic-level insights into model fairness gaps&lt;/li&gt;&lt;li&gt;Aims to enhance security against LLM-generated adversarial content&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Shaz Furniturewala', 'Arkaitz Zubiaga']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial attacks', 'red teaming', 'robustness', 'model interpretability', 'fairness', 'toxicity detection']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.12672</guid><pubDate>Wed, 17 Sep 2025 00:00:00 +0000</pubDate></item><item><title>MORABLES: A Benchmark for Assessing Abstract Moral Reasoning in LLMs with Fables</title><link>https://arxiv.org/abs/2509.12371</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces MORABLES benchmark for moral reasoning in LLMs&lt;/li&gt;&lt;li&gt;Includes adversarial variants to test model vulnerabilities&lt;/li&gt;&lt;li&gt;Finds significant self-contradiction in model responses&lt;/li&gt;&lt;li&gt;Highlights issues with superficial pattern matching&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Matteo Marcuzzo', 'Alessandro Zangari', 'Andrea Albarelli', 'Jose Camacho-Collados', 'Mohammad Taher Pilehvar']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial prompting', 'robustness', 'safety evaluation', 'red teaming', 'model extraction']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.12371</guid><pubDate>Wed, 17 Sep 2025 00:00:00 +0000</pubDate></item><item><title>Murphys Laws of AI Alignment: Why the Gap Always Wins</title><link>https://arxiv.org/abs/2509.05381</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Analyzes the difficulty of learning true rewards from biased human feedback&lt;/li&gt;&lt;li&gt;Proves exponential sample complexity without a calibration oracle&lt;/li&gt;&lt;li&gt;Quantifies the alignment gap based on misspecification parameters&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Madhava Gaikwad']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'safety', 'reinforcement learning', 'human feedback', 'theoretical analysis']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.05381</guid><pubDate>Wed, 17 Sep 2025 00:00:00 +0000</pubDate></item><item><title>Multi-Agent Systems Execute Arbitrary Malicious Code</title><link>https://arxiv.org/abs/2503.12188</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Demonstrates adversarial attacks on multi-agent LLM systems leading to arbitrary code execution&lt;/li&gt;&lt;li&gt;Attacks succeed even when individual agents are resistant to prompt injection&lt;/li&gt;&lt;li&gt;High success rates (58-90%) with GPT-4o, sometimes 100% in certain configurations&lt;/li&gt;&lt;li&gt;Highlights need for security models in multi-agent systems&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Harold Triedman', 'Rishi Jha', 'Vitaly Shmatikov']&lt;/li&gt;&lt;li&gt;Tags: ['red teaming', 'adversarial prompting', 'multi-agent systems', 'security', 'LLM']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2503.12188</guid><pubDate>Wed, 17 Sep 2025 00:00:00 +0000</pubDate></item><item><title>Enhancing Prompt Injection Attacks to LLMs via Poisoning Alignment</title><link>https://arxiv.org/abs/2410.14827</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces PoisonedAlign method for poisoning LLM alignment data&lt;/li&gt;&lt;li&gt;Enhances prompt injection attack success across multiple LLMs&lt;/li&gt;&lt;li&gt;Maintains model performance on standard benchmarks while increasing vulnerability&lt;/li&gt;&lt;li&gt;Demonstrates critical security threat through alignment process manipulation&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Zedian Shao', 'Hongbin Liu', 'Jaden Mu', 'Neil Zhenqiang Gong']&lt;/li&gt;&lt;li&gt;Tags: ['prompt injection', 'data poisoning', 'alignment', 'security', 'red teaming']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2410.14827</guid><pubDate>Wed, 17 Sep 2025 00:00:00 +0000</pubDate></item><item><title>Prompt Injection Attacks on LLM Generated Reviews of Scientific Publications</title><link>https://arxiv.org/abs/2509.10248</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Investigates prompt injection attacks on LLM-generated scientific reviews&lt;/li&gt;&lt;li&gt;Finds simple prompt injections can achieve 100% acceptance rates&lt;/li&gt;&lt;li&gt;Discovers LLM reviews are generally biased towards acceptance (&gt;95% in many models)&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Janis Keuper']&lt;/li&gt;&lt;li&gt;Tags: ['prompt injection', 'adversarial prompting', 'red teaming', 'safety evaluation', 'model robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.10248</guid><pubDate>Wed, 17 Sep 2025 00:00:00 +0000</pubDate></item><item><title>Adversarial Examples Are Not Bugs, They Are Superposition</title><link>https://arxiv.org/abs/2508.17456</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes superposition as the primary cause of adversarial examples&lt;/li&gt;&lt;li&gt;Provides four lines of evidence including theoretical explanations and empirical results&lt;/li&gt;&lt;li&gt;Demonstrates interventions on superposition and robustness in toy models and ResNet18&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Liv Gorton', 'Owen Lewis']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial examples', 'robustness', 'superposition', 'security', 'interpretability']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2508.17456</guid><pubDate>Wed, 17 Sep 2025 00:00:00 +0000</pubDate></item><item><title>Safety Pretraining: Toward the Next Generation of Safe AI</title><link>https://arxiv.org/abs/2504.16980</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces a data-centric pretraining framework with four key steps:&lt;/li&gt;&lt;li&gt;1. Safety Filtering: Classifies web data into safe/unsafe&lt;/li&gt;&lt;li&gt;2. Safety Rephrasing: Converts unsafe data into safe narratives&lt;/li&gt;&lt;li&gt;3. Native Refusal: Teaches model to refuse unsafe content with moral reasoning&lt;/li&gt;&lt;li&gt;4. Harmfulness-Tag: Flags unsafe content during pretraining to guide inference&lt;/li&gt;&lt;li&gt;Reduces attack success rates from 38.8% to 8.4% on safety benchmarks without performance loss&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Pratyush Maini', 'Sachin Goyal', 'Dylan Sam', 'Alex Robey', 'Yash Savani', 'Yiding Jiang', 'Andy Zou', 'Matt Fredrikson', 'Zacharcy C. Lipton', 'J. Zico Kolter']&lt;/li&gt;&lt;li&gt;Tags: ['safety_pretraining', 'LLM_safety', 'alignment', 'data_poisoning_prevention', 'adversarial_robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2504.16980</guid><pubDate>Wed, 17 Sep 2025 00:00:00 +0000</pubDate></item><item><title>SafeSwitch: Steering Unsafe LLM Behavior via Internal Activation Signals</title><link>https://arxiv.org/abs/2502.01042</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces SafeSwitch, a dynamic framework for regulating unsafe LLM outputs using internal activation signals&lt;/li&gt;&lt;li&gt;Reduces harmful outputs by ~80% while maintaining utility through context-aware safety activation&lt;/li&gt;&lt;li&gt;Efficiently tunes &lt;6% of original parameters for safety control&lt;/li&gt;&lt;li&gt;Demonstrates LLM capacity for self-aware safety reflection&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Peixuan Han', 'Cheng Qian', 'Xiusi Chen', 'Yuji Zhang', 'Heng Ji', 'Denghui Zhang']&lt;/li&gt;&lt;li&gt;Tags: ['LLM safety', 'internal state monitoring', 'safety mechanisms', 'parameter efficiency', 'alignment']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2502.01042</guid><pubDate>Wed, 17 Sep 2025 00:00:00 +0000</pubDate></item><item><title>Security of Deep Reinforcement Learning for Autonomous Driving: A Survey</title><link>https://arxiv.org/abs/2212.06123</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Comprehensive survey of 86 studies on RL security&lt;/li&gt;&lt;li&gt;Focuses on autonomous driving context&lt;/li&gt;&lt;li&gt;Categorizes attacks and defenses by threat models&lt;/li&gt;&lt;li&gt;Evaluates applicability of security measures&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Survey&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Ambra Demontis', 'Srishti Gupta', 'Maura Pintor', 'Luca Demetrio', 'Kathrin Grosse', 'Hsiao-Ying Lin', 'Chengfang Fang', 'Battista Biggio', 'Fabio Roli']&lt;/li&gt;&lt;li&gt;Tags: ['RL security', 'autonomous driving', 'survey', 'attacks', 'defenses']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2212.06123</guid><pubDate>Wed, 17 Sep 2025 00:00:00 +0000</pubDate></item><item><title>Your Compiler is Backdooring Your Model: Understanding and Exploiting Compilation Inconsistency Vulnerabilities in Deep Learning Compilers</title><link>https://arxiv.org/abs/2509.11173</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Unveils a fundamental vulnerability in DL compilers that can introduce backdoors during compilation&lt;/li&gt;&lt;li&gt;Adversarial attacks achieve 100% success while maintaining normal accuracy&lt;/li&gt;&lt;li&gt;Natural triggers found in 31 HuggingFace models, including popular ones&lt;/li&gt;&lt;li&gt;Compilers studied include three commercial ones across two hardware platforms&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Simin Chen', 'Jinjun Peng', 'Yixin He', 'Junfeng Yang', 'Baishakhi Ray']&lt;/li&gt;&lt;li&gt;Tags: ['security', 'backdoors', 'compilers', 'adversarial_attacks', 'model_manipulation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.11173</guid><pubDate>Wed, 17 Sep 2025 00:00:00 +0000</pubDate></item><item><title>LLM in the Middle: A Systematic Review of Threats and Mitigations to Real-World LLM-based Systems</title><link>https://arxiv.org/abs/2509.10682</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Systematic review of security threats and mitigations for LLM-based systems&lt;/li&gt;&lt;li&gt;Categorizes threats across the software and LLM lifecycle&lt;/li&gt;&lt;li&gt;Provides defense strategies mapped to attack types and lifecycle phases&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Survey&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Vitor Hugo Galhardo Moia', 'Igor Jochem Sanz', 'Gabriel Antonio Fontes Rebello', 'Rodrigo Duarte de Meneses', 'Briland Hitaj', 'Ulf Lindqvist']&lt;/li&gt;&lt;li&gt;Tags: ['LLM security', 'threat modeling', 'mitigations', 'survey', 'real-world scenarios']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.10682</guid><pubDate>Wed, 17 Sep 2025 00:00:00 +0000</pubDate></item><item><title>Uncovering the Vulnerability of Large Language Models in the Financial Domain via Risk Concealment</title><link>https://arxiv.org/abs/2509.10546</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces Risk-Concealment Attacks (RCA) for red-teaming financial LLMs&lt;/li&gt;&lt;li&gt;Constructs FIN-Bench benchmark for financial LLM safety evaluation&lt;/li&gt;&lt;li&gt;Demonstrates high attack success rates (93.18% avg) against major LLMs including GPT-4.1 and OpenAI o1&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Gang Cheng', 'Haibo Jin', 'Wenbin Zhang', 'Haohan Wang', 'Jun Zhuang']&lt;/li&gt;&lt;li&gt;Tags: ['red teaming', 'adversarial prompting', 'safety evaluation', 'financial domain', 'alignment']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.10546</guid><pubDate>Wed, 17 Sep 2025 00:00:00 +0000</pubDate></item><item><title>DRAG: Data Reconstruction Attack using Guided Diffusion</title><link>https://arxiv.org/abs/2509.11724</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes DRAG, a data reconstruction attack using guided diffusion&lt;/li&gt;&lt;li&gt;Targets foundation models in split inference scenarios&lt;/li&gt;&lt;li&gt;Outperforms state-of-the-art methods in reconstructing images from intermediate representations&lt;/li&gt;&lt;li&gt;Highlights privacy risks of large models in split inference&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Wa-Kin Lei', 'Jun-Cheng Chen', 'Shang-Tse Chen']&lt;/li&gt;&lt;li&gt;Tags: ['data reconstruction', 'privacy attack', 'split inference', 'diffusion models', 'foundation models']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.11724</guid><pubDate>Wed, 17 Sep 2025 00:00:00 +0000</pubDate></item><item><title>Reasoned Safety Alignment: Ensuring Jailbreak Defense via Answer-Then-Check</title><link>https://arxiv.org/abs/2509.11629</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces Answer-Then-Check method to enhance LLM jailbreak defense&lt;/li&gt;&lt;li&gt;Creates ReSA dataset with 80K examples for safety alignment training&lt;/li&gt;&lt;li&gt;Demonstrates improved safety while reducing over-refusal rates&lt;/li&gt;&lt;li&gt;Maintains general reasoning capabilities on benchmarks like MMLU&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Chentao Cao', 'Xiaojun Xu', 'Bo Han', 'Hang Li']&lt;/li&gt;&lt;li&gt;Tags: ['jailbreaking', 'safety alignment', 'LLM security', 'adversarial prompting', 'dataset']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.11629</guid><pubDate>Wed, 17 Sep 2025 00:00:00 +0000</pubDate></item><item><title>DARD: Dice Adversarial Robustness Distillation against Adversarial Attacks</title><link>https://arxiv.org/abs/2509.11525</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces DARD for distilling robustness from large to small models&lt;/li&gt;&lt;li&gt;Proposes DPGD for generating adversarial examples&lt;/li&gt;&lt;li&gt;Demonstrates improved robustness and accuracy over adversarial training&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jing Zou', 'Shungeng Zhang', 'Meikang Qiu', 'Chong Li']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial robustness', 'knowledge distillation', 'defense']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.11525</guid><pubDate>Wed, 17 Sep 2025 00:00:00 +0000</pubDate></item><item><title>From Firewalls to Frontiers: AI Red-Teaming is a Domain-Specific Evolution of Cyber Red-Teaming</title><link>https://arxiv.org/abs/2509.11398</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Argues that AI red-teaming should be recognized as a domain-specific evolution of cyber red-teaming&lt;/li&gt;&lt;li&gt;Emphasizes new risks and failure modes in AI systems requiring adapted red team strategies&lt;/li&gt;&lt;li&gt;Proposes merging AI and cyber red teaming to create a robust security ecosystem&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Position&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Anusha Sinha', 'Keltin Grimes', 'James Lucassen', 'Michael Feffer', 'Nathan VanHoudnos', 'Zhiwei Steven Wu', 'Hoda Heidari']&lt;/li&gt;&lt;li&gt;Tags: ['red teaming', 'AI security', 'cybersecurity', 'adversarial attacks', 'security strategy']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.11398</guid><pubDate>Wed, 17 Sep 2025 00:00:00 +0000</pubDate></item><item><title>Stabilizing Data-Free Model Extraction</title><link>https://arxiv.org/abs/2509.11159</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes MetaDFME, a data-free model extraction method using meta-learning to stabilize substitute model accuracy&lt;/li&gt;&lt;li&gt;Addresses oscillating accuracy issue caused by distribution shift in generated data&lt;/li&gt;&lt;li&gt;Demonstrates improved performance and stability on image datasets (MNIST, SVHN, CIFAR-10, CIFAR-100)&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Dat-Thinh Nguyen', 'Kim-Hung Le', 'Nhien-An Le-Khac']&lt;/li&gt;&lt;li&gt;Tags: ['model extraction', 'adversarial attacks', 'security', 'meta-learning', 'data-free']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.11159</guid><pubDate>Wed, 17 Sep 2025 00:00:00 +0000</pubDate></item><item><title>The Psychogenic Machine: Simulating AI Psychosis, Delusion Reinforcement and Harm Enablement in Large Language Models</title><link>https://arxiv.org/abs/2509.10970</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduced psychosis-bench to evaluate LLM psychogenicity&lt;/li&gt;&lt;li&gt;Assessed delusion confirmation, harm enablement, and safety interventions&lt;/li&gt;&lt;li&gt;Found models perpetuate delusions and enable harm&lt;/li&gt;&lt;li&gt;Emphasizes need for safety in LLM training&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Joshua Au Yeung', 'Jacopo Dalmasso', 'Luca Foschini', 'Richard JB Dobson', 'Zeljko Kraljevic']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'safety evaluation', 'delusion reinforcement', 'harm enablement', 'benchmarking']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.10970</guid><pubDate>Wed, 17 Sep 2025 00:00:00 +0000</pubDate></item><item><title>The Anti-Ouroboros Effect: Emergent Resilience in Large Language Models from Recursive Selective Feedback</title><link>https://arxiv.org/abs/2509.10509</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces selective feedback mechanism to prevent model collapse in recursive LLM training&lt;/li&gt;&lt;li&gt;Demonstrates statistically significant performance improvement (6.6% ROUGE-L F1) over 5 generations&lt;/li&gt;&lt;li&gt;Contrasts with foundational classifier experiment validating degenerative loop&lt;/li&gt;&lt;li&gt;Highlights emergent resilience in high-dimensional models under selection pressure&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Sai Teja Reddy Adapala']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'robustness', 'AI safety', 'model stability']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.10509</guid><pubDate>Wed, 17 Sep 2025 00:00:00 +0000</pubDate></item><item><title>Your Compiler is Backdooring Your Model: Understanding and Exploiting Compilation Inconsistency Vulnerabilities in Deep Learning Compilers</title><link>https://arxiv.org/abs/2509.11173</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Unveils a fundamental vulnerability in DL compilers that can introduce backdoors during compilation&lt;/li&gt;&lt;li&gt;Adversarial attacks achieve 100% success while maintaining normal accuracy&lt;/li&gt;&lt;li&gt;Natural triggers found in 31 HuggingFace models, including popular ones&lt;/li&gt;&lt;li&gt;Compilers studied include three commercial ones across two hardware platforms&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Simin Chen', 'Jinjun Peng', 'Yixin He', 'Junfeng Yang', 'Baishakhi Ray']&lt;/li&gt;&lt;li&gt;Tags: ['security', 'backdoors', 'compilers', 'adversarial_attacks', 'model_manipulation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.11173</guid><pubDate>Wed, 17 Sep 2025 00:00:00 +0000</pubDate></item><item><title>Adversarial Prompt Distillation for Vision-Language Models</title><link>https://arxiv.org/abs/2411.15244</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes Adversarial Prompt Distillation (APD) to enhance robustness of Vision-Language Models (VLMs) against adversarial attacks&lt;/li&gt;&lt;li&gt;Combines adversarial training with bimodal knowledge distillation from a clean teacher model&lt;/li&gt;&lt;li&gt;Demonstrates improved adversarial robustness and clean accuracy over state-of-the-art methods&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Lin Luo', 'Xin Wang', 'Bojia Zi', 'Shihao Zhao', 'Xingjun Ma', 'Yu-Gang Jiang']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial_prompting', 'robustness', 'vision_language_models', 'knowledge_distillation', 'multi_modal']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2411.15244</guid><pubDate>Wed, 17 Sep 2025 00:00:00 +0000</pubDate></item><item><title>TrojanRobot: Physical-world Backdoor Attacks Against VLM-based Robotic Manipulation</title><link>https://arxiv.org/abs/2411.11683</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces TrojanRobot, a physical-world backdoor attack against VLM-based robotic manipulation&lt;/li&gt;&lt;li&gt;Uses module-poisoning to embed backdoor into visual perception module&lt;/li&gt;&lt;li&gt;Demonstrates effectiveness on UR3e manipulator with 4 VLMs and 18 tasks&lt;/li&gt;&lt;li&gt;Proposes prime attacks with permutation, stagnation, and intentional variants&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Xianlong Wang', 'Hewen Pan', 'Hangtao Zhang', 'Minghui Li', 'Shengshan Hu', 'Ziqi Zhou', 'Lulu Xue', 'Aishan Liu', 'Yunpeng Jiang', 'Leo Yu Zhang', 'Xiaohua Jia']&lt;/li&gt;&lt;li&gt;Tags: ['backdoor attack', 'physical-world', 'VLM', 'robotic manipulation', 'module poisoning']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2411.11683</guid><pubDate>Wed, 17 Sep 2025 00:00:00 +0000</pubDate></item><item><title>Context-Aware Membership Inference Attacks against Pre-trained Large Language Models</title><link>https://arxiv.org/abs/2409.13745</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces a novel context-aware membership inference attack on pre-trained LLMs&lt;/li&gt;&lt;li&gt;Adapts MIA statistical tests to perplexity dynamics of token subsequences&lt;/li&gt;&lt;li&gt;Significantly outperforms prior approaches in revealing context-dependent memorization&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Hongyan Chang', 'Ali Shahin Shamsabadi', 'Kleomenis Katevas', 'Hamed Haddadi', 'Reza Shokri']&lt;/li&gt;&lt;li&gt;Tags: ['privacy attacks', 'LLM', 'membership inference', 'data poisoning']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2409.13745</guid><pubDate>Wed, 17 Sep 2025 00:00:00 +0000</pubDate></item><item><title>When Safe Unimodal Inputs Collide: Optimizing Reasoning Chains for Cross-Modal Safety in Multimodal Large Language Models</title><link>https://arxiv.org/abs/2509.12060</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces SSUI dataset for cross-modal safety challenges&lt;/li&gt;&lt;li&gt;Proposes SRPO training framework to align reasoning with safety&lt;/li&gt;&lt;li&gt;Achieves state-of-the-art results on safety benchmarks&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Wei Cai', 'Shujuan Liu', 'Jian Zhao', 'Ziyan Shi', 'Yusheng Zhao', 'Yuchen Yuan', 'Tianle Zhang', 'Chi Zhang', 'Xuelong Li']&lt;/li&gt;&lt;li&gt;Tags: ['safety evaluation', 'robustness', 'multimodal', 'alignment', 'reasoning']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.12060</guid><pubDate>Wed, 17 Sep 2025 00:00:00 +0000</pubDate></item><item><title>TRiSM for Agentic AI: A Review of Trust, Risk, and Security Management in LLM-based Agentic Multi-Agent Systems</title><link>https://arxiv.org/abs/2506.04133</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Reviews Trust, Risk, and Security Management (TRiSM) for LLM-based Agentic Multi-Agent Systems (AMAS)&lt;/li&gt;&lt;li&gt;Proposes a risk taxonomy including adversarial manipulation threats&lt;/li&gt;&lt;li&gt;Introduces security strategies like encryption and adversarial robustness&lt;/li&gt;&lt;li&gt;Develops TRiSM framework pillars: Explainability, ModelOps, Security, Privacy, Lifecycle Governance&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Survey&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Shaina Raza', 'Ranjan Sapkota', 'Manoj Karkee', 'Christos Emmanouilidis']&lt;/li&gt;&lt;li&gt;Tags: ['security', 'risk management', 'adversarial manipulation', 'privacy', 'trust']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.04133</guid><pubDate>Wed, 17 Sep 2025 00:00:00 +0000</pubDate></item><item><title>Probing LLM Hallucination from Within: Perturbation-Driven Approach via Internal Knowledge</title><link>https://arxiv.org/abs/2411.09689</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces hallucination probing to classify LLM-generated text into aligned, misaligned, or fabricated&lt;/li&gt;&lt;li&gt;Proposes SHINE method using prompt perturbations for hallucination detection without external knowledge or training&lt;/li&gt;&lt;li&gt;Achieves state-of-the-art performance across multiple LLMs and datasets&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Seongmin Lee', 'Hsiang Hsu', 'Chun-Fu Chen', 'Duen Horng Chau']&lt;/li&gt;&lt;li&gt;Tags: ['hallucination detection', 'adversarial prompting', 'safety evaluation', 'robustness', 'red teaming']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2411.09689</guid><pubDate>Wed, 17 Sep 2025 00:00:00 +0000</pubDate></item><item><title>xOffense: An AI-driven autonomous penetration testing framework with offensive knowledge-enhanced LLMs and multi agent systems</title><link>https://arxiv.org/abs/2509.13021</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces xOffense, an AI-driven multi-agent penetration testing framework&lt;/li&gt;&lt;li&gt;Leverages fine-tuned LLM (Qwen3-32B) for reasoning and decision-making&lt;/li&gt;&lt;li&gt;Evaluates on AutoPenBench and AI-Pentest-Benchmark with superior results&lt;/li&gt;&lt;li&gt;Outperforms leading systems like VulnBot and PentestGPT&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Phung Duc Luong', 'Le Tran Gia Bao', 'Nguyen Vu Khai Tam', 'Dong Huu Nguyen Khoa', 'Nguyen Huu Quyen', 'Van-Hau Pham', 'Phan The Duy']&lt;/li&gt;&lt;li&gt;Tags: ['red teaming', 'LLM applications', 'penetration testing', 'multi-agent systems']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.13021</guid><pubDate>Wed, 17 Sep 2025 00:00:00 +0000</pubDate></item><item><title>Sy-FAR: Symmetry-based Fair Adversarial Robustness</title><link>https://arxiv.org/abs/2509.12939</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces Sy-FAR, a technique for symmetry-based fair adversarial robustness&lt;/li&gt;&lt;li&gt;Aims to balance attack success rates between classes to ensure fairness&lt;/li&gt;&lt;li&gt;Evaluated on five datasets with three model architectures against realistic attacks&lt;/li&gt;&lt;li&gt;Shows improved fair adversarial robustness and consistency compared to state-of-the-art methods&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Haneen Najjar', 'Eyal Ronen', 'Mahmood Sharif']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial_robustness', 'security', 'fairness', 'red_teaming', 'face_recognition']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.12939</guid><pubDate>Wed, 17 Sep 2025 00:00:00 +0000</pubDate></item><item><title>Jailbreaking Large Language Models Through Content Concretization</title><link>https://arxiv.org/abs/2509.12937</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces Content Concretization (CC), a novel jailbreaking technique for LLMs&lt;/li&gt;&lt;li&gt;Two-stage process: initial response generation followed by iterative refinement&lt;/li&gt;&lt;li&gt;Evaluated with 350 cybersecurity prompts, SR increased from 7% to 62% after 3 refinements&lt;/li&gt;&lt;li&gt;Highlights critical vulnerabilities in current LLM safety mechanisms&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ["Johan Wahr\\'eus", 'Ahmed Hussain', 'Panos Papadimitratos']&lt;/li&gt;&lt;li&gt;Tags: ['jailbreaking', 'adversarial prompting', 'red teaming', 'safety evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.12937</guid><pubDate>Wed, 17 Sep 2025 00:00:00 +0000</pubDate></item><item><title>Defense-to-Attack: Bypassing Weak Defenses Enables Stronger Jailbreaks in Vision-Language Models</title><link>https://arxiv.org/abs/2509.12724</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes Defense2Attack, a novel jailbreak method for VLMs using weak defenses&lt;/li&gt;&lt;li&gt;Incorporates visual and textual optimizers plus a red-team suffix generator&lt;/li&gt;&lt;li&gt;Empirically evaluated on four VLMs and four safety benchmarks&lt;/li&gt;&lt;li&gt;Outperforms state-of-the-art attack methods in single attempts&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yunhan Zhao', 'Xiang Zheng', 'Xingjun Ma']&lt;/li&gt;&lt;li&gt;Tags: ['jailbreak', 'red teaming', 'adversarial attacks', 'vision-language models', 'prompt engineering']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.12724</guid><pubDate>Wed, 17 Sep 2025 00:00:00 +0000</pubDate></item><item><title>A Systematic Evaluation of Parameter-Efficient Fine-Tuning Methods for the Security of Code LLMs</title><link>https://arxiv.org/abs/2509.12649</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Evaluates 7 parameter-efficient fine-tuning methods for improving code LLM security&lt;/li&gt;&lt;li&gt;Prompt-tuning achieved 80.86% secure code rate (13.5% improvement)&lt;/li&gt;&lt;li&gt;Increased robustness against poisoning attacks in TrojanPuzzle evaluation&lt;/li&gt;&lt;li&gt;Effective across Python and Java&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Kiho Lee', 'Jungkon Kim', 'Doowon Kim', 'Hyoungshick Kim']&lt;/li&gt;&lt;li&gt;Tags: ['code LLM security', 'parameter-efficient fine-tuning', 'prompt-tuning', 'poisoning attacks', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.12649</guid><pubDate>Wed, 17 Sep 2025 00:00:00 +0000</pubDate></item><item><title>CIARD: Cyclic Iterative Adversarial Robustness Distillation</title><link>https://arxiv.org/abs/2509.12633</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes CIARD method for adversarial robustness distillation&lt;/li&gt;&lt;li&gt;Introduces multi-teacher framework with contrastive push-loss alignment&lt;/li&gt;&lt;li&gt;Implements continuous adversarial retraining to maintain teacher robustness&lt;/li&gt;&lt;li&gt;Achieves improved adversarial defense rates and clean accuracy&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Liming Lu', 'Shuchao Pang', 'Xu Zheng', 'Xiang Gu', 'Anan Du', 'Yunhuai Liu', 'Yongbin Zhou']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial_robustness', 'knowledge_distillation', 'multi_teacher', 'continuous_retraining']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.12633</guid><pubDate>Wed, 17 Sep 2025 00:00:00 +0000</pubDate></item><item><title>Amulet: a Python Library for Assessing Interactions Among ML Defenses and Risks</title><link>https://arxiv.org/abs/2509.12386</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces Amulet, a Python library for evaluating unintended interactions among ML defenses and risks&lt;/li&gt;&lt;li&gt;Covers security, privacy, and fairness risks with a comprehensive set of attacks, defenses, and metrics&lt;/li&gt;&lt;li&gt;Modular design allows extensibility for new attacks and defenses&lt;/li&gt;&lt;li&gt;User-friendly API for practitioners and researchers&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Asim Waheed', 'Vasisht Duddu', 'Rui Zhang', 'Sebastian Szyller', 'N. Asokan']&lt;/li&gt;&lt;li&gt;Tags: ['security evaluation', 'risk assessment', 'ML safety', 'privacy', 'fairness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.12386</guid><pubDate>Wed, 17 Sep 2025 00:00:00 +0000</pubDate></item><item><title>MORABLES: A Benchmark for Assessing Abstract Moral Reasoning in LLMs with Fables</title><link>https://arxiv.org/abs/2509.12371</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces MORABLES benchmark for moral reasoning in LLMs&lt;/li&gt;&lt;li&gt;Includes adversarial variants to test model vulnerabilities&lt;/li&gt;&lt;li&gt;Finds significant self-contradiction in model responses&lt;/li&gt;&lt;li&gt;Highlights issues with superficial pattern matching&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Matteo Marcuzzo', 'Alessandro Zangari', 'Andrea Albarelli', 'Jose Camacho-Collados', 'Mohammad Taher Pilehvar']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial prompting', 'robustness', 'safety evaluation', 'red teaming', 'model extraction']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.12371</guid><pubDate>Wed, 17 Sep 2025 00:00:00 +0000</pubDate></item><item><title>MEUV: Achieving Fine-Grained Capability Activation in Large Language Models via Mutually Exclusive Unlock Vectors</title><link>https://arxiv.org/abs/2509.12221</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces MEUV for fine-grained capability activation in LLMs&lt;/li&gt;&lt;li&gt;Uses mutually exclusive vectors to unlock specific sensitive topics&lt;/li&gt;&lt;li&gt;Achieves high success rates with reduced cross-topic leakage&lt;/li&gt;&lt;li&gt;Demonstrates language transferability between Chinese and English&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Xin Tong', 'Zhi Lin', 'Jingya Wang', 'Meng Han', 'Bo Jin']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'adversarial prompting', 'alignment', 'safety evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.12221</guid><pubDate>Wed, 17 Sep 2025 00:00:00 +0000</pubDate></item><item><title>The Anatomy of Alignment: Decomposing Preference Optimization by Steering Sparse Features</title><link>https://arxiv.org/abs/2509.12934</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces Feature Steering with Reinforcement Learning (FSRL) for transparent alignment of LLMs&lt;/li&gt;&lt;li&gt;Demonstrates FSRL's effectiveness comparable to RLHF&lt;/li&gt;&lt;li&gt;Analyzes adapter's policy promoting style features over explicit alignment concepts&lt;/li&gt;&lt;li&gt;Highlights need for interpretable alignment mechanisms&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jeremias Ferrao', 'Matthijs van der Lende', 'Ilija Lichkovski', 'Clement Neo']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'safety', 'FSRL', 'SAE', 'mechanistic_analysis']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.12934</guid><pubDate>Wed, 17 Sep 2025 00:00:00 +0000</pubDate></item></channel></rss>