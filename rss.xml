<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>AI Security Paper Digest</title><link>https://kentaroh-toyoda.github.io/ai-security-paper-digest-rss/rss.xml</link><description>Curated papers on AI security from OpenAlex and ArXiv</description><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><lastBuildDate>Thu, 12 Jun 2025 22:40:30 +0000</lastBuildDate><item><title>MCA-Bench: A Multimodal Benchmark for Evaluating CAPTCHA Robustness Against VLM-based Attacks</title><link>https://arxiv.org/abs/2506.05982</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces MCA-Bench, a multimodal benchmark to evaluate CAPTCHA robustness against attacks using vision-language models.&lt;/li&gt;&lt;li&gt;Develops specialized cracking agents fine-tuned on various CAPTCHA types to assess security vulnerabilities consistently across modalities.&lt;/li&gt;&lt;li&gt;Provides quantitative analysis of CAPTCHA challenge complexity, interaction depth, and model solvability, revealing security weaknesses.&lt;/li&gt;&lt;li&gt;Proposes design principles and identifies open challenges for improving CAPTCHA security and benchmarking standards.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Quality Assessment&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Clarity: 5/5&lt;/li&gt;&lt;li&gt;Novelty: 5/5&lt;/li&gt;&lt;li&gt;Significance: 4/5&lt;/li&gt;&lt;li&gt;Try-worthiness: Yes&lt;/li&gt;&lt;li&gt;Justification: The abstract is very well-written, clearly outlining the motivation, contributions, and findings of the work. The introduction of a unified, multimodal benchmark for evaluating CAPTCHA robustness against vision-language model (VLM) attacks is highly novel, as no such comprehensive suite appears to exist. The work is significant for both the security and AI communities, as it enables systematic evaluation and comparison of CAPTCHA schemes in the face of rapidly advancing automated attacks. While the paper is very recent and not yet cited, its potential impact is high, especially given the open release of datasets and code. The benchmark and findings are likely to be valuable for researchers and practitioners, making it worth trying and building upon.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: Zonglin Wu, Yule Xue, Xin Wei, Yiren Song&lt;/li&gt;&lt;li&gt;Tags: ['security vulnerabilities', 'multimodal attacks', 'benchmarking', 'adversarial attacks', 'AI security']&lt;/li&gt;&lt;li&gt;Code Repository: &lt;a href='Not specified in the metadata or abstract, but the abstract states that datasets and code are available online. The actual URL is not provided.'&gt;Not specified in the metadata or abstract, but the abstract states that datasets and code are available online. The actual URL is not provided.&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.05982</guid><pubDate>Thu, 12 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Is Perturbation-Based Image Protection Disruptive to Image Editing?</title><link>https://arxiv.org/abs/2506.04394</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Examines the effectiveness of perturbation-based image protection methods against diffusion model-based image editing.&lt;/li&gt;&lt;li&gt;Finds that current perturbation techniques often fail to prevent desirable edits, as diffusion models can still generate outputs closely aligned with prompts.&lt;/li&gt;&lt;li&gt;Highlights a paradox where added noise may enhance the association between images and text prompts, undermining protection efforts.&lt;/li&gt;&lt;li&gt;Suggests that existing perturbation-based defenses are insufficient for robust protection against misuse of image generation models.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research Paper&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Quality Assessment&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Clarity: 5/5&lt;/li&gt;&lt;li&gt;Novelty: 4/5&lt;/li&gt;&lt;li&gt;Significance: 4/5&lt;/li&gt;&lt;li&gt;Try-worthiness: Yes&lt;/li&gt;&lt;li&gt;Justification: The abstract is well-written, clearly outlining the motivation, methodology, and findings of the paper. The work is novel in that it systematically evaluates the effectiveness of perturbation-based image protection methods against diffusion-based editing, revealing counterintuitive results that challenge current assumptions in the field. While the paper is a preprint and has not yet accumulated citations, the topic is highly relevant given the rapid adoption of diffusion models and the need for robust image protection. The findings are significant as they suggest that current protection methods may be insufficient, which is important for both researchers and practitioners. The paper is worth experimenting with, especially for those interested in image security and generative models. No code repository is provided.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: Qiuyu Tang, Bonor Ayambem, Mooi Choo Chuah, Aparna Bharati&lt;/li&gt;&lt;li&gt;Tags: ['image protection', 'adversarial perturbations', 'diffusion models', 'image editing security', 'model robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.04394</guid><pubDate>Thu, 12 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Diffusion-based Adversarial Purification from the Perspective of the Frequency Domain</title><link>https://arxiv.org/abs/2505.01267</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a novel diffusion-based adversarial purification method targeting adversarial perturbations in images.&lt;/li&gt;&lt;li&gt;Analyzes adversarial perturbations from the frequency domain perspective, focusing on amplitude and phase spectra.&lt;/li&gt;&lt;li&gt;Introduces a technique to selectively preserve low-frequency components to maintain image content and structure while removing adversarial noise.&lt;/li&gt;&lt;li&gt;Demonstrates superior performance compared to existing adversarial defense methods through extensive experiments.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research Paper&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Quality Assessment&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Clarity: 4/5&lt;/li&gt;&lt;li&gt;Novelty: 5/5&lt;/li&gt;&lt;li&gt;Significance: 4/5&lt;/li&gt;&lt;li&gt;Try-worthiness: Yes&lt;/li&gt;&lt;li&gt;Justification: The abstract is well-written and clearly explains both the motivation and the proposed method, though some technical details may require domain knowledge. The approach is novel, leveraging frequency domain analysis for diffusion-based adversarial purification, which is a fresh perspective compared to prior pixel-domain methods. While the paper is very recent and has no citations yet, the topic is significant due to the ongoing importance of adversarial robustness in machine learning. The method claims strong empirical results and a theoretically motivated approach, making it worth trying for researchers or practitioners in adversarial defense. No code repository is provided in the metadata.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: Gaozheng Pei, Ke Ma, Yingfei Sun, Qianqian Xu, Qingming Huang&lt;/li&gt;&lt;li&gt;Tags: ['adversarial examples', 'adversarial purification', 'robustness', 'image security', 'defense methods']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2505.01267</guid><pubDate>Thu, 12 Jun 2025 00:00:00 +0000</pubDate></item><item><title>MVTamperBench: Evaluating Robustness of Vision-Language Models</title><link>https://arxiv.org/abs/2412.19794</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces MVTamperBench, a benchmark for evaluating robustness of multimodal large language models (MLLMs) against five types of adversarial tampering in video data.&lt;/li&gt;&lt;li&gt;Focuses on real-world tampering scenarios such as surveillance interference, social media content edits, and misinformation injection.&lt;/li&gt;&lt;li&gt;Evaluates 45 recent MLLMs across 19 video manipulation tasks, revealing variability in model resilience and that larger models are not necessarily more robust.&lt;/li&gt;&lt;li&gt;Aims to improve tamper-resilience in safety-critical applications like detecting clickbait, preventing harmful content distribution, and enforcing media platform policies.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Quality Assessment&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Clarity: 5/5&lt;/li&gt;&lt;li&gt;Novelty: 5/5&lt;/li&gt;&lt;li&gt;Significance: 4/5&lt;/li&gt;&lt;li&gt;Try-worthiness: Yes&lt;/li&gt;&lt;li&gt;Justification: The abstract is well-written, clearly outlining the motivation, methodology, and contributions of the work. The introduction of a comprehensive benchmark (MVTamperBench) for evaluating the robustness of vision-language models against a wide range of realistic tampering techniques is highly novel and timely, given the increasing deployment of MLLMs in safety-critical applications. The dataset's scale and the systematic evaluation of 45 models across 19 manipulation tasks further enhance its significance. While the paper is very recent and not yet cited, its potential impact is high, especially for researchers and practitioners concerned with trustworthy video understanding. The public release of code and data increases its try-worthiness.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: Amit Agarwal, Srikant Panda, Angeline Charles, Bhargava Kumar, Hitesh Patel, Priyaranjan Pattnayak, Taki Hasan Rafi, Tejaswini Kumar, Hansa Meghwani, Karan Gupta, Dong-Kyu Chae&lt;/li&gt;&lt;li&gt;Tags: ['adversarial robustness', 'multimodal models', 'video tampering', 'misinformation detection', 'safety-critical applications']&lt;/li&gt;&lt;li&gt;Code Repository: &lt;a href='https://amitbcp.github.io/MVTamperBench/'&gt;https://amitbcp.github.io/MVTamperBench/&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2412.19794</guid><pubDate>Thu, 12 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Dynamic Negative Guidance of Diffusion Models</title><link>https://arxiv.org/abs/2410.14398</link><description>&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Other&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: Felix Koulischer, Johannes Deleu, Gabriel Raya, Thomas Demeester, Luca Ambrogioni&lt;/li&gt;&lt;li&gt;Tags: []&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2410.14398</guid><pubDate>Thu, 12 Jun 2025 00:00:00 +0000</pubDate></item><item><title>DAVSP: Safety Alignment for Large Vision-Language Models via Deep Aligned Visual Safety Prompt</title><link>https://arxiv.org/abs/2506.09353</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes DAVSP, a method to enhance safety alignment in large vision-language models against malicious visual queries.&lt;/li&gt;&lt;li&gt;Introduces a Visual Safety Prompt that appends a trainable padding region around input images to preserve features and expand optimization.&lt;/li&gt;&lt;li&gt;Develops Deep Alignment, training the visual safety prompt via supervision in the model's activation space to better detect malicious inputs.&lt;/li&gt;&lt;li&gt;Demonstrates effectiveness across multiple benchmarks and models in resisting malicious queries while maintaining performance on benign inputs.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research Paper&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Quality Assessment&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Clarity: 4/5&lt;/li&gt;&lt;li&gt;Novelty: 4/5&lt;/li&gt;&lt;li&gt;Significance: 3/5&lt;/li&gt;&lt;li&gt;Try-worthiness: Yes&lt;/li&gt;&lt;li&gt;Justification: The abstract is well-written and clearly explains both the problem (LVLM vulnerability to malicious queries) and the proposed solution (DAVSP, with Visual Safety Prompt and Deep Alignment). The approach appears novel, especially the idea of a trainable padding region and supervision in the activation space. While the paper is very recent and on arXiv (preprint), the topic is timely and relevant, and the authors provide extensive experiments and ablation studies. The availability of code further increases the try-worthiness. However, as it is a preprint with no citations yet, its significance is moderate but could grow if the approach proves effective.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: Yitong Zhang, Jia Li, Liyi Cai, Ge Li&lt;/li&gt;&lt;li&gt;Tags: ['AI safety', 'alignment', 'vision-language models', 'adversarial robustness', 'safety evaluation']&lt;/li&gt;&lt;li&gt;Code Repository: &lt;a href='https://github.com/zhangyitonggg/DAVSP'&gt;https://github.com/zhangyitonggg/DAVSP&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.09353</guid><pubDate>Thu, 12 Jun 2025 00:00:00 +0000</pubDate></item><item><title>AngleRoCL: Angle-Robust Concept Learning for Physically View-Invariant T2I Adversarial Patches</title><link>https://arxiv.org/abs/2506.09538</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;The paper investigates the angle robustness of text-to-image (T2I) adversarial patches designed to mislead object detectors in the physical world.&lt;/li&gt;&lt;li&gt;It identifies that current T2I adversarial patches lack robustness when viewed from different angles and that linguistic instructions alone do not improve this robustness.&lt;/li&gt;&lt;li&gt;Proposes Angle-Robust Concept Learning (AngleRoCL), a method to learn text embeddings that guide T2I models to generate adversarial patches with improved angle robustness.&lt;/li&gt;&lt;li&gt;Demonstrates through simulations and physical experiments that AngleRoCL significantly enhances attack success rates across multiple viewing angles on state-of-the-art detectors.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research Paper&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Quality Assessment&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Clarity: 4/5&lt;/li&gt;&lt;li&gt;Novelty: 5/5&lt;/li&gt;&lt;li&gt;Significance: 4/5&lt;/li&gt;&lt;li&gt;Try-worthiness: Yes&lt;/li&gt;&lt;li&gt;Justification: The abstract is well-written and clearly explains the motivation, problem, and proposed solution (AngleRoCL) for improving the angle robustness of T2I adversarial patches. The focus on physical-world angle robustness is novel and addresses a gap in current adversarial patch research. The work is significant as it advances understanding of physical adversarial attacks and their robustness, and demonstrates strong empirical results across multiple detectors and views. While the paper is a preprint and very recent (no citations yet), the topic is timely and relevant for both security and generative AI communities. The approach appears practical and worth experimenting with, especially for researchers in adversarial machine learning and T2I systems. No code repository is provided in the metadata.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: Wenjun Ji, Yuxiang Fu, Luyang Ying, Deng-Ping Fan, Yuyi Wang, Ming-Ming Cheng, Ivor Tsang, Qing Guo&lt;/li&gt;&lt;li&gt;Tags: ['adversarial patches', 'physical-world attacks', 'angle robustness', 'text-to-image models', 'security vulnerabilities']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.09538</guid><pubDate>Thu, 12 Jun 2025 00:00:00 +0000</pubDate></item><item><title>One Pic is All it Takes: Poisoning Visual Document Retrieval Augmented Generation with a Single Image</title><link>https://arxiv.org/abs/2504.02132</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces the first poisoning attack on multi-modal retrieval augmented generation (M-RAG) systems focusing on visual document retrieval.&lt;/li&gt;&lt;li&gt;Proposes two types of attacks: a universal denial-of-service (DoS) attack and a targeted misinformation attack via injecting a single adversarial image into the knowledge base.&lt;/li&gt;&lt;li&gt;Uses a multi-objective gradient-based adversarial method to craft the poisoned image affecting both retrieval and generation components.&lt;/li&gt;&lt;li&gt;Evaluates attack effectiveness across multiple datasets, state-of-the-art embedding retrievers, and large multi-modal models, including analysis of defenses and attack transferability.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research Paper&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Quality Assessment&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Clarity: 4/5&lt;/li&gt;&lt;li&gt;Novelty: 5/5&lt;/li&gt;&lt;li&gt;Significance: 4/5&lt;/li&gt;&lt;li&gt;Try-worthiness: Yes&lt;/li&gt;&lt;li&gt;Justification: The paper is clearly written, with a well-structured abstract that concisely explains the motivation, methods, and results. The novelty is high, as this is the first work to demonstrate a poisoning attack on multi-modal retrieval augmented generation (M-RAG) systems using a single adversarial image in the knowledge base. The significance is also high, given the increasing deployment of M-RAG systems and the practical implications of such vulnerabilities, though the impact is yet to be seen due to the paper's recency and preprint status. The work is worth trying for researchers and practitioners interested in AI security, adversarial attacks, and robust multi-modal systems. No code repository is provided in the metadata.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: Ezzeldin Shereen, Dan Ristea, Shae McFadden, Burak Hasircioglu, Vasilios Mavroudis, Chris Hicks&lt;/li&gt;&lt;li&gt;Tags: ['data poisoning', 'adversarial attack', 'multi-modal models', 'retrieval augmented generation', 'security vulnerabilities']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2504.02132</guid><pubDate>Thu, 12 Jun 2025 00:00:00 +0000</pubDate></item><item><title>GenARM: Reward Guided Generation with Autoregressive Reward Model for Test-time Alignment</title><link>https://arxiv.org/abs/2410.08193</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces GenARM, a test-time alignment method using an Autoregressive Reward Model to guide frozen LLMs without retraining.&lt;/li&gt;&lt;li&gt;Addresses limitations of trajectory-level reward models by predicting next-token rewards for autoregressive generation.&lt;/li&gt;&lt;li&gt;Demonstrates theoretical guarantees within a KL-regularized reinforcement learning framework.&lt;/li&gt;&lt;li&gt;Enables efficient multi-objective alignment and real-time trade-offs between user preferences.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research Paper&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Quality Assessment&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Clarity: 5/5&lt;/li&gt;&lt;li&gt;Novelty: 5/5&lt;/li&gt;&lt;li&gt;Significance: 4/5&lt;/li&gt;&lt;li&gt;Try-worthiness: Yes&lt;/li&gt;&lt;li&gt;Justification: The abstract is very well-written, clearly explaining the motivation, limitations of prior work, and the proposed solution (GenARM). The novelty is high, introducing an autoregressive reward model for test-time alignment, which is a new approach compared to trajectory-level RMs. The significance is promising, as the method claims to match training-time alignment performance and enables new capabilities like multi-objective alignment and efficient weak-to-strong guidance. While the paper is very recent and on arXiv (preprint), the described contributions and experimental results suggest it is worth trying for practitioners interested in LLM alignment. The project page is provided, which likely contains code.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: Yuancheng Xu, Udari Madhushani Sehwag, Alec Koppel, Sicheng Zhu, Bang An, Furong Huang, Sumitra Ganesh&lt;/li&gt;&lt;li&gt;Tags: ['AI safety', 'alignment', 'test-time alignment', 'reward modeling', 'LLM alignment']&lt;/li&gt;&lt;li&gt;Code Repository: &lt;a href='https://genarm.github.io'&gt;https://genarm.github.io&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2410.08193</guid><pubDate>Thu, 12 Jun 2025 00:00:00 +0000</pubDate></item><item><title>MOSAIC: Multiple Observers Spotting AI Content</title><link>https://arxiv.org/abs/2409.07615</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes MOSAIC, a method using multiple observer LLMs to detect AI-generated text content.&lt;/li&gt;&lt;li&gt;Addresses the challenge of distinguishing machine-generated from human-written text, a key AI safety concern.&lt;/li&gt;&lt;li&gt;Improves robustness over single or fixed pairs of detector models by ensembling multiple LLMs.&lt;/li&gt;&lt;li&gt;Provides theoretical grounding and empirical validation for enhanced detection performance.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research Paper&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Quality Assessment&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Clarity: 5/5&lt;/li&gt;&lt;li&gt;Novelty: 4/5&lt;/li&gt;&lt;li&gt;Significance: 4/5&lt;/li&gt;&lt;li&gt;Try-worthiness: Yes&lt;/li&gt;&lt;li&gt;Justification: The abstract is well-written, clearly outlining the motivation, prior work, and the proposed extension (ensembling multiple LLMs for AI-generated text detection). The novelty is solid, as the paper moves beyond existing binary or dual-model approaches by introducing a theoretically grounded ensemble method. While the venue is arXiv (preprint), the topic is highly relevant given the proliferation of LLM-generated content and the need for robust detection. The significance is high due to the practical importance of the problem, though the lack of peer review and citations (due to recency) tempers this slightly. The presence of open-source code and data further increases the paper's try-worthiness for experimentation.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: Matthieu Dubois, Fran\c{c}ois Yvon, Pablo Piantanida&lt;/li&gt;&lt;li&gt;Tags: ['AI safety', 'harmful content detection', 'LLM-generated text detection', 'robustness', 'text modality']&lt;/li&gt;&lt;li&gt;Code Repository: &lt;a href='https://github.com/BaggerOfWords/MOSAIC'&gt;https://github.com/BaggerOfWords/MOSAIC&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2409.07615</guid><pubDate>Thu, 12 Jun 2025 00:00:00 +0000</pubDate></item><item><title>You Are What You Say: Exploiting Linguistic Content for VoicePrivacy Attacks</title><link>https://arxiv.org/abs/2506.09521</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;The paper investigates privacy attacks on speaker anonymization systems by exploiting linguistic content using a language model (BERT) adapted as an automatic speaker verification system.&lt;/li&gt;&lt;li&gt;It demonstrates that speaker identity can be inferred from textual content alone, achieving low error rates for some speakers, highlighting a privacy vulnerability.&lt;/li&gt;&lt;li&gt;The study critiques current privacy evaluation datasets and metrics, suggesting improvements for fairer and unbiased privacy assessments.&lt;/li&gt;&lt;li&gt;It provides insights into the security risks of speaker anonymization systems when linguistic content is not adequately protected.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research Paper&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Quality Assessment&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Clarity: 4/5&lt;/li&gt;&lt;li&gt;Novelty: 4/5&lt;/li&gt;&lt;li&gt;Significance: 3/5&lt;/li&gt;&lt;li&gt;Try-worthiness: Yes&lt;/li&gt;&lt;li&gt;Justification: The paper is clearly written, with a well-structured abstract that explains the motivation, method, and findings. The use of BERT as an ASV system to exploit linguistic content for privacy attacks is a novel angle, especially in the context of speaker anonymization. The significance is moderate: while the topic is important for privacy research, the impact is yet to be seen due to the paper's recency and preprint status. The findings challenge current evaluation protocols and suggest actionable improvements, making it worth experimenting with for researchers in privacy and speech processing. No code repository is provided.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: \"Unal Ege Gaznepoglu, Anna Leschanowsky, Ahmad Aloradi, Prachi Singh, Daniel Tenbrinck, Emanu\"el A. P. Habets, Nils Peters&lt;/li&gt;&lt;li&gt;Tags: ['privacy attacks', 'speaker anonymization', 'automatic speaker verification', 'voice privacy', 'security vulnerabilities']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.09521</guid><pubDate>Thu, 12 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Adversarial Text Generation with Dynamic Contextual Perturbation</title><link>https://arxiv.org/abs/2506.09148</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes Dynamic Contextual Perturbation (DCP), a novel adversarial text attack method that generates context-aware perturbations across multiple text levels.&lt;/li&gt;&lt;li&gt;DCP aims to induce misclassification in NLP models while preserving semantic fidelity and fluency, making attacks more subtle and effective.&lt;/li&gt;&lt;li&gt;The approach leverages pre-trained language models and an adversarial objective to iteratively refine perturbations.&lt;/li&gt;&lt;li&gt;Experimental results demonstrate DCP's ability to challenge the robustness of state-of-the-art NLP systems, emphasizing the importance of context in adversarial attacks.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research Paper&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Quality Assessment&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Clarity: 5/5&lt;/li&gt;&lt;li&gt;Novelty: 4/5&lt;/li&gt;&lt;li&gt;Significance: 3/5&lt;/li&gt;&lt;li&gt;Try-worthiness: Yes&lt;/li&gt;&lt;li&gt;Justification: The abstract is well-written, clearly outlining the motivation, method (Dynamic Contextual Perturbation), and contributions. The focus on context-aware adversarial attacks across multiple text granularities (sentences, paragraphs, documents) is a novel extension over traditional word-level attacks. While the paper is very recent and has no citations yet, the problem addressed is significant for NLP robustness. The use of pre-trained language models and iterative refinement is promising. The lack of a code repository is a drawback, but the described approach is worth experimenting with for researchers in adversarial NLP.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: Hetvi Waghela, Jaydip Sen, Sneha Rakshit, Subhasis Dasgupta&lt;/li&gt;&lt;li&gt;Tags: ['adversarial attacks', 'NLP robustness', 'text perturbation', 'adversarial examples', 'model robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.09148</guid><pubDate>Thu, 12 Jun 2025 00:00:00 +0000</pubDate></item><item><title>From Judgment to Interference: Early Stopping LLM Harmful Outputs via Streaming Content Monitoring</title><link>https://arxiv.org/abs/2506.09996</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a streaming content monitor (SCM) to detect harmful outputs from LLMs early during generation, reducing latency in moderation.&lt;/li&gt;&lt;li&gt;Introduces FineHarm, a dataset with fine-grained token-level annotations for training models to detect harmful content in partial outputs.&lt;/li&gt;&lt;li&gt;Demonstrates that SCM achieves high detection performance (macro F1 &gt; 0.95) by observing only the first 18% of tokens, enabling timely intervention.&lt;/li&gt;&lt;li&gt;Shows that SCM can improve safety alignment by serving as a pseudo-harmfulness annotator, outperforming existing methods like DPO.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research Paper&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Quality Assessment&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Clarity: 4/5&lt;/li&gt;&lt;li&gt;Novelty: 4/5&lt;/li&gt;&lt;li&gt;Significance: 3/5&lt;/li&gt;&lt;li&gt;Try-worthiness: Yes&lt;/li&gt;&lt;li&gt;Justification: The paper is clearly written, with a well-structured abstract that explains the motivation, problem, proposed solution, and results. The novelty is notable: it addresses the training-inference gap in partial detection for LLM output moderation by introducing a new dataset (FineHarm) and a streaming content monitor trained with dual supervision. While the idea of early stopping for harmful outputs is not entirely new, the approach of native partial detection and fine-grained token-level annotation is a meaningful advancement. The significance is moderate, as it targets a practical and timely problem in LLM safety, but is currently a preprint without citations or peer-reviewed validation. The strong experimental results and practical relevance make it worth trying for those working on LLM safety or moderation. No code repository is provided in the metadata.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: Yang Li, Qiang Sheng, Yehan Yang, Xueyao Zhang, Juan Cao&lt;/li&gt;&lt;li&gt;Tags: ['LLM safety', 'harmful output detection', 'early stopping', 'content moderation', 'safety alignment']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.09996</guid><pubDate>Thu, 12 Jun 2025 00:00:00 +0000</pubDate></item><item><title>When Detection Fails: The Power of Fine-Tuned Models to Generate Human-Like Social Media Text</title><link>https://arxiv.org/abs/2506.09975</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Investigates the challenge of detecting AI-generated social media text, especially when generated by fine-tuned language models.&lt;/li&gt;&lt;li&gt;Creates a large dataset of AI-generated social media posts across controversial topics using various LLMs including fine-tuned models.&lt;/li&gt;&lt;li&gt;Finds that detection methods fail significantly when the attacker uses fine-tuned models that are not publicly available.&lt;/li&gt;&lt;li&gt;Highlights security implications for detection systems and influence campaign defenses against sophisticated AI-generated content.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research Paper&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Quality Assessment&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Clarity: 5/5&lt;/li&gt;&lt;li&gt;Novelty: 4/5&lt;/li&gt;&lt;li&gt;Significance: 4/5&lt;/li&gt;&lt;li&gt;Try-worthiness: Yes&lt;/li&gt;&lt;li&gt;Justification: The abstract is well-written, clearly outlining the motivation, methodology, and findings. The focus on fine-tuned LLMs generating undetectable social media text is timely and addresses a realistic threat model, which is a novel angle compared to prior work that often assumes access to the generating model. The creation of a large dataset and the inclusion of a human study add to the paper's significance. Although it is a preprint and very recent (hence no citations yet), the topic is highly relevant for both research and practical applications in AI safety and social media integrity. The lack of a code repository is a minor drawback, but the work is still worth experimenting with, especially for those interested in detection robustness.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: Hillary Dawkins, Kathleen C. Fraser, Svetlana Kiritchenko&lt;/li&gt;&lt;li&gt;Tags: ['AI security', 'adversarial prompting', 'LLM red teaming', 'detection evasion', 'fine-tuning attacks']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.09975</guid><pubDate>Thu, 12 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Memorization in Language Models through the Lens of Intrinsic Dimension</title><link>https://arxiv.org/abs/2506.09591</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Investigates unintended memorization in language models, which can lead to privacy leakage and intellectual property disclosure.&lt;/li&gt;&lt;li&gt;Analyzes the role of Intrinsic Dimension (ID) as a factor influencing the likelihood of memorization in latent space.&lt;/li&gt;&lt;li&gt;Finds that sequences with higher intrinsic dimension are less likely to be memorized, especially in overparameterized models and sparse exposure scenarios.&lt;/li&gt;&lt;li&gt;Highlights the interaction between model scale, data exposure, and sequence complexity in shaping memorization behavior.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research Paper&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Quality Assessment&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Clarity: 4/5&lt;/li&gt;&lt;li&gt;Novelty: 4/5&lt;/li&gt;&lt;li&gt;Significance: 3/5&lt;/li&gt;&lt;li&gt;Try-worthiness: Yes&lt;/li&gt;&lt;li&gt;Justification: The abstract is well-written and clearly explains the motivation, methodology, and findings of the paper, though some technical terms (e.g., Intrinsic Dimension) may require background knowledge. The work is novel in its focus on the geometric property of intrinsic dimension as a factor in memorization, which is less explored compared to other factors like context length or parameter size. The significance is moderate: while memorization in LMs is an important topic, the paper is a preprint and has not yet been peer-reviewed or cited, so its impact is still uncertain. However, the findings could be valuable for researchers concerned with privacy and model behavior, making it worth experimenting with. No code repository is provided in the metadata.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: Stefan Arnold&lt;/li&gt;&lt;li&gt;Tags: ['privacy attacks', 'membership inference', 'model memorization', 'language models', 'latent space analysis']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.09591</guid><pubDate>Thu, 12 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Token Constraint Decoding Improves Robustness on Question Answering for Large Language Models</title><link>https://arxiv.org/abs/2506.09408</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces Token Constraint Decoding (TCD), an inference-time algorithm to improve robustness of LLMs against input perturbations in multiple-choice question answering tasks.&lt;/li&gt;&lt;li&gt;Demonstrates that TCD significantly restores performance degraded by noisy inputs, with up to +39% gains for smaller models.&lt;/li&gt;&lt;li&gt;Shows that TCD acts as a regularizer to reduce overconfident outputs, enhancing reasoning stability.&lt;/li&gt;&lt;li&gt;Highlights the potential of TCD to improve reliability and safety of LLM deployment in real-world, safety-critical applications.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research Paper&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Quality Assessment&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Clarity: 5/5&lt;/li&gt;&lt;li&gt;Novelty: 4/5&lt;/li&gt;&lt;li&gt;Significance: 4/5&lt;/li&gt;&lt;li&gt;Try-worthiness: Yes&lt;/li&gt;&lt;li&gt;Justification: The abstract is well-written, clearly explaining the motivation, method (Token Constraint Decoding), and results, making the paper easy to understand (clarity: 5). The approach of enforcing token-level constraints at inference to improve robustness in LLMs appears to be a novel and practical contribution, especially as it is model-agnostic and inference-time only (novelty: 4). The significance is high given the importance of robustness in real-world LLM deployments, and the reported improvements (up to +39% for weaker models) are substantial; however, as an arXiv preprint with no citations yet, it is not yet fully validated by the community (significance: 4). The method is simple, practical, and shows strong empirical gains, making it worth trying for practitioners concerned with LLM robustness (try_worthiness: true). No code repository is listed in the metadata.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: Jui-Ming Yao, Hao-Yuan Chen, Zi-Xian Tang, Bing-Jia Tan, Sheng-Wei Peng, Bing-Cheng Xie, Shun-Feng Su&lt;/li&gt;&lt;li&gt;Tags: ['robustness', 'adversarial robustness', 'LLM safety', 'inference-time defenses', 'prompt engineering']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.09408</guid><pubDate>Thu, 12 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Towards Efficient and Effective Alignment of Large Language Models</title><link>https://arxiv.org/abs/2506.09329</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes novel methodologies for efficient and effective alignment of large language models with human expectations.&lt;/li&gt;&lt;li&gt;Introduces Lion, an adversarial distillation framework to iteratively refine training data by generating challenging instructions.&lt;/li&gt;&lt;li&gt;Develops Learning to Edit (LTE) for efficient knowledge integration and Bridging and Modeling Correlations (BMC) to improve preference optimization.&lt;/li&gt;&lt;li&gt;Presents FollowBench, a fine-grained benchmark to evaluate LLMs' adherence to complex constraints, highlighting current weaknesses.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research Paper&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Quality Assessment&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Clarity: 5/5&lt;/li&gt;&lt;li&gt;Novelty: 5/5&lt;/li&gt;&lt;li&gt;Significance: 4/5&lt;/li&gt;&lt;li&gt;Try-worthiness: Yes&lt;/li&gt;&lt;li&gt;Justification: The abstract is exceptionally clear, outlining specific contributions in data collection (Lion, WebR), training (LTE, BMC), and evaluation (FollowBench) for LLM alignment. The methodologies introduced—such as adversarial distillation, automated web-based data synthesis, meta-learning for knowledge editing, and token-level preference modeling—are novel and address current limitations in the field. The significance is high given the centrality of alignment in LLM research, though as a very recent preprint, it has not yet accumulated citations or peer-reviewed validation. The breadth and depth of contributions make it highly worth trying for researchers and practitioners interested in LLM alignment. No code repository is provided in the metadata or abstract.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: Yuxin Jiang&lt;/li&gt;&lt;li&gt;Tags: ['AI safety', 'alignment', 'LLM alignment', 'safety evaluation', 'benchmarking']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.09329</guid><pubDate>Thu, 12 Jun 2025 00:00:00 +0000</pubDate></item><item><title>AdversariaL attacK sAfety aLIgnment(ALKALI): Safeguarding LLMs through GRACE: Geometric Representation-Aware Contrastive Enhancement- Introducing Adversarial Vulnerability Quality Index (AVQI)</title><link>https://arxiv.org/abs/2506.08885</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces ALKALI, a comprehensive adversarial benchmark with 9,000 prompts targeting LLM safety alignment vulnerabilities.&lt;/li&gt;&lt;li&gt;Identifies a novel vulnerability called latent camouflage where adversarial prompts mimic the latent geometry of safe completions to evade defenses.&lt;/li&gt;&lt;li&gt;Proposes GRACE, a geometric representation-aware contrastive enhancement framework that improves alignment by enforcing latent space separation between safe and adversarial outputs.&lt;/li&gt;&lt;li&gt;Presents AVQI, a new metric quantifying latent alignment failures by measuring cluster separation and compactness in model embeddings.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research Paper&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Quality Assessment&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Clarity: 4/5&lt;/li&gt;&lt;li&gt;Novelty: 5/5&lt;/li&gt;&lt;li&gt;Significance: 4/5&lt;/li&gt;&lt;li&gt;Try-worthiness: Yes&lt;/li&gt;&lt;li&gt;Justification: The abstract is dense but well-written, clearly outlining the problem (latent geometric vulnerabilities in LLM alignment), the contributions (ALKALI benchmark, GRACE framework, AVQI metric), and the results (up to 39% ASR reduction). The work is highly novel, introducing new concepts (latent camouflage, AVQI) and a comprehensive adversarial benchmark. Its significance is high given the current focus on LLM safety and alignment, though as a very recent preprint, it has not yet accumulated citations. The public code and benchmark, along with promising results, make it worth trying for researchers and practitioners interested in LLM robustness and safety.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: Danush Khanna, Krishna Kumar, Basab Ghosh, Vinija Jain, Vasu Sharma, Aman Chadha, Amitava Das&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'adversarial prompting', 'alignment', 'safety evaluation', 'latent space regularization']&lt;/li&gt;&lt;li&gt;Code Repository: &lt;a href='https://anonymous.4open.science/r/alkali-B416/README.md'&gt;https://anonymous.4open.science/r/alkali-B416/README.md&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.08885</guid><pubDate>Thu, 12 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Generate-then-Verify: Reconstructing Data from Limited Published Statistics</title><link>https://arxiv.org/abs/2504.21199</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;The paper addresses reconstructing sensitive tabular data from limited published aggregate statistics, highlighting privacy risks.&lt;/li&gt;&lt;li&gt;It introduces a novel generate-then-verify integer programming approach to identify subsets of data guaranteed to be correct despite incomplete information.&lt;/li&gt;&lt;li&gt;The work demonstrates that privacy violations can occur even when only sparse aggregate data is published, emphasizing risks in data release practices.&lt;/li&gt;&lt;li&gt;The focus is on privacy attacks related to data reconstruction, which is a key concern in AI security and privacy.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research Paper&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Quality Assessment&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Clarity: 4/5&lt;/li&gt;&lt;li&gt;Novelty: 5/5&lt;/li&gt;&lt;li&gt;Significance: 4/5&lt;/li&gt;&lt;li&gt;Try-worthiness: Yes&lt;/li&gt;&lt;li&gt;Justification: The abstract is well-written and clearly explains the motivation, problem setting, and proposed approach, though some technical terms may require background knowledge (clarity: 4). The paper introduces a new problem setting (partial data reconstruction) and a novel generate-then-verify integer programming method, which appears to be a significant departure from prior work that focused on full reconstruction (novelty: 5). The significance is high, as the work addresses privacy risks in real-world census data even when published statistics are sparse, and the evaluation on U.S. Census data adds practical relevance; however, as a preprint with no citations yet, its impact is still emerging (significance: 4). The approach is worth trying for researchers in privacy, data security, or statistical disclosure control, as it offers a new attack vector and methodology (try_worthiness: true). No code repository is provided in the metadata.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: Terrance Liu, Eileen Xiao, Adam Smith, Pratiksha Thaker, Zhiwei Steven Wu&lt;/li&gt;&lt;li&gt;Tags: ['data reconstruction', 'privacy attacks', 'membership inference', 'data privacy', 'security vulnerabilities']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2504.21199</guid><pubDate>Thu, 12 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Model Attribution and Detection of Synthetic Speech via Vocoder Fingerprints</title><link>https://arxiv.org/abs/2411.14013</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes methods for attributing synthetic speech to specific vocoder models, aiding in model attribution and detection.&lt;/li&gt;&lt;li&gt;Develops techniques to distinguish synthetic speech from real speech with high accuracy.&lt;/li&gt;&lt;li&gt;Introduces vocoder fingerprints based on residuals between audio signals and filtered versions as a robust detection feature.&lt;/li&gt;&lt;li&gt;Evaluates robustness of the detection method against noise, demonstrating resilience.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research Paper&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Quality Assessment&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Clarity: 5/5&lt;/li&gt;&lt;li&gt;Novelty: 4/5&lt;/li&gt;&lt;li&gt;Significance: 4/5&lt;/li&gt;&lt;li&gt;Try-worthiness: Yes&lt;/li&gt;&lt;li&gt;Justification: The abstract is well-written, clearly outlining the three main tasks addressed and the methodology (vocoder fingerprints via standardized average residuals). The approach appears novel, especially in leveraging residuals after low-pass or EnCodec filtering for attribution and detection, and the results (AUROC &gt;99%) are impressive. The significance is high given the growing importance of synthetic speech detection, though the paper is a preprint and very new, so real-world impact and citations are yet to be seen. The robustness to noise further increases its practical value. No code repository is provided in the metadata.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: Mat\'ias Pizarro, Mike Laszkiewicz, Shawkat Hesso, Dorothea Kolossa, Asja Fischer&lt;/li&gt;&lt;li&gt;Tags: ['model attribution', 'synthetic speech detection', 'audio deepfake detection', 'security vulnerabilities', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2411.14013</guid><pubDate>Thu, 12 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Automatic Pseudo-Harmful Prompt Generation for Evaluating False Refusals in Large Language Models</title><link>https://arxiv.org/abs/2409.00598</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a novel method to automatically generate pseudo-harmful prompts to evaluate false refusal behaviors in safety-aligned LLMs.&lt;/li&gt;&lt;li&gt;Constructs PHTest, a large-scale evaluation dataset that captures diverse false refusal patterns and controversial prompts.&lt;/li&gt;&lt;li&gt;Evaluates 20 LLMs on PHTest, revealing a trade-off between reducing false refusals and maintaining robustness against jailbreak attacks.&lt;/li&gt;&lt;li&gt;Demonstrates that many jailbreak defenses increase false refusal rates, impacting model usability and safety balance.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research Paper&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Quality Assessment&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Clarity: 5/5&lt;/li&gt;&lt;li&gt;Novelty: 5/5&lt;/li&gt;&lt;li&gt;Significance: 4/5&lt;/li&gt;&lt;li&gt;Try-worthiness: Yes&lt;/li&gt;&lt;li&gt;Justification: The abstract is very clear, well-written, and precisely explains the motivation, method, and contributions. The work is novel as it proposes the first automatic method for generating pseudo-harmful prompts and introduces a large, labeled dataset (PHTest) for evaluating false refusals in LLMs—a topic of growing importance. The significance is high given the practical impact on LLM usability and safety, though the paper is very new and only on arXiv, so it has not yet been peer-reviewed or cited. The availability of code and data makes it highly try-worthy for researchers and practitioners interested in LLM safety and alignment.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: Bang An, Sicheng Zhu, Ruiyi Zhang, Michael-Andrei Panaitescu-Liess, Yuancheng Xu, Furong Huang&lt;/li&gt;&lt;li&gt;Tags: ['LLM safety', 'false refusal', 'jailbreak defense', 'prompt generation', 'safety evaluation']&lt;/li&gt;&lt;li&gt;Code Repository: &lt;a href='https://github.com/umd-huang-lab/FalseRefusal'&gt;https://github.com/umd-huang-lab/FalseRefusal&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2409.00598</guid><pubDate>Thu, 12 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Distributionally and Adversarially Robust Logistic Regression via Intersecting Wasserstein Balls</title><link>https://arxiv.org/abs/2407.13625</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;The paper addresses adversarially robust optimization (ARO) to improve model robustness against adversarial attacks.&lt;/li&gt;&lt;li&gt;It proposes a distributionally robust (DR) approach using intersecting Wasserstein balls to better hedge against distributional shifts and adversarial examples.&lt;/li&gt;&lt;li&gt;The method is applied to logistic regression and includes tractable convex optimization reformulations and efficient algorithms.&lt;/li&gt;&lt;li&gt;Empirical results demonstrate improved robustness compared to benchmark methods.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research Paper&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Quality Assessment&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Clarity: 4/5&lt;/li&gt;&lt;li&gt;Novelty: 4/5&lt;/li&gt;&lt;li&gt;Significance: 3/5&lt;/li&gt;&lt;li&gt;Try-worthiness: Yes&lt;/li&gt;&lt;li&gt;Justification: The abstract is well-written and clearly explains the motivation, approach, and contributions of the paper, though it is somewhat dense with technical terms. The work appears novel in its intersection of adversarial and distributional robustness via intersecting Wasserstein balls, and the convex reformulation for logistic regression is a notable contribution. The significance is moderate at this stage, as it is a very recent preprint with no citations yet and is published on arXiv, but the approach addresses a relevant problem in robust machine learning. The method is worth trying, especially for those interested in robust optimization and adversarial defense, as the abstract claims empirical improvements and efficient algorithms. No code repository is provided in the metadata.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: Aras Selvi, Eleonora Kreacic, Mohsen Ghassemi, Vamsi Potluru, Tucker Balch, Manuela Veloso&lt;/li&gt;&lt;li&gt;Tags: ['adversarial robustness', 'distributionally robust optimization', 'adversarial examples', 'robustness against attacks', 'optimization algorithms']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2407.13625</guid><pubDate>Thu, 12 Jun 2025 00:00:00 +0000</pubDate></item><item><title>FastLloyd: Federated, Accurate, Secure, and Tunable $k$-Means Clustering with Differential Privacy</title><link>https://arxiv.org/abs/2405.02437</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a federated k-means clustering algorithm that ensures privacy using differential privacy (DP) and secure computation.&lt;/li&gt;&lt;li&gt;Addresses privacy-preserving clustering in a federated setting, improving both privacy guarantees and computational efficiency.&lt;/li&gt;&lt;li&gt;Introduces enhancements to DP mechanisms and secure aggregation to achieve significant speed-ups and better utility compared to prior work.&lt;/li&gt;&lt;li&gt;Focuses on privacy protection and security in distributed machine learning, relevant to AI security and privacy attacks.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research Paper&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Quality Assessment&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Clarity: 4/5&lt;/li&gt;&lt;li&gt;Novelty: 5/5&lt;/li&gt;&lt;li&gt;Significance: 4/5&lt;/li&gt;&lt;li&gt;Try-worthiness: Yes&lt;/li&gt;&lt;li&gt;Justification: The abstract is well-written and clearly outlines the motivation, challenges, and contributions of the paper, though some technical terms may require background knowledge. The work appears highly novel, proposing a federated k-means clustering approach that is both differentially private and computationally efficient, addressing limitations of both secure computation and DP models. The claimed five orders of magnitude speed-up and improved utility over state-of-the-art methods suggest significant impact, especially in privacy-preserving machine learning. While the paper is very recent and has no citations yet, its relevance to federated learning and privacy makes it significant. The approach seems promising and worth experimenting with, particularly for researchers or practitioners interested in privacy-preserving clustering. No code repository is provided in the metadata.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: Abdulrahman Diaa, Thomas Humphries, Florian Kerschbaum&lt;/li&gt;&lt;li&gt;Tags: ['federated learning', 'differential privacy', 'privacy attacks', 'secure computation', 'privacy-preserving machine learning']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2405.02437</guid><pubDate>Thu, 12 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Design Patterns for Securing LLM Agents against Prompt Injections</title><link>https://arxiv.org/abs/2506.08837</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Addresses security challenges of LLM-powered AI agents, focusing on prompt injection attacks.&lt;/li&gt;&lt;li&gt;Proposes principled design patterns to build AI agents with provable resistance to prompt injection.&lt;/li&gt;&lt;li&gt;Analyzes trade-offs between utility and security of the proposed design patterns.&lt;/li&gt;&lt;li&gt;Demonstrates real-world applicability through case studies.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research Paper&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Quality Assessment&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Clarity: 4/5&lt;/li&gt;&lt;li&gt;Novelty: 4/5&lt;/li&gt;&lt;li&gt;Significance: 4/5&lt;/li&gt;&lt;li&gt;Try-worthiness: Yes&lt;/li&gt;&lt;li&gt;Justification: The paper is clearly written, with a well-structured abstract that outlines the motivation, problem, and contributions. The focus on design patterns for securing LLM agents against prompt injection is timely and relevant, given the increasing deployment of LLM-powered agents and the growing awareness of prompt injection vulnerabilities. The approach of proposing 'principled design patterns' and analyzing their trade-offs appears novel, as most prior work focuses on detection or mitigation rather than systematic design principles. The significance is high due to the critical nature of prompt injection threats, though the impact is yet to be seen as the paper is very recent and published as a preprint. The inclusion of case studies suggests practical applicability, making it worth experimenting with or implementing. No code repository is provided.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: Luca Beurer-Kellner, Beat Buesser Ana-Maria Cre\c{t}u, Edoardo Debenedetti, Daniel Dobos, Daniel Fabian, Marc Fischer, David Froelicher, Kathrin Grosse, Daniel Naeff, Ezinwanne Ozoani, Andrew Paverd, Florian Tram\`er, V\'aclav Volhejn&lt;/li&gt;&lt;li&gt;Tags: ['prompt injection', 'LLM security', 'AI agent security', 'adversarial prompting', 'security design patterns']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.08837</guid><pubDate>Thu, 12 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Mitigating Reward Over-optimization in Direct Alignment Algorithms with Importance Sampling</title><link>https://arxiv.org/abs/2506.08681</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Addresses alignment of large language models (LLMs) with human values using Direct Alignment Algorithms (DAAs).&lt;/li&gt;&lt;li&gt;Identifies and mitigates the problem of reward over-optimization where models drift away from reference policies during training.&lt;/li&gt;&lt;li&gt;Proposes an importance-sampling based method (IS-DAAs) to reduce over-optimization and stabilize training.&lt;/li&gt;&lt;li&gt;Demonstrates improved performance and robustness compared to existing methods, contributing to safer and more aligned AI behavior.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research Paper&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Quality Assessment&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Clarity: 4/5&lt;/li&gt;&lt;li&gt;Novelty: 4/5&lt;/li&gt;&lt;li&gt;Significance: 3/5&lt;/li&gt;&lt;li&gt;Try-worthiness: Yes&lt;/li&gt;&lt;li&gt;Justification: The abstract is well-written and clearly explains the motivation, method, and results, though some technical terms may require background knowledge. The use of importance sampling to mitigate over-optimization in Direct Alignment Algorithms appears to be a novel contribution, especially with the addition of clipping to control variance. The significance is moderate: while the problem is important and the solution promising, the paper is very new, has no citations yet, and is only a preprint. However, the method addresses a real pain point in DAA training and claims better performance than existing solutions, making it worth trying for practitioners in LLM alignment. The code is mentioned as publicly available, but no direct link is provided in the metadata.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: Phuc Minh Nguyen, Ngoc-Hieu Nguyen, Duy H. M. Nguyen, Anji Liu, An Mai, Binh T. Nguyen, Daniel Sonntag, Khoa D. Doan&lt;/li&gt;&lt;li&gt;Tags: ['AI safety', 'alignment', 'reward over-optimization', 'importance sampling', 'large language models']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.08681</guid><pubDate>Thu, 12 Jun 2025 00:00:00 +0000</pubDate></item><item><title>AsFT: Anchoring Safety During LLM Fine-Tuning Within Narrow Safety Basin</title><link>https://arxiv.org/abs/2506.08473</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Identifies a narrow safety basin in LLM parameter space where perturbations along alignment direction preserve safety, while orthogonal perturbations degrade it.&lt;/li&gt;&lt;li&gt;Proposes AsFT, a fine-tuning method that anchors updates along the alignment direction to maintain safety during LLM fine-tuning.&lt;/li&gt;&lt;li&gt;Demonstrates that AsFT reduces harmful behaviors and improves model performance compared to existing methods like Safe LoRA.&lt;/li&gt;&lt;li&gt;Provides extensive experimental validation and open-source code for reproducibility.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research Paper&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Quality Assessment&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Clarity: 4/5&lt;/li&gt;&lt;li&gt;Novelty: 4/5&lt;/li&gt;&lt;li&gt;Significance: 3/5&lt;/li&gt;&lt;li&gt;Try-worthiness: Yes&lt;/li&gt;&lt;li&gt;Justification: The paper is clearly written, with a well-structured abstract that explains the motivation, method (AsFT), and results. The idea of using the alignment direction as an anchor for safety during LLM fine-tuning appears novel and is well-motivated by the observed geometry of the parameter space. While the venue is arXiv (preprint), the topic is highly relevant given ongoing concerns about LLM safety, and the reported improvements over Safe LoRA are promising. The significance is moderate due to the early stage (no citations yet), but the method's practical focus and available code make it worth trying for researchers and practitioners interested in safe LLM fine-tuning.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: Shuo Yang, Qihui Zhang, Yuyang Liu, Yue Huang, Xiaojun Jia, Kunpeng Ning, Jiayu Yao, Jigang Wang, Hailiang Dai, Yibing Song, Li Yuan&lt;/li&gt;&lt;li&gt;Tags: ['LLM safety', 'fine-tuning safety', 'alignment', 'harmful output prevention', 'model robustness']&lt;/li&gt;&lt;li&gt;Code Repository: &lt;a href='https://github.com/PKU-YuanGroup/AsFT'&gt;https://github.com/PKU-YuanGroup/AsFT&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.08473</guid><pubDate>Thu, 12 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Certified Unlearning for Neural Networks</title><link>https://arxiv.org/abs/2506.06985</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a novel certified machine unlearning method for neural networks to remove the influence of specific training data upon request.&lt;/li&gt;&lt;li&gt;Addresses privacy concerns and regulatory requirements such as the 'right to be forgotten' with formal unlearning guarantees.&lt;/li&gt;&lt;li&gt;Uses noisy fine-tuning on retained data to achieve privacy amplification and certified unlearning without restrictive assumptions.&lt;/li&gt;&lt;li&gt;Demonstrates theoretical trade-offs and empirical effectiveness, outperforming existing baselines.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research Paper&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Quality Assessment&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Clarity: 5/5&lt;/li&gt;&lt;li&gt;Novelty: 5/5&lt;/li&gt;&lt;li&gt;Significance: 4/5&lt;/li&gt;&lt;li&gt;Try-worthiness: Yes&lt;/li&gt;&lt;li&gt;Justification: The abstract is well-written, clearly stating the problem, motivation, limitations of prior work, and the proposed solution. The method introduces a novel approach to certified machine unlearning using noisy fine-tuning and privacy amplification, which appears to be a significant advancement over existing methods that lack formal guarantees or require restrictive assumptions. While the paper is very recent and has no citations yet, the topic is highly relevant due to increasing privacy regulations and the need for practical unlearning solutions in neural networks. The availability of code further increases its try-worthiness for practitioners and researchers.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: Anastasia Koloskova, Youssef Allouah, Animesh Jha, Rachid Guerraoui, Sanmi Koyejo&lt;/li&gt;&lt;li&gt;Tags: ['machine unlearning', 'privacy', 'data removal', 'regulatory compliance', 'neural networks']&lt;/li&gt;&lt;li&gt;Code Repository: &lt;a href='https://github.com/stair-lab/certified-unlearning-neural-networks-icml-2025'&gt;https://github.com/stair-lab/certified-unlearning-neural-networks-icml-2025&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.06985</guid><pubDate>Thu, 12 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Why Are Web AI Agents More Vulnerable Than Standalone LLMs? A Security Analysis</title><link>https://arxiv.org/abs/2502.20383</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Analyzes why Web AI agents are more vulnerable to adversarial inputs compared to standalone LLMs despite using the same underlying models.&lt;/li&gt;&lt;li&gt;Identifies three key factors increasing vulnerability: embedding user goals in system prompts, multi-step action generation, and observational capabilities.&lt;/li&gt;&lt;li&gt;Proposes a component-level, systematic evaluation framework to better assess security weaknesses beyond simple success metrics.&lt;/li&gt;&lt;li&gt;Provides insights and recommendations for improving security and robustness in the design of Web AI agents.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research Paper&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Quality Assessment&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Clarity: 4/5&lt;/li&gt;&lt;li&gt;Novelty: 4/5&lt;/li&gt;&lt;li&gt;Significance: 3/5&lt;/li&gt;&lt;li&gt;Try-worthiness: Yes&lt;/li&gt;&lt;li&gt;Justification: The paper is clearly written, with a well-structured abstract that outlines the motivation, problem, methodology, and findings. The focus on the increased vulnerability of Web AI agents compared to standalone LLMs is timely and relevant, especially as web-based agents become more prevalent. The identification of specific factors (system prompt embedding, multi-step action generation, observational capabilities) contributing to vulnerabilities is a novel and actionable contribution. While the paper is a preprint and very recent (hence no citations yet), the topic is significant for the AI security community. The proposed systematic evaluation framework and component-level analysis make it worth experimenting with or building upon. No code repository is provided in the metadata.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: Jeffrey Yang Fan Chiang, Seungjae Lee, Jia-Bin Huang, Furong Huang, Yizheng Chen&lt;/li&gt;&lt;li&gt;Tags: ['AI security', 'adversarial prompting', 'LLM vulnerabilities', 'AI agent robustness', 'security evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2502.20383</guid><pubDate>Thu, 12 Jun 2025 00:00:00 +0000</pubDate></item><item><title>WaKA: Data Attribution using K-Nearest Neighbors and Membership Privacy Principles</title><link>https://arxiv.org/abs/2411.01357</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces WaKA, a novel data attribution method combining k-nearest neighbors and membership inference attack principles.&lt;/li&gt;&lt;li&gt;WaKA measures individual data points' contributions to model loss distribution without sampling subsets, improving computational efficiency.&lt;/li&gt;&lt;li&gt;Can be used both a posteriori as a membership inference attack to assess privacy risks and a priori for privacy influence measurement and data valuation.&lt;/li&gt;&lt;li&gt;Demonstrates robustness and effectiveness on real-world datasets, outperforming or matching existing methods like LiRA and Shapley Values.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research Paper&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Quality Assessment&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Clarity: 4/5&lt;/li&gt;&lt;li&gt;Novelty: 4/5&lt;/li&gt;&lt;li&gt;Significance: 3/5&lt;/li&gt;&lt;li&gt;Try-worthiness: Yes&lt;/li&gt;&lt;li&gt;Justification: The abstract is well-written and clearly explains the motivation, methodology, and contributions of WaKA. The method appears novel, combining Wasserstein distances, k-NN, and membership privacy principles in a unified framework for data attribution and privacy risk assessment. The approach is positioned as more computationally efficient than LiRA and more robust than Shapley Values for certain tasks, which is promising. However, as the paper is a very recent arXiv preprint with no citations yet, its significance is not fully established, though the addressed problem is important. The method's versatility and claimed efficiency make it worth trying for practitioners interested in data attribution, privacy, or k-NN models. No code repository is provided in the metadata.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: Patrick Mesana, Cl\'ement B\'enesse, Hadrien Lautraite, Gilles Caporossi, S\'ebastien Gambs&lt;/li&gt;&lt;li&gt;Tags: ['membership inference attack', 'data attribution', 'privacy attacks', 'model privacy risk', 'data valuation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2411.01357</guid><pubDate>Thu, 12 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Rewind-to-Delete: Certified Machine Unlearning for Nonconvex Functions</title><link>https://arxiv.org/abs/2409.09778</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a certified machine unlearning algorithm for general nonconvex loss functions, enabling efficient removal of data from trained models without full retraining.&lt;/li&gt;&lt;li&gt;Provides theoretical guarantees on privacy (differential privacy-based certified unlearning), utility, and computational complexity.&lt;/li&gt;&lt;li&gt;Introduces a novel 'rewind-to-delete' approach that rewinds model training to an earlier step before continuing gradient descent on retained data.&lt;/li&gt;&lt;li&gt;Demonstrates improved performance over existing certified unlearning methods in experiments reflecting practical data removal scenarios.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research Paper&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Quality Assessment&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Clarity: 4/5&lt;/li&gt;&lt;li&gt;Novelty: 5/5&lt;/li&gt;&lt;li&gt;Significance: 4/5&lt;/li&gt;&lt;li&gt;Try-worthiness: Yes&lt;/li&gt;&lt;li&gt;Justification: The abstract is well-written and clearly explains the motivation, contributions, and results of the paper, though some technical terms may be challenging for non-experts. The work is highly novel, proposing the first certified unlearning algorithm for general nonconvex loss functions that is black-box and first-order, addressing a significant gap in the literature. The significance is high given the importance of machine unlearning for privacy and compliance, and the theoretical and empirical results strengthen its impact, though the venue (arXiv) and lack of citations (due to recency) slightly temper this. The approach appears practical and broadly applicable, making it worth trying for researchers and practitioners interested in privacy-preserving machine learning. No code repository is provided in the metadata.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: Siqiao Mu, Diego Klabjan&lt;/li&gt;&lt;li&gt;Tags: ['machine unlearning', 'data privacy', 'differential privacy', 'model update security', 'nonconvex optimization']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2409.09778</guid><pubDate>Thu, 12 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Share Secrets for Privacy: Confidential Forecasting with Vertical Federated Learning</title><link>https://arxiv.org/abs/2405.20761</link><description>&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Other&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Quality Assessment&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Clarity: 4/5&lt;/li&gt;&lt;li&gt;Novelty: 5/5&lt;/li&gt;&lt;li&gt;Significance: 4/5&lt;/li&gt;&lt;li&gt;Try-worthiness: Yes&lt;/li&gt;&lt;li&gt;Justification: The abstract is well-written and clearly outlines the motivation, challenges, and contributions of the paper, though some technical details may require domain knowledge. The work is highly novel, proposing a new framework (STV) for privacy-preserving time series forecasting in vertical federated learning, with new algorithms for secure matrix operations and exact optimization. The significance is high given the practical importance of privacy in federated learning and the strong reported improvements over state-of-the-art methods, though the venue (arXiv) is not peer-reviewed and the paper is very recent, so impact is yet to be established. The presence of a public code repository and promising results make this paper worth trying for practitioners and researchers interested in privacy-preserving forecasting.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: Aditya Shankar, J\'er\'emie Decouchant, Dimitra Gkorou, Rihan Hai, Lydia Y. Chen&lt;/li&gt;&lt;li&gt;Tags: []&lt;/li&gt;&lt;li&gt;Code Repository: &lt;a href='https://github.com/adis98/STV'&gt;https://github.com/adis98/STV&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2405.20761</guid><pubDate>Thu, 12 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Byzantine-Resilient Decentralized Multi-Armed Bandits</title><link>https://arxiv.org/abs/2310.07320</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;The paper addresses decentralized multi-armed bandit problems in the presence of Byzantine agents that can send arbitrary, potentially malicious information.&lt;/li&gt;&lt;li&gt;It proposes a Byzantine-resilient decentralized UCB algorithm that truncates inconsistent and extreme values to mitigate the impact of adversarial agents.&lt;/li&gt;&lt;li&gt;The approach ensures that normal agents achieve regret performance comparable to classic single-agent UCB, and collectively outperform non-cooperative agents under certain network connectivity conditions.&lt;/li&gt;&lt;li&gt;The work includes theoretical guarantees, extensions to dynamic network topologies, and empirical validation.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research Paper&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Quality Assessment&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Clarity: 4/5&lt;/li&gt;&lt;li&gt;Novelty: 5/5&lt;/li&gt;&lt;li&gt;Significance: 4/5&lt;/li&gt;&lt;li&gt;Try-worthiness: Yes&lt;/li&gt;&lt;li&gt;Justification: The abstract is well-written and clearly explains the problem, motivation, and main contributions, though it is somewhat dense with technical terms. The work is highly novel, addressing Byzantine resilience in decentralized multi-armed bandits—a challenging and timely topic with practical implications in adversarial environments. The significance is high given the broad applicability (network security, recommender systems, finance), though the paper is very new and only on arXiv, so it has not yet been peer-reviewed or cited. The proposed algorithm appears theoretically sound and is supported by experiments, making it worth trying for researchers or practitioners in decentralized learning or adversarial settings. No code repository is provided in the metadata.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: Jingxuan Zhu, Alec Koppel, Alvaro Velasquez, Ji Liu&lt;/li&gt;&lt;li&gt;Tags: ['Byzantine resilience', 'decentralized learning', 'adversarial robustness', 'security vulnerabilities', 'multi-agent systems']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2310.07320</guid><pubDate>Thu, 12 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Evasion Attacks Against Bayesian Predictive Models</title><link>https://arxiv.org/abs/2506.09640</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;The paper studies evasion attacks specifically targeting Bayesian predictive models, an underexplored area in adversarial machine learning.&lt;/li&gt;&lt;li&gt;It introduces a general methodology for designing optimal evasion attacks against Bayesian models.&lt;/li&gt;&lt;li&gt;Two adversarial objectives are considered: perturbing specific point predictions and altering the entire posterior predictive distribution.&lt;/li&gt;&lt;li&gt;Novel gradient-based attack methods are proposed and analyzed across different computational setups.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research Paper&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Quality Assessment&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Clarity: 4/5&lt;/li&gt;&lt;li&gt;Novelty: 5/5&lt;/li&gt;&lt;li&gt;Significance: 4/5&lt;/li&gt;&lt;li&gt;Try-worthiness: Yes&lt;/li&gt;&lt;li&gt;Justification: The abstract is well-written and clearly outlines the motivation, gap in the literature, and the contributions (clarity: 4). The focus on evasion attacks against Bayesian predictive models is novel, as most adversarial ML research targets classical models (novelty: 5). The significance is high given the growing use of Bayesian models and the lack of prior work in this area, though the impact is yet to be seen due to the paper's recency and preprint status (significance: 4). The proposed general methodology and gradient-based attacks are likely to be of practical interest to researchers and practitioners, making the paper worth implementing or experimenting with (try_worthiness: true). No code repository is provided in the metadata or abstract.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: Pablo G. Arce, Roi Naveiro, David R\'ios Insua&lt;/li&gt;&lt;li&gt;Tags: ['adversarial attacks', 'evasion attacks', 'Bayesian models', 'model robustness', 'adversarial machine learning']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.09640</guid><pubDate>Thu, 12 Jun 2025 00:00:00 +0000</pubDate></item><item><title>TooBadRL: Trigger Optimization to Boost Effectiveness of Backdoor Attacks on Deep Reinforcement Learning</title><link>https://arxiv.org/abs/2506.09562</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces TooBadRL, a novel framework for optimizing backdoor triggers in deep reinforcement learning (DRL) systems.&lt;/li&gt;&lt;li&gt;Focuses on enhancing the effectiveness of backdoor attacks by optimizing triggers along temporal, spatial, and magnitude dimensions.&lt;/li&gt;&lt;li&gt;Proposes techniques including adaptive freezing for injection timing, Shapley value analysis for dimension selection, and gradient-based adversarial optimization for trigger magnitude.&lt;/li&gt;&lt;li&gt;Demonstrates significant improvements in attack success rates on multiple DRL algorithms and benchmark tasks with minimal impact on normal performance.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research Paper&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Quality Assessment&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Clarity: 5/5&lt;/li&gt;&lt;li&gt;Novelty: 5/5&lt;/li&gt;&lt;li&gt;Significance: 4/5&lt;/li&gt;&lt;li&gt;Try-worthiness: Yes&lt;/li&gt;&lt;li&gt;Justification: The abstract is very well-written, clearly outlining the motivation, methodology, and results. The work is novel as it is the first to systematically optimize backdoor triggers in DRL along temporal, spatial, and magnitude axes, introducing new mechanisms such as adaptive freezing, Shapley value-based dimension selection, and gradient-based magnitude optimization. While the paper is a preprint and very recent (hence no citations yet), the problem addressed is significant for the security of DRL systems, and the results show substantial improvements in attack effectiveness. The presence of a public code repository further increases its try-worthiness for researchers and practitioners interested in adversarial machine learning or DRL security.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: Songze Li, Mingxuan Zhang, Oubo Ma, Kang Wei, Shouling Ji&lt;/li&gt;&lt;li&gt;Tags: ['backdoor attacks', 'deep reinforcement learning', 'trigger optimization', 'adversarial attacks', 'AI security']&lt;/li&gt;&lt;li&gt;Code Repository: &lt;a href='https://github.com/S3IC-Lab/TooBadRL'&gt;https://github.com/S3IC-Lab/TooBadRL&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.09562</guid><pubDate>Thu, 12 Jun 2025 00:00:00 +0000</pubDate></item><item><title>What is the Cost of Differential Privacy for Deep Learning-Based Trajectory Generation?</title><link>https://arxiv.org/abs/2506.09312</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Investigates the impact of Differential Privacy (DP) mechanisms on deep learning-based trajectory generation models.&lt;/li&gt;&lt;li&gt;Evaluates the utility-privacy trade-off when applying DP-SGD and a novel DP mechanism for conditional generation.&lt;/li&gt;&lt;li&gt;Compares the performance of different generative model types (Diffusion, VAE, GAN) under DP constraints.&lt;/li&gt;&lt;li&gt;Finds that DP-SGD significantly affects model utility but formal privacy guarantees are achievable with large datasets and specific model choices.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research Paper&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Quality Assessment&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Clarity: 4/5&lt;/li&gt;&lt;li&gt;Novelty: 4/5&lt;/li&gt;&lt;li&gt;Significance: 3/5&lt;/li&gt;&lt;li&gt;Try-worthiness: Yes&lt;/li&gt;&lt;li&gt;Justification: The paper is clearly written, with a well-structured abstract that outlines the motivation, research questions, methods, and key findings. The novelty is reasonably high: it addresses the under-explored intersection of differential privacy and deep generative models for trajectory data, and proposes a new DP mechanism for conditional generation. The significance is moderate; while the topic is important and timely, the impact is limited by the early stage (preprint, no citations yet) and the practical constraints highlighted in the results (DP guarantees are only feasible with large datasets). The work is worth trying for researchers interested in privacy-preserving synthetic data generation, especially given the practical insights into model choice and DP trade-offs. No code repository is provided in the metadata.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: Erik Buchholz, Natasha Fernandes, David D. Nguyen, Alsharif Abuadbba, Surya Nepal, Salil S. Kanhere&lt;/li&gt;&lt;li&gt;Tags: ['differential privacy', 'privacy attacks', 'deep learning', 'generative models', 'utility-privacy trade-off']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.09312</guid><pubDate>Thu, 12 Jun 2025 00:00:00 +0000</pubDate></item><item><title>PatchGuard: Adversarially Robust Anomaly Detection and Localization through Vision Transformers and Pseudo Anomalies</title><link>https://arxiv.org/abs/2506.09237</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces PatchGuard, a method for adversarially robust anomaly detection and localization using Vision Transformers.&lt;/li&gt;&lt;li&gt;Incorporates pseudo anomalies with localization masks to enhance robustness against adversarial attacks.&lt;/li&gt;&lt;li&gt;Provides theoretical insights into attention mechanisms to improve adversarial robustness in AD and AL systems.&lt;/li&gt;&lt;li&gt;Demonstrates significant performance improvements in adversarial settings on industrial and medical datasets.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research Paper&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Quality Assessment&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Clarity: 4/5&lt;/li&gt;&lt;li&gt;Novelty: 4/5&lt;/li&gt;&lt;li&gt;Significance: 4/5&lt;/li&gt;&lt;li&gt;Try-worthiness: Yes&lt;/li&gt;&lt;li&gt;Justification: The abstract is well-written and clearly outlines the motivation, methodology, and results of the paper, though some technical details are dense. The use of Vision Transformers with adversarially generated pseudo anomalies for robust anomaly detection and localization is a novel approach, especially with the theoretical analysis of attention mechanisms. The reported performance gains in adversarial settings are substantial and relevant for high-stakes domains like medical imaging and industrial monitoring. While the paper is a recent preprint and has no citations yet, the strong results and open-source code make it worth experimenting with, especially for researchers or practitioners interested in robust anomaly detection.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: Mojtaba Nafez, Amirhossein Koochakian, Arad Maleki, Jafar Habibi, Mohammad Hossein Rohban&lt;/li&gt;&lt;li&gt;Tags: ['adversarial robustness', 'anomaly detection', 'vision transformers', 'adversarial attacks', 'adversarial training']&lt;/li&gt;&lt;li&gt;Code Repository: &lt;a href='https://github.com/rohban-lab/PatchGuard'&gt;https://github.com/rohban-lab/PatchGuard&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.09237</guid><pubDate>Thu, 12 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Apollo: A Posteriori Label-Only Membership Inference Attack Towards Machine Unlearning</title><link>https://arxiv.org/abs/2506.09923</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces Apollo, a novel label-only membership inference attack targeting machine unlearning models.&lt;/li&gt;&lt;li&gt;Focuses on privacy attacks that infer whether specific data samples have been unlearned from a model.&lt;/li&gt;&lt;li&gt;Operates under a strict threat model where the attacker only has access to the label outputs of the unlearned model.&lt;/li&gt;&lt;li&gt;Demonstrates that Apollo achieves high precision despite limited access, highlighting new privacy vulnerabilities in machine unlearning.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research Paper&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Quality Assessment&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Clarity: 4/5&lt;/li&gt;&lt;li&gt;Novelty: 5/5&lt;/li&gt;&lt;li&gt;Significance: 4/5&lt;/li&gt;&lt;li&gt;Try-worthiness: Yes&lt;/li&gt;&lt;li&gt;Justification: The paper is clearly written, with a well-structured abstract that explains the motivation, problem, and contribution. The novelty is high, as it introduces a new type of membership inference attack (Apollo) under a stricter and more realistic threat model (label-only access) in the context of machine unlearning, which has not been well-explored. The significance is strong given the growing importance of machine unlearning for privacy and compliance, and the fact that the attack works under more practical assumptions increases its impact. While the paper is very new and has no citations yet, its relevance to privacy and security in ML makes it worth experimenting with, especially for researchers or practitioners interested in privacy attacks and defenses. No code repository is provided in the metadata.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: Liou Tang, James Joshi, Ashish Kundu&lt;/li&gt;&lt;li&gt;Tags: ['membership inference', 'privacy attacks', 'machine unlearning', 'model privacy', 'security vulnerabilities']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.09923</guid><pubDate>Thu, 12 Jun 2025 00:00:00 +0000</pubDate></item><item><title>A look at adversarial attacks on radio waveforms from discrete latent space</title><link>https://arxiv.org/abs/2506.09896</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;The paper investigates adversarial attacks on radio-frequency (RF) waveforms represented in a discrete latent space using a Vector Quantized Variational Autoencoder (VQVAE).&lt;/li&gt;&lt;li&gt;It analyzes how VQVAE can suppress the effectiveness of adversarial attacks on digitally modulated waveform classes by reconstructing attacked inputs.&lt;/li&gt;&lt;li&gt;The study compares attacks that preserve phase information with those that do not, evaluating their impact on classification accuracy.&lt;/li&gt;&lt;li&gt;It explores the latent space distributions under attack to identify potential detection mechanisms for adversarial examples.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research Paper&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Quality Assessment&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Clarity: 4/5&lt;/li&gt;&lt;li&gt;Novelty: 4/5&lt;/li&gt;&lt;li&gt;Significance: 3/5&lt;/li&gt;&lt;li&gt;Try-worthiness: Yes&lt;/li&gt;&lt;li&gt;Justification: The abstract is well-written and clearly describes the methodology and findings, though some technical terms may require domain knowledge. The use of VQVAE (Vector Quantized Variational Autoencoder) for suppressing adversarial attacks on radio waveforms is a novel application, especially with the focus on discrete latent spaces and phase-preserving attacks. The significance is moderate as it addresses a niche but important area (RF adversarial robustness), and the results could be impactful for secure communications. As a recent arXiv preprint, it has no citations yet, but the approach and findings make it worth experimenting with, especially for researchers in adversarial ML or RF signal processing. No code repository is provided in the metadata.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: Attanasia Garuso, Silvija Kokalj-Filipovic, Yagna Kaasaragadda&lt;/li&gt;&lt;li&gt;Tags: ['adversarial attacks', 'robustness', 'discrete latent space', 'radio-frequency security', 'attack detection']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.09896</guid><pubDate>Thu, 12 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Private Aggregation for Byzantine-Resilient Heterogeneous Federated Learning</title><link>https://arxiv.org/abs/2506.09870</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Addresses Byzantine resilience in federated learning, focusing on robustness against malicious clients.&lt;/li&gt;&lt;li&gt;Proposes a multi-stage method combining verifiable secret sharing, secure aggregation, and private information retrieval to ensure privacy and robustness.&lt;/li&gt;&lt;li&gt;Specifically tackles challenges arising from heterogeneous client data in federated learning.&lt;/li&gt;&lt;li&gt;Evaluates the method's effectiveness against various attacks and discusses communication overhead and scalability.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research Paper&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Quality Assessment&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Clarity: 4/5&lt;/li&gt;&lt;li&gt;Novelty: 5/5&lt;/li&gt;&lt;li&gt;Significance: 4/5&lt;/li&gt;&lt;li&gt;Try-worthiness: Yes&lt;/li&gt;&lt;li&gt;Justification: The abstract is well-written and clearly outlines the problem (Byzantine resilience and privacy in heterogeneous federated learning), the limitations of prior work, and the proposed multi-stage solution. The approach appears novel, especially in its co-design of verifiable secret sharing, secure aggregation, and private information retrieval tailored for heterogeneous data—a gap in existing literature. The significance is high given the importance of privacy and robustness in federated learning, though as a recent arXiv preprint, it has not yet been peer-reviewed or cited. The method's evaluation against a variety of attacks and its focus on scalability make it promising for practical experimentation. No code repository is provided.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: Maximilian Egger, Rawad Bitar&lt;/li&gt;&lt;li&gt;Tags: ['Byzantine resilience', 'Federated learning security', 'Secure aggregation', 'Privacy-preserving machine learning', 'Robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.09870</guid><pubDate>Thu, 12 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Weighted Loss Methods for Robust Federated Learning under Data Heterogeneity</title><link>https://arxiv.org/abs/2506.09824</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Addresses security threats in federated learning from Byzantine (malicious) participants contributing poisonous gradients.&lt;/li&gt;&lt;li&gt;Proposes a novel weighted loss method (Worker Label Alignment Loss) to improve robustness against Byzantine attacks under data heterogeneity.&lt;/li&gt;&lt;li&gt;Focuses on distinguishing malicious gradients from honest but heterogeneous gradients to ensure secure model aggregation.&lt;/li&gt;&lt;li&gt;Provides theoretical and empirical validation of the proposed method's effectiveness in enhancing federated learning security.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research Paper&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Quality Assessment&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Clarity: 4/5&lt;/li&gt;&lt;li&gt;Novelty: 4/5&lt;/li&gt;&lt;li&gt;Significance: 3/5&lt;/li&gt;&lt;li&gt;Try-worthiness: Yes&lt;/li&gt;&lt;li&gt;Justification: The abstract is well-written and clearly explains the problem of Byzantine robustness in federated learning under data heterogeneity. The proposed method, Worker Label Alignment Loss (WoLA), is described succinctly, and the motivation is clear. The novelty appears solid, as it addresses a known challenge (distinguishing honest from Byzantine gradients in heterogeneous settings) with a new weighted loss approach. The significance is moderate: while the problem is important and the solution claims strong empirical and theoretical results, the paper is a preprint on arXiv with no citations yet, so its impact is not yet established. Given the relevance of robust federated learning and the claimed improvements, the paper is worth trying for researchers or practitioners in the field. No code repository is provided in the metadata.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: Johan Erbani, Sonia Ben Mokhtar, Pierre-Edouard Portier, Elod Egyed-Zsigmond, Diana Nurbakova&lt;/li&gt;&lt;li&gt;Tags: ['federated learning', 'Byzantine resilience', 'data poisoning', 'robustness', 'security vulnerabilities']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.09824</guid><pubDate>Thu, 12 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Devil's Hand: Data Poisoning Attacks to Locally Private Graph Learning Protocols</title><link>https://arxiv.org/abs/2506.09803</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces the first data poisoning attack targeting locally private graph learning protocols using graph neural networks (GNNs).&lt;/li&gt;&lt;li&gt;The attack involves injecting fake users and manipulating connections to degrade the utility of privacy-preserving graph learning.&lt;/li&gt;&lt;li&gt;Demonstrates the attack's effectiveness both theoretically and empirically.&lt;/li&gt;&lt;li&gt;Explores defense strategies but finds them only partially effective, emphasizing the need for stronger security measures.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research Paper&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Quality Assessment&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Clarity: 4/5&lt;/li&gt;&lt;li&gt;Novelty: 5/5&lt;/li&gt;&lt;li&gt;Significance: 4/5&lt;/li&gt;&lt;li&gt;Try-worthiness: Yes&lt;/li&gt;&lt;li&gt;Justification: The abstract is well-written and clearly explains the motivation, threat model, and contributions, though some technical details are omitted (as is typical for abstracts). The work is highly novel, being the first to propose data poisoning attacks on locally private graph learning protocols—a timely and important topic as privacy-preserving GNNs gain traction. The significance is high given the growing deployment of GNNs in privacy-sensitive domains, and the lack of prior work on this specific vulnerability. While the paper is very new and has no citations yet, its topic and findings are likely to be influential. The work is worth trying for researchers interested in privacy, security, and graph learning, especially to validate the attack and explore defenses. No code repository is provided in the metadata.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: Longzhu He, Chaozhuo Li, Peng Tang, Litian Zhang, Sen Su&lt;/li&gt;&lt;li&gt;Tags: ['data poisoning', 'graph neural networks', 'local differential privacy', 'AI security', 'adversarial attacks']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.09803</guid><pubDate>Thu, 12 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Wavelet Scattering Transform and Fourier Representation for Offline Detection of Malicious Clients in Federated Learning</title><link>https://arxiv.org/abs/2506.09674</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes WAFFLE, a method to detect malicious or anomalous clients in federated learning before training begins.&lt;/li&gt;&lt;li&gt;Uses Wavelet Scattering Transform and Fourier Transform to create compressed, privacy-preserving client representations for anomaly detection.&lt;/li&gt;&lt;li&gt;Focuses on security in federated learning by identifying corrupted clients that could degrade model performance.&lt;/li&gt;&lt;li&gt;Demonstrates improved detection accuracy and downstream model performance compared to existing anomaly detection methods.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research Paper&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Quality Assessment&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Clarity: 4/5&lt;/li&gt;&lt;li&gt;Novelty: 4/5&lt;/li&gt;&lt;li&gt;Significance: 3/5&lt;/li&gt;&lt;li&gt;Try-worthiness: Yes&lt;/li&gt;&lt;li&gt;Justification: The abstract is well-written and clearly explains the motivation, method (WAFFLE), and contributions, though some technical terms may require background knowledge. The use of wavelet scattering and Fourier transforms for pre-training detection of malicious clients in federated learning appears novel, especially the focus on offline detection and the theoretical advantages of WST. The significance is moderate: while the problem is important and the approach promising, the paper is a recent arXiv preprint with no citations yet, so its impact is not yet established. The method is lightweight, claims improved detection, and could be valuable to practitioners, making it worth trying. No code repository is provided in the metadata.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: Alessandro Licciardi, Davide Leo, Davide Carbone&lt;/li&gt;&lt;li&gt;Tags: ['federated learning security', 'malicious client detection', 'data poisoning', 'anomaly detection', 'privacy-preserving security']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.09674</guid><pubDate>Thu, 12 Jun 2025 00:00:00 +0000</pubDate></item><item><title>In-Context Bias Propagation in LLM-Based Tabular Data Generation</title><link>https://arxiv.org/abs/2506.09630</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;The paper studies how biases in in-context examples propagate to synthetic tabular data generated by LLMs.&lt;/li&gt;&lt;li&gt;It identifies a security vulnerability where a malicious actor can inject bias via in-context examples, compromising fairness in downstream classifiers.&lt;/li&gt;&lt;li&gt;The work highlights risks in LLM-based data generation pipelines, especially in sensitive domains where fairness and bias are critical.&lt;/li&gt;&lt;li&gt;This introduces a novel adversarial attack vector related to prompt injection and data poisoning in LLMs.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research Paper&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Quality Assessment&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Clarity: 4/5&lt;/li&gt;&lt;li&gt;Novelty: 4/5&lt;/li&gt;&lt;li&gt;Significance: 4/5&lt;/li&gt;&lt;li&gt;Try-worthiness: Yes&lt;/li&gt;&lt;li&gt;Justification: The abstract is well-written and clearly outlines the motivation, problem, and contributions of the paper, though some sentences are dense. The work is novel in systematically studying bias propagation in LLM-based tabular data generation, especially highlighting adversarial scenarios, which is a timely and underexplored topic. The significance is high given the increasing use of LLMs for data augmentation and the critical importance of fairness in synthetic data generation. While the paper is new and not yet cited, its findings are likely to influence both research and practical deployments. The topic is important enough to warrant experimental follow-up, especially for those working with LLM-generated data in sensitive domains. No code repository is provided.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: Pol G. Recasens, Alberto Gutierrez, Jordi Torres, Josep. Ll Berral, Anisa Halimi, Kieran Fraser&lt;/li&gt;&lt;li&gt;Tags: ['LLM security', 'prompt injection', 'data poisoning', 'bias and fairness', 'adversarial attacks']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.09630</guid><pubDate>Thu, 12 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Learning Obfuscations Of LLM Embedding Sequences: Stained Glass Transform</title><link>https://arxiv.org/abs/2506.09452</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces the Stained Glass Transform, a learned stochastic transformation of LLM word embeddings to provide privacy for input data.&lt;/li&gt;&lt;li&gt;Addresses privacy concerns for sensitive data used in shared or multi-tenant LLM compute infrastructures.&lt;/li&gt;&lt;li&gt;Theoretical connection to mutual information theory to quantify privacy guarantees.&lt;/li&gt;&lt;li&gt;Empirical evaluation of privacy and utility trade-offs using token-level privacy metrics and standard LLM benchmarks.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research Paper&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Quality Assessment&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Clarity: 4/5&lt;/li&gt;&lt;li&gt;Novelty: 5/5&lt;/li&gt;&lt;li&gt;Significance: 4/5&lt;/li&gt;&lt;li&gt;Try-worthiness: Yes&lt;/li&gt;&lt;li&gt;Justification: The abstract is well-written and clearly outlines the motivation, problem, and proposed solution (Stained Glass Transform) for privacy-preserving LLM deployments. The approach—learning stochastic, sequence-dependent transformations of LLM embeddings to provide privacy while preserving utility—appears novel, especially with its theoretical grounding in mutual information and Gaussian Mixture Models. The significance is high given the growing importance of privacy in enterprise AI deployments, though the impact is yet to be seen due to the paper's recency and preprint status. The method is worth experimenting with, as it addresses a real and pressing problem in LLM deployment. No code repository is provided in the metadata.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: Jay Roberts, Kyle Mylonakis, Sidhartha Roy, Kaan Kale&lt;/li&gt;&lt;li&gt;Tags: ['privacy attacks', 'data privacy', 'LLM embeddings', 'mutual information', 'privacy-preserving transformations']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.09452</guid><pubDate>Thu, 12 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Generalization Error Analysis for Attack-Free and Byzantine-Resilient Decentralized Learning with Data Heterogeneity</title><link>https://arxiv.org/abs/2506.09438</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Analyzes generalization errors in decentralized learning under both attack-free and Byzantine-resilient settings.&lt;/li&gt;&lt;li&gt;Focuses on the impact of data heterogeneity and Byzantine attacks on model generalization error.&lt;/li&gt;&lt;li&gt;Provides theoretical insights into how malicious agents performing Byzantine attacks affect learning robustness.&lt;/li&gt;&lt;li&gt;Validates findings with numerical experiments on convex and non-convex tasks.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research Paper&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Quality Assessment&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Clarity: 4/5&lt;/li&gt;&lt;li&gt;Novelty: 4/5&lt;/li&gt;&lt;li&gt;Significance: 3/5&lt;/li&gt;&lt;li&gt;Try-worthiness: Yes&lt;/li&gt;&lt;li&gt;Justification: The abstract is well-written and clearly outlines the motivation, contributions, and findings of the paper, though it is somewhat dense with technical terms. The work is novel in that it addresses generalization error in decentralized learning under both attack-free and Byzantine-resilient settings with heterogeneous data, which is less explored compared to prior work focusing on homogeneous data or stronger assumptions. The significance is moderate: while the topic is important and timely, the paper is a preprint and has not yet been peer-reviewed or cited, so its impact is not yet established. The theoretical insights and validation experiments make it worth trying for researchers interested in decentralized learning robustness and generalization. No code repository is provided in the metadata.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: Haoxiang Ye, Tao Sun, Qing Ling&lt;/li&gt;&lt;li&gt;Tags: ['Byzantine attacks', 'decentralized learning', 'generalization error', 'adversarial robustness', 'data heterogeneity']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.09438</guid><pubDate>Thu, 12 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Adversarial Surrogate Risk Bounds for Binary Classification</title><link>https://arxiv.org/abs/2506.09348</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Focuses on adversarial robustness in binary classification models.&lt;/li&gt;&lt;li&gt;Analyzes adversarial surrogate risk and its convergence to optimal adversarial classification risk.&lt;/li&gt;&lt;li&gt;Provides theoretical bounds quantifying the rate of convergence under adversarial training.&lt;/li&gt;&lt;li&gt;Contributes to understanding robustness against adversarial attacks in machine learning.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research Paper&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Quality Assessment&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Clarity: 4/5&lt;/li&gt;&lt;li&gt;Novelty: 4/5&lt;/li&gt;&lt;li&gt;Significance: 3/5&lt;/li&gt;&lt;li&gt;Try-worthiness: Yes&lt;/li&gt;&lt;li&gt;Justification: The abstract is well-written and clearly explains the motivation, contributions, and context of the work, though it is somewhat technical and assumes familiarity with adversarial risk and surrogate losses. The novelty is reasonably high, as the paper addresses a gap in the literature by providing convergence rate bounds for adversarial surrogate risk minimization, which has not been thoroughly explored before. The significance is moderate: while the topic is important in robust machine learning, the paper is a preprint and has not yet been peer-reviewed or cited, so its impact is not yet established. The work is worth trying for researchers interested in adversarial robustness and theoretical guarantees, as it may provide useful insights or tools for developing robust classifiers. No code repository is provided.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: Natalie S. Frank&lt;/li&gt;&lt;li&gt;Tags: ['adversarial robustness', 'adversarial training', 'adversarial attacks', 'binary classification', 'security vulnerabilities']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.09348</guid><pubDate>Thu, 12 Jun 2025 00:00:00 +0000</pubDate></item><item><title>SoK: Machine Unlearning for Large Language Models</title><link>https://arxiv.org/abs/2506.09227</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Provides a comprehensive survey and new taxonomy of machine unlearning techniques for large language models, focusing on the intention behind unlearning (true removal vs. behavioral suppression).&lt;/li&gt;&lt;li&gt;Analyzes the effectiveness and limitations of current unlearning methods and evaluation metrics, highlighting challenges in reliably removing data influence.&lt;/li&gt;&lt;li&gt;Discusses practical challenges such as scalability and sequential unlearning that impact deployment in real-world AI systems.&lt;/li&gt;&lt;li&gt;Offers insights relevant to privacy, data removal, and policy guidance in the context of generative AI models.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Survey/Review&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Quality Assessment&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Clarity: 5/5&lt;/li&gt;&lt;li&gt;Novelty: 4/5&lt;/li&gt;&lt;li&gt;Significance: 4/5&lt;/li&gt;&lt;li&gt;Try-worthiness: No&lt;/li&gt;&lt;li&gt;Justification: The paper is very clearly written, with a well-structured abstract that outlines its contributions and motivation. It introduces a novel taxonomy for unlearning in LLMs based on intention (removal vs. suppression), which is a fresh perspective compared to prior technical categorizations. The significance is high given the growing importance of data removal and privacy in generative AI, though as a Systematization of Knowledge (SoK) paper, its primary value is in organizing and critiquing the field rather than proposing a new method. As such, it is not directly try-worthy for implementation, but is highly valuable for researchers seeking to understand or advance the area. No code repository is provided, which is typical for survey/SoK papers.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: Jie Ren, Yue Xing, Yingqian Cui, Charu C. Aggarwal, Hui Liu&lt;/li&gt;&lt;li&gt;Tags: ['machine unlearning', 'privacy attacks', 'data removal', 'LLM security', 'evaluation metrics']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.09227</guid><pubDate>Thu, 12 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Your Agent Can Defend Itself against Backdoor Attacks</title><link>https://arxiv.org/abs/2506.08336</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;The paper addresses security risks from backdoor attacks on LLM-powered agents during training and fine-tuning.&lt;/li&gt;&lt;li&gt;Proposes ReAgent, a novel defense mechanism that detects backdoors by checking consistency between user instructions, agent planning, and execution.&lt;/li&gt;&lt;li&gt;ReAgent uses a two-level approach: verifying consistency between agent thoughts and actions, and reconstructing instructions from thought trajectories to detect anomalies.&lt;/li&gt;&lt;li&gt;Evaluation shows ReAgent significantly reduces backdoor attack success rates, outperforming existing defenses.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research Paper&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Quality Assessment&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Clarity: 5/5&lt;/li&gt;&lt;li&gt;Novelty: 4/5&lt;/li&gt;&lt;li&gt;Significance: 4/5&lt;/li&gt;&lt;li&gt;Try-worthiness: Yes&lt;/li&gt;&lt;li&gt;Justification: The abstract is well-written, clearly outlining the problem (backdoor attacks on LLM agents), the proposed solution (ReAgent), and the evaluation results. The approach of using the agent's own reasoning and planning consistency to detect backdoors is novel and intuitive, especially in the context of LLM-powered agents. While the paper is very recent and has no citations yet, the problem addressed is highly significant given the increasing deployment of LLM agents and the real-world risks posed by backdoors. The reported effectiveness (up to 90% reduction in attack success) and outperforming existing defenses suggest practical value and make it worth trying or experimenting with. No code repository is provided in the metadata.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: Li Changjiang, Liang Jiacheng, Cao Bochuan, Chen Jinghui, Wang Ting&lt;/li&gt;&lt;li&gt;Tags: ['backdoor attacks', 'LLM security', 'adversarial defense', 'AI security', 'agent robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.08336</guid><pubDate>Thu, 12 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Auditing Black-Box LLM APIs with a Rank-Based Uniformity Test</title><link>https://arxiv.org/abs/2506.06975</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a rank-based uniformity test to audit black-box LLM APIs for behavioral equality with authentic models.&lt;/li&gt;&lt;li&gt;Addresses security concerns such as detecting harmful fine-tuning, model substitution, and quantization that may degrade performance or safety.&lt;/li&gt;&lt;li&gt;Method is designed to be query-efficient and resistant to adversarial detection by API providers.&lt;/li&gt;&lt;li&gt;Evaluates the approach under threat scenarios including jailbreak prompts and malicious model alterations.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research Paper&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Quality Assessment&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Clarity: 5/5&lt;/li&gt;&lt;li&gt;Novelty: 5/5&lt;/li&gt;&lt;li&gt;Significance: 4/5&lt;/li&gt;&lt;li&gt;Try-worthiness: Yes&lt;/li&gt;&lt;li&gt;Justification: The paper is very clearly written, with a well-structured abstract that concisely explains the motivation, problem, and proposed solution. The novelty is high: auditing black-box LLM APIs for model substitution using a rank-based uniformity test is a new and timely contribution, especially as API-based access to LLMs becomes more prevalent. The significance is strong, as this addresses a real and growing concern in the LLM ecosystem, though the impact will depend on adoption and further validation. The method's robustness to adversarial providers and query efficiency make it particularly promising for practical use. Given the recency of the paper, the lack of citations is not a concern. The approach is worth implementing or experimenting with, especially for those interested in LLM security, auditing, or API reliability. No code repository is listed in the metadata or abstract.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: Xiaoyuan Zhu, Yaowen Ye, Tianyi Qiu, Hanlin Zhu, Sijun Tan, Ajraf Mannan, Jonathan Michala, Raluca Ada Popa, Willie Neiswanger&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'model substitution detection', 'API auditing', 'jailbreak detection', 'security evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.06975</guid><pubDate>Thu, 12 Jun 2025 00:00:00 +0000</pubDate></item><item><title>HoliSafe: Holistic Safety Benchmarking and Modeling with Safety Meta Token for Vision-Language Model</title><link>https://arxiv.org/abs/2506.04704</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces HoliSafe, a comprehensive safety dataset and benchmark for vision-language models addressing harmful image-text interactions.&lt;/li&gt;&lt;li&gt;Proposes SafeLLaVA, a vision-language model with a learnable safety meta token and safety head to intrinsically improve safety and provide interpretable harmfulness classification.&lt;/li&gt;&lt;li&gt;Demonstrates state-of-the-art safety performance on multiple benchmarks, revealing vulnerabilities in existing models.&lt;/li&gt;&lt;li&gt;Focuses on robust and interpretable safety evaluation and mitigation for multimodal AI systems.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research Paper&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Quality Assessment&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Clarity: 5/5&lt;/li&gt;&lt;li&gt;Novelty: 5/5&lt;/li&gt;&lt;li&gt;Significance: 4/5&lt;/li&gt;&lt;li&gt;Try-worthiness: Yes&lt;/li&gt;&lt;li&gt;Justification: The abstract is well-written, clearly outlining the motivation, contributions, and results. The work is highly novel, introducing both a comprehensive safety benchmark (HoliSafe) and a new model architecture (SafeLLaVA) with a learnable safety meta token and safety head—both of which are new directions for VLM safety. The significance is high given the growing importance of safety in multimodal AI, though as a very recent preprint, it has not yet accumulated citations or peer-reviewed validation. The benchmark and model are likely to be influential and worth experimenting with, especially for researchers in VLM safety and alignment. No code repository link is provided in the metadata.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: Youngwan Lee, Kangsan Kim, Kwanyong Park, Ilcahe Jung, Soojin Jang, Seanie Lee, Yong-Ju Lee, Sung Ju Hwang&lt;/li&gt;&lt;li&gt;Tags: ['AI safety', 'vision-language models', 'safety benchmarking', 'harmful content prevention', 'multimodal alignment']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.04704</guid><pubDate>Thu, 12 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Discovering Forbidden Topics in Language Models</title><link>https://arxiv.org/abs/2505.17441</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces refusal discovery, a method to identify topics that language models refuse to discuss.&lt;/li&gt;&lt;li&gt;Develops Iterated Prefill Crawler (IPC) to systematically find forbidden topics in various large language models.&lt;/li&gt;&lt;li&gt;Demonstrates IPC's effectiveness on multiple models, revealing censorship and alignment issues including politically aligned refusals.&lt;/li&gt;&lt;li&gt;Highlights the importance of refusal discovery for detecting biases, boundaries, and alignment failures in AI systems.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research Paper&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Quality Assessment&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Clarity: 5/5&lt;/li&gt;&lt;li&gt;Novelty: 5/5&lt;/li&gt;&lt;li&gt;Significance: 4/5&lt;/li&gt;&lt;li&gt;Try-worthiness: Yes&lt;/li&gt;&lt;li&gt;Justification: The paper is very clearly written, with a well-structured abstract that introduces a new problem (refusal discovery), a novel method (Iterated Prefill Crawler), and concrete results on both open and frontier models. The novelty is high, as refusal discovery is a new task and the method appears original. The significance is strong given the importance of understanding and auditing language model refusals, especially in the context of safety and censorship, though the impact is not yet proven due to the recency of the work and lack of citations. The work is worth trying, as it provides a practical tool for probing model boundaries and alignment failures. No code repository is listed in the metadata or abstract.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: Can Rager, Chris Wendler, Rohit Gandikota, David Bau&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'alignment failures', 'censorship detection', 'refusal discovery', 'AI safety evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2505.17441</guid><pubDate>Thu, 12 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Persona-judge: Personalized Alignment of Large Language Models via Token-level Self-judgment</title><link>https://arxiv.org/abs/2504.12663</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces Persona-judge, a novel method for personalized alignment of large language models without additional training.&lt;/li&gt;&lt;li&gt;Uses a draft model to generate tokens and a judge model to evaluate token acceptance based on different preferences.&lt;/li&gt;&lt;li&gt;Addresses challenges in aligning LLMs with diverse human values efficiently and scalably.&lt;/li&gt;&lt;li&gt;Demonstrates a training-free approach leveraging intrinsic preference judgment capabilities of LLMs.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research Paper&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Quality Assessment&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Clarity: 4/5&lt;/li&gt;&lt;li&gt;Novelty: 5/5&lt;/li&gt;&lt;li&gt;Significance: 4/5&lt;/li&gt;&lt;li&gt;Try-worthiness: Yes&lt;/li&gt;&lt;li&gt;Justification: The abstract is well-written and clearly explains the motivation, method, and contributions of the paper, though some technical details are left for the full text. The approach—training-free, token-level personalized alignment using intrinsic model judgment—appears novel compared to standard RLHF or reward-based methods. The significance is promising, as personalized alignment is a key challenge in LLM deployment, and the method claims scalability and efficiency. While the paper is very new and only on arXiv, the topic and approach are timely and relevant, making it worth experimenting with. The code is mentioned as available, but no direct link is provided in the metadata.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: Xiaotian Zhang, Ruizhe Chen, Yang Feng, Zuozhu Liu&lt;/li&gt;&lt;li&gt;Tags: ['AI safety', 'alignment', 'personalized alignment', 'large language models', 'preference modeling']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2504.12663</guid><pubDate>Thu, 12 Jun 2025 00:00:00 +0000</pubDate></item><item><title>FC-Attack: Jailbreaking Multimodal Large Language Models via Auto-Generated Flowcharts</title><link>https://arxiv.org/abs/2502.21059</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces FC-Attack, a novel jailbreak attack on multimodal large language models (MLLMs) using auto-generated flowcharts as visual prompts.&lt;/li&gt;&lt;li&gt;Demonstrates high attack success rates (up to 96% via images and 78% via videos) on multiple MLLMs, revealing vulnerabilities in visual modality safety alignment.&lt;/li&gt;&lt;li&gt;Analyzes factors influencing attack effectiveness, such as number of steps and font styles in flowcharts, showing that subtle changes can significantly increase jailbreak success.&lt;/li&gt;&lt;li&gt;Evaluates defense mechanisms like AdaShield, which reduce attack success but at the cost of model utility.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research Paper&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Quality Assessment&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Clarity: 4/5&lt;/li&gt;&lt;li&gt;Novelty: 5/5&lt;/li&gt;&lt;li&gt;Significance: 4/5&lt;/li&gt;&lt;li&gt;Try-worthiness: Yes&lt;/li&gt;&lt;li&gt;Justification: The abstract is well-written and clearly explains the motivation, methodology, and results of the work, though some technical details could be clearer for non-experts. The approach—using auto-generated flowcharts as a novel vector for multimodal jailbreak attacks—is highly original and addresses a timely and important security concern in MLLMs. The reported attack success rates are impressive, and the exploration of factors like font style and flowchart shape adds depth. While the paper is a preprint and has not yet accumulated citations, its findings are significant for both researchers and practitioners concerned with LLM safety. The work is worth experimenting with, especially for those developing or defending MLLMs. No code repository is provided in the metadata.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: Ziyi Zhang, Zhen Sun, Zongmin Zhang, Jihui Guo, Xinlei He&lt;/li&gt;&lt;li&gt;Tags: ['jailbreaking', 'multimodal models', 'adversarial prompting', 'AI safety', 'defense mechanisms']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2502.21059</guid><pubDate>Thu, 12 Jun 2025 00:00:00 +0000</pubDate></item><item><title>On the Privacy Risks of Spiking Neural Networks: A Membership Inference Analysis</title><link>https://arxiv.org/abs/2502.13191</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Investigates privacy risks of Spiking Neural Networks (SNNs) through Membership Inference Attacks (MIAs).&lt;/li&gt;&lt;li&gt;Finds that SNNs are vulnerable to MIAs, challenging assumptions of inherent privacy robustness.&lt;/li&gt;&lt;li&gt;Introduces an input dropout strategy under black-box settings to enhance membership inference effectiveness.&lt;/li&gt;&lt;li&gt;Compares privacy vulnerabilities of SNNs to those of traditional Artificial Neural Networks (ANNs).&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research Paper&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Quality Assessment&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Clarity: 4/5&lt;/li&gt;&lt;li&gt;Novelty: 4/5&lt;/li&gt;&lt;li&gt;Significance: 3/5&lt;/li&gt;&lt;li&gt;Try-worthiness: Yes&lt;/li&gt;&lt;li&gt;Justification: The abstract is well-written and clearly outlines the motivation, methodology, and findings of the paper, though some technical details are omitted (clarity: 4). The work is novel as it addresses the underexplored privacy risks of Spiking Neural Networks (SNNs) in the context of Membership Inference Attacks, introducing new insights and an input dropout strategy (novelty: 4). While the significance is promising due to the growing interest in SNNs and privacy, the paper is a recent preprint on arXiv with no citations yet, so its impact is still uncertain (significance: 3). The availability of code and the relevance of the topic make it worth trying for researchers interested in neural network privacy (try_worthiness: true).&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: Junyi Guan, Abhijith Sharma, Chong Tian, Salem Lahlou&lt;/li&gt;&lt;li&gt;Tags: ['membership inference', 'privacy attacks', 'spiking neural networks', 'black-box attacks', 'AI security']&lt;/li&gt;&lt;li&gt;Code Repository: &lt;a href='https://github.com/sharmaabhijith/MIA_SNN'&gt;https://github.com/sharmaabhijith/MIA_SNN&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2502.13191</guid><pubDate>Thu, 12 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Trustworthy AI: Safety, Bias, and Privacy -- A Survey</title><link>https://arxiv.org/abs/2502.10450</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Surveys current challenges in AI trustworthiness focusing on safety, bias, and privacy.&lt;/li&gt;&lt;li&gt;Discusses safety alignment techniques to prevent harmful or toxic outputs in large language models.&lt;/li&gt;&lt;li&gt;Examines privacy attacks, specifically membership inference attacks on deep neural networks.&lt;/li&gt;&lt;li&gt;Addresses spurious biases that affect model reliability and trustworthiness.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Survey/Review&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Quality Assessment&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Clarity: 4/5&lt;/li&gt;&lt;li&gt;Novelty: 2/5&lt;/li&gt;&lt;li&gt;Significance: 3/5&lt;/li&gt;&lt;li&gt;Try-worthiness: No&lt;/li&gt;&lt;li&gt;Justification: The paper is clearly written, with a well-structured abstract that outlines its focus on safety, bias, and privacy in trustworthy AI. However, as a survey paper, its novelty is limited, especially since it covers well-known topics in the field. The significance is moderate: while trustworthy AI is an important area, the paper is a preprint on arXiv and has not yet been peer-reviewed or cited, which limits its immediate impact. There is no indication of new methods or frameworks to implement, so it is not particularly try-worthy for experimentation. No code repository is provided.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: Xingli Fang, Jianwei Li, Varun Mulchandani, Jung-Eun Kim&lt;/li&gt;&lt;li&gt;Tags: ['AI safety', 'alignment', 'harmful outputs prevention', 'membership inference attacks', 'bias mitigation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2502.10450</guid><pubDate>Thu, 12 Jun 2025 00:00:00 +0000</pubDate></item><item><title>MELON: Provable Defense Against Indirect Prompt Injection Attacks in AI Agents</title><link>https://arxiv.org/abs/2502.05174</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces MELON, a novel defense mechanism against indirect prompt injection (IPI) attacks targeting LLM agents.&lt;/li&gt;&lt;li&gt;MELON detects attacks by re-executing the agent's trajectory with masked user prompts and comparing actions to identify malicious influence.&lt;/li&gt;&lt;li&gt;Demonstrates superior performance over state-of-the-art defenses in preventing attacks while preserving normal utility on the AgentDojo benchmark.&lt;/li&gt;&lt;li&gt;Includes an enhanced version MELON-Aug that combines MELON with prompt augmentation for improved defense.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research Paper&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Quality Assessment&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Clarity: 5/5&lt;/li&gt;&lt;li&gt;Novelty: 4/5&lt;/li&gt;&lt;li&gt;Significance: 4/5&lt;/li&gt;&lt;li&gt;Try-worthiness: Yes&lt;/li&gt;&lt;li&gt;Justification: The abstract is well-written, clearly outlining the problem (indirect prompt injection attacks in LLM agents), the limitations of existing defenses, and the proposed solution (MELON). The method is described with sufficient detail to understand its core mechanism and evaluation. The novelty is high, as MELON introduces a new defense strategy based on masked re-execution and action comparison, which appears distinct from prior work. The significance is strong given the growing importance of LLM agent security, and the paper demonstrates superior performance over state-of-the-art methods on a relevant benchmark. The presence of a code repository further increases its try-worthiness, making it practical for researchers and practitioners to experiment with. The only limitation is that it is a preprint and very recent, so its impact is not yet established in the community.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: Kaijie Zhu, Xianjun Yang, Jindong Wang, Wenbo Guo, William Yang Wang&lt;/li&gt;&lt;li&gt;Tags: ['indirect prompt injection', 'LLM security', 'adversarial prompting', 'AI agent defense', 'prompt injection detection']&lt;/li&gt;&lt;li&gt;Code Repository: &lt;a href='https://github.com/kaijiezhu11/MELON'&gt;https://github.com/kaijiezhu11/MELON&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2502.05174</guid><pubDate>Thu, 12 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Position: Emergent Machina Sapiens Urge Rethinking Multi-Agent Paradigms</title><link>https://arxiv.org/abs/2502.04388</link><description>&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Other&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Quality Assessment&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Clarity: 4/5&lt;/li&gt;&lt;li&gt;Novelty: 4/5&lt;/li&gt;&lt;li&gt;Significance: 3/5&lt;/li&gt;&lt;li&gt;Try-worthiness: No&lt;/li&gt;&lt;li&gt;Justification: The paper is clearly written and articulates the need to rethink multi-agent paradigms in the context of emergent, self-organizing AI systems, especially for critical infrastructure. The call to move beyond static objectives and rules is a novel and timely perspective, given the increasing deployment of autonomous agents. However, as a position paper (rather than a technical or empirical contribution), its significance is more in framing future research directions than in providing implementable methods. The lack of a code repository and the conceptual nature of the work mean it is not directly try-worthy for implementation or experimentation at this stage.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: Hepeng Li, Yuhong Liu, Jun Yan, Jie Gao, Xiaoou Yang, Mohamed Naili&lt;/li&gt;&lt;li&gt;Tags: []&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2502.04388</guid><pubDate>Thu, 12 Jun 2025 00:00:00 +0000</pubDate></item><item><title>An LLM-Empowered Adaptive Evolutionary Algorithm For Multi-Component Deep Learning Systems</title><link>https://arxiv.org/abs/2501.00829</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes an LLM-empowered adaptive evolutionary algorithm ($\mu$MOEA) to detect safety violations in multi-component deep learning (MCDL) systems.&lt;/li&gt;&lt;li&gt;Leverages the context-understanding and quantitative reasoning capabilities of LLMs to improve search efficiency and diversity in evolutionary algorithms.&lt;/li&gt;&lt;li&gt;Integrates evolutionary experience back into the LLM to avoid local optima and generate diverse candidate solutions.&lt;/li&gt;&lt;li&gt;Evaluates the approach against state-of-the-art multi-objective evolutionary algorithms, demonstrating improved efficiency and diversity in finding safety violations.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research Paper&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Quality Assessment&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Clarity: 4/5&lt;/li&gt;&lt;li&gt;Novelty: 5/5&lt;/li&gt;&lt;li&gt;Significance: 3/5&lt;/li&gt;&lt;li&gt;Try-worthiness: Yes&lt;/li&gt;&lt;li&gt;Justification: The abstract is well-written and clearly explains the motivation, approach, and results, though some technical terms may require background knowledge (clarity: 4). The use of LLMs to guide and adapt evolutionary algorithms for multi-component deep learning systems appears novel, especially the integration of LLM feedback into the evolutionary loop (novelty: 5). As a recent arXiv preprint, it has no citations yet and is not peer-reviewed, which limits its immediate significance, but the topic is timely and relevant (significance: 3). Given the promising experimental results and the innovative approach, it is worth trying or experimenting with, especially for researchers in evolutionary algorithms or LLM applications (try_worthiness: true). No code repository is provided in the metadata.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: Haoxiang Tian, Xingshuo Han, Guoquan Wu, An Guo, Yuan Zhou. Jie Zhang, Shuo Li, Jun Wei, Tianwei Zhang&lt;/li&gt;&lt;li&gt;Tags: ['AI safety', 'safety evaluation', 'LLM red teaming', 'evolutionary algorithms', 'multi-component deep learning systems']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2501.00829</guid><pubDate>Thu, 12 Jun 2025 00:00:00 +0000</pubDate></item><item><title>CROW: Eliminating Backdoors from Large Language Models via Internal Consistency Regularization</title><link>https://arxiv.org/abs/2411.12768</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Identifies vulnerability of large language models (LLMs) to backdoor attacks that manipulate outputs via hidden triggers.&lt;/li&gt;&lt;li&gt;Proposes Internal Consistency Regularization (CROW), a novel defense method that enforces layer-wise representation consistency to neutralize backdoors.&lt;/li&gt;&lt;li&gt;CROW does not require knowledge of triggers or clean reference models, only a small clean dataset for fine-tuning.&lt;/li&gt;&lt;li&gt;Demonstrates effectiveness across multiple LLM architectures and backdoor attack types while preserving model performance.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research Paper&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Quality Assessment&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Clarity: 5/5&lt;/li&gt;&lt;li&gt;Novelty: 4/5&lt;/li&gt;&lt;li&gt;Significance: 4/5&lt;/li&gt;&lt;li&gt;Try-worthiness: Yes&lt;/li&gt;&lt;li&gt;Justification: The abstract is well-written, clearly explaining the motivation, method (CROW), and results. The approach appears novel, targeting backdoor defenses specifically for text generation in LLMs, which is less explored compared to classification tasks. The method does not require trigger knowledge or clean reference models, which is a practical advantage. The significance is high given the increasing concern about LLM security, and the evaluation on multiple popular models (Llama-2, CodeLlama, Mistral) adds credibility. While it is a preprint and has no citations yet, the topic and results suggest it is worth trying for researchers or practitioners interested in LLM security. No code repository is provided in the metadata.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: Nay Myat Min, Long H. Pham, Yige Li, Jun Sun&lt;/li&gt;&lt;li&gt;Tags: ['backdoor attacks', 'LLM security', 'defense methods', 'adversarial regularization', 'model robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2411.12768</guid><pubDate>Thu, 12 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Multimodal Pragmatic Jailbreak on Text-to-image Models</title><link>https://arxiv.org/abs/2409.19149</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces a novel multimodal jailbreak attack on text-to-image (T2I) diffusion models that generates unsafe content by combining visual text and images.&lt;/li&gt;&lt;li&gt;Evaluates nine T2I models, including commercial ones, revealing a high rate of unsafe content generation despite existing safety filters.&lt;/li&gt;&lt;li&gt;Demonstrates that common safety filters (keyword blocklists, prompt filters, NSFW image filters) are ineffective against this multimodal jailbreak.&lt;/li&gt;&lt;li&gt;Analyzes the causes of the vulnerability related to text rendering capabilities and training data, providing a foundation for improving T2I model security.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research Paper&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Quality Assessment&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Clarity: 5/5&lt;/li&gt;&lt;li&gt;Novelty: 5/5&lt;/li&gt;&lt;li&gt;Significance: 4/5&lt;/li&gt;&lt;li&gt;Try-worthiness: Yes&lt;/li&gt;&lt;li&gt;Justification: The abstract is well-written, clearly outlining the motivation, methodology, and findings. The work introduces a novel type of 'jailbreak' attack on text-to-image (T2I) diffusion models, focusing on multimodal unsafe content that evades existing filters—a new and timely concern as T2I models proliferate. The authors provide a new dataset, benchmark multiple models (including commercial ones), and analyze the failure of current safety filters, which is both novel and significant for the community. While the paper is very recent and has no citations yet, its topic is highly relevant and impactful for both researchers and practitioners concerned with AI safety. The project page is provided, which may contain further resources or code.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: Tong Liu, Zhixin Lai, Jiawen Wang, Gengyuan Zhang, Shuo Chen, Philip Torr, Vera Demberg, Volker Tresp, Jindong Gu&lt;/li&gt;&lt;li&gt;Tags: ['jailbreaking', 'multimodal attacks', 'text-to-image models', 'AI safety', 'security vulnerabilities']&lt;/li&gt;&lt;li&gt;Code Repository: &lt;a href='https://multimodalpragmatic.github.io/'&gt;https://multimodalpragmatic.github.io/&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2409.19149</guid><pubDate>Thu, 12 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Code Vulnerability Repair with Large Language Model using Context-Aware Prompt Tuning</title><link>https://arxiv.org/abs/2409.18395</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;The paper addresses the challenge of detecting and repairing buffer overflow vulnerabilities in code using Large Language Models (LLMs), specifically GitHub Copilot.&lt;/li&gt;&lt;li&gt;It identifies a significant gap in Copilot's ability to repair vulnerabilities despite a reasonable detection rate.&lt;/li&gt;&lt;li&gt;Proposes context-aware prompt tuning techniques that inject domain-specific security knowledge to improve repair success rates.&lt;/li&gt;&lt;li&gt;Demonstrates a substantial improvement in vulnerability repair performance, increasing from 15% to 63% with the proposed method.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research Paper&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Quality Assessment&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Clarity: 4/5&lt;/li&gt;&lt;li&gt;Novelty: 4/5&lt;/li&gt;&lt;li&gt;Significance: 3/5&lt;/li&gt;&lt;li&gt;Try-worthiness: Yes&lt;/li&gt;&lt;li&gt;Justification: The paper is clearly written, with a well-defined problem (buffer overflow repair with LLMs), a clear baseline (Copilot's current performance), and a specific proposed solution (context-aware prompt tuning with domain knowledge). The novelty is reasonably high, as prompt engineering for vulnerability repair is an emerging area, and the focus on buffer overflows and Copilot is specific. The significance is moderate: while the problem is important, the venue is arXiv (preprint), and the paper is very new, so impact is not yet established. The reported improvement (from 15% to 63% repair rate) is substantial, making the approach worth trying for practitioners or researchers in code security and LLMs. No code repository is provided in the metadata.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: Arshiya Khan, Guannan Liu, Xing Gao&lt;/li&gt;&lt;li&gt;Tags: ['code vulnerability repair', 'LLM security', 'prompt tuning', 'buffer overflow', 'AI security']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2409.18395</guid><pubDate>Thu, 12 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Raising the Bar: Investigating the Values of Large Language Models via Generative Evolving Testing</title><link>https://arxiv.org/abs/2406.14230</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes GETA, a generative evolving testing approach to dynamically evaluate value alignment in LLMs.&lt;/li&gt;&lt;li&gt;Addresses the problem of static benchmarks becoming outdated due to rapid model evolution (evaluation chronoeffect).&lt;/li&gt;&lt;li&gt;GETA generates difficulty-tailored test items that adapt to the model's capabilities, probing moral boundaries effectively.&lt;/li&gt;&lt;li&gt;Evaluation shows GETA provides more consistent and robust assessments of LLMs' value alignment, including on out-of-distribution data.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research Paper&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Quality Assessment&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Clarity: 4/5&lt;/li&gt;&lt;li&gt;Novelty: 5/5&lt;/li&gt;&lt;li&gt;Significance: 4/5&lt;/li&gt;&lt;li&gt;Try-worthiness: Yes&lt;/li&gt;&lt;li&gt;Justification: The abstract is well-written and clearly outlines the motivation, problem (evaluation chronoeffect), and the proposed solution (GETA). The approach of generative evolving testing for value alignment in LLMs is novel, especially as it adapts to model capabilities and avoids benchmark saturation. The significance is high given the importance of robust LLM evaluation, though the paper is very new and not yet peer-reviewed or cited. The method appears promising for researchers and practitioners interested in LLM safety and evaluation, making it worth experimenting with. No code repository is provided in the metadata.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: Han Jiang, Xiaoyuan Yi, Zhihua Wei, Ziang Xiao, Shu Wang, Xing Xie&lt;/li&gt;&lt;li&gt;Tags: ['AI safety', 'value alignment', 'LLM evaluation', 'harmful output prevention', 'adaptive testing']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2406.14230</guid><pubDate>Thu, 12 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Language Models Resist Alignment: Evidence From Data Compression</title><link>https://arxiv.org/abs/2406.06144</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Investigates the robustness of alignment fine-tuning in large language models (LLMs) and finds that alignment effects can be superficial and easily circumvented.&lt;/li&gt;&lt;li&gt;Introduces the concept of 'elasticity' where post-alignment models tend to revert to pre-training behavior distributions upon further fine-tuning.&lt;/li&gt;&lt;li&gt;Uses compression theory to theoretically explain why fine-tuning undermines alignment more than pre-training.&lt;/li&gt;&lt;li&gt;Empirically validates elasticity across different model types and scales, showing that larger models and more pre-training data increase elasticity.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research Paper&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Quality Assessment&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Clarity: 4/5&lt;/li&gt;&lt;li&gt;Novelty: 5/5&lt;/li&gt;&lt;li&gt;Significance: 4/5&lt;/li&gt;&lt;li&gt;Try-worthiness: Yes&lt;/li&gt;&lt;li&gt;Justification: The abstract is well-written and clearly explains the motivation, methodology, and findings of the paper, though some technical terms (like 'elasticity') may require background knowledge. The work is highly novel, being the first to theoretically and empirically analyze the 'elasticity' of LLM alignment using compression theory. Its significance is high given the centrality of alignment in LLM safety and deployment, and the findings have important implications for future alignment strategies. Although it is a preprint and very recent (hence no citations yet), the topic and results are timely and relevant. The availability of code and model weights further increases its try-worthiness for researchers interested in LLM alignment.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: Jiaming Ji, Kaile Wang, Tianyi Qiu, Boyuan Chen, Jiayi Zhou, Changye Li, Hantao Lou, Juntao Dai, Yunhuai Liu, Yaodong Yang&lt;/li&gt;&lt;li&gt;Tags: ['AI Safety', 'Alignment', 'Robustness', 'LLM Alignment', 'Fine-tuning']&lt;/li&gt;&lt;li&gt;Code Repository: &lt;a href='https://pku-lm-resist-alignment.github.io'&gt;https://pku-lm-resist-alignment.github.io&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2406.06144</guid><pubDate>Thu, 12 Jun 2025 00:00:00 +0000</pubDate></item><item><title>SafeCoT: Improving VLM Safety with Minimal Reasoning</title><link>https://arxiv.org/abs/2506.08399</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces SafeCoT, a framework to improve safety in vision-language models (VLMs) by enhancing refusal behavior.&lt;/li&gt;&lt;li&gt;Uses rule-based chain-of-thought supervision to enable models to reason about safety risks with minimal supervision.&lt;/li&gt;&lt;li&gt;Demonstrates improved safety alignment and generalization across multiple benchmarks with limited training data.&lt;/li&gt;&lt;li&gt;Focuses on reducing harmful or inappropriate outputs in VLMs, addressing AI safety and alignment challenges.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research Paper&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Quality Assessment&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Clarity: 5/5&lt;/li&gt;&lt;li&gt;Novelty: 4/5&lt;/li&gt;&lt;li&gt;Significance: 4/5&lt;/li&gt;&lt;li&gt;Try-worthiness: Yes&lt;/li&gt;&lt;li&gt;Justification: The abstract is well-written and clearly communicates the motivation, method (SafeCoT), and results. The approach is novel in that it uses minimal, interpretable rule-based chain-of-thought supervision for VLM safety, contrasting with prior work that relies on large-scale annotations or complex models. The significance is promising, as safety in VLMs is a critical and timely issue, and the method claims strong results with limited data. While the paper is very new and has no citations yet, its focus and results make it worth trying for researchers or practitioners interested in VLM safety. No code repository is provided in the metadata.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: Jiachen Ma, Zhanhui Zhou, Chao Yang, Chaochao Lu&lt;/li&gt;&lt;li&gt;Tags: ['AI safety', 'vision-language models', 'refusal behavior', 'chain-of-thought reasoning', 'safety alignment']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.08399</guid><pubDate>Thu, 12 Jun 2025 00:00:00 +0000</pubDate></item><item><title>RSafe: Incentivizing proactive reasoning to build robust and adaptive LLM safeguards</title><link>https://arxiv.org/abs/2506.07736</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes RSafe, a novel adaptive reasoning-based safeguard for LLMs to detect and block policy-violating content.&lt;/li&gt;&lt;li&gt;Addresses limitations of existing guard models that rely on curated datasets and struggle with out-of-distribution threats and jailbreak attacks.&lt;/li&gt;&lt;li&gt;Uses a two-stage training approach: guided safety reasoning and rule-based reinforcement learning to improve safety prediction and generalization.&lt;/li&gt;&lt;li&gt;Supports user-specified safety policies to tailor safeguards to specific safety requirements during inference.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research Paper&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Quality Assessment&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Clarity: 4/5&lt;/li&gt;&lt;li&gt;Novelty: 4/5&lt;/li&gt;&lt;li&gt;Significance: 3/5&lt;/li&gt;&lt;li&gt;Try-worthiness: Yes&lt;/li&gt;&lt;li&gt;Justification: The abstract is well-written and clearly explains the motivation, methodology, and contributions of the paper. The proposed RSafe framework introduces a two-stage, reasoning-based approach to LLM safety, which appears novel compared to existing guard models that rely mainly on curated datasets. The ability to generalize to out-of-distribution threats and accept user-specified safety policies is a meaningful advancement. However, as an arXiv preprint with no citations yet, its significance is not fully established, though the topic is highly relevant. The approach is promising and worth experimenting with, especially for those working on LLM safety and alignment. No code repository is provided in the metadata.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: Jingnan Zheng, Xiangtian Ji, Yijun Lu, Chenhang Cui, Weixiang Zhao, Gelei Deng, Zhenkai Liang, An Zhang, Tat-Seng Chua&lt;/li&gt;&lt;li&gt;Tags: ['LLM safety', 'jailbreaking', 'safety alignment', 'adversarial robustness', 'reinforcement learning']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.07736</guid><pubDate>Thu, 12 Jun 2025 00:00:00 +0000</pubDate></item><item><title>SAFEFLOW: A Principled Protocol for Trustworthy and Transactional Autonomous Agent Systems</title><link>https://arxiv.org/abs/2506.07564</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces SAFEFLOW, a protocol framework for trustworthy autonomous agents based on LLMs and VLMs, focusing on secure information flow control.&lt;/li&gt;&lt;li&gt;Enforces fine-grained tracking of data provenance, integrity, and confidentiality to prevent contamination from untrusted or adversarial inputs.&lt;/li&gt;&lt;li&gt;Implements transactional execution, conflict resolution, and secure scheduling to ensure robustness and consistency in multi-agent concurrent settings.&lt;/li&gt;&lt;li&gt;Presents SAFEFLOWBENCH, a benchmark suite to evaluate agent reliability and security under adversarial, noisy, and concurrent conditions.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research Paper&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Quality Assessment&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Clarity: 5/5&lt;/li&gt;&lt;li&gt;Novelty: 5/5&lt;/li&gt;&lt;li&gt;Significance: 4/5&lt;/li&gt;&lt;li&gt;Try-worthiness: Yes&lt;/li&gt;&lt;li&gt;Justification: The abstract is exceptionally clear, outlining both the motivation and the technical contributions of SAFEFLOW in a structured manner. The work appears highly novel, introducing a principled protocol for information flow control, transactional execution, and robust multi-agent coordination in LLM/VLM-based agent systems—areas that are currently under-addressed. The significance is high given the growing importance of trustworthy autonomous agents, though the lack of peer-reviewed publication and citations (due to recency) tempers this slightly. The introduction of a benchmark suite (SAFEFLOWBENCH) further increases its practical value. The paper is definitely worth trying for those interested in secure and reliable agent systems. No code repository is listed in the metadata or abstract.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: Peiran Li, Xinkai Zou, Zhuohang Wu, Ruifeng Li, Shuo Xing, Hanwen Zheng, Zhikai Hu, Yuping Wang, Haoxi Li, Qin Yuan, Yingmo Zhang, Zhengzhong Tu&lt;/li&gt;&lt;li&gt;Tags: ['information flow control', 'multi-agent security', 'adversarial robustness', 'secure autonomous agents', 'benchmarking security']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.07564</guid><pubDate>Thu, 12 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Supervision policies can shape long-term risk management in general-purpose AI models</title><link>https://arxiv.org/abs/2501.06137</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;The paper investigates supervision policies for managing risks and incidents in general-purpose AI models, including LLMs.&lt;/li&gt;&lt;li&gt;It develops a simulation framework to evaluate different risk supervision strategies such as priority-based and diversity-prioritized approaches.&lt;/li&gt;&lt;li&gt;Findings highlight trade-offs in risk management policies, showing how some approaches may overlook systemic issues reported by the community.&lt;/li&gt;&lt;li&gt;The study validates simulation results with real-world datasets, including a large corpus of ChatGPT interactions flagged as risky.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research Paper&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Quality Assessment&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Clarity: 4/5&lt;/li&gt;&lt;li&gt;Novelty: 4/5&lt;/li&gt;&lt;li&gt;Significance: 4/5&lt;/li&gt;&lt;li&gt;Try-worthiness: Yes&lt;/li&gt;&lt;li&gt;Justification: The abstract is well-written and clearly outlines the motivation, methodology, and findings of the paper, though some sentences are dense and could be more concise. The work is novel in its simulation-based approach to evaluating supervision policies for AI risk management, especially in the context of large-scale, real-world datasets. The significance is high given the growing importance of AI risk management and the use of real-world data (e.g., ChatGPT interactions) for validation. While the paper is a preprint and has no citations yet, its relevance to current AI governance challenges makes it worth experimenting with, especially for organizations or researchers interested in AI safety and oversight. No code repository is provided in the metadata.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: Manuel Cebrian, Emilia Gomez, David Fernandez Llorca&lt;/li&gt;&lt;li&gt;Tags: ['AI safety', 'risk management', 'incident reporting', 'supervision policies', 'LLM safety']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2501.06137</guid><pubDate>Thu, 12 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Beyond Bradley-Terry Models: A General Preference Model for Language Model Alignment</title><link>https://arxiv.org/abs/2410.02197</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces a novel preference embedding model to better capture complex human preferences for language model alignment.&lt;/li&gt;&lt;li&gt;Proposes General Preference Optimization (GPO), a generalization of reward-based RLHF to improve alignment.&lt;/li&gt;&lt;li&gt;Demonstrates improved performance over traditional Bradley-Terry reward models on benchmarks and downstream tasks.&lt;/li&gt;&lt;li&gt;Focuses on enhancing alignment of foundation models with nuanced human values, addressing AI safety concerns.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research Paper&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Quality Assessment&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Clarity: 4/5&lt;/li&gt;&lt;li&gt;Novelty: 5/5&lt;/li&gt;&lt;li&gt;Significance: 4/5&lt;/li&gt;&lt;li&gt;Try-worthiness: Yes&lt;/li&gt;&lt;li&gt;Justification: The abstract is well-written and clearly explains the motivation, methodology, and results, though some technical terms may require background knowledge (clarity: 4). The work is highly novel, proposing a general preference model that goes beyond the widely used Bradley-Terry model, especially in handling intransitive and cyclic preferences, which is a recognized limitation in current alignment methods (novelty: 5). The significance is high given the centrality of preference modeling in language model alignment and the reported improvements on benchmarks, though as a recent preprint, its impact is not yet established (significance: 4). The presence of a code repository and promising experimental results make this paper worth trying for researchers and practitioners in the field (try_worthiness: true).&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: Yifan Zhang, Ge Zhang, Yue Wu, Kangping Xu, Quanquan Gu&lt;/li&gt;&lt;li&gt;Tags: ['AI safety', 'alignment', 'reward modeling', 'reinforcement learning from human feedback', 'preference modeling']&lt;/li&gt;&lt;li&gt;Code Repository: &lt;a href='https://github.com/general-preference/general-preference-model'&gt;https://github.com/general-preference/general-preference-model&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2410.02197</guid><pubDate>Thu, 12 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Code-Switching Red-Teaming: LLM Evaluation for Safety and Multilingual Understanding</title><link>https://arxiv.org/abs/2406.15481</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces CSRT, a framework for generating code-switching red-teaming queries to evaluate LLM safety and multilingual understanding.&lt;/li&gt;&lt;li&gt;Demonstrates that code-switching queries can more effectively elicit undesirable behaviors in LLMs compared to standard English-only attacks.&lt;/li&gt;&lt;li&gt;Evaluates ten state-of-the-art LLMs across up to 10 languages, showing significant improvements in attack success rates.&lt;/li&gt;&lt;li&gt;Analyzes the relationship between language resource availability and safety alignment in multilingual LLMs.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research Paper&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Quality Assessment&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Clarity: 4/5&lt;/li&gt;&lt;li&gt;Novelty: 5/5&lt;/li&gt;&lt;li&gt;Significance: 4/5&lt;/li&gt;&lt;li&gt;Try-worthiness: Yes&lt;/li&gt;&lt;li&gt;Justification: The paper is clearly written, with a well-structured abstract that outlines the motivation, method (CSRT), experiments, and findings. The focus on code-switching in red-teaming for LLM safety is novel and addresses a real gap in current evaluation methods, especially as code-switching is common in natural language but underexplored in safety testing. The work is significant as it demonstrates substantial improvements (46.7% more attacks) over standard approaches and provides insights into multilingual LLM safety, which is increasingly important. Although it is a preprint with no citations yet, the topic and results make it worth experimenting with, especially for those interested in LLM safety and multilingual NLP. No code repository is provided in the metadata.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: Haneul Yoo, Yongjin Yang, Hwaran Lee&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'multilingual safety evaluation', 'adversarial prompting', 'safety testing', 'code-switching attacks']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2406.15481</guid><pubDate>Thu, 12 Jun 2025 00:00:00 +0000</pubDate></item><item><title>LLMail-Inject: A Dataset from a Realistic Adaptive Prompt Injection Challenge</title><link>https://arxiv.org/abs/2506.09956</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Presents LLMail-Inject, a dataset from a realistic adaptive prompt injection challenge targeting LLM-based email assistants.&lt;/li&gt;&lt;li&gt;Focuses on indirect prompt injection attacks that exploit LLMs' inability to distinguish instructions from data.&lt;/li&gt;&lt;li&gt;Includes analysis of 208,095 unique attack submissions from 839 participants across multiple defense strategies and LLM architectures.&lt;/li&gt;&lt;li&gt;Aims to provide insights into instruction-data separation and foster development of practical defenses against prompt injection.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research Paper&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Quality Assessment&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Clarity: 5/5&lt;/li&gt;&lt;li&gt;Novelty: 5/5&lt;/li&gt;&lt;li&gt;Significance: 4/5&lt;/li&gt;&lt;li&gt;Try-worthiness: Yes&lt;/li&gt;&lt;li&gt;Justification: The abstract is well-written and clearly explains the motivation, methodology, and contributions of the work. The paper introduces a large, realistic dataset from a public challenge focused on adaptive prompt injection attacks, which is a novel and timely contribution given the current security concerns around LLMs. The release of both the dataset and challenge code is highly valuable for the research community. While the paper is very recent and has no citations yet, its significance is high due to the scale of the dataset and the relevance of the problem. The work is definitely worth experimenting with, especially for those researching LLM security and prompt injection defenses.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: Sahar Abdelnabi, Aideen Fay, Ahmed Salem, Egor Zverev, Kai-Chieh Liao, Chi-Huang Liu, Chun-Chih Kuo, Jannis Weigend, Danyael Manlangit, Alex Apostolov, Haris Umair, Jo\~ao Donato, Masayuki Kawakita, Athar Mahboob, Tran Huu Bach, Tsun-Han Chiang, Myeongjin Cho, Hajin Choi, Byeonghyeon Kim, Hyeonjin Lee, Benjamin Pannell, Conor McCauley, Mark Russinovich, Andrew Paverd, Giovanni Cherubin&lt;/li&gt;&lt;li&gt;Tags: ['prompt injection', 'adversarial prompting', 'LLM security', 'red teaming', 'security evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.09956</guid><pubDate>Thu, 12 Jun 2025 00:00:00 +0000</pubDate></item><item><title>SAFE: Multitask Failure Detection for Vision-Language-Action Models</title><link>https://arxiv.org/abs/2506.09937</link><description>&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Other&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Quality Assessment&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Clarity: 5/5&lt;/li&gt;&lt;li&gt;Novelty: 4/5&lt;/li&gt;&lt;li&gt;Significance: 4/5&lt;/li&gt;&lt;li&gt;Try-worthiness: Yes&lt;/li&gt;&lt;li&gt;Justification: The abstract is well-written, clearly stating the problem, motivation, and contributions. The paper introduces the multitask failure detection problem for vision-language-action (VLA) models, which is a novel and timely topic as VLAs are increasingly used in robotics. The approach (SAFE) leverages internal VLA features for generalizable failure detection, which is a new direction compared to prior work focused on single tasks. The significance is high given the growing importance of safe deployment of generalist robot policies, and the extensive evaluation on multiple architectures and environments adds credibility. Although the paper is very recent and has no citations yet, its relevance and the promise of state-of-the-art results make it worth trying. The abstract provides a project page (https://vla-safe.github.io/) but does not list a direct code repository.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: Qiao Gu, Yuanliang Ju, Shengxiang Sun, Igor Gilitschenski, Haruki Nishimura, Masha Itkina, Florian Shkurti&lt;/li&gt;&lt;li&gt;Tags: []&lt;/li&gt;&lt;li&gt;Code Repository: &lt;a href='https://vla-safe.github.io/'&gt;https://vla-safe.github.io/&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.09937</guid><pubDate>Thu, 12 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Attention Head Embeddings with Trainable Deep Kernels for Hallucination Detection in LLMs</title><link>https://arxiv.org/abs/2506.09886</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a novel intrinsic method to detect hallucinations in LLM outputs by analyzing probabilistic divergence between prompt and response hidden-state distributions.&lt;/li&gt;&lt;li&gt;Finds that hallucinated responses show smaller deviations from prompts, indicating hallucinations stem from superficial rephrasing rather than deep reasoning.&lt;/li&gt;&lt;li&gt;Introduces trainable deep kernels to better capture geometric differences between distributions, improving detection sensitivity.&lt;/li&gt;&lt;li&gt;Demonstrates state-of-the-art performance on hallucination detection benchmarks without relying on external knowledge or auxiliary models.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research Paper&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Quality Assessment&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Clarity: 4/5&lt;/li&gt;&lt;li&gt;Novelty: 5/5&lt;/li&gt;&lt;li&gt;Significance: 4/5&lt;/li&gt;&lt;li&gt;Try-worthiness: Yes&lt;/li&gt;&lt;li&gt;Justification: The abstract is well-written and clearly explains the motivation, method, and results, though some technical terms may require background knowledge (clarity: 4). The approach is novel, leveraging probabilistic divergence between prompt and response hidden states and introducing trainable deep kernels for hallucination detection, which is a fresh perspective in the field (novelty: 5). The significance is high given the importance of hallucination detection in LLMs, and the claim of state-of-the-art results on several benchmarks, though the preprint status and lack of citations (due to recency) temper this slightly (significance: 4). The method appears robust, scalable, and competitive even without kernel training, making it worth trying for practitioners interested in LLM reliability (try_worthiness: true). No code repository is provided in the metadata.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: Rodion Oblovatny, Alexandra Bazarova, Alexey Zaytsev&lt;/li&gt;&lt;li&gt;Tags: ['hallucination detection', 'LLM safety', 'model-intrinsic evaluation', 'adversarial robustness', 'deep kernel learning']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.09886</guid><pubDate>Thu, 12 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Inverting Black-Box Face Recognition Systems via Zero-Order Optimization in Eigenface Space</title><link>https://arxiv.org/abs/2506.09777</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces DarkerBB, a method for reconstructing facial images from black-box face recognition models using only similarity scores.&lt;/li&gt;&lt;li&gt;Addresses a privacy attack scenario known as model inversion, which can reveal sensitive information from AI systems.&lt;/li&gt;&lt;li&gt;Uses zero-order optimization in eigenface space to perform the inversion without access to embeddings.&lt;/li&gt;&lt;li&gt;Demonstrates state-of-the-art verification accuracy and query efficiency on multiple face recognition benchmarks.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research Paper&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Quality Assessment&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Clarity: 4/5&lt;/li&gt;&lt;li&gt;Novelty: 5/5&lt;/li&gt;&lt;li&gt;Significance: 4/5&lt;/li&gt;&lt;li&gt;Try-worthiness: Yes&lt;/li&gt;&lt;li&gt;Justification: The paper is clearly written, with a well-defined problem statement and a concise description of the proposed method (DarkerBB). The novelty is high, as it tackles the challenging task of inverting black-box face recognition systems using only similarity scores, rather than embeddings, and introduces a zero-order optimization approach in eigenface space. The significance is strong due to the privacy implications and the achievement of state-of-the-art results on standard benchmarks, though the impact is yet to be seen given its recent publication and preprint status. The method appears promising and worth experimenting with, especially for researchers interested in model inversion, privacy, and adversarial attacks. No code repository is provided in the metadata.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: Anton Razzhigaev, Matvey Mikhalchuk, Klim Kireev, Igor Udovichenko, Andrey Kuznetsov, Aleksandr Petiushko&lt;/li&gt;&lt;li&gt;Tags: ['model inversion', 'privacy attacks', 'black-box attacks', 'face recognition security', 'adversarial techniques']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.09777</guid><pubDate>Thu, 12 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Reasoning Models Are More Easily Gaslighted Than You Think</title><link>https://arxiv.org/abs/2506.09677</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Evaluates the vulnerability of state-of-the-art reasoning models to gaslighting negation prompts, showing significant accuracy degradation.&lt;/li&gt;&lt;li&gt;Introduces GaslightingBench-R, a diagnostic benchmark to assess reasoning models' susceptibility to manipulative user inputs.&lt;/li&gt;&lt;li&gt;Highlights fundamental robustness limitations in reasoning models, especially in maintaining belief persistence under adversarial prompting.&lt;/li&gt;&lt;li&gt;Focuses on security implications related to adversarial prompting and model robustness against manipulative inputs.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research Paper&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Quality Assessment&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Clarity: 5/5&lt;/li&gt;&lt;li&gt;Novelty: 4/5&lt;/li&gt;&lt;li&gt;Significance: 4/5&lt;/li&gt;&lt;li&gt;Try-worthiness: Yes&lt;/li&gt;&lt;li&gt;Justification: The paper is clearly written, with a well-structured abstract that explains the motivation, methodology, and findings. The focus on 'gaslighting' prompts and the systematic evaluation of state-of-the-art reasoning models on this vulnerability is a novel angle, especially with the introduction of a new benchmark (GaslightingBench-R). While the venue is arXiv and the paper is very recent (hence no citations yet), the topic is highly relevant given the widespread deployment of reasoning-centric LLMs. The significant accuracy drops reported and the new benchmark make this work both significant and worth experimenting with, especially for those interested in model robustness and adversarial prompting.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: Bin Zhu, Hailong Yin, Jingjing Chen, Yu-Gang Jiang&lt;/li&gt;&lt;li&gt;Tags: ['adversarial prompting', 'robustness', 'LLM security', 'prompt injection', 'AI safety']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.09677</guid><pubDate>Thu, 12 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Effective Red-Teaming of Policy-Adherent Agents</title><link>https://arxiv.org/abs/2506.09600</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces a novel threat model targeting policy-adherent LLM-based agents vulnerable to adversarial user manipulation.&lt;/li&gt;&lt;li&gt;Proposes CRAFT, a multi-agent red-teaming system that uses policy-aware persuasive strategies to test agent robustness.&lt;/li&gt;&lt;li&gt;Develops tau-break, a new benchmark to rigorously evaluate agent resilience against manipulative user behavior.&lt;/li&gt;&lt;li&gt;Evaluates defense strategies against adversarial attacks, finding current methods insufficient and calling for stronger safeguards.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research Paper&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Quality Assessment&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Clarity: 4/5&lt;/li&gt;&lt;li&gt;Novelty: 5/5&lt;/li&gt;&lt;li&gt;Significance: 4/5&lt;/li&gt;&lt;li&gt;Try-worthiness: Yes&lt;/li&gt;&lt;li&gt;Justification: The paper is clearly written, with a well-defined problem statement and methodology described in the abstract. It introduces a novel threat model and a new red-teaming system (CRAFT) specifically targeting policy-adherent agents, which is a timely and underexplored area. The introduction of a new benchmark (tau-break) further adds to its novelty. While the paper is very recent and has no citations yet, the topic is highly significant given the increasing deployment of LLM agents in policy-sensitive domains. The proposed methods and benchmarks are likely to be of interest to both researchers and practitioners, making the paper worth trying out. No code repository is provided in the metadata.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: Itay Nakash, George Kour, Koren Lazar, Matan Vetzler, Guy Uziel, Ateret Anaby-Tavor&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'jailbreaking', 'adversarial prompting', 'policy adherence', 'security evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.09600</guid><pubDate>Thu, 12 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Adv-BMT: Bidirectional Motion Transformer for Safety-Critical Traffic Scenario Generation</title><link>https://arxiv.org/abs/2506.09485</link><description>&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Other&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Quality Assessment&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Clarity: 4/5&lt;/li&gt;&lt;li&gt;Novelty: 5/5&lt;/li&gt;&lt;li&gt;Significance: 4/5&lt;/li&gt;&lt;li&gt;Try-worthiness: Yes&lt;/li&gt;&lt;li&gt;Justification: The abstract is well-written and clearly explains the motivation, methodology, and results of the paper. The use of a bidirectional motion transformer for inverse traffic motion prediction and scenario generation appears novel, especially the ability to generate realistic collision scenarios without requiring collision data for pretraining. The significance is high given the importance of safety-critical scenario generation for autonomous driving validation, though the impact is yet to be fully established due to the paper's recency and preprint status. The reported 20% reduction in collision rates in training is promising and suggests practical value, making the approach worth trying for researchers or practitioners in autonomous driving simulation and testing.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: Yuxin Liu, Zhenghao Peng, Xuanhao Cui, Bolei Zhou&lt;/li&gt;&lt;li&gt;Tags: []&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.09485</guid><pubDate>Thu, 12 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Abstraction-Based Proof Production in Formal Verification of Neural Networks</title><link>https://arxiv.org/abs/2506.09455</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces a novel framework for proof-producing abstraction-based verification of deep neural networks (DNNs).&lt;/li&gt;&lt;li&gt;Separates verification into proving correctness of an abstract network and proving soundness of the abstraction relative to the original DNN.&lt;/li&gt;&lt;li&gt;Addresses the gap between scalability and provable guarantees in DNN verification tools.&lt;/li&gt;&lt;li&gt;Aims to enhance reliability and trustworthiness of DNN verification results through formal proofs.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research Paper&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Quality Assessment&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Clarity: 4/5&lt;/li&gt;&lt;li&gt;Novelty: 5/5&lt;/li&gt;&lt;li&gt;Significance: 4/5&lt;/li&gt;&lt;li&gt;Try-worthiness: Yes&lt;/li&gt;&lt;li&gt;Justification: The abstract is well-written and clearly explains the motivation, gap, and proposed solution, though some technical details are left for the full paper. The work is highly novel, as it claims to be the first to provide formal proof production for abstraction-based DNN verification—a key challenge in the field. The significance is high given the increasing demand for scalable and trustworthy neural network verification, though as a preprint, its impact is yet to be seen. The approach is worth trying for researchers in formal verification or neural network safety, as it addresses a real and pressing need. No code repository is provided in the metadata.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: Yizhak Yisrael Elboher, Omri Isac, Guy Katz, Tobias Ladner, Haoze Wu&lt;/li&gt;&lt;li&gt;Tags: ['formal verification', 'neural network verification', 'proof production', 'AI safety', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.09455</guid><pubDate>Thu, 12 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Reasoning as a Resource: Optimizing Fast and Slow Thinking in Code Generation Models</title><link>https://arxiv.org/abs/2506.09396</link><description>&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Other&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Quality Assessment&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Clarity: 4/5&lt;/li&gt;&lt;li&gt;Novelty: 5/5&lt;/li&gt;&lt;li&gt;Significance: 3/5&lt;/li&gt;&lt;li&gt;Try-worthiness: No&lt;/li&gt;&lt;li&gt;Justification: The paper is clearly written, with a well-articulated abstract that introduces the concept of treating reasoning depth as a controllable resource in code generation models. The idea of explicitly managing the trade-off between 'fast' and 'slow' thinking is novel and thought-provoking, especially in the context of code generation. However, as a position paper, it primarily outlines a vision and proposes directions rather than presenting concrete methods or empirical results. Its significance is moderate at this stage, as it may influence future research but does not yet provide actionable techniques or benchmarks. Given the lack of implementation details or code, it is not currently try-worthy for direct experimentation.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: Zongjie Li, Shuai Wang&lt;/li&gt;&lt;li&gt;Tags: []&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.09396</guid><pubDate>Thu, 12 Jun 2025 00:00:00 +0000</pubDate></item><item><title>SAGE: Exploring the Boundaries of Unsafe Concept Domain with Semantic-Augment Erasing</title><link>https://arxiv.org/abs/2506.09363</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Addresses safety risks in diffusion models related to unsafe content generation by proposing a novel concept erasing method.&lt;/li&gt;&lt;li&gt;Introduces semantic-augment erasing to generalize unsafe concept removal beyond fixed word erasure, improving safety in text-to-image generation.&lt;/li&gt;&lt;li&gt;Proposes a global-local collaborative retention mechanism to preserve irrelevant concepts while erasing unsafe ones, enhancing model robustness.&lt;/li&gt;&lt;li&gt;Demonstrates superior performance in safe generation of diffusion models compared to existing methods.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research Paper&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Quality Assessment&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Clarity: 4/5&lt;/li&gt;&lt;li&gt;Novelty: 5/5&lt;/li&gt;&lt;li&gt;Significance: 4/5&lt;/li&gt;&lt;li&gt;Try-worthiness: Yes&lt;/li&gt;&lt;li&gt;Justification: The abstract is well-written and clearly explains both the problem (unsafe concept erasure in diffusion models) and the proposed solution (semantic-augment erasing and a global-local retention mechanism). The approach appears novel, moving beyond simple word-based erasure to a more semantic, domain-level method, which is a meaningful advancement in the area of safe generative AI. While the paper is very new and thus has no citations yet, the topic is highly relevant and the proposed method could have significant impact on the safety of generative models. The promise of open-sourced code and weights further increases its try-worthiness for practitioners and researchers.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: Hongguang Zhu, Yunchao Wei, Mengyu Wang, Siyu Jiao, Yan Fang, Jiannan Huang, Yao Zhao&lt;/li&gt;&lt;li&gt;Tags: ['AI safety', 'safe content generation', 'concept erasing', 'diffusion models', 'adversarial mitigation']&lt;/li&gt;&lt;li&gt;Code Repository: &lt;a href='https://github.com/KevinLight831/SAGE'&gt;https://github.com/KevinLight831/SAGE&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.09363</guid><pubDate>Thu, 12 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Enhancing the Safety of Medical Vision-Language Models by Synthetic Demonstrations</title><link>https://arxiv.org/abs/2506.09067</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Addresses security vulnerabilities in medical vision-language models (Med-VLMs), focusing on mitigating harmful queries such as jailbreak attacks.&lt;/li&gt;&lt;li&gt;Proposes an inference-time defense strategy using synthetic clinical demonstrations to enhance model safety against both visual and textual adversarial inputs.&lt;/li&gt;&lt;li&gt;Demonstrates the approach on diverse medical imaging datasets across nine modalities, showing improved safety without significant performance degradation.&lt;/li&gt;&lt;li&gt;Introduces a mixed demonstration strategy to balance security and performance under limited demonstration budgets.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research Paper&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Quality Assessment&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Clarity: 4/5&lt;/li&gt;&lt;li&gt;Novelty: 5/5&lt;/li&gt;&lt;li&gt;Significance: 4/5&lt;/li&gt;&lt;li&gt;Try-worthiness: Yes&lt;/li&gt;&lt;li&gt;Justification: The abstract is well-written and clearly outlines the motivation, problem, and proposed solution, though some technical terms may require domain knowledge. The work is highly novel, addressing the underexplored area of security vulnerabilities in medical vision-language models and proposing a new inference-time defense using synthetic demonstrations. The significance is high given the critical importance of safety in medical AI, though the impact is yet to be established due to the paper's recency and preprint status. The approach appears practical and directly applicable, making it worth experimenting with, especially for those working on secure medical AI systems. No code repository is provided in the metadata.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: Zhiyu Xue, Reza Abbasi-Asl, Ramtin Pedarsani&lt;/li&gt;&lt;li&gt;Tags: ['AI safety', 'jailbreaking', 'adversarial prompting', 'multimodal security', 'defense strategies']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.09067</guid><pubDate>Thu, 12 Jun 2025 00:00:00 +0000</pubDate></item><item><title>An Interpretable N-gram Perplexity Threat Model for Large Language Model Jailbreaks</title><link>https://arxiv.org/abs/2410.16222</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a unified, interpretable N-gram perplexity threat model to evaluate and compare jailbreak attacks on safety-tuned large language models (LLMs).&lt;/li&gt;&lt;li&gt;The threat model is LLM-agnostic and nonparametric, enabling principled benchmarking of various jailbreaking methods on equal footing.&lt;/li&gt;&lt;li&gt;Finds that attack success rates against modern safety-tuned models are lower than previously reported and that discrete optimization-based attacks outperform recent LLM-based attacks.&lt;/li&gt;&lt;li&gt;Analyzes how effective jailbreaks exploit rare or out-of-distribution bigrams, providing insights into attack mechanisms.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research Paper&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Quality Assessment&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Clarity: 4/5&lt;/li&gt;&lt;li&gt;Novelty: 5/5&lt;/li&gt;&lt;li&gt;Significance: 4/5&lt;/li&gt;&lt;li&gt;Try-worthiness: Yes&lt;/li&gt;&lt;li&gt;Justification: The paper is clearly written, with a well-structured abstract that explains the motivation, methodology, and findings. The proposal of an interpretable, LLM-agnostic N-gram perplexity threat model for benchmarking jailbreak attacks is novel and addresses a gap in principled, comparable evaluation of such attacks. The work is significant as it provides new insights into the effectiveness of jailbreak methods and highlights the importance of infrequent bigrams in successful attacks. Although it is a preprint and very recent (hence no citations yet), the topic is highly relevant to LLM safety research. The approach is practical and interpretable, making it worth trying for researchers and practitioners interested in LLM security. No code repository is provided in the metadata.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: Valentyn Boreiko, Alexander Panfilov, Vaclav Voracek, Matthias Hein, Jonas Geiping&lt;/li&gt;&lt;li&gt;Tags: ['LLM jailbreaking', 'threat modeling', 'adversarial attacks', 'AI safety', 'security evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2410.16222</guid><pubDate>Thu, 12 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Application-Driven Value Alignment in Agentic AI Systems: Survey and Perspectives</title><link>https://arxiv.org/abs/2506.09656</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Surveys value alignment challenges and methods in agentic AI systems, focusing on ensuring AI goals and behaviors align with human values.&lt;/li&gt;&lt;li&gt;Discusses hierarchical value principles and their application in multi-agent autonomous decision-making environments.&lt;/li&gt;&lt;li&gt;Reviews evaluation datasets and methods for assessing value alignment in AI agents.&lt;/li&gt;&lt;li&gt;Explores value coordination among multiple agents and proposes future research directions in AI value alignment.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Survey/Review&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Quality Assessment&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Clarity: 4/5&lt;/li&gt;&lt;li&gt;Novelty: 3/5&lt;/li&gt;&lt;li&gt;Significance: 3/5&lt;/li&gt;&lt;li&gt;Try-worthiness: No&lt;/li&gt;&lt;li&gt;Justification: The paper is clearly written, with a well-structured abstract that outlines its scope, including a hierarchical review of value alignment principles, application scenarios, and evaluation methods in agentic AI systems. However, as a survey paper, its novelty is moderate—it synthesizes existing work rather than presenting new algorithms or empirical results. Its significance is reasonable given the growing importance of value alignment in multi-agent and LLM-driven systems, but as a recent preprint without citations and not yet peer-reviewed, its impact is still to be determined. Since it is a survey and not a technical contribution, there is no direct implementation or experimentation to try, so try-worthiness is rated as false. No code repository is provided.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: Wei Zeng, Hengshu Zhu, Chuan Qin, Han Wu, Yihang Cheng, Sirui Zhang, Xiaowei Jin, Yinuo Shen, Zhenxing Wang, Feimin Zhong, Hui Xiong&lt;/li&gt;&lt;li&gt;Tags: ['AI safety', 'value alignment', 'agentic AI', 'multi-agent systems', 'safety evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.09656</guid><pubDate>Thu, 12 Jun 2025 00:00:00 +0000</pubDate></item></channel></rss>