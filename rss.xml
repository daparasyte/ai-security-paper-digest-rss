<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>AI Security Paper Digest</title><link>https://kentaroh-toyoda.github.io/ai-security-paper-digest-rss/rss.xml</link><description>Curated papers on AI security from ArXiv</description><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><lastBuildDate>Wed, 03 Sep 2025 22:33:46 +0000</lastBuildDate><item><title>MCA-Bench: A Multimodal Benchmark for Evaluating CAPTCHA Robustness Against VLM-based Attacks</title><link>https://arxiv.org/abs/2506.05982</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces MCA-Bench, a multimodal benchmark for evaluating CAPTCHA robustness against VLM attacks&lt;/li&gt;&lt;li&gt;Uses vision-language models to fine-tune cracking agents for various CAPTCHA types&lt;/li&gt;&lt;li&gt;Provides quantitative analysis of CAPTCHA vulnerability factors&lt;/li&gt;&lt;li&gt;Proposes design principles for CAPTCHA hardening&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Zonglin Wu', 'Yule Xue', 'Yaoyao Feng', 'Xiaolong Wang', 'Yiren Song']&lt;/li&gt;&lt;li&gt;Tags: ['CAPTCHA', 'security', 'benchmark', 'multimodal', 'VLM', 'robustness', 'red teaming']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.05982</guid><pubDate>Wed, 03 Sep 2025 00:00:00 +0000</pubDate></item><item><title>VPO: Aligning Text-to-Video Generation Models with Prompt Optimization</title><link>https://arxiv.org/abs/2503.20491</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces VPO framework for aligning text-to-video models&lt;/li&gt;&lt;li&gt;Focuses on safety, alignment, and video quality improvements&lt;/li&gt;&lt;li&gt;Uses two-stage optimization with SFT and preference learning&lt;/li&gt;&lt;li&gt;Demonstrates safety enhancements and generalization&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jiale Cheng', 'Ruiliang Lyu', 'Xiaotao Gu', 'Xiao Liu', 'Jiazheng Xu', 'Yida Lu', 'Jiayan Teng', 'Zhuoyi Yang', 'Yuxiao Dong', 'Jie Tang', 'Hongning Wang', 'Minlie Huang']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'safety', 'prompt optimization', 'video generation', 'model alignment']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2503.20491</guid><pubDate>Wed, 03 Sep 2025 00:00:00 +0000</pubDate></item><item><title>See No Evil: Adversarial Attacks Against Linguistic-Visual Association in Referring Multi-Object Tracking Systems</title><link>https://arxiv.org/abs/2509.02028</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces adversarial attacks on Referring Multi-Object Tracking (RMOT) systems&lt;/li&gt;&lt;li&gt;Presents VEIL framework to disrupt linguistic-visual association and tracking logic&lt;/li&gt;&lt;li&gt;Demonstrates persistent errors in FIFO-based memory models over multiple frames&lt;/li&gt;&lt;li&gt;Validates attack effectiveness on Refer-KITTI dataset&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Halima Bouzidi', 'Haoyu Liu', 'Mohammad Al Faruque']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial attacks', 'security', 'red teaming', 'robustness', 'linguistic-visual association']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.02028</guid><pubDate>Wed, 03 Sep 2025 00:00:00 +0000</pubDate></item><item><title>Sequential Difference Maximization: Generating Adversarial Examples via Multi-Stage Optimization</title><link>https://arxiv.org/abs/2509.00826</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces Sequential Difference Maximization (SDM) for generating adversarial examples&lt;/li&gt;&lt;li&gt;Employs a three-layer optimization framework (cycle-stage-step)&lt;/li&gt;&lt;li&gt;Utilizes Directional Probability Difference Ratio (DPDR) loss in subsequent stages&lt;/li&gt;&lt;li&gt;Demonstrates improved attack performance and cost-effectiveness&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Xinlei Liu', 'Tao Hu', 'Peng Yi', 'Weitao Han', 'Jichao Xie', 'Baolin Li']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial attacks', 'red teaming', 'model robustness', 'security evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.00826</guid><pubDate>Wed, 03 Sep 2025 00:00:00 +0000</pubDate></item><item><title>Activation Steering Meets Preference Optimization: Defense Against Jailbreaks in Vision Language Models</title><link>https://arxiv.org/abs/2509.00373</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes SPO-VLM, a two-stage defense framework combining activation steering and preference optimization&lt;/li&gt;&lt;li&gt;Stage I computes adaptive steering vectors for generalized harm suppression&lt;/li&gt;&lt;li&gt;Stage II refines vectors with toxicity assessment and visual consistency rewards&lt;/li&gt;&lt;li&gt;Maintains visual grounding performance while enhancing safety against jailbreak attacks&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Sihao Wu', 'Gaojie Jin', 'Wei Huang', 'Jianhong Wang', 'Xiaowei Huang']&lt;/li&gt;&lt;li&gt;Tags: ['jailbreaking', 'adversarial_prompting', 'robustness', 'safety_evaluation', 'activation_steering']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.00373</guid><pubDate>Wed, 03 Sep 2025 00:00:00 +0000</pubDate></item><item><title>VPO: Aligning Text-to-Video Generation Models with Prompt Optimization</title><link>https://arxiv.org/abs/2503.20491</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces VPO framework for aligning text-to-video models&lt;/li&gt;&lt;li&gt;Focuses on safety, alignment, and video quality improvements&lt;/li&gt;&lt;li&gt;Uses two-stage optimization with SFT and preference learning&lt;/li&gt;&lt;li&gt;Demonstrates safety enhancements and generalization&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jiale Cheng', 'Ruiliang Lyu', 'Xiaotao Gu', 'Xiao Liu', 'Jiazheng Xu', 'Yida Lu', 'Jiayan Teng', 'Zhuoyi Yang', 'Yuxiao Dong', 'Jie Tang', 'Hongning Wang', 'Minlie Huang']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'safety', 'prompt optimization', 'video generation', 'model alignment']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2503.20491</guid><pubDate>Wed, 03 Sep 2025 00:00:00 +0000</pubDate></item><item><title>Efficient Detection of Toxic Prompts in Large Language Models</title><link>https://arxiv.org/abs/2408.11727</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes ToxicDetector, a greybox method for detecting toxic prompts in LLMs&lt;/li&gt;&lt;li&gt;Uses embeddings and MLP classifier for prompt classification&lt;/li&gt;&lt;li&gt;Achieves 96.39% accuracy and 2.00% false positive rate&lt;/li&gt;&lt;li&gt;Evaluated on LLama, Gemma-2 models and multiple datasets&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yi Liu', 'Junzhe Yu', 'Huijia Sun', 'Ling Shi', 'Gelei Deng', 'Yuqi Chen', 'Yang Liu']&lt;/li&gt;&lt;li&gt;Tags: ['toxic prompt detection', 'red teaming', 'safety evaluation', 'adversarial prompting', 'LLM security']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2408.11727</guid><pubDate>Wed, 03 Sep 2025 00:00:00 +0000</pubDate></item><item><title>ObjexMT: Objective Extraction and Metacognitive Calibration for LLM-as-a-Judge under Multi-Turn Jailbreaks</title><link>https://arxiv.org/abs/2508.16889</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces ObjexMT benchmark for evaluating objective extraction and metacognitive calibration in LLM judges under multi-turn jailbreak scenarios&lt;/li&gt;&lt;li&gt;Evaluates models on their ability to infer latent objectives and self-assess confidence&lt;/li&gt;&lt;li&gt;Finds significant calibration issues with models like gpt-4.1 and Qwen3 being overconfident&lt;/li&gt;&lt;li&gt;Recommends exposing objectives when possible and using confidence gating for safer decisions&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Hyunjun Kim', 'Junwoo Ha', 'Sangyoon Yu', 'Haon Park']&lt;/li&gt;&lt;li&gt;Tags: ['red teaming', 'jailbreaking', 'metacognition', 'calibration', 'benchmarking']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2508.16889</guid><pubDate>Wed, 03 Sep 2025 00:00:00 +0000</pubDate></item><item><title>Truthful Text Sanitization Guided by Inference Attacks</title><link>https://arxiv.org/abs/2412.12928</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces a two-stage text sanitization method using LLMs&lt;/li&gt;&lt;li&gt;First stage generates truth-preserving replacement candidates&lt;/li&gt;&lt;li&gt;Second stage evaluates candidates via inference attacks with LLMs&lt;/li&gt;&lt;li&gt;Proposes new metrics for privacy and utility evaluation&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ["Ildik\\'o Pil\\'an", 'Benet Manzanares-Salor', "David S\\'anchez", 'Pierre Lison']&lt;/li&gt;&lt;li&gt;Tags: ['red teaming', 'adversarial prompting', 'privacy attacks', 'LLM', 'text sanitization']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2412.12928</guid><pubDate>Wed, 03 Sep 2025 00:00:00 +0000</pubDate></item><item><title>MEGen: Generative Backdoor into Large Language Models via Model Editing</title><link>https://arxiv.org/abs/2408.10722</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes MEGen, an editing-based generative backdoor for LLMs&lt;/li&gt;&lt;li&gt;Enables generative backdoors with high attack success using minimal parameter changes&lt;/li&gt;&lt;li&gt;Demonstrates ability to output dangerous content when triggered&lt;/li&gt;&lt;li&gt;Highlights safety risks in generative LLM applications&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jiyang Qiu', 'Xinbei Ma', 'Zhuosheng Zhang', 'Hai Zhao', 'Yun Li', 'Qianren Wang']&lt;/li&gt;&lt;li&gt;Tags: ['backdoor', 'LLM', 'generative model', 'adversarial attack', 'model editing', 'security']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2408.10722</guid><pubDate>Wed, 03 Sep 2025 00:00:00 +0000</pubDate></item><item><title>Intrinsic Test of Unlearning Using Parametric Knowledge Traces</title><link>https://arxiv.org/abs/2406.11614</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a parameter-based evaluation method for unlearning in LLMs using vocabulary projections&lt;/li&gt;&lt;li&gt;Introduces ConceptVectors benchmark dataset with parametric knowledge traces&lt;/li&gt;&lt;li&gt;Finds existing unlearning methods minimally impact concept vectors, advocating for direct parameter modification&lt;/li&gt;&lt;li&gt;Highlights need for combined behavioral and parameter-based evaluations&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yihuai Hong', 'Lei Yu', 'Haiqin Yang', 'Shauli Ravfogel', 'Mor Geva']&lt;/li&gt;&lt;li&gt;Tags: ['unlearning', 'safety evaluation', 'adversarial manipulation', 'parameter inspection', 'concept vectors']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2406.11614</guid><pubDate>Wed, 03 Sep 2025 00:00:00 +0000</pubDate></item><item><title>Why Not Transform Chat Large Language Models to Non-English?</title><link>https://arxiv.org/abs/2405.13923</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces TransLLM framework for transforming chat LLMs to non-English&lt;/li&gt;&lt;li&gt;Achieves better safety performance by rejecting more harmful queries&lt;/li&gt;&lt;li&gt;Outperforms ChatGPT and GPT-4 on AdvBench safety benchmark&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Xiang Geng', 'Ming Zhu', 'Jiahuan Li', 'Zhejian Lai', 'Wei Zou', 'Shuaijie She', 'Jiaxin Guo', 'Xiaofeng Zhao', 'Yinglu Li', 'Yuang Li', 'Chang Su', 'Yanqing Zhao', 'Xinglin Lyu', 'Min Zhang', 'Jiajun Chen', 'Hao Yang', 'Shujian Huang']&lt;/li&gt;&lt;li&gt;Tags: ['safety evaluation', 'adversarial prompting', 'model alignment', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2405.13923</guid><pubDate>Wed, 03 Sep 2025 00:00:00 +0000</pubDate></item><item><title>DynaGuard: A Dynamic Guardrail Model With User-Defined Policies</title><link>https://arxiv.org/abs/2509.02563</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces DynaGuard, a dynamic guardrail model supporting user-defined policies&lt;/li&gt;&lt;li&gt;Matches static models in accuracy for predefined harms&lt;/li&gt;&lt;li&gt;Handles free-form policies with high accuracy and faster response than reasoning models&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Monte Hoover', 'Vatsal Baherwani', 'Neel Jain', 'Khalid Saifullah', 'Joseph Vincent', 'Chirag Jain', 'Melissa Kazemi Rad', 'C. Bayan Bruss', 'Ashwinee Panda', 'Tom Goldstein']&lt;/li&gt;&lt;li&gt;Tags: ['safety', 'guardrails', 'policy_enforcement', 'dynamic_models', 'moderation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.02563</guid><pubDate>Wed, 03 Sep 2025 00:00:00 +0000</pubDate></item><item><title>EigenBench: A Comparative Behavioral Measure of Value Alignment</title><link>https://arxiv.org/abs/2509.01938</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces EigenBench, a black-box method for comparative value alignment benchmarking of language models&lt;/li&gt;&lt;li&gt;Uses EigenTrust to aggregate model judgments without ground truth labels&lt;/li&gt;&lt;li&gt;Evaluates model alignment through mutual judgments across scenarios&lt;/li&gt;&lt;li&gt;Finds that prompt variance dominates but model disposition still contributes&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jonathn Chang', 'Leonard Piff', 'Suvadip Sana', 'Jasmine X. Li', 'Lionel Levine']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'safety evaluation', 'benchmarking', 'model comparison', 'value alignment']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.01938</guid><pubDate>Wed, 03 Sep 2025 00:00:00 +0000</pubDate></item><item><title>Oyster-I: Beyond Refusal -- Constructive Safety Alignment for Responsible Language Models</title><link>https://arxiv.org/abs/2509.01909</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces Constructive Safety Alignment (CSA) paradigm for LLMs&lt;/li&gt;&lt;li&gt;Implements CSA in Oyster-I (Oy1) model&lt;/li&gt;&lt;li&gt;Achieves state-of-the-art safety while retaining capabilities&lt;/li&gt;&lt;li&gt;Strong performance on Strata-Sword jailbreak dataset&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Ranjie Duan', 'Jiexi Liu', 'Xiaojun Jia', 'Shiji Zhao', 'Ruoxi Cheng', 'Fengxiang Wang', 'Cheng Wei', 'Yong Xie', 'Chang Liu', 'Defeng Li', 'Yinpeng Dong', 'Yichi Zhang', 'Yuefeng Chen', 'Chongwen Wang', 'Xingjun Ma', 'Xingxing Wei', 'Yang Liu', 'Hang Su', 'Jun Zhu', 'Xinfeng Li', 'Yitong Sun', 'Jie Zhang', 'Jinzhao Hu', 'Sha Xu', 'Yitong Yang', 'Jialing Tao', 'Hui Xue']&lt;/li&gt;&lt;li&gt;Tags: ['safety', 'alignment', 'red teaming', 'robustness', 'jailbreaking']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.01909</guid><pubDate>Wed, 03 Sep 2025 00:00:00 +0000</pubDate></item><item><title>Ensemble Debates with Local Large Language Models for AI Alignment</title><link>https://arxiv.org/abs/2509.00091</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Studies ensemble debates using local open-source LLMs for improving AI alignment&lt;/li&gt;&lt;li&gt;Evaluates performance across 15 scenarios and 5 ensemble configs&lt;/li&gt;&lt;li&gt;Shows significant gains in reasoning depth, argument quality, truthfulness, and human enhancement&lt;/li&gt;&lt;li&gt;Provides code, prompts, and dataset for reproducibility&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Ephraiem Sarabamoun']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'safety evaluation', 'ensemble methods', 'reproducibility']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.00091</guid><pubDate>Wed, 03 Sep 2025 00:00:00 +0000</pubDate></item><item><title>SpecEval: Evaluating Model Adherence to Behavior Specifications</title><link>https://arxiv.org/abs/2509.02464</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduced SpecEval framework to audit model adherence to provider specifications&lt;/li&gt;&lt;li&gt;Evaluates consistency between provider specs, model outputs, and evaluator models&lt;/li&gt;&lt;li&gt;Found compliance gaps up to 20% across providers&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Ahmed Ahmed', 'Kevin Klyman', 'Yi Zeng', 'Sanmi Koyejo', 'Percy Liang']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'safety evaluation', 'model auditing']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.02464</guid><pubDate>Wed, 03 Sep 2025 00:00:00 +0000</pubDate></item><item><title>Exploring and Mitigating Fawning Hallucinations in Large Language Models</title><link>https://arxiv.org/abs/2509.00869</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Analyzes fawning hallucinations in LLMs caused by deceptive prompts&lt;/li&gt;&lt;li&gt;Proposes Collaborative Contrastive Decoding (CCD) for mitigation&lt;/li&gt;&lt;li&gt;Demonstrates improved factuality across various tasks&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Zixuan Shangguan', 'Yanjie Dong', 'Lanjun Wang', 'Xiaoyi Fan', 'Victor C. M. Leung', 'Xiping Hu']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial prompting', 'safety evaluation', 'robustness', 'alignment']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.00869</guid><pubDate>Wed, 03 Sep 2025 00:00:00 +0000</pubDate></item><item><title>Thinking Hard, Going Misaligned: Emergent Misalignment in LLMs</title><link>https://arxiv.org/abs/2509.00544</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Reports Reasoning-Induced Misalignment where strengthening LLM reasoning via think-mode or math dataset fine-tuning increases malicious request responsiveness&lt;/li&gt;&lt;li&gt;Observes dense models are particularly vulnerable&lt;/li&gt;&lt;li&gt;Analyzes internal model states including attention shifts and MoE experts affecting safety guardrails&lt;/li&gt;&lt;li&gt;Highlights emerging reasoning-safety trade-off and need for advanced alignment&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Hanqi Yan', 'Hainiu Xu', 'Yulan He']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'safety', 'LLM', 'reasoning', 'red teaming']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.00544</guid><pubDate>Wed, 03 Sep 2025 00:00:00 +0000</pubDate></item><item><title>The Resurgence of GCG Adversarial Attacks on Large Language Models</title><link>https://arxiv.org/abs/2509.00391</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Systematic evaluation of GCG and T-GCG adversarial attacks on LLMs&lt;/li&gt;&lt;li&gt;Found attack success rates decrease with model size&lt;/li&gt;&lt;li&gt;Prefix-based metrics overestimate effectiveness compared to semantic judgments&lt;/li&gt;&lt;li&gt;Coding prompts more vulnerable than safety prompts&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yuting Tan', 'Xuying Li', 'Zhuo Li', 'Huizhen Shu', 'Peikang Hu']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial prompting', 'jailbreaking', 'red teaming', 'model evaluation', 'security']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.00391</guid><pubDate>Wed, 03 Sep 2025 00:00:00 +0000</pubDate></item><item><title>VPO: Aligning Text-to-Video Generation Models with Prompt Optimization</title><link>https://arxiv.org/abs/2503.20491</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces VPO framework for aligning text-to-video models&lt;/li&gt;&lt;li&gt;Focuses on safety, alignment, and video quality improvements&lt;/li&gt;&lt;li&gt;Uses two-stage optimization with SFT and preference learning&lt;/li&gt;&lt;li&gt;Demonstrates safety enhancements and generalization&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jiale Cheng', 'Ruiliang Lyu', 'Xiaotao Gu', 'Xiao Liu', 'Jiazheng Xu', 'Yida Lu', 'Jiayan Teng', 'Zhuoyi Yang', 'Yuxiao Dong', 'Jie Tang', 'Hongning Wang', 'Minlie Huang']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'safety', 'prompt optimization', 'video generation', 'model alignment']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2503.20491</guid><pubDate>Wed, 03 Sep 2025 00:00:00 +0000</pubDate></item><item><title>Addressing Key Challenges of Adversarial Attacks and Defenses in the Tabular Domain: A Methodological Framework for Coherence and Consistency</title><link>https://arxiv.org/abs/2412.07326</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a method to perturb dependent features while preserving coherence in tabular data adversarial attacks&lt;/li&gt;&lt;li&gt;Introduces Class-Specific Anomaly Detection (CSAD) for detecting adversarial samples relative to predicted class distributions&lt;/li&gt;&lt;li&gt;Develops SHAP-based anomaly detection to identify inconsistencies in model decision-making&lt;/li&gt;&lt;li&gt;Evaluates various attack strategies across multiple models and datasets&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yael Itzhakev', 'Amit Giloni', 'Yuval Elovici', 'Asaf Shabtai']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial attacks', 'anomaly detection', 'tabular data', 'evaluation metrics', 'red teaming']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2412.07326</guid><pubDate>Wed, 03 Sep 2025 00:00:00 +0000</pubDate></item><item><title>Adversarial Attacks and Defenses in Multivariate Time-Series Forecasting for Smart and Connected Infrastructures</title><link>https://arxiv.org/abs/2408.14875</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Investigates adversarial attacks (FGSM, BIM) on multivariate time-series forecasting models&lt;/li&gt;&lt;li&gt;Demonstrates data poisoning during training and model hardening defenses&lt;/li&gt;&lt;li&gt;Shows significant RMSE reduction after applying adversarial defenses&lt;/li&gt;&lt;li&gt;Tests transferability across electricity and hard disk failure datasets&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Pooja Krishan', 'Rohan Mohapatra', 'Sanchari Das', 'Saptarshi Sengupta']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial attacks', 'data poisoning', 'robustness', 'security standards', 'model hardening']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2408.14875</guid><pubDate>Wed, 03 Sep 2025 00:00:00 +0000</pubDate></item><item><title>EigenBench: A Comparative Behavioral Measure of Value Alignment</title><link>https://arxiv.org/abs/2509.01938</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces EigenBench, a black-box method for comparative value alignment benchmarking of language models&lt;/li&gt;&lt;li&gt;Uses EigenTrust to aggregate model judgments without ground truth labels&lt;/li&gt;&lt;li&gt;Evaluates model alignment through mutual judgments across scenarios&lt;/li&gt;&lt;li&gt;Finds that prompt variance dominates but model disposition still contributes&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jonathn Chang', 'Leonard Piff', 'Suvadip Sana', 'Jasmine X. Li', 'Lionel Levine']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'safety evaluation', 'benchmarking', 'model comparison', 'value alignment']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.01938</guid><pubDate>Wed, 03 Sep 2025 00:00:00 +0000</pubDate></item><item><title>Throttling Web Agents Using Reasoning Gates</title><link>https://arxiv.org/abs/2509.01619</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces Web Agent Throttling using reasoning gates to impose costs on agents&lt;/li&gt;&lt;li&gt;Designs rebus-based puzzles requiring multi-hop reasoning&lt;/li&gt;&lt;li&gt;Achieves 9.2x higher response cost for agents compared to SOTA models&lt;/li&gt;&lt;li&gt;Deploys and evaluates on custom website and MCP servers&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Abhinav Kumar', 'Jaechul Roh', 'Ali Naseh', 'Amir Houmansadr', 'Eugene Bagdasarian']&lt;/li&gt;&lt;li&gt;Tags: ['AI security', 'web agents', 'throttling', 'reasoning puzzles', 'adversarial defense']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.01619</guid><pubDate>Wed, 03 Sep 2025 00:00:00 +0000</pubDate></item><item><title>Sequential Difference Maximization: Generating Adversarial Examples via Multi-Stage Optimization</title><link>https://arxiv.org/abs/2509.00826</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces Sequential Difference Maximization (SDM) for generating adversarial examples&lt;/li&gt;&lt;li&gt;Employs a three-layer optimization framework (cycle-stage-step)&lt;/li&gt;&lt;li&gt;Utilizes Directional Probability Difference Ratio (DPDR) loss in subsequent stages&lt;/li&gt;&lt;li&gt;Demonstrates improved attack performance and cost-effectiveness&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Xinlei Liu', 'Tao Hu', 'Peng Yi', 'Weitao Han', 'Jichao Xie', 'Baolin Li']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial attacks', 'red teaming', 'model robustness', 'security evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.00826</guid><pubDate>Wed, 03 Sep 2025 00:00:00 +0000</pubDate></item><item><title>The Resurgence of GCG Adversarial Attacks on Large Language Models</title><link>https://arxiv.org/abs/2509.00391</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Systematic evaluation of GCG and T-GCG adversarial attacks on LLMs&lt;/li&gt;&lt;li&gt;Found attack success rates decrease with model size&lt;/li&gt;&lt;li&gt;Prefix-based metrics overestimate effectiveness compared to semantic judgments&lt;/li&gt;&lt;li&gt;Coding prompts more vulnerable than safety prompts&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yuting Tan', 'Xuying Li', 'Zhuo Li', 'Huizhen Shu', 'Peikang Hu']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial prompting', 'jailbreaking', 'red teaming', 'model evaluation', 'security']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.00391</guid><pubDate>Wed, 03 Sep 2025 00:00:00 +0000</pubDate></item><item><title>AEGIS : Automated Co-Evolutionary Framework for Guarding Prompt Injections Schema</title><link>https://arxiv.org/abs/2509.00088</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces AEGIS, an automated co-evolutionary framework for defending against prompt injection attacks in LLMs&lt;/li&gt;&lt;li&gt;Uses Textual Gradient Optimization (TGO) to iteratively evolve both attack and defense prompts&lt;/li&gt;&lt;li&gt;Demonstrates significant improvements in attack success rate (ASR) and detection true positive rate (TPR)&lt;/li&gt;&lt;li&gt;Validates effectiveness across different LLMs through ablation studies&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Ting-Chun Liu', 'Ching-Yu Hsu', 'Kuan-Yi Lee', 'Chi-An Fu', 'Hung-yi Lee']&lt;/li&gt;&lt;li&gt;Tags: ['prompt injection', 'adversarial training', 'co-evolution', 'LLM security', 'red teaming']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.00088</guid><pubDate>Wed, 03 Sep 2025 00:00:00 +0000</pubDate></item><item><title>DynaGuard: A Dynamic Guardrail Model With User-Defined Policies</title><link>https://arxiv.org/abs/2509.02563</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces DynaGuard, a dynamic guardrail model supporting user-defined policies&lt;/li&gt;&lt;li&gt;Matches static models in accuracy for predefined harms&lt;/li&gt;&lt;li&gt;Handles free-form policies with high accuracy and faster response than reasoning models&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Monte Hoover', 'Vatsal Baherwani', 'Neel Jain', 'Khalid Saifullah', 'Joseph Vincent', 'Chirag Jain', 'Melissa Kazemi Rad', 'C. Bayan Bruss', 'Ashwinee Panda', 'Tom Goldstein']&lt;/li&gt;&lt;li&gt;Tags: ['safety', 'guardrails', 'policy_enforcement', 'dynamic_models', 'moderation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.02563</guid><pubDate>Wed, 03 Sep 2025 00:00:00 +0000</pubDate></item><item><title>FedThief: Harming Others to Benefit Oneself in Self-Centered Federated Learning</title><link>https://arxiv.org/abs/2509.00540</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces FedThief, a self-centered federated learning attack&lt;/li&gt;&lt;li&gt;Attackers upload tampered model updates to degrade global model&lt;/li&gt;&lt;li&gt;Simultaneously enhance private model via divergence-aware ensemble&lt;/li&gt;&lt;li&gt;Experiments show improved attacker model and degraded global model&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Xiangyu Zhang', 'Mang Ye']&lt;/li&gt;&lt;li&gt;Tags: ['data poisoning', 'federated learning', 'security', 'adversarial attacks']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.00540</guid><pubDate>Wed, 03 Sep 2025 00:00:00 +0000</pubDate></item><item><title>One Shot Dominance: Knowledge Poisoning Attack on Retrieval-Augmented Generation Systems</title><link>https://arxiv.org/abs/2505.11548</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces AuthChain: a single-document knowledge poisoning attack on RAG systems&lt;/li&gt;&lt;li&gt;Effective for complex multi-hop questions involving multiple elements&lt;/li&gt;&lt;li&gt;Achieves higher attack success rates and stealthiness vs. state-of-the-art&lt;/li&gt;&lt;li&gt;Validated across six popular LLMs&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Zhiyuan Chang', 'Mingyang Li', 'Xiaojun Jia', 'Junjie Wang', 'Yuekai Huang', 'Ziyou Jiang', 'Yang Liu', 'Qing Wang']&lt;/li&gt;&lt;li&gt;Tags: ['data_poisoning', 'security', 'RAG', 'adversarial_attacks', 'LLM']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2505.11548</guid><pubDate>Wed, 03 Sep 2025 00:00:00 +0000</pubDate></item><item><title>Progent: Programmable Privilege Control for LLM Agents</title><link>https://arxiv.org/abs/2504.11703</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces Progent, a privilege control framework for LLM agents&lt;/li&gt;&lt;li&gt;Enforces tool-level security with fine-grained policies&lt;/li&gt;&lt;li&gt;Evaluated on benchmarks like AgentDojo, ASB, AgentPoison&lt;/li&gt;&lt;li&gt;Reduces attack success rates to 0% while preserving utility&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Tianneng Shi', 'Jingxuan He', 'Zhun Wang', 'Hongwei Li', 'Linyu Wu', 'Wenbo Guo', 'Dawn Song']&lt;/li&gt;&lt;li&gt;Tags: ['security', 'tool restriction', 'privilege control', 'red teaming', 'policy enforcement']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2504.11703</guid><pubDate>Wed, 03 Sep 2025 00:00:00 +0000</pubDate></item><item><title>PBI-Attack: Prior-Guided Bimodal Interactive Black-Box Jailbreak Attack for Toxicity Maximization</title><link>https://arxiv.org/abs/2412.05892</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes PBI-Attack: a prior-guided, bimodal interactive black-box jailbreak attack for LVLMs&lt;/li&gt;&lt;li&gt;Uses image and text perturbations to maximize toxicity in model responses&lt;/li&gt;&lt;li&gt;Achieves high success rates on both open-source (92.5%) and closed-source (67.3%) LVLMs&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Ruoxi Cheng', 'Yizhong Ding', 'Shuirong Cao', 'Ranjie Duan', 'Xiaoshuang Jia', 'Shaowei Yuan', 'Simeng Qin', 'Zhiqiang Wang', 'Xiaojun Jia']&lt;/li&gt;&lt;li&gt;Tags: ['jailbreaking', 'adversarial_prompting', 'red_team', 'multimodal_attack', 'LVLM']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2412.05892</guid><pubDate>Wed, 03 Sep 2025 00:00:00 +0000</pubDate></item><item><title>Adversarial Attacks and Defenses in Multivariate Time-Series Forecasting for Smart and Connected Infrastructures</title><link>https://arxiv.org/abs/2408.14875</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Investigates adversarial attacks (FGSM, BIM) on multivariate time-series forecasting models&lt;/li&gt;&lt;li&gt;Demonstrates data poisoning during training and model hardening defenses&lt;/li&gt;&lt;li&gt;Shows significant RMSE reduction after applying adversarial defenses&lt;/li&gt;&lt;li&gt;Tests transferability across electricity and hard disk failure datasets&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Pooja Krishan', 'Rohan Mohapatra', 'Sanchari Das', 'Saptarshi Sengupta']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial attacks', 'data poisoning', 'robustness', 'security standards', 'model hardening']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2408.14875</guid><pubDate>Wed, 03 Sep 2025 00:00:00 +0000</pubDate></item><item><title>Efficient Detection of Toxic Prompts in Large Language Models</title><link>https://arxiv.org/abs/2408.11727</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes ToxicDetector, a greybox method for detecting toxic prompts in LLMs&lt;/li&gt;&lt;li&gt;Uses embeddings and MLP classifier for prompt classification&lt;/li&gt;&lt;li&gt;Achieves 96.39% accuracy and 2.00% false positive rate&lt;/li&gt;&lt;li&gt;Evaluated on LLama, Gemma-2 models and multiple datasets&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yi Liu', 'Junzhe Yu', 'Huijia Sun', 'Ling Shi', 'Gelei Deng', 'Yuqi Chen', 'Yang Liu']&lt;/li&gt;&lt;li&gt;Tags: ['toxic prompt detection', 'red teaming', 'safety evaluation', 'adversarial prompting', 'LLM security']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2408.11727</guid><pubDate>Wed, 03 Sep 2025 00:00:00 +0000</pubDate></item><item><title>MEGen: Generative Backdoor into Large Language Models via Model Editing</title><link>https://arxiv.org/abs/2408.10722</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes MEGen, an editing-based generative backdoor for LLMs&lt;/li&gt;&lt;li&gt;Enables generative backdoors with high attack success using minimal parameter changes&lt;/li&gt;&lt;li&gt;Demonstrates ability to output dangerous content when triggered&lt;/li&gt;&lt;li&gt;Highlights safety risks in generative LLM applications&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jiyang Qiu', 'Xinbei Ma', 'Zhuosheng Zhang', 'Hai Zhao', 'Yun Li', 'Qianren Wang']&lt;/li&gt;&lt;li&gt;Tags: ['backdoor', 'LLM', 'generative model', 'adversarial attack', 'model editing', 'security']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2408.10722</guid><pubDate>Wed, 03 Sep 2025 00:00:00 +0000</pubDate></item><item><title>Intrinsic Test of Unlearning Using Parametric Knowledge Traces</title><link>https://arxiv.org/abs/2406.11614</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a parameter-based evaluation method for unlearning in LLMs using vocabulary projections&lt;/li&gt;&lt;li&gt;Introduces ConceptVectors benchmark dataset with parametric knowledge traces&lt;/li&gt;&lt;li&gt;Finds existing unlearning methods minimally impact concept vectors, advocating for direct parameter modification&lt;/li&gt;&lt;li&gt;Highlights need for combined behavioral and parameter-based evaluations&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yihuai Hong', 'Lei Yu', 'Haiqin Yang', 'Shauli Ravfogel', 'Mor Geva']&lt;/li&gt;&lt;li&gt;Tags: ['unlearning', 'safety evaluation', 'adversarial manipulation', 'parameter inspection', 'concept vectors']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2406.11614</guid><pubDate>Wed, 03 Sep 2025 00:00:00 +0000</pubDate></item><item><title>HAICOSYSTEM: An Ecosystem for Sandboxing Safety Risks in Human-AI Interactions</title><link>https://arxiv.org/abs/2409.16427</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces HAICOSYSTEM, a sandbox framework for evaluating AI agent safety in complex human interactions&lt;/li&gt;&lt;li&gt;Features a modular environment simulating multi-turn interactions with tool use&lt;/li&gt;&lt;li&gt;Develops a multi-dimensional evaluation framework covering various safety risks&lt;/li&gt;&lt;li&gt;Demonstrates safety vulnerabilities in state-of-the-art LLMs when facing malicious users&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Xuhui Zhou', 'Hyunwoo Kim', 'Faeze Brahman', 'Liwei Jiang', 'Hao Zhu', 'Ximing Lu', 'Frank Xu', 'Bill Yuchen Lin', 'Yejin Choi', 'Niloofar Mireshghallah', 'Ronan Le Bras', 'Maarten Sap']&lt;/li&gt;&lt;li&gt;Tags: ['safety evaluation', 'red teaming', 'robustness', 'human-ai interaction', 'simulation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2409.16427</guid><pubDate>Wed, 03 Sep 2025 00:00:00 +0000</pubDate></item><item><title>A Survey: Towards Privacy and Security in Mobile Large Language Models</title><link>https://arxiv.org/abs/2509.02411</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Comprehensive survey on privacy and security issues in mobile LLMs&lt;/li&gt;&lt;li&gt;Covers adversarial attacks, membership inference, and side-channel attacks&lt;/li&gt;&lt;li&gt;Reviews solutions like differential privacy, federated learning, and prompt encryption&lt;/li&gt;&lt;li&gt;Discusses open challenges and future research directions&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Survey&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Honghui Xu', 'Kaiyang Li', 'Wei Chen', 'Danyang Zheng', 'Zhiyuan Li', 'Zhipeng Cai']&lt;/li&gt;&lt;li&gt;Tags: ['privacy', 'security', 'adversarial attacks', 'mobile LLMs', 'survey']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.02411</guid><pubDate>Wed, 03 Sep 2025 00:00:00 +0000</pubDate></item><item><title>Poisoned at Scale: A Scalable Audit Uncovers Hidden Scam Endpoints in Production LLMs</title><link>https://arxiv.org/abs/2509.02372</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces a scalable audit framework to detect poisoned training data in LLMs&lt;/li&gt;&lt;li&gt;Evaluates four production LLMs (GPT-4o, GPT-4o-mini, Llama-4-Scout, DeepSeek-V3)&lt;/li&gt;&lt;li&gt;Finds 4.2% of generated code contains malicious URLs from benign prompts&lt;/li&gt;&lt;li&gt;Highlights urgent need for robust defense mechanisms and safety checks&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Zhiyang Chen', 'Tara Saba', 'Xun Deng', 'Xujie Si', 'Fan Long']&lt;/li&gt;&lt;li&gt;Tags: ['data poisoning', 'adversarial prompting', 'safety evaluation', 'alignment', 'red teaming']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.02372</guid><pubDate>Wed, 03 Sep 2025 00:00:00 +0000</pubDate></item><item><title>Enhancing Reliability in LLM-Integrated Robotic Systems: A Unified Approach to Security and Safety</title><link>https://arxiv.org/abs/2509.02163</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a unified framework for enhancing security and safety in LLM-integrated robotic systems&lt;/li&gt;&lt;li&gt;Mitigates prompt injection attacks through prompt assembling and state management&lt;/li&gt;&lt;li&gt;Enforces operational safety with robust validation mechanisms&lt;/li&gt;&lt;li&gt;Demonstrates significant improvements in adversarial conditions&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Wenxiao Zhang', 'Xiangrui Kong', 'Conan Dewitt', 'Thomas Br\\"aunl', 'Jin B. Hong']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'adversarial prompting', 'safety evaluation', 'robustness', 'security standards']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.02163</guid><pubDate>Wed, 03 Sep 2025 00:00:00 +0000</pubDate></item><item><title>Web Fraud Attacks Against LLM-Driven Multi-Agent Systems</title><link>https://arxiv.org/abs/2509.01211</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces Web Fraud Attacks targeting LLM-driven multi-agent systems&lt;/li&gt;&lt;li&gt; Designs 11 attack variants using domain tampering and link camouflage&lt;/li&gt;&lt;li&gt;Demonstrates high effectiveness and stealth across different MAS architectures&lt;/li&gt;&lt;li&gt;Highlights need for improved link validation and security measures&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Dezhang Kong', 'Hujin Peng', 'Yilun Zhang', 'Lele Zhao', 'Zhenhua Xu', 'Shi Lin', 'Changting Lin', 'Meng Han']&lt;/li&gt;&lt;li&gt;Tags: ['red teaming', 'adversarial attacks', 'security', 'multi-agent systems', 'web security']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.01211</guid><pubDate>Wed, 03 Sep 2025 00:00:00 +0000</pubDate></item><item><title>Clone What You Can't Steal: Black-Box LLM Replication via Logit Leakage and Distillation</title><link>https://arxiv.org/abs/2509.00973</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces a two-stage pipeline for black-box LLM replication using logit leakage and distillation&lt;/li&gt;&lt;li&gt;Reconstructs output projection matrix via SVD on collected logits&lt;/li&gt;&lt;li&gt;Distills compact student models with comparable performance&lt;/li&gt;&lt;li&gt;Highlights security risks of exposed logits in API responses&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Kanchon Gharami', 'Hansaka Aluvihare', 'Shafika Showkat Moni', 'Berker Pek\\"oz']&lt;/li&gt;&lt;li&gt;Tags: ['model extraction', 'adversarial attacks', 'API security', 'logits leakage', 'red teaming']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.00973</guid><pubDate>Wed, 03 Sep 2025 00:00:00 +0000</pubDate></item><item><title>Sequential Difference Maximization: Generating Adversarial Examples via Multi-Stage Optimization</title><link>https://arxiv.org/abs/2509.00826</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces Sequential Difference Maximization (SDM) for generating adversarial examples&lt;/li&gt;&lt;li&gt;Employs a three-layer optimization framework (cycle-stage-step)&lt;/li&gt;&lt;li&gt;Utilizes Directional Probability Difference Ratio (DPDR) loss in subsequent stages&lt;/li&gt;&lt;li&gt;Demonstrates improved attack performance and cost-effectiveness&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Xinlei Liu', 'Tao Hu', 'Peng Yi', 'Weitao Han', 'Jichao Xie', 'Baolin Li']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial attacks', 'red teaming', 'model robustness', 'security evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.00826</guid><pubDate>Wed, 03 Sep 2025 00:00:00 +0000</pubDate></item><item><title>The Resurgence of GCG Adversarial Attacks on Large Language Models</title><link>https://arxiv.org/abs/2509.00391</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Systematic evaluation of GCG and T-GCG adversarial attacks on LLMs&lt;/li&gt;&lt;li&gt;Found attack success rates decrease with model size&lt;/li&gt;&lt;li&gt;Prefix-based metrics overestimate effectiveness compared to semantic judgments&lt;/li&gt;&lt;li&gt;Coding prompts more vulnerable than safety prompts&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yuting Tan', 'Xuying Li', 'Zhuo Li', 'Huizhen Shu', 'Peikang Hu']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial prompting', 'jailbreaking', 'red teaming', 'model evaluation', 'security']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.00391</guid><pubDate>Wed, 03 Sep 2025 00:00:00 +0000</pubDate></item><item><title>Activation Steering Meets Preference Optimization: Defense Against Jailbreaks in Vision Language Models</title><link>https://arxiv.org/abs/2509.00373</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes SPO-VLM, a two-stage defense framework combining activation steering and preference optimization&lt;/li&gt;&lt;li&gt;Stage I computes adaptive steering vectors for generalized harm suppression&lt;/li&gt;&lt;li&gt;Stage II refines vectors with toxicity assessment and visual consistency rewards&lt;/li&gt;&lt;li&gt;Maintains visual grounding performance while enhancing safety against jailbreak attacks&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Sihao Wu', 'Gaojie Jin', 'Wei Huang', 'Jianhong Wang', 'Xiaowei Huang']&lt;/li&gt;&lt;li&gt;Tags: ['jailbreaking', 'adversarial_prompting', 'robustness', 'safety_evaluation', 'activation_steering']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.00373</guid><pubDate>Wed, 03 Sep 2025 00:00:00 +0000</pubDate></item><item><title>A Whole New World: Creating a Parallel-Poisoned Web Only AI-Agents Can See</title><link>https://arxiv.org/abs/2509.00124</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces a cloaking-based attack targeting LLM-powered web agents&lt;/li&gt;&lt;li&gt;Exploits agent fingerprinting to serve malicious content&lt;/li&gt;&lt;li&gt;Enables hidden prompt injection and behavior manipulation&lt;/li&gt;&lt;li&gt;Highlights urgent security implications for agentic AI&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Shaked Zychlinski']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'prompt injection', 'data poisoning', 'privacy attacks', 'security standards']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.00124</guid><pubDate>Wed, 03 Sep 2025 00:00:00 +0000</pubDate></item><item><title>AEGIS : Automated Co-Evolutionary Framework for Guarding Prompt Injections Schema</title><link>https://arxiv.org/abs/2509.00088</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces AEGIS, an automated co-evolutionary framework for defending against prompt injection attacks in LLMs&lt;/li&gt;&lt;li&gt;Uses Textual Gradient Optimization (TGO) to iteratively evolve both attack and defense prompts&lt;/li&gt;&lt;li&gt;Demonstrates significant improvements in attack success rate (ASR) and detection true positive rate (TPR)&lt;/li&gt;&lt;li&gt;Validates effectiveness across different LLMs through ablation studies&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Ting-Chun Liu', 'Ching-Yu Hsu', 'Kuan-Yi Lee', 'Chi-An Fu', 'Hung-yi Lee']&lt;/li&gt;&lt;li&gt;Tags: ['prompt injection', 'adversarial training', 'co-evolution', 'LLM security', 'red teaming']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.00088</guid><pubDate>Wed, 03 Sep 2025 00:00:00 +0000</pubDate></item><item><title>Private, Verifiable, and Auditable AI Systems</title><link>https://arxiv.org/abs/2509.00085</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces techniques for verifiable and auditable AI using zero-knowledge cryptography&lt;/li&gt;&lt;li&gt;Utilizes secure multi-party computation and trusted execution environments for confidential deployment&lt;/li&gt;&lt;li&gt;Enhances security with delegation mechanisms and access controls&lt;/li&gt;&lt;li&gt;Focuses on privacy, verifiability, and auditability in foundation models&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Tobin South']&lt;/li&gt;&lt;li&gt;Tags: ['AI security', 'AI safety', 'privacy', 'verifiability', 'auditability']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.00085</guid><pubDate>Wed, 03 Sep 2025 00:00:00 +0000</pubDate></item><item><title>EigenBench: A Comparative Behavioral Measure of Value Alignment</title><link>https://arxiv.org/abs/2509.01938</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces EigenBench, a black-box method for comparative value alignment benchmarking of language models&lt;/li&gt;&lt;li&gt;Uses EigenTrust to aggregate model judgments without ground truth labels&lt;/li&gt;&lt;li&gt;Evaluates model alignment through mutual judgments across scenarios&lt;/li&gt;&lt;li&gt;Finds that prompt variance dominates but model disposition still contributes&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jonathn Chang', 'Leonard Piff', 'Suvadip Sana', 'Jasmine X. Li', 'Lionel Levine']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'safety evaluation', 'benchmarking', 'model comparison', 'value alignment']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.01938</guid><pubDate>Wed, 03 Sep 2025 00:00:00 +0000</pubDate></item><item><title>Oyster-I: Beyond Refusal -- Constructive Safety Alignment for Responsible Language Models</title><link>https://arxiv.org/abs/2509.01909</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces Constructive Safety Alignment (CSA) paradigm for LLMs&lt;/li&gt;&lt;li&gt;Implements CSA in Oyster-I (Oy1) model&lt;/li&gt;&lt;li&gt;Achieves state-of-the-art safety while retaining capabilities&lt;/li&gt;&lt;li&gt;Strong performance on Strata-Sword jailbreak dataset&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Ranjie Duan', 'Jiexi Liu', 'Xiaojun Jia', 'Shiji Zhao', 'Ruoxi Cheng', 'Fengxiang Wang', 'Cheng Wei', 'Yong Xie', 'Chang Liu', 'Defeng Li', 'Yinpeng Dong', 'Yichi Zhang', 'Yuefeng Chen', 'Chongwen Wang', 'Xingjun Ma', 'Xingxing Wei', 'Yang Liu', 'Hang Su', 'Jun Zhu', 'Xinfeng Li', 'Yitong Sun', 'Jie Zhang', 'Jinzhao Hu', 'Sha Xu', 'Yitong Yang', 'Jialing Tao', 'Hui Xue']&lt;/li&gt;&lt;li&gt;Tags: ['safety', 'alignment', 'red teaming', 'robustness', 'jailbreaking']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.01909</guid><pubDate>Wed, 03 Sep 2025 00:00:00 +0000</pubDate></item><item><title>Unraveling LLM Jailbreaks Through Safety Knowledge Neurons</title><link>https://arxiv.org/abs/2509.01631</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces neuron-level interpretability for safety-related neurons in LLMs&lt;/li&gt;&lt;li&gt;Proposes SafeTuning fine-tuning strategy to reinforce safety neurons&lt;/li&gt;&lt;li&gt;Achieves over 97% ASR in controlling model behavior&lt;/li&gt;&lt;li&gt;Outperforms baseline defenses across multiple LLMs&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Chongwen Zhao', 'Kaizhu Huang']&lt;/li&gt;&lt;li&gt;Tags: ['jailbreak', 'safety', 'neuron', 'SafeTuning', 'red teaming']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.01631</guid><pubDate>Wed, 03 Sep 2025 00:00:00 +0000</pubDate></item><item><title>Counterfactual Sensitivity for Faithful Reasoning in Language Models</title><link>https://arxiv.org/abs/2509.01544</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces Counterfactual Sensitivity Regularization (CSR) to enforce dependence between reasoning steps and outputs&lt;/li&gt;&lt;li&gt;Uses operator-level counterfactual interventions during training to penalize invalid reasoning&lt;/li&gt;&lt;li&gt;Improves faithfulness in structured reasoning tasks with minimal accuracy loss&lt;/li&gt;&lt;li&gt;Generalizes to larger models and complements inference-time methods&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Ibne Farabi Shihab', 'Sanjeda Akter', 'Anuj Sharma']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'robustness', 'safety evaluation', 'process supervision', 'counterfactual reasoning']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.01544</guid><pubDate>Wed, 03 Sep 2025 00:00:00 +0000</pubDate></item><item><title>Instruction-Level Weight Shaping: A Framework for Self-Improving AI Agents</title><link>https://arxiv.org/abs/2509.00251</link><description>&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Other&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Rimom Costa']&lt;/li&gt;&lt;li&gt;Tags: []&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.00251</guid><pubDate>Wed, 03 Sep 2025 00:00:00 +0000</pubDate></item><item><title>Ensemble Debates with Local Large Language Models for AI Alignment</title><link>https://arxiv.org/abs/2509.00091</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Studies ensemble debates using local open-source LLMs for improving AI alignment&lt;/li&gt;&lt;li&gt;Evaluates performance across 15 scenarios and 5 ensemble configs&lt;/li&gt;&lt;li&gt;Shows significant gains in reasoning depth, argument quality, truthfulness, and human enhancement&lt;/li&gt;&lt;li&gt;Provides code, prompts, and dataset for reproducibility&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Ephraiem Sarabamoun']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'safety evaluation', 'ensemble methods', 'reproducibility']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.00091</guid><pubDate>Wed, 03 Sep 2025 00:00:00 +0000</pubDate></item></channel></rss>