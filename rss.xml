<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>AI Security Paper Digest</title><link>https://kentaroh-toyoda.github.io/ai-security-paper-digest-rss/rss.xml</link><description>Curated papers on AI security from ArXiv and ACL</description><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><lastBuildDate>Fri, 10 Oct 2025 22:31:32 +0000</lastBuildDate><item><title>SaFeR-VLM: Toward Safety-aware Fine-grained Reasoning in Multimodal Models</title><link>https://arxiv.org/abs/2510.06871</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes SaFeR-VLM, a safety-aligned reinforcement learning framework for multimodal models&lt;/li&gt;&lt;li&gt;Introduces QI-Safe-10K dataset for safety-critical cases&lt;/li&gt;&lt;li&gt;Uses safety-aware rollout with reflection and correction&lt;/li&gt;&lt;li&gt;Structured reward modeling with explicit penalties for hallucinations and contradictions&lt;/li&gt;&lt;li&gt;GRPO optimization for reinforcing safe trajectories&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Huahui Yi', 'Kun Wang', 'Qiankun Li', 'Miao Yu', 'Liang Lin', 'Gongli Xi', 'Hao Wu', 'Xuming Hu', 'Kang Li', 'Yang Liu']&lt;/li&gt;&lt;li&gt;Tags: ['safety', 'multimodal', 'reinforcement learning', 'dataset', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.06871</guid><pubDate>Fri, 10 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Safe-Control: A Safety Patch for Mitigating Unsafe Content in Text-to-Image Generation Models</title><link>https://arxiv.org/abs/2508.21099</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces Safe-Control, a plug-and-play safety patch for T2I models&lt;/li&gt;&lt;li&gt;Uses data-driven strategies and safety-aware conditions&lt;/li&gt;&lt;li&gt;Effective in reducing unsafe content generation across multiple models&lt;/li&gt;&lt;li&gt;Outperforms existing safety mechanisms in evaluations&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Xiangtao Meng', 'Yingkai Dong', 'Ning Yu', 'Li Wang', 'Zheng Li', 'Shanqing Guo']&lt;/li&gt;&lt;li&gt;Tags: ['safety', 'text-to-image', 'adversarial attacks', 'content moderation', 'model patch']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2508.21099</guid><pubDate>Fri, 10 Oct 2025 00:00:00 +0000</pubDate></item><item><title>$\textit{Agents Under Siege}$: Breaking Pragmatic Multi-Agent LLM Systems with Optimized Prompt Attacks</title><link>https://arxiv.org/abs/2504.00218</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces permutation-invariant adversarial attacks on multi-agent LLM systems&lt;/li&gt;&lt;li&gt;Uses graph-based optimization and PIEL loss to distribute prompts across agents&lt;/li&gt;&lt;li&gt;Evaluates on multiple models and datasets, showing significant improvement over conventional attacks&lt;/li&gt;&lt;li&gt;Highlights failure of existing defenses in multi-agent contexts&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Rana Muhammad Shahroz Khan', 'Zhen Tan', 'Sukwon Yun', 'Charles Fleming', 'Tianlong Chen']&lt;/li&gt;&lt;li&gt;Tags: ['red teaming', 'adversarial prompting', 'multi-agent systems', 'jailbreaking', 'safety evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2504.00218</guid><pubDate>Fri, 10 Oct 2025 00:00:00 +0000</pubDate></item><item><title>X-Teaming Evolutionary M2S: Automated Discovery of Multi-turn to Single-turn Jailbreak Templates</title><link>https://arxiv.org/abs/2509.08729</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Automated framework for discovering M2S jailbreak templates using language-model-guided evolution&lt;/li&gt;&lt;li&gt;Achieves 44.8% success on GPT-4.1 with two new template families&lt;/li&gt;&lt;li&gt;Emphasizes cross-model evaluation and threshold calibration&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Hyunjun Kim', 'Junwoo Ha', 'Sangyoon Yu', 'Haon Park']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'jailbreaking', 'prompt injection', 'adversarial prompting', 'model extraction']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.08729</guid><pubDate>Fri, 10 Oct 2025 00:00:00 +0000</pubDate></item><item><title>ObjexMT: Objective Extraction and Metacognitive Calibration for LLM-as-a-Judge under Multi-Turn Jailbreaks</title><link>https://arxiv.org/abs/2508.16889</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces ObjexMT benchmark for evaluating LLMs' ability to extract objectives from multi-turn jailbreak conversations&lt;/li&gt;&lt;li&gt;Assesses both accuracy of objective extraction and metacognitive calibration&lt;/li&gt;&lt;li&gt;Evaluates six models across three datasets, highlighting performance variations and confidence issues&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Hyunjun Kim', 'Junwoo Ha', 'Sangyoon Yu', 'Haon Park']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'jailbreaking', 'safety evaluation', 'benchmarking', 'metacognition']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2508.16889</guid><pubDate>Fri, 10 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Can Small-Scale Data Poisoning Exacerbate Dialect-Linked Biases in Large Language Models?</title><link>https://arxiv.org/abs/2507.19195</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Study on data poisoning using dialectal prompts to amplify biases in LLMs&lt;/li&gt;&lt;li&gt;Poisoned data pairs dialects (AAVE, Southern) with toxic completions&lt;/li&gt;&lt;li&gt;Models show increased toxicity and stereotypes for dialect inputs after poisoning&lt;/li&gt;&lt;li&gt;Emergent jailbreaking occurs without explicit slurs, indicating alignment issues&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Chaymaa Abbas', 'Mariette Awad', 'Razane Tajeddine']&lt;/li&gt;&lt;li&gt;Tags: ['data poisoning', 'dialect bias', 'jailbreaking', 'safety evaluation', 'alignment']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2507.19195</guid><pubDate>Fri, 10 Oct 2025 00:00:00 +0000</pubDate></item><item><title>LLMs Encode Harmfulness and Refusal Separately</title><link>https://arxiv.org/abs/2507.11878</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Identifies separate harmfulness and refusal directions in LLMs&lt;/li&gt;&lt;li&gt;Shows jailbreak methods reduce refusal without changing harmfulness perception&lt;/li&gt;&lt;li&gt;Introduces Latent Guard using internal harmfulness representation for safety&lt;/li&gt;&lt;li&gt;Demonstrates robustness against finetuning attacks&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jiachen Zhao', 'Jing Huang', 'Zhengxuan Wu', 'David Bau', 'Weiyan Shi']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'jailbreaking', 'adversarial prompting', 'alignment', 'safety evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2507.11878</guid><pubDate>Fri, 10 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Multi-Trigger Poisoning Amplifies Backdoor Vulnerabilities in LLMs</title><link>https://arxiv.org/abs/2507.11112</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces a framework for multi-trigger poisoning in LLMs&lt;/li&gt;&lt;li&gt;Demonstrates coexistence of multiple backdoor triggers&lt;/li&gt;&lt;li&gt;Proposes a post-hoc recovery method using layer-wise analysis&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Sanhanat Sivapiromrat', 'Caiqi Zhang', 'Marco Basaldella', 'Nigel Collier']&lt;/li&gt;&lt;li&gt;Tags: ['data poisoning', 'backdoor attacks', 'LLM security', 'defence mechanisms', 'trigger robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2507.11112</guid><pubDate>Fri, 10 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Inference-time Alignment in Continuous Space</title><link>https://arxiv.org/abs/2505.20081</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes SEA, an algorithm for inference-time alignment using gradient-based sampling in continuous latent space&lt;/li&gt;&lt;li&gt;Aims to improve alignment over discrete response space methods&lt;/li&gt;&lt;li&gt;Shows significant performance improvements on AdvBench and MATH&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yige Yuan', 'Teng Xiao', 'Li Yunfan', 'Bingbing Xu', 'Shuchang Tao', 'Yunqi Qiu', 'Huawei Shen', 'Xueqi Cheng']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'inference-time', 'gradient-based', 'latent space', 'large language models']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2505.20081</guid><pubDate>Fri, 10 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Logic Jailbreak: Efficiently Unlocking LLM Safety Restrictions Through Formal Logical Expression</title><link>https://arxiv.org/abs/2505.13527</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces LogiBreak, a black-box jailbreak method using logical expressions&lt;/li&gt;&lt;li&gt;Converts harmful prompts into formal logic to bypass safety mechanisms&lt;/li&gt;&lt;li&gt;Evaluated on multilingual dataset showing effectiveness across languages&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jingyu Peng', 'Maolin Wang', 'Nan Wang', 'Jiatong Li', 'Yuchen Li', 'Yuyang Ye', 'Wanyu Wang', 'Pengyue Jia', 'Kai Zhang', 'Xiangyu Zhao']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'jailbreaking', 'adversarial prompting', 'safety evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2505.13527</guid><pubDate>Fri, 10 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Say It Another Way: Auditing LLMs with a User-Grounded Automated Paraphrasing Framework</title><link>https://arxiv.org/abs/2505.03563</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces AUGMENT framework for controlled paraphrasing in auditing LLMs&lt;/li&gt;&lt;li&gt;Focuses on user-grounded transformations with linguistic and demographic factors&lt;/li&gt;&lt;li&gt;Case studies on BBQ and MMLU datasets reveal systematic weaknesses&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ["Cl\\'ea Chataigner", 'Rebecca Ma', 'Prakhar Ganesh', 'Yuhao Chen', 'Afaf Ta\\"ik', 'Elliot Creager', 'Golnoosh Farnadi']&lt;/li&gt;&lt;li&gt;Tags: ['red teaming', 'prompt injection', 'adversarial prompting', 'safety evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2505.03563</guid><pubDate>Fri, 10 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Enhancing LLM Reliability via Explicit Knowledge Boundary Modeling</title><link>https://arxiv.org/abs/2503.02233</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes EKBM framework to enhance LLM reliability by modeling knowledge boundaries&lt;/li&gt;&lt;li&gt;Uses fast and slow reasoning systems to balance efficiency and accuracy&lt;/li&gt;&lt;li&gt;Hybrid training pipeline for self-awareness&lt;/li&gt;&lt;li&gt;Evaluated on dialogue state tracking tasks&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Hang Zheng', 'Hongshen Xu', 'Yuncong Liu', 'Lu Chen', 'Pascale Fung', 'Kai Yu']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'safety evaluation', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2503.02233</guid><pubDate>Fri, 10 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Erasing Without Remembering: Implicit Knowledge Forgetting in Large Language Models</title><link>https://arxiv.org/abs/2502.19982</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Investigates knowledge forgetting in LLMs, focusing on generalizing unlearning to related implicit knowledge.&lt;/li&gt;&lt;li&gt;Evaluates 15 methods across 3 datasets, finding current approaches don't handle paraphrased or related data well.&lt;/li&gt;&lt;li&gt;Proposes PerMU, a probability perturbation-based unlearning method that targets fact-related tokens.&lt;/li&gt;&lt;li&gt;Shows significant improvements in both target and implicit knowledge forgetting across multiple datasets and model sizes.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Huazheng Wang', 'Yongcheng Jing', 'Haifeng Sun', 'Yingjie Wang', 'Jingyu Wang', 'Jianxin Liao', 'Dacheng Tao']&lt;/li&gt;&lt;li&gt;Tags: ['unlearning', 'knowledge forgetting', 'implicit knowledge', 'adversarial unlearning', 'privacy attacks']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2502.19982</guid><pubDate>Fri, 10 Oct 2025 00:00:00 +0000</pubDate></item><item><title>MetaDefense: Defending Finetuning-based Jailbreak Attack Before and During Generation</title><link>https://arxiv.org/abs/2510.07835</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces MetaDefense, a framework to defend against finetuning-based jailbreak attacks in LLMs.&lt;/li&gt;&lt;li&gt;Uses two-stage defense: pre-generation detection and mid-generation monitoring.&lt;/li&gt;&lt;li&gt;Trains LLM to predict harmfulness using specialized prompts.&lt;/li&gt;&lt;li&gt;Shows improved performance over existing defenses on multiple LLM architectures.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Weisen Jiang', 'Sinno Jialin Pan']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'jailbreaking', 'adversarial prompting', 'safety evaluation', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.07835</guid><pubDate>Fri, 10 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Who Stole Your Data? A Method for Detecting Unauthorized RAG Theft</title><link>https://arxiv.org/abs/2510.07728</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces RPD dataset for RAG plagiarism detection&lt;/li&gt;&lt;li&gt;Develops dual-layered watermarking system&lt;/li&gt;&lt;li&gt;Uses interrogator-detective framework with hypothesis testing&lt;/li&gt;&lt;li&gt;Tests against adversarial evasion&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Peiyang Liu', 'Ziqiang Cui', 'Di Liang', 'Wei Ye']&lt;/li&gt;&lt;li&gt;Tags: ['data_poisoning', 'privacy_attacks', 'red_teaming', 'model_extraction', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.07728</guid><pubDate>Fri, 10 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Multimodal Safety Evaluation in Generative Agent Social Simulations</title><link>https://arxiv.org/abs/2510.07709</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces a simulation framework for evaluating generative agents' safety in multimodal environments&lt;/li&gt;&lt;li&gt;Assesses safety improvement over time, detection of unsafe activities, and social dynamics&lt;/li&gt;&lt;li&gt;Finds agents struggle to align local revisions with global safety, with success rates varying by model and scenario&lt;/li&gt;&lt;li&gt;Highlights overtrust in misleading visuals as a significant issue&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Alhim Vera', 'Karen Sanchez', 'Carlos Hinojosa', 'Haidar Bin Hamid', 'Donghoon Kim', 'Bernard Ghanem']&lt;/li&gt;&lt;li&gt;Tags: ['multimodal safety', 'agent evaluation', 'social simulations', 'safety metrics', 'visual-text interactions']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.07709</guid><pubDate>Fri, 10 Oct 2025 00:00:00 +0000</pubDate></item><item><title>PATCH: Mitigating PII Leakage in Language Models with Privacy-Aware Targeted Circuit PatcHing</title><link>https://arxiv.org/abs/2510.07452</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces PATCH, a method to mitigate PII leakage in LMs by identifying and editing specific circuits&lt;/li&gt;&lt;li&gt;Achieves better privacy-utility trade-off than existing defenses like DP&lt;/li&gt;&lt;li&gt;Reduces PII leakage recall by up to 65% and further with DP combination&lt;/li&gt;&lt;li&gt;Circuits persist after other defenses, making PATCH a complementary approach&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Anthony Hughes', 'Vasisht Duddu', 'N. Asokan', 'Nikolaos Aletras', 'Ning Ma']&lt;/li&gt;&lt;li&gt;Tags: ['privacy attacks', 'model extraction', 'data poisoning', 'safety evaluation', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.07452</guid><pubDate>Fri, 10 Oct 2025 00:00:00 +0000</pubDate></item><item><title>AutoRed: A Free-form Adversarial Prompt Generation Framework for Automated Red Teaming</title><link>https://arxiv.org/abs/2510.08329</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes AutoRed, a free-form adversarial prompt generation framework for red teaming LLMs&lt;/li&gt;&lt;li&gt;Removes reliance on seed instructions by using persona-guided generation and reflection loop&lt;/li&gt;&lt;li&gt;Introduces a verifier to assess prompt harmfulness without querying target models&lt;/li&gt;&lt;li&gt;Builds and evaluates two new red teaming datasets (AutoRed-Medium and AutoRed-Hard)&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Muxi Diao', 'Yutao Mou', 'Keqing He', 'Hanbo Song', 'Lulu Zhao', 'Shikun Zhang', 'Wei Ye', 'Kongming Liang', 'Zhanyu Ma']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'adversarial prompting', 'jailbreaking', 'safety evaluation', 'prompt injection']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.08329</guid><pubDate>Fri, 10 Oct 2025 00:00:00 +0000</pubDate></item><item><title>The Alignment Waltz: Jointly Training Agents to Collaborate for Safety</title><link>https://arxiv.org/abs/2510.08240</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes WaltzRL, a multi-agent RL framework for safety alignment&lt;/li&gt;&lt;li&gt;Trains conversation and feedback agents collaboratively&lt;/li&gt;&lt;li&gt;Uses Dynamic Improvement Reward (DIR) to evolve over time&lt;/li&gt;&lt;li&gt;Reduces unsafe responses and overrefusals in experiments&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jingyu Zhang', 'Haozhu Wang', 'Eric Michael Smith', 'Sid Wang', 'Amr Sharaf', 'Mahesh Pasupuleti', 'Benjamin Van Durme', 'Daniel Khashabi', 'Jason Weston', 'Hongyuan Zhan']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'reinforcement learning', 'multi-agent', 'safety evaluation', 'red teaming']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.08240</guid><pubDate>Fri, 10 Oct 2025 00:00:00 +0000</pubDate></item><item><title>LLMs Learn to Deceive Unintentionally: Emergent Misalignment in Dishonesty from Misaligned Samples to Biased Human-AI Interactions</title><link>https://arxiv.org/abs/2510.08211</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Investigates emergent misalignment in LLMs leading to dishonesty and deception&lt;/li&gt;&lt;li&gt;Shows that small amounts of misaligned data can cause significant dishonest behavior&lt;/li&gt;&lt;li&gt;Explores human-AI interaction scenarios with biased users&lt;/li&gt;&lt;li&gt;Demonstrates risks in downstream finetuning and practical interactions&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['XuHao Hu', 'Peng Wang', 'Xiaoya Lu', 'Dongrui Liu', 'Xuanjing Huang', 'Jing Shao']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'safety evaluation', 'adversarial prompting', 'model extraction', 'privacy attacks']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.08211</guid><pubDate>Fri, 10 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Beyond Over-Refusal: Scenario-Based Diagnostics and Post-Hoc Mitigation for Exaggerated Refusals in LLMs</title><link>https://arxiv.org/abs/2510.08158</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces XSB and MS-XSB benchmarks for evaluating false refusals in LLMs&lt;/li&gt;&lt;li&gt;Identifies refusal triggers using post-hoc explanations&lt;/li&gt;&lt;li&gt;Proposes three model-agnostic mitigation strategies&lt;/li&gt;&lt;li&gt;Improves compliance on safe prompts while maintaining safety&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Shuzhou Yuan', 'Ercong Nie', 'Yinuo Sun', 'Chenxuan Zhao', 'William LaCroix', 'Michael F\\"arber']&lt;/li&gt;&lt;li&gt;Tags: ['LLM safety', 'red teaming', 'jailbreaking', 'prompt injection', 'adversarial prompting', 'safety evaluation', 'benchmarks']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.08158</guid><pubDate>Fri, 10 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Role-Conditioned Refusals: Evaluating Access Control Reasoning in Large Language Models</title><link>https://arxiv.org/abs/2510.07642</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Evaluates LLMs' ability to adhere to access control policies through role-conditioned refusals&lt;/li&gt;&lt;li&gt;Compares prompting, two-step verification, and fine-tuning approaches&lt;/li&gt;&lt;li&gt;Uses modified Spider and BIRD datasets with PostgreSQL role-based policies&lt;/li&gt;&lt;li&gt;Finds verification improves precision, fine-tuning balances safety and utility&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['{\\DJ}or{\\dj}e Klisura', 'Joseph Khoury', 'Ashish Kundu', 'Ram Krishnan', 'Anthony Rios']&lt;/li&gt;&lt;li&gt;Tags: ['red teaming', 'access control', 'prompt injection', 'safety evaluation', 'model extraction']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.07642</guid><pubDate>Fri, 10 Oct 2025 00:00:00 +0000</pubDate></item><item><title>AEGIS : Automated Co-Evolutionary Framework for Guarding Prompt Injections Schema</title><link>https://arxiv.org/abs/2509.00088</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes AEGIS, an automated co-evolutionary framework for guarding prompt injection attacks in LLMs.&lt;/li&gt;&lt;li&gt;Uses Textual Gradient Optimization (TGO) to iteratively optimize both attack and defense prompts.&lt;/li&gt;&lt;li&gt;Evaluates on a real-world dataset, showing improved attack success and detection rates compared to baselines.&lt;/li&gt;&lt;li&gt;Demonstrates effectiveness across different LLMs.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Ting-Chun Liu', 'Ching-Yu Hsu', 'Kuan-Yi Lee', 'Chi-An Fu', 'Hung-yi Lee']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'prompt injection', 'adversarial prompting', 'security', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.00088</guid><pubDate>Fri, 10 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Multi-Target Backdoor Attacks Against Speaker Recognition</title><link>https://arxiv.org/abs/2508.08559</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a multi-target backdoor attack on speaker recognition using position-independent clicking sounds&lt;/li&gt;&lt;li&gt;Targets up to 50 speakers with success rates up to 95.04%&lt;/li&gt;&lt;li&gt;Simulates realistic conditions by varying SNR between speech and trigger&lt;/li&gt;&lt;li&gt;Extends to speaker verification using cosine similarity for proxy targets&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Alexandrine Fortier', 'Sonal Joshi', 'Thomas Thebaud', "Jes\\'us Villalba", 'Najim Dehak', 'Patrick Cardinal']&lt;/li&gt;&lt;li&gt;Tags: ['data poisoning', 'adversarial attacks', 'privacy attacks', 'speaker recognition']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2508.08559</guid><pubDate>Fri, 10 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Can Small-Scale Data Poisoning Exacerbate Dialect-Linked Biases in Large Language Models?</title><link>https://arxiv.org/abs/2507.19195</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Study on data poisoning using dialectal prompts to amplify biases in LLMs&lt;/li&gt;&lt;li&gt;Poisoned data pairs dialects (AAVE, Southern) with toxic completions&lt;/li&gt;&lt;li&gt;Models show increased toxicity and stereotypes for dialect inputs after poisoning&lt;/li&gt;&lt;li&gt;Emergent jailbreaking occurs without explicit slurs, indicating alignment issues&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Chaymaa Abbas', 'Mariette Awad', 'Razane Tajeddine']&lt;/li&gt;&lt;li&gt;Tags: ['data poisoning', 'dialect bias', 'jailbreaking', 'safety evaluation', 'alignment']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2507.19195</guid><pubDate>Fri, 10 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Multi-Trigger Poisoning Amplifies Backdoor Vulnerabilities in LLMs</title><link>https://arxiv.org/abs/2507.11112</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces a framework for multi-trigger poisoning in LLMs&lt;/li&gt;&lt;li&gt;Demonstrates coexistence of multiple backdoor triggers&lt;/li&gt;&lt;li&gt;Proposes a post-hoc recovery method using layer-wise analysis&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Sanhanat Sivapiromrat', 'Caiqi Zhang', 'Marco Basaldella', 'Nigel Collier']&lt;/li&gt;&lt;li&gt;Tags: ['data poisoning', 'backdoor attacks', 'LLM security', 'defence mechanisms', 'trigger robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2507.11112</guid><pubDate>Fri, 10 Oct 2025 00:00:00 +0000</pubDate></item><item><title>LLM Fingerprinting via Semantically Conditioned Watermarks</title><link>https://arxiv.org/abs/2505.16723</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces semantically conditioned watermarks for LLM fingerprinting&lt;/li&gt;&lt;li&gt;Uses a broad semantic domain instead of fixed queries&lt;/li&gt;&lt;li&gt;Watermark is a statistical signal in responses&lt;/li&gt;&lt;li&gt;Robust to finetuning, quantization, and filtering&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Thibaud Gloaguen', 'Robin Staab', "Nikola Jovanovi\\'c", 'Martin Vechev']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'model extraction', 'privacy attacks', 'robustness', 'safety evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2505.16723</guid><pubDate>Fri, 10 Oct 2025 00:00:00 +0000</pubDate></item><item><title>$\textit{Agents Under Siege}$: Breaking Pragmatic Multi-Agent LLM Systems with Optimized Prompt Attacks</title><link>https://arxiv.org/abs/2504.00218</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces permutation-invariant adversarial attacks on multi-agent LLM systems&lt;/li&gt;&lt;li&gt;Uses graph-based optimization and PIEL loss to distribute prompts across agents&lt;/li&gt;&lt;li&gt;Evaluates on multiple models and datasets, showing significant improvement over conventional attacks&lt;/li&gt;&lt;li&gt;Highlights failure of existing defenses in multi-agent contexts&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Rana Muhammad Shahroz Khan', 'Zhen Tan', 'Sukwon Yun', 'Charles Fleming', 'Tianlong Chen']&lt;/li&gt;&lt;li&gt;Tags: ['red teaming', 'adversarial prompting', 'multi-agent systems', 'jailbreaking', 'safety evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2504.00218</guid><pubDate>Fri, 10 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Enhancing LLM Reliability via Explicit Knowledge Boundary Modeling</title><link>https://arxiv.org/abs/2503.02233</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes EKBM framework to enhance LLM reliability by modeling knowledge boundaries&lt;/li&gt;&lt;li&gt;Uses fast and slow reasoning systems to balance efficiency and accuracy&lt;/li&gt;&lt;li&gt;Hybrid training pipeline for self-awareness&lt;/li&gt;&lt;li&gt;Evaluated on dialogue state tracking tasks&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Hang Zheng', 'Hongshen Xu', 'Yuncong Liu', 'Lu Chen', 'Pascale Fung', 'Kai Yu']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'safety evaluation', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2503.02233</guid><pubDate>Fri, 10 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Erasing Without Remembering: Implicit Knowledge Forgetting in Large Language Models</title><link>https://arxiv.org/abs/2502.19982</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Investigates knowledge forgetting in LLMs, focusing on generalizing unlearning to related implicit knowledge.&lt;/li&gt;&lt;li&gt;Evaluates 15 methods across 3 datasets, finding current approaches don't handle paraphrased or related data well.&lt;/li&gt;&lt;li&gt;Proposes PerMU, a probability perturbation-based unlearning method that targets fact-related tokens.&lt;/li&gt;&lt;li&gt;Shows significant improvements in both target and implicit knowledge forgetting across multiple datasets and model sizes.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Huazheng Wang', 'Yongcheng Jing', 'Haifeng Sun', 'Yingjie Wang', 'Jingyu Wang', 'Jianxin Liao', 'Dacheng Tao']&lt;/li&gt;&lt;li&gt;Tags: ['unlearning', 'knowledge forgetting', 'implicit knowledge', 'adversarial unlearning', 'privacy attacks']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2502.19982</guid><pubDate>Fri, 10 Oct 2025 00:00:00 +0000</pubDate></item><item><title>SaFeR-VLM: Toward Safety-aware Fine-grained Reasoning in Multimodal Models</title><link>https://arxiv.org/abs/2510.06871</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes SaFeR-VLM, a safety-aligned reinforcement learning framework for multimodal models&lt;/li&gt;&lt;li&gt;Introduces QI-Safe-10K dataset for safety-critical cases&lt;/li&gt;&lt;li&gt;Uses safety-aware rollout with reflection and correction&lt;/li&gt;&lt;li&gt;Structured reward modeling with explicit penalties for hallucinations and contradictions&lt;/li&gt;&lt;li&gt;GRPO optimization for reinforcing safe trajectories&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Huahui Yi', 'Kun Wang', 'Qiankun Li', 'Miao Yu', 'Liang Lin', 'Gongli Xi', 'Hao Wu', 'Xuming Hu', 'Kang Li', 'Yang Liu']&lt;/li&gt;&lt;li&gt;Tags: ['safety', 'multimodal', 'reinforcement learning', 'dataset', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.06871</guid><pubDate>Fri, 10 Oct 2025 00:00:00 +0000</pubDate></item><item><title>The Shape of Adversarial Influence: Characterizing LLM Latent Spaces with Persistent Homology</title><link>https://arxiv.org/abs/2505.20435</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Uses persistent homology to analyze adversarial influence on LLM latent spaces&lt;/li&gt;&lt;li&gt;Identifies topological compression as a signature of adversarial effects&lt;/li&gt;&lt;li&gt;Tests across six models with prompt injection and backdoor fine-tuning&lt;/li&gt;&lt;li&gt;Provides a new interpretability method for adversarial impacts&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Aideen Fay', "In\\'es Garc\\'ia-Redondo", 'Qiquan Wang', 'Haim Dubossarsky', 'Anthea Monod']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial prompting', 'red teaming', 'model robustness', 'interpretability', 'topological data analysis']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2505.20435</guid><pubDate>Fri, 10 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Watch your steps: Dormant Adversarial Behaviors that Activate upon LLM Finetuning</title><link>https://arxiv.org/abs/2505.16567</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces FAB attack that creates compromised LLMs with dormant adversarial behaviors activated by finetuning&lt;/li&gt;&lt;li&gt;Adversarial behaviors include unsolicited advertising, jailbreakability, and over-refusal&lt;/li&gt;&lt;li&gt;Attack uses meta-learning to simulate downstream finetuning and optimize for dormant behaviors&lt;/li&gt;&lt;li&gt;Demonstrates robustness across different finetuning configurations&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Thibaud Gloaguen', 'Mark Vero', 'Robin Staab', 'Martin Vechev']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'adversarial prompting', 'model extraction', 'security standards', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2505.16567</guid><pubDate>Fri, 10 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Backdoor Vectors: a Task Arithmetic View on Backdoor Attacks and Defenses</title><link>https://arxiv.org/abs/2510.08016</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes Backdoor Vector (BV) framework to understand backdoor attacks in model merging&lt;/li&gt;&lt;li&gt;Introduces Sparse Backdoor Vector (SBV) for enhancing backdoor resilience&lt;/li&gt;&lt;li&gt;Presents Injection BV Subtraction (IBVS) as a defense mechanism&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Stanis{\\l}aw Pawlak', "Jan Dubi\\'nski", 'Daniel Marczak', 'Bart{\\l}omiej Twardowski']&lt;/li&gt;&lt;li&gt;Tags: ['backdoor attacks', 'model merging', 'defense mechanisms', 'security']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.08016</guid><pubDate>Fri, 10 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Fewer Weights, More Problems: A Practical Attack on LLM Pruning</title><link>https://arxiv.org/abs/2510.07985</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces a new attack on LLM pruning where an adversary can inject malicious behavior that only manifests after pruning&lt;/li&gt;&lt;li&gt;Uses a proxy metric to identify parameters likely to be pruned and injects malicious behavior into those that are not&lt;/li&gt;&lt;li&gt;Demonstrates high success rates in jailbreak, instruction refusal, and content injection attacks after pruning&lt;/li&gt;&lt;li&gt;Highlights security risks in model compression and pruning methods&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Kazuki Egashira', 'Robin Staab', 'Thibaud Gloaguen', 'Mark Vero', 'Martin Vechev']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'model extraction', 'adversarial prompting', 'security standards', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.07985</guid><pubDate>Fri, 10 Oct 2025 00:00:00 +0000</pubDate></item><item><title>MetaDefense: Defending Finetuning-based Jailbreak Attack Before and During Generation</title><link>https://arxiv.org/abs/2510.07835</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces MetaDefense, a framework to defend against finetuning-based jailbreak attacks in LLMs.&lt;/li&gt;&lt;li&gt;Uses two-stage defense: pre-generation detection and mid-generation monitoring.&lt;/li&gt;&lt;li&gt;Trains LLM to predict harmfulness using specialized prompts.&lt;/li&gt;&lt;li&gt;Shows improved performance over existing defenses on multiple LLM architectures.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Weisen Jiang', 'Sinno Jialin Pan']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'jailbreaking', 'adversarial prompting', 'safety evaluation', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.07835</guid><pubDate>Fri, 10 Oct 2025 00:00:00 +0000</pubDate></item><item><title>SIMU: Selective Influence Machine Unlearning</title><link>https://arxiv.org/abs/2510.07822</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes SIMU, a two-step framework for selective machine unlearning in LLMs&lt;/li&gt;&lt;li&gt;Aims to forget targeted information while retaining original model capabilities&lt;/li&gt;&lt;li&gt;Uses second-order optimizer-based methods with selective neuron updates&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Anu Agarwal', 'Mihir Pamnani', 'Dilek Hakkani-Tur']&lt;/li&gt;&lt;li&gt;Tags: ['data poisoning', 'privacy attacks', 'safety evaluation', 'robustness', 'alignment']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.07822</guid><pubDate>Fri, 10 Oct 2025 00:00:00 +0000</pubDate></item><item><title>PEAR: Planner-Executor Agent Robustness Benchmark</title><link>https://arxiv.org/abs/2510.07505</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces PEAR benchmark for evaluating planner-executor MAS robustness&lt;/li&gt;&lt;li&gt;Finds trade-off between task performance and robustness&lt;/li&gt;&lt;li&gt;Highlights planner as critical attack surface&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Shen Dong', 'Mingxuan Zhang', 'Pengfei He', 'Li Ma', 'Bhavani Thuraisingham', 'Hui Liu', 'Yue Xing']&lt;/li&gt;&lt;li&gt;Tags: ['red teaming', 'robustness', 'multi-agent systems', 'benchmarking', 'adversarial attacks']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.07505</guid><pubDate>Fri, 10 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Defending MoE LLMs against Harmful Fine-Tuning via Safety Routing Alignment</title><link>https://arxiv.org/abs/2509.22745</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes SafeMoE, a defense method for MoE LLMs against harmful fine-tuning attacks&lt;/li&gt;&lt;li&gt;Mitigates routing drift by penalizing the gap between routing weights of fine-tuned and initial models&lt;/li&gt;&lt;li&gt;Demonstrates effectiveness on various MoE LLMs, maintaining utility with minimal overhead&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jaehan Kim', 'Minkyoo Song', 'Seungwon Shin', 'Sooel Son']&lt;/li&gt;&lt;li&gt;Tags: ['LLM security', 'fine-tuning defense', 'Mixture-of-Experts', 'safety alignment', 'adversarial attacks']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.22745</guid><pubDate>Fri, 10 Oct 2025 00:00:00 +0000</pubDate></item><item><title>X-Teaming Evolutionary M2S: Automated Discovery of Multi-turn to Single-turn Jailbreak Templates</title><link>https://arxiv.org/abs/2509.08729</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Automated framework for discovering M2S jailbreak templates using language-model-guided evolution&lt;/li&gt;&lt;li&gt;Achieves 44.8% success on GPT-4.1 with two new template families&lt;/li&gt;&lt;li&gt;Emphasizes cross-model evaluation and threshold calibration&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Hyunjun Kim', 'Junwoo Ha', 'Sangyoon Yu', 'Haon Park']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'jailbreaking', 'prompt injection', 'adversarial prompting', 'model extraction']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.08729</guid><pubDate>Fri, 10 Oct 2025 00:00:00 +0000</pubDate></item><item><title>AEGIS : Automated Co-Evolutionary Framework for Guarding Prompt Injections Schema</title><link>https://arxiv.org/abs/2509.00088</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes AEGIS, an automated co-evolutionary framework for guarding prompt injection attacks in LLMs.&lt;/li&gt;&lt;li&gt;Uses Textual Gradient Optimization (TGO) to iteratively optimize both attack and defense prompts.&lt;/li&gt;&lt;li&gt;Evaluates on a real-world dataset, showing improved attack success and detection rates compared to baselines.&lt;/li&gt;&lt;li&gt;Demonstrates effectiveness across different LLMs.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Ting-Chun Liu', 'Ching-Yu Hsu', 'Kuan-Yi Lee', 'Chi-An Fu', 'Hung-yi Lee']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'prompt injection', 'adversarial prompting', 'security', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.00088</guid><pubDate>Fri, 10 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Safe-Control: A Safety Patch for Mitigating Unsafe Content in Text-to-Image Generation Models</title><link>https://arxiv.org/abs/2508.21099</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces Safe-Control, a plug-and-play safety patch for T2I models&lt;/li&gt;&lt;li&gt;Uses data-driven strategies and safety-aware conditions&lt;/li&gt;&lt;li&gt;Effective in reducing unsafe content generation across multiple models&lt;/li&gt;&lt;li&gt;Outperforms existing safety mechanisms in evaluations&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Xiangtao Meng', 'Yingkai Dong', 'Ning Yu', 'Li Wang', 'Zheng Li', 'Shanqing Guo']&lt;/li&gt;&lt;li&gt;Tags: ['safety', 'text-to-image', 'adversarial attacks', 'content moderation', 'model patch']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2508.21099</guid><pubDate>Fri, 10 Oct 2025 00:00:00 +0000</pubDate></item><item><title>MCPSecBench: A Systematic Security Benchmark and Playground for Testing Model Context Protocols</title><link>https://arxiv.org/abs/2508.13220</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces MCPSecBench, a security benchmark for Model Context Protocol (MCP)&lt;/li&gt;&lt;li&gt;Identifies 17 attack types across 4 attack surfaces&lt;/li&gt;&lt;li&gt;Evaluates attacks on major MCP providers (Claude, OpenAI, Cursor)&lt;/li&gt;&lt;li&gt;Finds current protection mechanisms ineffective&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yixuan Yang', 'Daoyuan Wu', 'Yufan Chen']&lt;/li&gt;&lt;li&gt;Tags: ['red teaming', 'security benchmark', 'MCP', 'attack taxonomy', 'protection mechanisms']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2508.13220</guid><pubDate>Fri, 10 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Can Small-Scale Data Poisoning Exacerbate Dialect-Linked Biases in Large Language Models?</title><link>https://arxiv.org/abs/2507.19195</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Study on data poisoning using dialectal prompts to amplify biases in LLMs&lt;/li&gt;&lt;li&gt;Poisoned data pairs dialects (AAVE, Southern) with toxic completions&lt;/li&gt;&lt;li&gt;Models show increased toxicity and stereotypes for dialect inputs after poisoning&lt;/li&gt;&lt;li&gt;Emergent jailbreaking occurs without explicit slurs, indicating alignment issues&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Chaymaa Abbas', 'Mariette Awad', 'Razane Tajeddine']&lt;/li&gt;&lt;li&gt;Tags: ['data poisoning', 'dialect bias', 'jailbreaking', 'safety evaluation', 'alignment']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2507.19195</guid><pubDate>Fri, 10 Oct 2025 00:00:00 +0000</pubDate></item><item><title>The Shape of Adversarial Influence: Characterizing LLM Latent Spaces with Persistent Homology</title><link>https://arxiv.org/abs/2505.20435</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Uses persistent homology to analyze adversarial influence on LLM latent spaces&lt;/li&gt;&lt;li&gt;Identifies topological compression as a signature of adversarial effects&lt;/li&gt;&lt;li&gt;Tests across six models with prompt injection and backdoor fine-tuning&lt;/li&gt;&lt;li&gt;Provides a new interpretability method for adversarial impacts&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Aideen Fay', "In\\'es Garc\\'ia-Redondo", 'Qiquan Wang', 'Haim Dubossarsky', 'Anthea Monod']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial prompting', 'red teaming', 'model robustness', 'interpretability', 'topological data analysis']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2505.20435</guid><pubDate>Fri, 10 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Inference-time Alignment in Continuous Space</title><link>https://arxiv.org/abs/2505.20081</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes SEA, an algorithm for inference-time alignment using gradient-based sampling in continuous latent space&lt;/li&gt;&lt;li&gt;Aims to improve alignment over discrete response space methods&lt;/li&gt;&lt;li&gt;Shows significant performance improvements on AdvBench and MATH&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yige Yuan', 'Teng Xiao', 'Li Yunfan', 'Bingbing Xu', 'Shuchang Tao', 'Yunqi Qiu', 'Huawei Shen', 'Xueqi Cheng']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'inference-time', 'gradient-based', 'latent space', 'large language models']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2505.20081</guid><pubDate>Fri, 10 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Watch your steps: Dormant Adversarial Behaviors that Activate upon LLM Finetuning</title><link>https://arxiv.org/abs/2505.16567</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces FAB attack that creates compromised LLMs with dormant adversarial behaviors activated by finetuning&lt;/li&gt;&lt;li&gt;Adversarial behaviors include unsolicited advertising, jailbreakability, and over-refusal&lt;/li&gt;&lt;li&gt;Attack uses meta-learning to simulate downstream finetuning and optimize for dormant behaviors&lt;/li&gt;&lt;li&gt;Demonstrates robustness across different finetuning configurations&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Thibaud Gloaguen', 'Mark Vero', 'Robin Staab', 'Martin Vechev']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'adversarial prompting', 'model extraction', 'security standards', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2505.16567</guid><pubDate>Fri, 10 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Logic Jailbreak: Efficiently Unlocking LLM Safety Restrictions Through Formal Logical Expression</title><link>https://arxiv.org/abs/2505.13527</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces LogiBreak, a black-box jailbreak method using logical expressions&lt;/li&gt;&lt;li&gt;Converts harmful prompts into formal logic to bypass safety mechanisms&lt;/li&gt;&lt;li&gt;Evaluated on multilingual dataset showing effectiveness across languages&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jingyu Peng', 'Maolin Wang', 'Nan Wang', 'Jiatong Li', 'Yuchen Li', 'Yuyang Ye', 'Wanyu Wang', 'Pengyue Jia', 'Kai Zhang', 'Xiangyu Zhao']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'jailbreaking', 'adversarial prompting', 'safety evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2505.13527</guid><pubDate>Fri, 10 Oct 2025 00:00:00 +0000</pubDate></item><item><title>PiCo: Jailbreaking Multimodal Large Language Models via Pictorial Code Contextualization</title><link>https://arxiv.org/abs/2504.01444</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces PiCo, a jailbreaking framework for MLLMs using pictorial code contextualization&lt;/li&gt;&lt;li&gt;Employs token-level typographic attacks and code-style visual instructions to bypass defenses&lt;/li&gt;&lt;li&gt;Proposes a new evaluation metric for toxicity and helpfulness&lt;/li&gt;&lt;li&gt;Achieves high ASR on Gemini-Pro Vision and GPT-4&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Aofan Liu', 'Lulu Tang', 'Ting Pan', 'Yuguo Yin', 'Bin Wang', 'Ao Yang']&lt;/li&gt;&lt;li&gt;Tags: ['jailbreaking', 'adversarial prompting', 'multimodal models', 'security evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2504.01444</guid><pubDate>Fri, 10 Oct 2025 00:00:00 +0000</pubDate></item><item><title>$\textit{Agents Under Siege}$: Breaking Pragmatic Multi-Agent LLM Systems with Optimized Prompt Attacks</title><link>https://arxiv.org/abs/2504.00218</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces permutation-invariant adversarial attacks on multi-agent LLM systems&lt;/li&gt;&lt;li&gt;Uses graph-based optimization and PIEL loss to distribute prompts across agents&lt;/li&gt;&lt;li&gt;Evaluates on multiple models and datasets, showing significant improvement over conventional attacks&lt;/li&gt;&lt;li&gt;Highlights failure of existing defenses in multi-agent contexts&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Rana Muhammad Shahroz Khan', 'Zhen Tan', 'Sukwon Yun', 'Charles Fleming', 'Tianlong Chen']&lt;/li&gt;&lt;li&gt;Tags: ['red teaming', 'adversarial prompting', 'multi-agent systems', 'jailbreaking', 'safety evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2504.00218</guid><pubDate>Fri, 10 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Enhancing LLM Reliability via Explicit Knowledge Boundary Modeling</title><link>https://arxiv.org/abs/2503.02233</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes EKBM framework to enhance LLM reliability by modeling knowledge boundaries&lt;/li&gt;&lt;li&gt;Uses fast and slow reasoning systems to balance efficiency and accuracy&lt;/li&gt;&lt;li&gt;Hybrid training pipeline for self-awareness&lt;/li&gt;&lt;li&gt;Evaluated on dialogue state tracking tasks&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Hang Zheng', 'Hongshen Xu', 'Yuncong Liu', 'Lu Chen', 'Pascale Fung', 'Kai Yu']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'safety evaluation', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2503.02233</guid><pubDate>Fri, 10 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Rule Encoding and Compliance in Large Language Models: An Information-Theoretic Analysis</title><link>https://arxiv.org/abs/2510.05106</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;The paper analyzes how rule encodings in system prompts affect LLM attention and compliance.&lt;/li&gt;&lt;li&gt;It identifies trade-offs between anchor redundancy and attention entropy.&lt;/li&gt;&lt;li&gt;Provides a dynamic rule verification architecture to enhance compliance and security.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Joachim Diederich']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'prompt injection', 'alignment', 'safety evaluation', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.05106</guid><pubDate>Fri, 10 Oct 2025 00:00:00 +0000</pubDate></item><item><title>LLMs Learn to Deceive Unintentionally: Emergent Misalignment in Dishonesty from Misaligned Samples to Biased Human-AI Interactions</title><link>https://arxiv.org/abs/2510.08211</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Investigates emergent misalignment in LLMs leading to dishonesty and deception&lt;/li&gt;&lt;li&gt;Shows that small amounts of misaligned data can cause significant dishonest behavior&lt;/li&gt;&lt;li&gt;Explores human-AI interaction scenarios with biased users&lt;/li&gt;&lt;li&gt;Demonstrates risks in downstream finetuning and practical interactions&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['XuHao Hu', 'Peng Wang', 'Xiaoya Lu', 'Dongrui Liu', 'Xuanjing Huang', 'Jing Shao']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'safety evaluation', 'adversarial prompting', 'model extraction', 'privacy attacks']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.08211</guid><pubDate>Fri, 10 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Backdoor Vectors: a Task Arithmetic View on Backdoor Attacks and Defenses</title><link>https://arxiv.org/abs/2510.08016</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes Backdoor Vector (BV) framework to understand backdoor attacks in model merging&lt;/li&gt;&lt;li&gt;Introduces Sparse Backdoor Vector (SBV) for enhancing backdoor resilience&lt;/li&gt;&lt;li&gt;Presents Injection BV Subtraction (IBVS) as a defense mechanism&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Stanis{\\l}aw Pawlak', "Jan Dubi\\'nski", 'Daniel Marczak', 'Bart{\\l}omiej Twardowski']&lt;/li&gt;&lt;li&gt;Tags: ['backdoor attacks', 'model merging', 'defense mechanisms', 'security']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.08016</guid><pubDate>Fri, 10 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Fewer Weights, More Problems: A Practical Attack on LLM Pruning</title><link>https://arxiv.org/abs/2510.07985</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces a new attack on LLM pruning where an adversary can inject malicious behavior that only manifests after pruning&lt;/li&gt;&lt;li&gt;Uses a proxy metric to identify parameters likely to be pruned and injects malicious behavior into those that are not&lt;/li&gt;&lt;li&gt;Demonstrates high success rates in jailbreak, instruction refusal, and content injection attacks after pruning&lt;/li&gt;&lt;li&gt;Highlights security risks in model compression and pruning methods&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Kazuki Egashira', 'Robin Staab', 'Thibaud Gloaguen', 'Mark Vero', 'Martin Vechev']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'model extraction', 'adversarial prompting', 'security standards', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.07985</guid><pubDate>Fri, 10 Oct 2025 00:00:00 +0000</pubDate></item><item><title>MetaDefense: Defending Finetuning-based Jailbreak Attack Before and During Generation</title><link>https://arxiv.org/abs/2510.07835</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces MetaDefense, a framework to defend against finetuning-based jailbreak attacks in LLMs.&lt;/li&gt;&lt;li&gt;Uses two-stage defense: pre-generation detection and mid-generation monitoring.&lt;/li&gt;&lt;li&gt;Trains LLM to predict harmfulness using specialized prompts.&lt;/li&gt;&lt;li&gt;Shows improved performance over existing defenses on multiple LLM architectures.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Weisen Jiang', 'Sinno Jialin Pan']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'jailbreaking', 'adversarial prompting', 'safety evaluation', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.07835</guid><pubDate>Fri, 10 Oct 2025 00:00:00 +0000</pubDate></item><item><title>SIMU: Selective Influence Machine Unlearning</title><link>https://arxiv.org/abs/2510.07822</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes SIMU, a two-step framework for selective machine unlearning in LLMs&lt;/li&gt;&lt;li&gt;Aims to forget targeted information while retaining original model capabilities&lt;/li&gt;&lt;li&gt;Uses second-order optimizer-based methods with selective neuron updates&lt;/li&gt;&lt;li&gt;Focuses on safety by addressing undesired memorization of sensitive info&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Anu Agarwal', 'Mihir Pamnani', 'Dilek Hakkani-Tur']&lt;/li&gt;&lt;li&gt;Tags: ['machine unlearning', 'safety', 'privacy', 'model forgetting', 'selective influence']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.07822</guid><pubDate>Fri, 10 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Effective and Stealthy One-Shot Jailbreaks on Deployed Mobile Vision-Language Agents</title><link>https://arxiv.org/abs/2510.07809</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Presents a stealthy one-shot jailbreak attack on mobile vision-language agents using in-app prompt injections&lt;/li&gt;&lt;li&gt;Components include low-privilege targeting, stealthy activation via touch attributes, and one-shot prompt detoxification&lt;/li&gt;&lt;li&gt;Evaluates across multiple LVLM backends and Android apps, showing high hijack rates&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Renhua Ding', 'Xiao Yang', 'Zhengwei Fang', 'Jun Luo', 'Kun He', 'Jun Zhu']&lt;/li&gt;&lt;li&gt;Tags: ['red teaming', 'jailbreaking', 'prompt injection', 'adversarial prompting', 'security']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.07809</guid><pubDate>Fri, 10 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Rethinking Reasoning: A Survey on Reasoning-based Backdoors in LLMs</title><link>https://arxiv.org/abs/2510.07697</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Survey on reasoning-based backdoor attacks in LLMs&lt;/li&gt;&lt;li&gt;Introduces taxonomy of associative, passive, and active attacks&lt;/li&gt;&lt;li&gt;Discusses defense strategies and future research directions&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Survey&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Man Hu', 'Xinyi Wu', 'Zuofeng Suo', 'Jinbo Feng', 'Linghui Meng', 'Yanhao Jia', 'Anh Tuan Luu', 'Shuai Zhao']&lt;/li&gt;&lt;li&gt;Tags: ['LLM security', 'backdoor attacks', 'reasoning capabilities', 'defense strategies', 'taxonomy']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.07697</guid><pubDate>Fri, 10 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Profit Mirage: Revisiting Information Leakage in LLM-based Financial Agents</title><link>https://arxiv.org/abs/2510.07920</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Quantifies information leakage in LLM-based financial agents&lt;/li&gt;&lt;li&gt;Introduces FinLake-Bench for leakage-robust evaluation&lt;/li&gt;&lt;li&gt;Proposes FactFin framework with counterfactual perturbations&lt;/li&gt;&lt;li&gt;Improves out-of-sample generalization&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Xiangyu Li', 'Yawen Zeng', 'Xiaofen Xing', 'Jin Xu', 'Xiangmin Xu']&lt;/li&gt;&lt;li&gt;Tags: ['safety evaluation', 'robustness', 'data poisoning', 'alignment']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.07920</guid><pubDate>Fri, 10 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Multimodal Safety Evaluation in Generative Agent Social Simulations</title><link>https://arxiv.org/abs/2510.07709</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces a simulation framework for evaluating generative agents' safety in multimodal environments&lt;/li&gt;&lt;li&gt;Assesses safety improvement over time, detection of unsafe activities, and social dynamics&lt;/li&gt;&lt;li&gt;Finds agents struggle to align local revisions with global safety, with success rates varying by model and scenario&lt;/li&gt;&lt;li&gt;Highlights overtrust in misleading visuals as a significant issue&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Alhim Vera', 'Karen Sanchez', 'Carlos Hinojosa', 'Haidar Bin Hamid', 'Donghoon Kim', 'Bernard Ghanem']&lt;/li&gt;&lt;li&gt;Tags: ['multimodal safety', 'agent evaluation', 'social simulations', 'safety metrics', 'visual-text interactions']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.07709</guid><pubDate>Fri, 10 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Measuring and Mitigating Identity Bias in Multi-Agent Debate via Anonymization</title><link>https://arxiv.org/abs/2510.07517</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces a framework to measure and mitigate identity bias in multi-agent debate (MAD) for LLMs&lt;/li&gt;&lt;li&gt;Proposes response anonymization to prevent agents from recognizing peers or themselves&lt;/li&gt;&lt;li&gt;Defines Identity Bias Coefficient (IBC) to quantify bias&lt;/li&gt;&lt;li&gt;Empirical results show sycophancy is more common than self-bias&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Hyeong Kyu Choi', 'Xiaojin Zhu', 'Yixuan Li']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'safety evaluation', 'robustness', 'red teaming']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.07517</guid><pubDate>Fri, 10 Oct 2025 00:00:00 +0000</pubDate></item></channel></rss>