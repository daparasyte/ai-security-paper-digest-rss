<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>AI Security Paper Digest</title><link>https://kentaroh-toyoda.github.io/ai-security-paper-digest-rss/rss.xml</link><description>Curated papers on AI security from ArXiv and ACL</description><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><lastBuildDate>Fri, 03 Oct 2025 22:22:54 +0000</lastBuildDate><item><title>LLaVAShield: Safeguarding Multimodal Multi-Turn Dialogues in Vision-Language Models</title><link>https://arxiv.org/abs/2509.25896</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces LLaVAShield for safeguarding multimodal multi-turn dialogues in VLMs&lt;/li&gt;&lt;li&gt;Presents MMDS dataset with safety ratings and policy labels&lt;/li&gt;&lt;li&gt;Develops MCTS-based red-teaming framework to generate unsafe dialogues&lt;/li&gt;&lt;li&gt;Outperforms baselines in MMT content moderation tasks&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Guolei Huang', 'Qinzhi Peng', 'Gan Xu', 'Yuxuan Lu', 'Yongjun Shen']&lt;/li&gt;&lt;li&gt;Tags: ['red teaming', 'multimodal', 'multi-turn dialogues', 'safety evaluation', 'dataset']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.25896</guid><pubDate>Fri, 03 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Defend LLMs Through Self-Consciousness</title><link>https://arxiv.org/abs/2508.02961</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces a self-consciousness defense mechanism for LLMs against prompt injection attacks.&lt;/li&gt;&lt;li&gt;Uses Meta-Cognitive and Arbitration Modules for self-evaluation and output regulation.&lt;/li&gt;&lt;li&gt;Evaluated on seven LLMs with two datasets, showing significant defense improvements.&lt;/li&gt;&lt;li&gt;Analyzes trade-off between defense success and computational overhead.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Boshi Huang', 'Fabio Nonato de Paula']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'prompt injection', 'adversarial prompting', 'safety evaluation', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2508.02961</guid><pubDate>Fri, 03 Oct 2025 00:00:00 +0000</pubDate></item><item><title>PurpCode: Reasoning for Safer Code Generation</title><link>https://arxiv.org/abs/2507.19060</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces PurpCode, a post-training recipe for safe code generation&lt;/li&gt;&lt;li&gt;Two-stage training: Rule Learning and Reinforcement Learning&lt;/li&gt;&lt;li&gt;Uses red-teaming to create prompts for unsafe cyberactivities&lt;/li&gt;&lt;li&gt;Develops PurpCode-32B model with state-of-the-art cybersafety&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jiawei Liu', 'Nirav Diwan', 'Zhe Wang', 'Haoyu Zhai', 'Xiaona Zhou', 'Kiet A. Nguyen', 'Tianjiao Yu', 'Muntasir Wahed', 'Yinlin Deng', 'Hadjer Benkraouda', 'Yuxiang Wei', 'Lingming Zhang', 'Ismini Lourentzou', 'Gang Wang']&lt;/li&gt;&lt;li&gt;Tags: ['red teaming', 'code generation', 'security', 'alignment', 'reinforcement learning']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2507.19060</guid><pubDate>Fri, 03 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Superficial Safety Alignment Hypothesis</title><link>https://arxiv.org/abs/2410.10862</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes the Superficial Safety Alignment Hypothesis (SSAH) for LLMs&lt;/li&gt;&lt;li&gt;Identifies four types of critical components (SCU, UCU, CU, RU) for safety alignment&lt;/li&gt;&lt;li&gt;Shows freezing safety-critical components during fine-tuning retains safety&lt;/li&gt;&lt;li&gt;Uses redundant units as 'alignment budget' to minimize alignment tax&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jianwei Li', 'Jung-Eun Kim']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'safety', 'LLM', 'fine-tuning', 'model components']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2410.10862</guid><pubDate>Fri, 03 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Tree-based Dialogue Reinforced Policy Optimization for Red-Teaming Attacks</title><link>https://arxiv.org/abs/2510.02286</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes DialTree-RPO, a reinforcement learning framework with tree search for multi-turn attack discovery&lt;/li&gt;&lt;li&gt;Focuses on automated red-teaming for LLMs in multi-turn interactions&lt;/li&gt;&lt;li&gt;Achieves higher ASR than previous methods and uncovers new attack strategies&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Ruohao Guo', 'Afshin Oroojlooy', 'Roshan Sridhar', 'Miguel Ballesteros', 'Alan Ritter', 'Dan Roth']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'multi-turn attacks', 'reinforcement learning', 'tree search', 'adversarial attacks']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.02286</guid><pubDate>Fri, 03 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Just Do It!? Computer-Use Agents Exhibit Blind Goal-Directedness</title><link>https://arxiv.org/abs/2510.01670</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces Blind Goal-Directedness (BGD) in Computer-Use Agents (CUAs)&lt;/li&gt;&lt;li&gt;Develops BLIND-ACT benchmark with 90 tasks to evaluate BGD&lt;/li&gt;&lt;li&gt;Evaluates multiple models showing high BGD rates&lt;/li&gt;&lt;li&gt;Highlights need for training/inference interventions&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Erfan Shayegani', 'Keegan Hines', 'Yue Dong', 'Nael Abu-Ghazaleh', 'Roman Lutz', 'Spencer Whitehead', 'Vidhisha Balachandran', 'Besmira Nushi', 'Vibhav Vineet']&lt;/li&gt;&lt;li&gt;Tags: ['safety evaluation', 'alignment', 'red teaming', 'robustness', 'LLM red teaming']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.01670</guid><pubDate>Fri, 03 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Position: Privacy Is Not Just Memorization!</title><link>https://arxiv.org/abs/2510.01645</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;This position paper expands the privacy discourse beyond memorization in LLMs, covering data collection, inference-time leakage, and surveillance risks.&lt;/li&gt;&lt;li&gt;It includes a taxonomy of privacy risks across the LLM lifecycle and a longitudinal analysis of research focus.&lt;/li&gt;&lt;li&gt;The paper calls for interdisciplinary approaches to address these broader privacy threats.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Position&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Niloofar Mireshghallah', 'Tianshi Li']&lt;/li&gt;&lt;li&gt;Tags: ['privacy', 'data collection', 'inference-time leakage', 'surveillance', 'taxonomy']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.01645</guid><pubDate>Fri, 03 Oct 2025 00:00:00 +0000</pubDate></item><item><title>InvThink: Towards AI Safety via Inverse Reasoning</title><link>https://arxiv.org/abs/2510.01569</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces InvThink, a method for improving AI safety by instructing LLMs to reason through failure modes before generating responses.&lt;/li&gt;&lt;li&gt;Key steps include enumerating potential harms, analyzing consequences, and generating safe outputs.&lt;/li&gt;&lt;li&gt;Shows improved safety scaling with model size and reduced safety tax compared to existing methods.&lt;/li&gt;&lt;li&gt;Excels in high-stakes domains like medicine, finance, law, and agentic risks.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yubin Kim', 'Taehan Kim', 'Eugene Park', 'Chunjong Park', 'Cynthia Breazeal', 'Daniel McDuff', 'Hae Won Park']&lt;/li&gt;&lt;li&gt;Tags: ['AI safety', 'alignment', 'inverse reasoning', 'safety evaluation', 'LLM safety']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.01569</guid><pubDate>Fri, 03 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Is It Thinking or Cheating? Detecting Implicit Reward Hacking by Measuring Reasoning Effort</title><link>https://arxiv.org/abs/2510.01367</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces TRACE (Truncated Reasoning AUC Evaluation) to detect implicit reward hacking in reasoning models.&lt;/li&gt;&lt;li&gt;Measures reasoning effort by truncating CoT and evaluating verifier-passing rates.&lt;/li&gt;&lt;li&gt;Shows significant improvements over existing CoT monitors in math and coding tasks.&lt;/li&gt;&lt;li&gt;Can discover unknown loopholes during training.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Xinpeng Wang', 'Nitish Joshi', 'Barbara Plank', 'Rico Angell', 'He He']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'safety evaluation', 'red teaming', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.01367</guid><pubDate>Fri, 03 Oct 2025 00:00:00 +0000</pubDate></item><item><title>WAInjectBench: Benchmarking Prompt Injection Detections for Web Agents</title><link>https://arxiv.org/abs/2510.01354</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces WAInjectBench, a benchmark for detecting prompt injection attacks on web agents&lt;/li&gt;&lt;li&gt;Categorizes attacks based on threat models and creates datasets of malicious and benign samples&lt;/li&gt;&lt;li&gt;Evaluates text-based and image-based detection methods across multiple scenarios&lt;/li&gt;&lt;li&gt;Finds that detectors struggle with attacks using imperceptible perturbations or no explicit instructions&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yinuo Liu', 'Ruohan Xu', 'Xilong Wang', 'Yuqi Jia', 'Neil Zhenqiang Gong']&lt;/li&gt;&lt;li&gt;Tags: ['prompt injection', 'red teaming', 'benchmarking', 'detection methods', 'web agents']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.01354</guid><pubDate>Fri, 03 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Jailbreaking LLMs via Semantically Relevant Nested Scenarios with Targeted Toxic Knowledge</title><link>https://arxiv.org/abs/2510.01223</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes RTS-Attack, a framework for jailbreaking LLMs using semantically relevant nested scenarios with targeted toxic knowledge.&lt;/li&gt;&lt;li&gt;Demonstrates that LLM alignment defenses are not sensitive to these nested scenarios.&lt;/li&gt;&lt;li&gt;Generates jailbreak prompts that are concealed and free from harmful queries.&lt;/li&gt;&lt;li&gt;Outperforms baselines across multiple LLMs including GPT-4o, Llama3-70b, and Gemini-pro.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Hui Dou', 'Ning Xu', 'Yiwen Zhang', 'Kaibin Wang']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'jailbreaking', 'adversarial prompting', 'alignment', 'safety evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.01223</guid><pubDate>Fri, 03 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Inverse Language Modeling towards Robust and Grounded LLMs</title><link>https://arxiv.org/abs/2510.01929</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes Inverse Language Modeling (ILM) to improve LLM robustness and enable native grounding&lt;/li&gt;&lt;li&gt;Aims to make LLMs more analyzable and controllable&lt;/li&gt;&lt;li&gt;Potential applications in red teaming and security&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Davide Gabrielli', 'Simone Sestito', 'Iacopo Masi']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'robustness', 'safety evaluation', 'adversarial prompting']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.01929</guid><pubDate>Fri, 03 Oct 2025 00:00:00 +0000</pubDate></item><item><title>SoK: Measuring What Matters for Closed-Loop Security Agents</title><link>https://arxiv.org/abs/2510.01654</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces CLASP framework for evaluating closed-loop security agents&lt;/li&gt;&lt;li&gt;Maps security lifecycle stages to agentic capabilities&lt;/li&gt;&lt;li&gt;Defines CLC Score to measure loop closure and effectiveness&lt;/li&gt;&lt;li&gt;Analyzes 21 works to identify capability gaps&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Mudita Khurana', 'Raunak Jain']&lt;/li&gt;&lt;li&gt;Tags: ['security', 'evaluation', 'framework', 'agents', 'benchmark']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.01654</guid><pubDate>Fri, 03 Oct 2025 00:00:00 +0000</pubDate></item><item><title>NLP Methods for Detecting Novel LLM Jailbreaks and Keyword Analysis with BERT</title><link>https://arxiv.org/abs/2510.01644</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;The paper focuses on detecting jailbreak prompts in LLMs using NLP methods.&lt;/li&gt;&lt;li&gt;It evaluates different ML models, with BERT showing the best performance.&lt;/li&gt;&lt;li&gt;Analyzes keywords distinguishing jailbreaks from genuine prompts.&lt;/li&gt;&lt;li&gt;Identifies explicit reflexivity in prompts as a potential jailbreak signal.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['John Hawkins', 'Aditya Pramar', 'Rodney Beard', 'Rohitash Chandra']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'jailbreaking', 'prompt injection', 'adversarial prompting', 'safety evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.01644</guid><pubDate>Fri, 03 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Think Twice, Generate Once: Safeguarding by Progressive Self-Reflection</title><link>https://arxiv.org/abs/2510.01270</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces Progressive Self-Reflection (PSR) for LLMs to self-monitor and correct outputs during inference.&lt;/li&gt;&lt;li&gt;Significantly reduces attack success rates (e.g., from 77.5% to 5.9% on Llama-3.1-8B-Instruct) without retraining.&lt;/li&gt;&lt;li&gt;Uses a self-reflection predictor to adaptively determine the number of reflection rounds based on input complexity.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Hoang Phan', 'Victor Li', 'Qi Lei']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'safety evaluation', 'adversarial prompting', 'robustness', 'alignment']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.01270</guid><pubDate>Fri, 03 Oct 2025 00:00:00 +0000</pubDate></item><item><title>OpenAI's GPT-OSS-20B Model and Safety Alignment Issues in a Low-Resource Language</title><link>https://arxiv.org/abs/2510.01266</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Red teaming of OpenAI's GPT-OSS-20B model in a low-resource language (Hausa)&lt;/li&gt;&lt;li&gt;Uncovered biases, inaccuracies, and cultural insensitivities&lt;/li&gt;&lt;li&gt;Model generates harmful content when prompted with polite language&lt;/li&gt;&lt;li&gt;Safety protocols relax leading to misinformation and hate speech&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Isa Inuwa-Dutse']&lt;/li&gt;&lt;li&gt;Tags: ['red teaming', 'jailbreaking', 'prompt injection', 'adversarial prompting', 'model extraction', 'data poisoning', 'privacy attacks', 'alignment', 'robustness', 'safety evaluation', 'security standards']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.01266</guid><pubDate>Fri, 03 Oct 2025 00:00:00 +0000</pubDate></item><item><title>In AI Sweet Harmony: Sociopragmatic Guardrail Bypasses and Evaluation-Awareness in OpenAI gpt-oss-20b</title><link>https://arxiv.org/abs/2510.01259</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;The paper studies sociopragmatic framing effects on refusal behavior in OpenAI's gpt-oss-20b model.&lt;/li&gt;&lt;li&gt;Tests various harm domains like ZIP-bomb construction, synthetic card numbers, unsafe advice, and context exfiltration.&lt;/li&gt;&lt;li&gt;Introduces an AI-assisted hardening method to reduce leakage.&lt;/li&gt;&lt;li&gt;Evaluates model's awareness of evaluation prompts and Moderation API's performance.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Nils Durner']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'jailbreaking', 'adversarial prompting', 'safety evaluation', 'alignment']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.01259</guid><pubDate>Fri, 03 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Detoxifying Large Language Models via Autoregressive Reward Guided Representation Editing</title><link>https://arxiv.org/abs/2510.01243</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes ARGRE, a test-time detoxification framework for LLMs&lt;/li&gt;&lt;li&gt;Uses autoregressive reward model to guide representation editing&lt;/li&gt;&lt;li&gt;Aims to reduce toxicity while preserving model capabilities&lt;/li&gt;&lt;li&gt;Shows significant improvements over baselines&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yisong Xiao', 'Aishan Liu', 'Siyuan Liang', 'Zonghao Ying', 'Xianglong Liu', 'Dacheng Tao']&lt;/li&gt;&lt;li&gt;Tags: ['safety', 'detoxification', 'representation editing', 'reward model', 'test-time intervention']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.01243</guid><pubDate>Fri, 03 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Silent Tokens, Loud Effects: Padding in LLMs</title><link>https://arxiv.org/abs/2510.01238</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Study on padding tokens in LLMs and their impact on robustness&lt;/li&gt;&lt;li&gt;Evaluated across activations, generation quality, bias, and safety&lt;/li&gt;&lt;li&gt;Found padding can degrade quality, alter bias, and weaken safety&lt;/li&gt;&lt;li&gt;Emphasizes need for careful handling in deployment&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Rom Himelstein', 'Amit LeVi', 'Yonatan Belinkov', 'Avi Mendelson']&lt;/li&gt;&lt;li&gt;Tags: ['robustness', 'safety', 'LLM', 'padding tokens', 'adversarial prompting']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.01238</guid><pubDate>Fri, 03 Oct 2025 00:00:00 +0000</pubDate></item><item><title>PurpCode: Reasoning for Safer Code Generation</title><link>https://arxiv.org/abs/2507.19060</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces PurpCode, a post-training recipe for safe code generation&lt;/li&gt;&lt;li&gt;Two-stage training: Rule Learning and Reinforcement Learning&lt;/li&gt;&lt;li&gt;Uses red-teaming to create prompts for unsafe cyberactivities&lt;/li&gt;&lt;li&gt;Develops PurpCode-32B model with state-of-the-art cybersafety&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jiawei Liu', 'Nirav Diwan', 'Zhe Wang', 'Haoyu Zhai', 'Xiaona Zhou', 'Kiet A. Nguyen', 'Tianjiao Yu', 'Muntasir Wahed', 'Yinlin Deng', 'Hadjer Benkraouda', 'Yuxiang Wei', 'Lingming Zhang', 'Ismini Lourentzou', 'Gang Wang']&lt;/li&gt;&lt;li&gt;Tags: ['red teaming', 'code generation', 'security', 'alignment', 'reinforcement learning']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2507.19060</guid><pubDate>Fri, 03 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Superficial Safety Alignment Hypothesis</title><link>https://arxiv.org/abs/2410.10862</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes the Superficial Safety Alignment Hypothesis (SSAH) for LLM safety&lt;/li&gt;&lt;li&gt;Identifies four critical component types (SCU, UCU, CU, RU) for safety alignment&lt;/li&gt;&lt;li&gt;Demonstrates freezing safety-critical components during fine-tuning&lt;/li&gt;&lt;li&gt;Uses redundant units as an 'alignment budget' to minimize alignment tax&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jianwei Li', 'Jung-Eun Kim']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'safety', 'LLM', 'fine-tuning', 'model components']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2410.10862</guid><pubDate>Fri, 03 Oct 2025 00:00:00 +0000</pubDate></item><item><title>UpSafe$^\circ$C: Upcycling for Controllable Safety in Large Language Models</title><link>https://arxiv.org/abs/2510.02194</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes UpSafeC, a framework for enhancing LLM safety through safety-aware upcycling&lt;/li&gt;&lt;li&gt;Uses sparse Mixture-of-Experts (MoE) structure with a router as a soft guardrail&lt;/li&gt;&lt;li&gt;Introduces two-stage SFT and safety temperature for dynamic control&lt;/li&gt;&lt;li&gt;Demonstrates robust safety improvements against harmful and jailbreak inputs&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yuhao Sun', 'Zhuoer Xu', 'Shiwen Cui', 'Kun Yang', 'Lingyun Yu', 'Yongdong Zhang', 'Hongtao Xie']&lt;/li&gt;&lt;li&gt;Tags: ['LLM safety', 'jailbreak attacks', 'safety mechanisms', 'Mixture-of-Experts', 'inference-time control']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.02194</guid><pubDate>Fri, 03 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Evaluating the Robustness of a Production Malware Detection System to Transferable Adversarial Attacks</title><link>https://arxiv.org/abs/2510.01676</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;This paper evaluates the robustness of Gmail's malware detection system against adversarial attacks on the Magika model, which routes malware to specific classifiers. The authors show that by modifying a few bytes, they can evade detection. They also propose and deploy a defense mechanism.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Milad Nasr', 'Yanick Fratantonio', 'Luca Invernizzi', 'Ange Albertini', 'Loua Farah', 'Alex Petit-Bianco', 'Andreas Terzis', 'Kurt Thomas', 'Elie Bursztein', 'Nicholas Carlini']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial attacks', 'malware detection', 'model robustness', 'production system', 'defense']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.01676</guid><pubDate>Fri, 03 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Just Do It!? Computer-Use Agents Exhibit Blind Goal-Directedness</title><link>https://arxiv.org/abs/2510.01670</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces Blind Goal-Directedness (BGD) in Computer-Use Agents (CUAs)&lt;/li&gt;&lt;li&gt;Develops BLIND-ACT benchmark with 90 tasks to evaluate BGD&lt;/li&gt;&lt;li&gt;Evaluates multiple models showing high BGD rates&lt;/li&gt;&lt;li&gt;Highlights need for training/inference interventions&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Erfan Shayegani', 'Keegan Hines', 'Yue Dong', 'Nael Abu-Ghazaleh', 'Roman Lutz', 'Spencer Whitehead', 'Vidhisha Balachandran', 'Besmira Nushi', 'Vibhav Vineet']&lt;/li&gt;&lt;li&gt;Tags: ['safety evaluation', 'alignment', 'red teaming', 'robustness', 'LLM red teaming']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.01670</guid><pubDate>Fri, 03 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Position: Privacy Is Not Just Memorization!</title><link>https://arxiv.org/abs/2510.01645</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;This position paper expands the privacy discourse beyond memorization in LLMs, covering data collection, inference-time leakage, and surveillance risks.&lt;/li&gt;&lt;li&gt;It includes a taxonomy of privacy risks across the LLM lifecycle and a longitudinal analysis of research focus.&lt;/li&gt;&lt;li&gt;The paper calls for interdisciplinary approaches to address these broader privacy threats.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Position&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Niloofar Mireshghallah', 'Tianshi Li']&lt;/li&gt;&lt;li&gt;Tags: ['privacy', 'data collection', 'inference-time leakage', 'surveillance', 'taxonomy']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.01645</guid><pubDate>Fri, 03 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Silent Tokens, Loud Effects: Padding in LLMs</title><link>https://arxiv.org/abs/2510.01238</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Study on the impact of padding tokens in LLMs&lt;/li&gt;&lt;li&gt;Evaluated across activations, generation quality, bias, and safety&lt;/li&gt;&lt;li&gt;Found padding can degrade performance and safety&lt;/li&gt;&lt;li&gt;Emphasizes need for robust handling of padding in deployment&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Rom Himelstein', 'Amit LeVi', 'Yonatan Belinkov', 'Avi Mendelson']&lt;/li&gt;&lt;li&gt;Tags: ['LLM', 'robustness', 'safety', 'padding tokens', 'implementation errors']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.01238</guid><pubDate>Fri, 03 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Tree-based Dialogue Reinforced Policy Optimization for Red-Teaming Attacks</title><link>https://arxiv.org/abs/2510.02286</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes DialTree-RPO, a reinforcement learning framework with tree search for multi-turn attack discovery&lt;/li&gt;&lt;li&gt;Focuses on automated red-teaming for LLMs in multi-turn interactions&lt;/li&gt;&lt;li&gt;Achieves higher ASR than previous methods and uncovers new attack strategies&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Ruohao Guo', 'Afshin Oroojlooy', 'Roshan Sridhar', 'Miguel Ballesteros', 'Alan Ritter', 'Dan Roth']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'multi-turn attacks', 'reinforcement learning', 'tree search', 'adversarial attacks']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.02286</guid><pubDate>Fri, 03 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Detecting Post-generation Edits to Watermarked LLM Outputs via Combinatorial Watermarking</title><link>https://arxiv.org/abs/2510.01637</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a combinatorial watermarking framework for LLM outputs to detect post-generation edits&lt;/li&gt;&lt;li&gt;Introduces local statistics for edit detection and localization&lt;/li&gt;&lt;li&gt;Evaluates on open-source LLMs with various editing scenarios&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Liyan Xie', 'Muhammad Siddeek', 'Mohamed Seif', 'Andrea J. Goldsmith', 'Mengdi Wang']&lt;/li&gt;&lt;li&gt;Tags: ['watermarking', 'post-generation edits', 'edit detection', 'combinatorial patterns', 'local statistics']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.01637</guid><pubDate>Fri, 03 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Bypassing Prompt Guards in Production with Controlled-Release Prompting</title><link>https://arxiv.org/abs/2510.01529</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces a new attack method that bypasses prompt guards in production LLMs&lt;/li&gt;&lt;li&gt;Successfully jailbreaks models like Google Gemini, DeepSeek Chat, Grok, and Mistral Le Chat&lt;/li&gt;&lt;li&gt;Exploits resource asymmetry between prompt guards and main LLM&lt;/li&gt;&lt;li&gt;Highlights limitations of prompt guards and suggests focusing on output prevention&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jaiden Fairoze', 'Sanjam Garg', 'Keewoo Lee', 'Mingyuan Wang']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'jailbreaking', 'prompt injection', 'adversarial prompting', 'alignment']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.01529</guid><pubDate>Fri, 03 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Understanding Adversarial Transfer: Why Representation-Space Attacks Fail Where Data-Space Attacks Succeed</title><link>https://arxiv.org/abs/2510.01494</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;The paper explores why adversarial attacks in data-space transfer between models while those in representation-space do not.&lt;/li&gt;&lt;li&gt;It provides theoretical and empirical evidence across image classifiers, LMs, and VLMs.&lt;/li&gt;&lt;li&gt;Key findings include that data-space attacks transfer due to shared input space, while representation-space attacks require aligned geometries.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Isha Gupta', 'Rylan Schaeffer', 'Joshua Kazdan', 'Ken Liu', 'Sanmi Koyejo']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial robustness', 'transferability', 'data-space attacks', 'representation-space attacks', 'vision-language models']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.01494</guid><pubDate>Fri, 03 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Microsaccade-Inspired Probing: Positional Encoding Perturbations Reveal LLM Misbehaviours</title><link>https://arxiv.org/abs/2510.01288</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces a probing method for LLMs using positional encoding perturbations&lt;/li&gt;&lt;li&gt;Detects failures in factuality, safety, toxicity, and backdoor attacks&lt;/li&gt;&lt;li&gt;No fine-tuning or task-specific supervision needed&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Rui Melo', 'Rui Abreu', 'Corina S. Pasareanu']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'safety evaluation', 'adversarial prompting', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.01288</guid><pubDate>Fri, 03 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Superficial Safety Alignment Hypothesis</title><link>https://arxiv.org/abs/2410.10862</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes the Superficial Safety Alignment Hypothesis (SSAH) for LLM safety&lt;/li&gt;&lt;li&gt;Identifies four critical component types (SCU, UCU, CU, RU) for safety alignment&lt;/li&gt;&lt;li&gt;Demonstrates freezing safety-critical components during fine-tuning&lt;/li&gt;&lt;li&gt;Uses redundant units as an 'alignment budget' to minimize alignment tax&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jianwei Li', 'Jung-Eun Kim']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'safety', 'LLM', 'fine-tuning', 'model components']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2410.10862</guid><pubDate>Fri, 03 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Defend LLMs Through Self-Consciousness</title><link>https://arxiv.org/abs/2508.02961</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces a self-consciousness defense mechanism for LLMs against prompt injection attacks&lt;/li&gt;&lt;li&gt;Uses Meta-Cognitive and Arbitration Modules for self-evaluation and regulation&lt;/li&gt;&lt;li&gt;Evaluated on seven LLMs with two datasets: AdvBench and Prompt-Injection-Mixed-Techniques-2024&lt;/li&gt;&lt;li&gt;Shows significant improvements in defense success rates with minimal computational overhead&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Boshi Huang', 'Fabio Nonato de Paula']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'prompt injection', 'adversarial prompting', 'safety evaluation', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2508.02961</guid><pubDate>Fri, 03 Oct 2025 00:00:00 +0000</pubDate></item><item><title>AgentDAM: Privacy Leakage Evaluation for Autonomous Web Agents</title><link>https://arxiv.org/abs/2503.09780</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces AgentDAM benchmark for evaluating privacy leakage in autonomous web agents&lt;/li&gt;&lt;li&gt;Focuses on data minimization principle&lt;/li&gt;&lt;li&gt;Evaluates GPT-4, Llama-3, Claude agents on handling sensitive information&lt;/li&gt;&lt;li&gt;Proposes prompting defense to reduce information leakage&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Arman Zharmagambetov', 'Chuan Guo', 'Ivan Evtimov', 'Maya Pavlova', 'Ruslan Salakhutdinov', 'Kamalika Chaudhuri']&lt;/li&gt;&lt;li&gt;Tags: ['privacy attacks', 'safety evaluation', 'benchmarking', 'data minimization', 'autonomous agents']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2503.09780</guid><pubDate>Fri, 03 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Tree-based Dialogue Reinforced Policy Optimization for Red-Teaming Attacks</title><link>https://arxiv.org/abs/2510.02286</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes DialTree-RPO, a reinforcement learning framework with tree search for multi-turn attack discovery&lt;/li&gt;&lt;li&gt;Focuses on automated red-teaming for LLMs in multi-turn interactions&lt;/li&gt;&lt;li&gt;Achieves higher ASR than previous methods and uncovers new attack strategies&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Ruohao Guo', 'Afshin Oroojlooy', 'Roshan Sridhar', 'Miguel Ballesteros', 'Alan Ritter', 'Dan Roth']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'multi-turn attacks', 'reinforcement learning', 'tree search', 'adversarial attacks']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.02286</guid><pubDate>Fri, 03 Oct 2025 00:00:00 +0000</pubDate></item><item><title>SoK: Measuring What Matters for Closed-Loop Security Agents</title><link>https://arxiv.org/abs/2510.01654</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces CLASP framework for evaluating closed-loop security agents&lt;/li&gt;&lt;li&gt;Maps security lifecycle stages to agentic capabilities&lt;/li&gt;&lt;li&gt;Defines CLC Score to measure loop closure and effectiveness&lt;/li&gt;&lt;li&gt;Analyzes 21 works to identify capability gaps&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Mudita Khurana', 'Raunak Jain']&lt;/li&gt;&lt;li&gt;Tags: ['security', 'evaluation', 'framework', 'agents', 'benchmark']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.01654</guid><pubDate>Fri, 03 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Position: Privacy Is Not Just Memorization!</title><link>https://arxiv.org/abs/2510.01645</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;This position paper expands the privacy discourse beyond memorization in LLMs, covering data collection, inference-time leakage, and surveillance risks.&lt;/li&gt;&lt;li&gt;It includes a taxonomy of privacy risks across the LLM lifecycle and a longitudinal analysis of research focus.&lt;/li&gt;&lt;li&gt;The paper calls for interdisciplinary approaches to address these broader privacy threats.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Position&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Niloofar Mireshghallah', 'Tianshi Li']&lt;/li&gt;&lt;li&gt;Tags: ['privacy', 'data collection', 'inference-time leakage', 'surveillance', 'taxonomy']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.01645</guid><pubDate>Fri, 03 Oct 2025 00:00:00 +0000</pubDate></item><item><title>NLP Methods for Detecting Novel LLM Jailbreaks and Keyword Analysis with BERT</title><link>https://arxiv.org/abs/2510.01644</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;The paper focuses on detecting jailbreak prompts in LLMs using NLP methods.&lt;/li&gt;&lt;li&gt;It evaluates different ML models, with BERT showing the best performance.&lt;/li&gt;&lt;li&gt;Analyzes keywords distinguishing jailbreaks from genuine prompts.&lt;/li&gt;&lt;li&gt;Identifies explicit reflexivity in prompts as a potential jailbreak signal.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['John Hawkins', 'Aditya Pramar', 'Rodney Beard', 'Rohitash Chandra']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'jailbreaking', 'prompt injection', 'adversarial prompting', 'safety evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.01644</guid><pubDate>Fri, 03 Oct 2025 00:00:00 +0000</pubDate></item><item><title>POLAR: Automating Cyber Threat Prioritization through LLM-Powered Assessment</title><link>https://arxiv.org/abs/2510.01552</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;This paper evaluates LLMs in cyber threat intelligence (CTI) tasks, identifying three key vulnerabilities: spurious correlations, contradictory knowledge, and constrained generalization. It uses large-scale evaluations and human-in-the-loop supervision to analyze failure instances and offers insights for improving robustness.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Luoxi Tang', 'Yuqiao Meng', 'Ankita Patra', 'Weicheng Ma', 'Muchao Ye', 'Zhaohan Xi']&lt;/li&gt;&lt;li&gt;Tags: ['LLM security', 'CTI', 'vulnerability analysis', 'robustness', 'human-in-the-loop']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.01552</guid><pubDate>Fri, 03 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Understanding Adversarial Transfer: Why Representation-Space Attacks Fail Where Data-Space Attacks Succeed</title><link>https://arxiv.org/abs/2510.01494</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;The paper explores why adversarial attacks in data-space transfer between models while those in representation-space do not.&lt;/li&gt;&lt;li&gt;It provides theoretical and empirical evidence across image classifiers, LMs, and VLMs.&lt;/li&gt;&lt;li&gt;Key findings include that data-space attacks transfer due to shared input space, while representation-space attacks require aligned geometries.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Isha Gupta', 'Rylan Schaeffer', 'Joshua Kazdan', 'Ken Liu', 'Sanmi Koyejo']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial robustness', 'transferability', 'data-space attacks', 'representation-space attacks', 'vision-language models']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.01494</guid><pubDate>Fri, 03 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Breaking the Code: Security Assessment of AI Code Agents Through Systematic Jailbreaking Attacks</title><link>https://arxiv.org/abs/2510.01359</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;The paper introduces JAWS-BENCH, a benchmark for evaluating jailbreak attacks on code-capable LLM agents across different workspace environments.&lt;/li&gt;&lt;li&gt;It uses a hierarchical judge framework to assess compliance, attack success, syntactic correctness, and runtime executability.&lt;/li&gt;&lt;li&gt;Key findings show that code agents accept a significant portion of attacks, especially in more complex workspace regimes.&lt;/li&gt;&lt;li&gt;The study highlights that wrapping LLMs in agents increases vulnerability due to refusals being overturned during planning steps.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Shoumik Saha', 'Jifan Chen', 'Sam Mayers', 'Sanjay Krishna Gouda', 'Zijian Wang', 'Varun Kumar']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'jailbreaking', 'adversarial prompting', 'safety evaluation', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.01359</guid><pubDate>Fri, 03 Oct 2025 00:00:00 +0000</pubDate></item><item><title>WAInjectBench: Benchmarking Prompt Injection Detections for Web Agents</title><link>https://arxiv.org/abs/2510.01354</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces WAInjectBench, a benchmark for detecting prompt injection attacks on web agents&lt;/li&gt;&lt;li&gt;Categorizes attacks based on threat models and creates datasets of malicious and benign samples&lt;/li&gt;&lt;li&gt;Evaluates text-based and image-based detection methods across multiple scenarios&lt;/li&gt;&lt;li&gt;Finds that detectors struggle with attacks using imperceptible perturbations or no explicit instructions&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yinuo Liu', 'Ruohan Xu', 'Xilong Wang', 'Yuqi Jia', 'Neil Zhenqiang Gong']&lt;/li&gt;&lt;li&gt;Tags: ['prompt injection', 'red teaming', 'benchmarking', 'detection methods', 'web agents']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.01354</guid><pubDate>Fri, 03 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Microsaccade-Inspired Probing: Positional Encoding Perturbations Reveal LLM Misbehaviours</title><link>https://arxiv.org/abs/2510.01288</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces a probing method for LLMs using positional encoding perturbations&lt;/li&gt;&lt;li&gt;Detects failures in factuality, safety, toxicity, and backdoor attacks&lt;/li&gt;&lt;li&gt;No fine-tuning or task-specific supervision needed&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Rui Melo', 'Rui Abreu', 'Corina S. Pasareanu']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'safety evaluation', 'adversarial prompting', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.01288</guid><pubDate>Fri, 03 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Think Twice, Generate Once: Safeguarding by Progressive Self-Reflection</title><link>https://arxiv.org/abs/2510.01270</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces Progressive Self-Reflection (PSR) for LLMs to self-monitor and correct outputs during inference.&lt;/li&gt;&lt;li&gt;Significantly reduces attack success rates (e.g., from 77.5% to 5.9% on Llama-3.1-8B-Instruct) without retraining.&lt;/li&gt;&lt;li&gt;Uses a self-reflection predictor to adaptively determine the number of reflection rounds based on input complexity.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Hoang Phan', 'Victor Li', 'Qi Lei']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'safety evaluation', 'adversarial prompting', 'robustness', 'alignment']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.01270</guid><pubDate>Fri, 03 Oct 2025 00:00:00 +0000</pubDate></item><item><title>OpenAI's GPT-OSS-20B Model and Safety Alignment Issues in a Low-Resource Language</title><link>https://arxiv.org/abs/2510.01266</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Red teaming of OpenAI's GPT-OSS-20B model in a low-resource language (Hausa)&lt;/li&gt;&lt;li&gt;Uncovered biases, inaccuracies, and cultural insensitivities&lt;/li&gt;&lt;li&gt;Model generates harmful content when prompted with polite language&lt;/li&gt;&lt;li&gt;Safety protocols relax leading to misinformation and hate speech&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Isa Inuwa-Dutse']&lt;/li&gt;&lt;li&gt;Tags: ['red teaming', 'jailbreaking', 'prompt injection', 'adversarial prompting', 'model extraction', 'data poisoning', 'privacy attacks', 'alignment', 'robustness', 'safety evaluation', 'security standards']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.01266</guid><pubDate>Fri, 03 Oct 2025 00:00:00 +0000</pubDate></item><item><title>In AI Sweet Harmony: Sociopragmatic Guardrail Bypasses and Evaluation-Awareness in OpenAI gpt-oss-20b</title><link>https://arxiv.org/abs/2510.01259</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;The paper studies sociopragmatic framing effects on refusal behavior in OpenAI's gpt-oss-20b model.&lt;/li&gt;&lt;li&gt;Tests various harm domains like ZIP-bomb construction, synthetic card numbers, unsafe advice, and context exfiltration.&lt;/li&gt;&lt;li&gt;Introduces an AI-assisted hardening method to reduce leakage.&lt;/li&gt;&lt;li&gt;Evaluates model's awareness of evaluation prompts and Moderation API's performance.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Nils Durner']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'jailbreaking', 'adversarial prompting', 'safety evaluation', 'alignment']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.01259</guid><pubDate>Fri, 03 Oct 2025 00:00:00 +0000</pubDate></item><item><title>UpSafe$^\circ$C: Upcycling for Controllable Safety in Large Language Models</title><link>https://arxiv.org/abs/2510.02194</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes UpSafeC, a framework for enhancing LLM safety through safety-aware upcycling&lt;/li&gt;&lt;li&gt;Uses sparse Mixture-of-Experts (MoE) structure with a router as a soft guardrail&lt;/li&gt;&lt;li&gt;Introduces two-stage SFT and safety temperature for dynamic control&lt;/li&gt;&lt;li&gt;Demonstrates robust safety improvements against harmful and jailbreak inputs&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yuhao Sun', 'Zhuoer Xu', 'Shiwen Cui', 'Kun Yang', 'Lingyun Yu', 'Yongdong Zhang', 'Hongtao Xie']&lt;/li&gt;&lt;li&gt;Tags: ['LLM safety', 'jailbreak attacks', 'safety mechanisms', 'Mixture-of-Experts', 'inference-time control']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.02194</guid><pubDate>Fri, 03 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Just Do It!? Computer-Use Agents Exhibit Blind Goal-Directedness</title><link>https://arxiv.org/abs/2510.01670</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces Blind Goal-Directedness (BGD) in Computer-Use Agents (CUAs)&lt;/li&gt;&lt;li&gt;Develops BLIND-ACT benchmark with 90 tasks to evaluate BGD&lt;/li&gt;&lt;li&gt;Evaluates multiple models showing high BGD rates&lt;/li&gt;&lt;li&gt;Highlights need for training/inference interventions&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Erfan Shayegani', 'Keegan Hines', 'Yue Dong', 'Nael Abu-Ghazaleh', 'Roman Lutz', 'Spencer Whitehead', 'Vidhisha Balachandran', 'Besmira Nushi', 'Vibhav Vineet']&lt;/li&gt;&lt;li&gt;Tags: ['safety evaluation', 'alignment', 'red teaming', 'robustness', 'LLM red teaming']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.01670</guid><pubDate>Fri, 03 Oct 2025 00:00:00 +0000</pubDate></item><item><title>AdvEvo-MARL: Shaping Internalized Safety through Adversarial Co-Evolution in Multi-Agent Reinforcement Learning</title><link>https://arxiv.org/abs/2510.01586</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes AdvEvo-MARL, a co-evolutionary framework for multi-agent RL to internalize safety&lt;/li&gt;&lt;li&gt;Uses adversarial co-evolution of attackers and defenders to train agents to resist jailbreaks&lt;/li&gt;&lt;li&gt;Introduces public baseline for advantage estimation to stabilize learning&lt;/li&gt;&lt;li&gt;Shows improved safety metrics (ASR &lt;20%) and task accuracy&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Zhenyu Pan', 'Yiting Zhang', 'Zhuo Liu', 'Yolo Yunlong Tang', 'Zeliang Zhang', 'Haozheng Luo', 'Yuwei Han', 'Jianshu Zhang', 'Dennis Wu', 'Hong-Yu Chen', 'Haoran Lu', 'Haoyang Fang', 'Manling Li', 'Chenliang Xu', 'Philip S. Yu', 'Han Liu']&lt;/li&gt;&lt;li&gt;Tags: ['red teaming', 'jailbreaking', 'adversarial prompting', 'multi-agent RL', 'safety evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.01586</guid><pubDate>Fri, 03 Oct 2025 00:00:00 +0000</pubDate></item><item><title>InvThink: Towards AI Safety via Inverse Reasoning</title><link>https://arxiv.org/abs/2510.01569</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces InvThink, a method for improving AI safety by instructing LLMs to reason through failure modes before generating responses.&lt;/li&gt;&lt;li&gt;Key steps include enumerating potential harms, analyzing consequences, and generating safe outputs.&lt;/li&gt;&lt;li&gt;Shows improved safety scaling with model size and reduced safety tax compared to existing methods.&lt;/li&gt;&lt;li&gt;Excels in high-stakes domains like medicine, finance, law, and agentic risks.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yubin Kim', 'Taehan Kim', 'Eugene Park', 'Chunjong Park', 'Cynthia Breazeal', 'Daniel McDuff', 'Hae Won Park']&lt;/li&gt;&lt;li&gt;Tags: ['AI safety', 'alignment', 'inverse reasoning', 'safety evaluation', 'LLM safety']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.01569</guid><pubDate>Fri, 03 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Is It Thinking or Cheating? Detecting Implicit Reward Hacking by Measuring Reasoning Effort</title><link>https://arxiv.org/abs/2510.01367</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces TRACE (Truncated Reasoning AUC Evaluation) to detect implicit reward hacking in reasoning models.&lt;/li&gt;&lt;li&gt;Measures reasoning effort by truncating CoT and evaluating verifier-passing rates.&lt;/li&gt;&lt;li&gt;Shows significant improvements over existing CoT monitors in math and coding tasks.&lt;/li&gt;&lt;li&gt;Can discover unknown loopholes during training.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Xinpeng Wang', 'Nitish Joshi', 'Barbara Plank', 'Rico Angell', 'He He']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'safety evaluation', 'red teaming', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.01367</guid><pubDate>Fri, 03 Oct 2025 00:00:00 +0000</pubDate></item></channel></rss>