<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>AI Security Paper Digest</title><link>https://kentaroh-toyoda.github.io/ai-security-paper-digest-rss/rss.xml</link><description>Curated papers on AI security from ArXiv</description><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><lastBuildDate>Wed, 27 Aug 2025 22:21:57 +0000</lastBuildDate><item><title>Hidden Tail: Adversarial Image Causing Stealthy Resource Consumption in Vision-Language Models</title><link>https://arxiv.org/abs/2508.18805</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes Hidden Tail, a stealthy resource consumption attack on Vision-Language Models (VLMs)&lt;/li&gt;&lt;li&gt;Crafts adversarial images that induce maximum-length outputs with invisible special tokens&lt;/li&gt;&lt;li&gt;Uses composite loss balancing semantic preservation, special token induction, and EOS suppression&lt;/li&gt;&lt;li&gt;Demonstrates up to 19.2x output length increase while maintaining stealthiness&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Rui Zhang', 'Zihan Wang', 'Tianli Yang', 'Hongwei Li', 'Wenbo Jiang', 'Qingchuan Zhao', 'Yang Liu', 'Guowen Xu']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial_attacks', 'vision_language_models', 'resource_consumption', 'stealthy_attacks', 'adversarial_prompting', 'model_robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2508.18805</guid><pubDate>Wed, 27 Aug 2025 00:00:00 +0000</pubDate></item><item><title>SDGO: Self-Discrimination-Guided Optimization for Consistent Safety in Large Language Models</title><link>https://arxiv.org/abs/2508.15648</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces SDGO framework using self-discrimination as reward signal for safer LLM generation&lt;/li&gt;&lt;li&gt;Addresses safety inconsistency between model's discrimination and generation capabilities&lt;/li&gt;&lt;li&gt;Improves resistance to jailbreaking attacks without additional annotated data&lt;/li&gt;&lt;li&gt;Demonstrates robust performance against out-of-distribution adversarial prompts&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Peng Ding', 'Wen Sun', 'Dailin Li', 'Wei Zou', 'Jiaming Wang', 'Jiajun Chen', 'Shujian Huang']&lt;/li&gt;&lt;li&gt;Tags: ['jailbreaking', 'safety', 'reinforcement learning', 'self-discrimination', 'adversarial prompting']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2508.15648</guid><pubDate>Wed, 27 Aug 2025 00:00:00 +0000</pubDate></item><item><title>RePPL: Recalibrating Perplexity by Uncertainty in Semantic Propagation and Language Generation for Explainable QA Hallucination Detection</title><link>https://arxiv.org/abs/2505.15386</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes RePPL for detecting and explaining hallucinations in LLMs&lt;/li&gt;&lt;li&gt;Measures uncertainty in semantic propagation and language generation&lt;/li&gt;&lt;li&gt;Assigns token-level uncertainty scores to identify problematic inputs&lt;/li&gt;&lt;li&gt;Achieves high AUC (0.833) across multiple QA datasets&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yiming Huang', 'Junyan Zhang', 'Zihao Wang', 'Biquan Bie', 'Yunzhong Qiu', 'Yi R. Fung', 'Xinlei He']&lt;/li&gt;&lt;li&gt;Tags: ['hallucination detection', 'explainable AI', 'safety evaluation', 'uncertainty quantification', 'adversarial prompting']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2505.15386</guid><pubDate>Wed, 27 Aug 2025 00:00:00 +0000</pubDate></item><item><title>sudoLLM: On Multi-role Alignment of Language Models</title><link>https://arxiv.org/abs/2505.14607</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces sudoLLM framework for multi-role aligned LLMs using user access rights&lt;/li&gt;&lt;li&gt;Empirical results show improved alignment, generalization, and resistance to jailbreaking attacks&lt;/li&gt;&lt;li&gt;Injects user-based bias signals into queries to guide safe behavior&lt;/li&gt;&lt;li&gt;Complements existing guardrail mechanisms for enhanced end-to-end safety&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Soumadeep Saha', 'Akshay Chaturvedi', 'Joy Mahapatra', 'Utpal Garain']&lt;/li&gt;&lt;li&gt;Tags: ['LLM security', 'alignment', 'jailbreaking', 'access control', 'red teaming']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2505.14607</guid><pubDate>Wed, 27 Aug 2025 00:00:00 +0000</pubDate></item><item><title>The Double-edged Sword of LLM-based Data Reconstruction: Understanding and Mitigating Contextual Vulnerability in Word-level Differential Privacy Text Sanitization</title><link>https://arxiv.org/abs/2508.18976</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Investigates LLM-based data reconstruction attacks on differentially private text sanitization&lt;/li&gt;&lt;li&gt;Highlights 'contextual vulnerability' as a privacy risk&lt;/li&gt;&lt;li&gt;Demonstrates double-edged sword effect where LLMs can both exploit and enhance privacy&lt;/li&gt;&lt;li&gt;Proposes using adversarial LLM post-processing to improve privacy&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Stephen Meisenbacher', 'Alexandra Klymenko', 'Andreea-Elena Bodea', 'Florian Matthes']&lt;/li&gt;&lt;li&gt;Tags: ['Privacy attacks', 'LLM red teaming', 'Differential privacy', 'Data reconstruction', 'Adversarial AI']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2508.18976</guid><pubDate>Wed, 27 Aug 2025 00:00:00 +0000</pubDate></item><item><title>Membership Inference Attacks on LLM-based Recommender Systems</title><link>https://arxiv.org/abs/2508.18665</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces four membership inference attacks (direct inquiry, hallucination, similarity, poisoning) on LLM-based recommender systems&lt;/li&gt;&lt;li&gt;Evaluates attacks on three LLMs and two benchmark datasets&lt;/li&gt;&lt;li&gt;Direct inquiry and poisoning attacks show high effectiveness&lt;/li&gt;&lt;li&gt;Analyzes impact of prompt structure and user position on attack success&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jiajie He', 'Yuechun Gu', 'Min-Chun Chen', 'Keke Chen']&lt;/li&gt;&lt;li&gt;Tags: ['privacy attacks', 'membership inference', 'LLM', 'recommender systems', 'security']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2508.18665</guid><pubDate>Wed, 27 Aug 2025 00:00:00 +0000</pubDate></item><item><title>UniC-RAG: Universal Knowledge Corruption Attacks to Retrieval-Augmented Generation</title><link>https://arxiv.org/abs/2508.18652</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces UniC-RAG, a universal knowledge corruption attack on RAG systems&lt;/li&gt;&lt;li&gt;Optimizes a small set of adversarial texts to affect diverse user queries&lt;/li&gt;&lt;li&gt;Achieves high attack success rates (90%) with minimal injections&lt;/li&gt;&lt;li&gt;Existing defenses are insufficient against UniC-RAG&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Runpeng Geng', 'Yanting Wang', 'Ying Chen', 'Jinyuan Jia']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial attacks', 'data poisoning', 'RAG systems', 'universal attacks', 'red teaming']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2508.18652</guid><pubDate>Wed, 27 Aug 2025 00:00:00 +0000</pubDate></item><item><title>Training Language Model Agents to Find Vulnerabilities with CTF-Dojo</title><link>https://arxiv.org/abs/2508.18370</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces CTF-Dojo: a large-scale executable runtime environment for training LLMs on CTF-style security challenges&lt;/li&gt;&lt;li&gt;Develops CTF-Forge: an automated pipeline to create execution environments from public artifacts&lt;/li&gt;&lt;li&gt;Trains LLM agents achieving state-of-the-art performance on cybersecurity benchmarks (InterCode-CTF, NYU CTF Bench, Cybench)&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Terry Yue Zhuo', 'Dingmin Wang', 'Hantian Ding', 'Varun Kumar', 'Zijian Wang']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'vulnerability detection', 'CTF', 'execution environment', 'security training']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2508.18370</guid><pubDate>Wed, 27 Aug 2025 00:00:00 +0000</pubDate></item><item><title>Seal Your Backdoor with Variational Defense</title><link>https://arxiv.org/abs/2503.08829</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces VIBE, a model-agnostic framework for defending against backdoor attacks&lt;/li&gt;&lt;li&gt;Uses variational inference to recover clean labels from poisoned training data&lt;/li&gt;&lt;li&gt;Employs an EM algorithm with entropy-regularized optimal transport&lt;/li&gt;&lt;li&gt;Outperforms previous defenses in multiple experimental setups&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ["Ivan Saboli\\'c", "Matej Grci\\'c", "Sini\\v{s}a \\v{S}egvi\\'c"]&lt;/li&gt;&lt;li&gt;Tags: ['data poisoning', 'backdoor attacks', 'robustness', 'variational inference', 'EM algorithm']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2503.08829</guid><pubDate>Wed, 27 Aug 2025 00:00:00 +0000</pubDate></item><item><title>Membership Inference Attacks on LLM-based Recommender Systems</title><link>https://arxiv.org/abs/2508.18665</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces four membership inference attacks (direct inquiry, hallucination, similarity, poisoning) on LLM-based recommender systems&lt;/li&gt;&lt;li&gt;Evaluates attacks on three LLMs and two benchmark datasets&lt;/li&gt;&lt;li&gt;Direct inquiry and poisoning attacks show high effectiveness&lt;/li&gt;&lt;li&gt;Analyzes impact of prompt structure and user position on attack success&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jiajie He', 'Yuechun Gu', 'Min-Chun Chen', 'Keke Chen']&lt;/li&gt;&lt;li&gt;Tags: ['privacy attacks', 'membership inference', 'LLM', 'recommender systems', 'security']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2508.18665</guid><pubDate>Wed, 27 Aug 2025 00:00:00 +0000</pubDate></item><item><title>Training Language Model Agents to Find Vulnerabilities with CTF-Dojo</title><link>https://arxiv.org/abs/2508.18370</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces CTF-Dojo: a large-scale executable runtime environment for training LLMs on CTF-style security challenges&lt;/li&gt;&lt;li&gt;Develops CTF-Forge: an automated pipeline to create execution environments from public artifacts&lt;/li&gt;&lt;li&gt;Trains LLM agents achieving state-of-the-art performance on cybersecurity benchmarks (InterCode-CTF, NYU CTF Bench, Cybench)&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Terry Yue Zhuo', 'Dingmin Wang', 'Hantian Ding', 'Varun Kumar', 'Zijian Wang']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'vulnerability detection', 'CTF', 'execution environment', 'security training']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2508.18370</guid><pubDate>Wed, 27 Aug 2025 00:00:00 +0000</pubDate></item><item><title>FLAegis: A Two-Layer Defense Framework for Federated Learning Against Poisoning Attacks</title><link>https://arxiv.org/abs/2508.18737</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces FLAegis, a two-layer defense framework for federated learning against poisoning attacks&lt;/li&gt;&lt;li&gt;Uses SAX transformation and spectral clustering for Byzantine client detection&lt;/li&gt;&lt;li&gt;Incorporates FFT-based aggregation for robustness&lt;/li&gt;&lt;li&gt;Evaluated against multiple poisoning attacks with high performance&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ["Enrique M\\'armol Campos", "Aurora Gonz\\'alez Vidal", "Jos\\'e Luis Hern\\'andez Ramos", 'Antonio Skarmeta']&lt;/li&gt;&lt;li&gt;Tags: ['data poisoning', 'federated learning', 'defense mechanism', 'robustness', 'spectral clustering']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2508.18737</guid><pubDate>Wed, 27 Aug 2025 00:00:00 +0000</pubDate></item><item><title>RePPL: Recalibrating Perplexity by Uncertainty in Semantic Propagation and Language Generation for Explainable QA Hallucination Detection</title><link>https://arxiv.org/abs/2505.15386</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes RePPL for detecting and explaining hallucinations in LLMs&lt;/li&gt;&lt;li&gt;Measures uncertainty in semantic propagation and language generation&lt;/li&gt;&lt;li&gt;Assigns token-level uncertainty scores to identify problematic inputs&lt;/li&gt;&lt;li&gt;Achieves high AUC (0.833) across multiple QA datasets&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yiming Huang', 'Junyan Zhang', 'Zihao Wang', 'Biquan Bie', 'Yunzhong Qiu', 'Yi R. Fung', 'Xinlei He']&lt;/li&gt;&lt;li&gt;Tags: ['hallucination detection', 'explainable AI', 'safety evaluation', 'uncertainty quantification', 'adversarial prompting']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2505.15386</guid><pubDate>Wed, 27 Aug 2025 00:00:00 +0000</pubDate></item><item><title>Prefill-level Jailbreak: A Black-Box Risk Analysis of Large Language Models</title><link>https://arxiv.org/abs/2504.21038</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces prefill-level jailbreak attacks on LLMs&lt;/li&gt;&lt;li&gt;Evaluates attack effectiveness across 14 models with high success rates&lt;/li&gt;&lt;li&gt;Analyzes token probability manipulation and defense strategies&lt;/li&gt;&lt;li&gt;Shows prefill attacks enhance existing prompt-level attacks&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yakai Li', 'Jiekang Hu', 'Weiduan Sang', 'Luping Ma', 'Dongsheng Nie', 'Weijuan Zhang', 'Aimin Yu', 'Yi Su', 'Qingjia Huang', 'Qihang Zhou']&lt;/li&gt;&lt;li&gt;Tags: ['jailbreaking', 'red teaming', 'LLM security', 'adversarial attacks', 'black-box analysis']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2504.21038</guid><pubDate>Wed, 27 Aug 2025 00:00:00 +0000</pubDate></item><item><title>Generative Artificial Intelligence-Supported Pentesting: A Comparison between Claude Opus, GPT-4, and Copilot</title><link>https://arxiv.org/abs/2501.06963</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Evaluates GenAI tools (Claude Opus, GPT-4, Copilot) for pentesting&lt;/li&gt;&lt;li&gt;Analyzes their utility across PTES phases&lt;/li&gt;&lt;li&gt;Finds Claude Opus outperforms others&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ["Antonio L\\'opez Mart\\'inez", 'Alejandro Cano', "Antonio Ruiz-Mart\\'inez"]&lt;/li&gt;&lt;li&gt;Tags: ['red teaming', 'adversarial prompting', 'safety evaluation', 'pentesting', 'LLM']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2501.06963</guid><pubDate>Wed, 27 Aug 2025 00:00:00 +0000</pubDate></item><item><title>HonestCyberEval: An AI Cyber Risk Benchmark for Automated Software Exploitation</title><link>https://arxiv.org/abs/2410.21939</link><description>&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Other&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Dan Ristea', 'Vasilios Mavroudis']&lt;/li&gt;&lt;li&gt;Tags: []&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2410.21939</guid><pubDate>Wed, 27 Aug 2025 00:00:00 +0000</pubDate></item><item><title>FLAegis: A Two-Layer Defense Framework for Federated Learning Against Poisoning Attacks</title><link>https://arxiv.org/abs/2508.18737</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces FLAegis, a two-layer defense framework for federated learning against poisoning attacks&lt;/li&gt;&lt;li&gt;Uses SAX transformation and spectral clustering for Byzantine client detection&lt;/li&gt;&lt;li&gt;Incorporates FFT-based aggregation for robustness&lt;/li&gt;&lt;li&gt;Evaluated against multiple poisoning attacks with high performance&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ["Enrique M\\'armol Campos", "Aurora Gonz\\'alez Vidal", "Jos\\'e Luis Hern\\'andez Ramos", 'Antonio Skarmeta']&lt;/li&gt;&lt;li&gt;Tags: ['data poisoning', 'federated learning', 'defense mechanism', 'robustness', 'spectral clustering']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2508.18737</guid><pubDate>Wed, 27 Aug 2025 00:00:00 +0000</pubDate></item><item><title>Membership Inference Attacks on LLM-based Recommender Systems</title><link>https://arxiv.org/abs/2508.18665</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces four membership inference attacks (direct inquiry, hallucination, similarity, poisoning) on LLM-based recommender systems&lt;/li&gt;&lt;li&gt;Evaluates attacks on three LLMs and two benchmark datasets&lt;/li&gt;&lt;li&gt;Direct inquiry and poisoning attacks show high effectiveness&lt;/li&gt;&lt;li&gt;Analyzes impact of prompt structure and user position on attack success&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jiajie He', 'Yuechun Gu', 'Min-Chun Chen', 'Keke Chen']&lt;/li&gt;&lt;li&gt;Tags: ['privacy attacks', 'membership inference', 'LLM', 'recommender systems', 'security']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2508.18665</guid><pubDate>Wed, 27 Aug 2025 00:00:00 +0000</pubDate></item><item><title>PRISM: Robust VLM Alignment with Principled Reasoning for Integrated Safety in Multimodality</title><link>https://arxiv.org/abs/2508.18649</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces PRISM framework for VLM safety with principled reasoning&lt;/li&gt;&lt;li&gt;Utilizes PRISM-CoT and PRISM-DPO for safety-aware chain-of-thought reasoning&lt;/li&gt;&lt;li&gt;Demonstrates significant reduction in attack success rates on JailbreakV-28K and VLBreak&lt;/li&gt;&lt;li&gt;Maintains or improves model utility while enhancing safety&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Nanxi Li', 'Zhengyue Zhao', 'Chaowei Xiao']&lt;/li&gt;&lt;li&gt;Tags: ['VLM safety', 'red teaming', 'jailbreak defense', 'adversarial robustness', 'multimodal alignment']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2508.18649</guid><pubDate>Wed, 27 Aug 2025 00:00:00 +0000</pubDate></item></channel></rss>