<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>AI Security Paper Digest</title><link>https://kentaroh-toyoda.github.io/ai-security-paper-digest-rss/rss.xml</link><description>Curated papers on AI security from OpenAlex and ArXiv</description><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><lastBuildDate>Mon, 28 Jul 2025 22:12:27 +0000</lastBuildDate><item><title>ShrinkBox: Backdoor Attack on Object Detection to Disrupt Collision Avoidance in Machine Learning-based Advanced Driver Assistance Systems</title><link>https://arxiv.org/abs/2507.18656</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces ShrinkBox, a backdoor attack targeting object detection in ML-based ADAS&lt;/li&gt;&lt;li&gt;Attack shrinks ground truth bounding boxes to disrupt distance estimation&lt;/li&gt;&lt;li&gt;Demonstrates 96% ASR with 4% poisoning ratio on KITTI dataset&lt;/li&gt;&lt;li&gt;Increases MAE in distance estimation by 3x on poisoned samples&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Muhammad Zaeem Shahzad', 'Muhammad Abdullah Hanif', 'Bassem Ouni', 'Muhammad Shafique']&lt;/li&gt;&lt;li&gt;Tags: ['data poisoning', 'robustness', 'safety evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2507.18656</guid><pubDate>Mon, 28 Jul 2025 00:00:00 +0000</pubDate></item><item><title>Jailbreaking Large Language Diffusion Models: Revealing Hidden Safety Flaws in Diffusion-Based Text Generation</title><link>https://arxiv.org/abs/2507.19227</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces PArallel Decoding jailbreak (PAD) for diffusion-based language models&lt;/li&gt;&lt;li&gt;Achieves 97% jailbreak success rate across four LLDMs&lt;/li&gt;&lt;li&gt;Highlights 2x increase in harmful generation speed compared to autoregressive LLMs&lt;/li&gt;&lt;li&gt;Provides insights into LLDM architecture for secure deployment&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yuanhe Zhang', 'Fangzhou Xie', 'Zhenhong Zhou', 'Zherui Li', 'Hao Chen', 'Kun Wang', 'Yufei Guo']&lt;/li&gt;&lt;li&gt;Tags: ['jailbreaking', 'adversarial prompting', 'safety evaluation', 'diffusion models', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2507.19227</guid><pubDate>Mon, 28 Jul 2025 00:00:00 +0000</pubDate></item><item><title>PurpCode: Reasoning for Safer Code Generation</title><link>https://arxiv.org/abs/2507.19060</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces PurpCode, a post-training recipe for safe code reasoning models&lt;/li&gt;&lt;li&gt;Uses red-teaming to generate adversarial prompts for safety evaluation&lt;/li&gt;&lt;li&gt;Two-stage training: Rule Learning and Reinforcement Learning&lt;/li&gt;&lt;li&gt;PurpCode-32B model shows state-of-the-art cybersafety and reduced overrefusal&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jiawei Liu', 'Nirav Diwan', 'Zhe Wang', 'Haoyu Zhai', 'Xiaona Zhou', 'Kiet A. Nguyen', 'Tianjiao Yu', 'Muntasir Wahed', 'Yinlin Deng', 'Hadjer Benkraouda', 'Yuxiang Wei', 'Lingming Zhang', 'Ismini Lourentzou', 'Gang Wang']&lt;/li&gt;&lt;li&gt;Tags: ['red teaming', 'adversarial prompting', 'safety evaluation', 'model alignment', 'secure code generation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2507.19060</guid><pubDate>Mon, 28 Jul 2025 00:00:00 +0000</pubDate></item><item><title>Blind Spot Navigation: Evolutionary Discovery of Sensitive Semantic Concepts for LVLMs</title><link>https://arxiv.org/abs/2505.15265</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes an evolutionary framework using LLMs and T2I models to discover sensitive semantic concepts for LVLMs&lt;/li&gt;&lt;li&gt;Generates image descriptions via LLM crossover/mutation and converts them to visual inputs&lt;/li&gt;&lt;li&gt;Evaluates LVLM performance on these inputs to identify concepts causing hallucinations/errors&lt;/li&gt;&lt;li&gt;Demonstrates effectiveness on 7 LVLMs and 2 multimodal tasks&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Zihao Pan', 'Yu Tong', 'Weibin Wu', 'Jingyi Wang', 'Lifeng Chen', 'Zhe Zhao', 'Jiajia Wei', 'Yitong Qiao', 'Zibin Zheng']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial attacks', 'red teaming', 'semantic concepts', 'large vision-language models', 'evolutionary algorithms', 'robustness evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2505.15265</guid><pubDate>Mon, 28 Jul 2025 00:00:00 +0000</pubDate></item><item><title>Kill two birds with one stone: generalized and robust AI-generated text detection via dynamic perturbations</title><link>https://arxiv.org/abs/2504.21019</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes DP-Net for AI-generated text detection&lt;/li&gt;&lt;li&gt;Uses dynamic perturbations via reinforcement learning&lt;/li&gt;&lt;li&gt;Improves generalization across domains&lt;/li&gt;&lt;li&gt;Enhances robustness against adversarial attacks&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yinghan Zhou', 'Juan Wen', 'Wanli Peng', 'Yiming Xue', 'Ziwei Zhang', 'Zhengxian Wu']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial attacks', 'robustness', 'AIGT detection', 'text perturbations', 'reinforcement learning']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2504.21019</guid><pubDate>Mon, 28 Jul 2025 00:00:00 +0000</pubDate></item><item><title>Model Tampering Attacks Enable More Rigorous Evaluations of LLM Capabilities</title><link>https://arxiv.org/abs/2502.05209</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes model tampering attacks for evaluating LLM risks&lt;/li&gt;&lt;li&gt;Compares input-space and model tampering attacks&lt;/li&gt;&lt;li&gt;Shows resilience to capability elicitation is low-dimensional&lt;/li&gt;&lt;li&gt;Demonstrates unlearning methods can be undone with fine-tuning&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Zora Che', 'Stephen Casper', 'Robert Kirk', 'Anirudh Satheesh', 'Stewart Slocum', 'Lev E McKinney', 'Rohit Gandikota', 'Aidan Ewart', 'Domenic Rosati', 'Zichu Wu', 'Zikui Cai', 'Bilal Chughtai', 'Yarin Gal', 'Furong Huang', 'Dylan Hadfield-Menell']&lt;/li&gt;&lt;li&gt;Tags: ['red teaming', 'adversarial attacks', 'model tampering', 'safety evaluation', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2502.05209</guid><pubDate>Mon, 28 Jul 2025 00:00:00 +0000</pubDate></item><item><title>Evaluation of LLM Vulnerabilities to Being Misused for Personalized Disinformation Generation</title><link>https://arxiv.org/abs/2412.13666</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Evaluates LLM vulnerabilities to generating personalized disinformation&lt;/li&gt;&lt;li&gt;Finds that personalization reduces safety filter activations, acting as a jailbreak&lt;/li&gt;&lt;li&gt;Highlights need for stronger safety filters and disclaimers&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Aneta Zugecova', 'Dominik Macko', 'Ivan Srba', 'Robert Moro', 'Jakub Kopal', 'Katarina Marcincinova', 'Matus Mesarcik']&lt;/li&gt;&lt;li&gt;Tags: ['jailbreaking', 'safety evaluation', 'adversarial prompting', 'disinformation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2412.13666</guid><pubDate>Mon, 28 Jul 2025 00:00:00 +0000</pubDate></item><item><title>Secret Collusion among AI Agents: Multi-Agent Deception via Steganography</title><link>https://arxiv.org/abs/2402.07510</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Formalizes the problem of secret collusion via steganography in AI agents&lt;/li&gt;&lt;li&gt;Proposes mitigation measures and evaluation framework&lt;/li&gt;&lt;li&gt;Empirical testing across multiple LLMs including GPT-4&lt;/li&gt;&lt;li&gt;Highlights need for continuous monitoring of steganographic capabilities&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Sumeet Ramesh Motwani', 'Mikhail Baranchuk', 'Martin Strohmeier', 'Vijay Bolina', 'Philip H. S. Torr', 'Lewis Hammond', 'Christian Schroeder de Witt']&lt;/li&gt;&lt;li&gt;Tags: ['steganography', 'privacy attacks', 'AI security', 'collusion', 'mitigation measures']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2402.07510</guid><pubDate>Mon, 28 Jul 2025 00:00:00 +0000</pubDate></item><item><title>Running in CIRCLE? A Simple Benchmark for LLM Code Interpreter Security</title><link>https://arxiv.org/abs/2507.19399</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces CIRCLE benchmark for evaluating LLM code interpreter security&lt;/li&gt;&lt;li&gt;Assesses 1,260 prompts across CPU, memory, disk resource exhaustion risks&lt;/li&gt;&lt;li&gt;Evaluates model refusal rates and code execution safety&lt;/li&gt;&lt;li&gt;Highlights significant vulnerabilities and defense weaknesses against indirect prompts&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Gabriel Chua']&lt;/li&gt;&lt;li&gt;Tags: ['LLM security', 'red teaming', 'code interpreter', 'benchmark', 'adversarial prompting', 'safety evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2507.19399</guid><pubDate>Mon, 28 Jul 2025 00:00:00 +0000</pubDate></item><item><title>PrompTrend: Continuous Community-Driven Vulnerability Discovery and Assessment for Large Language Models</title><link>https://arxiv.org/abs/2507.19185</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces PrompTrend, a system for continuous LLM vulnerability discovery using community data&lt;/li&gt;&lt;li&gt;Evaluates 198 vulnerabilities across 9 models, revealing psychological attacks are more effective&lt;/li&gt;&lt;li&gt;Develops a multidimensional scoring framework with 78% classification accuracy&lt;/li&gt;&lt;li&gt;Challenges assumptions about capability vs security and highlights community-driven threats&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Tarek Gasmi', 'Ramzi Guesmi', 'Mootez Aloui', 'Jihene Bennaceur']&lt;/li&gt;&lt;li&gt;Tags: ['LLM security', 'red teaming', 'vulnerability assessment', 'community-driven', 'psychological attacks', 'adversarial prompting', 'safety evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2507.19185</guid><pubDate>Mon, 28 Jul 2025 00:00:00 +0000</pubDate></item><item><title>Specification Self-Correction: Mitigating In-Context Reward Hacking Through Test-Time Refinement</title><link>https://arxiv.org/abs/2507.18742</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces Specification Self-Correction (SSC) framework to mitigate in-context reward hacking&lt;/li&gt;&lt;li&gt;Reduces vulnerability to specification exploitation by over 90% in experiments&lt;/li&gt;&lt;li&gt;Test-time correction process without model weight modification&lt;/li&gt;&lt;li&gt;Improves robust alignment through dynamic specification repair&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ["V\\'ictor Gallego"]&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'robustness', 'safety evaluation', 'adversarial prompting']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2507.18742</guid><pubDate>Mon, 28 Jul 2025 00:00:00 +0000</pubDate></item></channel></rss>