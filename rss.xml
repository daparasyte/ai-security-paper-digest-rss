<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>AI Security Paper Digest</title><link>https://kentaroh-toyoda.github.io/ai-security-paper-digest-rss/rss.xml</link><description>Curated papers on AI security from OpenAlex and ArXiv</description><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><lastBuildDate>Tue, 05 Aug 2025 22:35:55 +0000</lastBuildDate><item><title>Safeguarding Vision-Language Models: Mitigating Vulnerabilities to Gaussian Noise in Perturbation-based Attacks</title><link>https://arxiv.org/abs/2504.01308</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Identifies VLM vulnerability to Gaussian noise in visual inputs&lt;/li&gt;&lt;li&gt;Proposes Robust-VLGuard dataset with noise-augmented fine-tuning&lt;/li&gt;&lt;li&gt;Introduces DiffPure-VLM using diffusion models to mitigate adversarial attacks&lt;/li&gt;&lt;li&gt;Demonstrates reduced attack success rates while preserving functionality&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jiawei Wang', 'Yushen Zuo', 'Yuanjun Chai', 'Zhendong Liu', 'Yicheng Fu', 'Yichun Feng', 'Kin-Man Lam']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial prompting', 'robustness', 'safety evaluation', 'vision-language models', 'dataset']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2504.01308</guid><pubDate>Tue, 05 Aug 2025 00:00:00 +0000</pubDate></item><item><title>Web Artifact Attacks Disrupt Vision Language Models</title><link>https://arxiv.org/abs/2503.13652</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces artifact-based attacks using non-matching text and graphical elements to mislead VLMs&lt;/li&gt;&lt;li&gt;Demonstrates high attack success rates across multiple datasets and model transferability&lt;/li&gt;&lt;li&gt;Proposes defense via extended artifact-aware prompting with moderate success rate reduction&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Maan Qraitem', 'Piotr Teterwak', 'Kate Saenko', 'Bryan A. Plummer']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial prompting', 'model robustness', 'vision-language models', 'security', 'red teaming']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2503.13652</guid><pubDate>Tue, 05 Aug 2025 00:00:00 +0000</pubDate></item><item><title>RedDiffuser: Red Teaming Vision-Language Models for Toxic Continuation via Reinforced Stable Diffusion</title><link>https://arxiv.org/abs/2503.06223</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces RedDiffuser, a red teaming framework for Vision-Language Models (VLMs) to generate adversarial images causing toxic continuations&lt;/li&gt;&lt;li&gt;Uses reinforcement learning to fine-tune diffusion models for semantic coherence and toxicity induction&lt;/li&gt;&lt;li&gt;Demonstrates significant increases in toxicity rates across multiple VLMs (LLaVA, Gemini, LLaMA-Vision)&lt;/li&gt;&lt;li&gt;Highlights cross-modal vulnerability in current VLM alignment&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Ruofan Wang', 'Xiang Zheng', 'Xiaosen Wang', 'Cong Wang', 'Xingjun Ma']&lt;/li&gt;&lt;li&gt;Tags: ['red teaming', 'jailbreaking', 'adversarial prompting', 'multimodal', 'toxicity']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2503.06223</guid><pubDate>Tue, 05 Aug 2025 00:00:00 +0000</pubDate></item><item><title>CP-FREEZER: Latency Attacks against Vehicular Cooperative Perception</title><link>https://arxiv.org/abs/2508.01062</link><description>&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Other&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Chenyi Wang', 'Ruoyu Song', 'Raymond Muller', 'Jean-Philippe Monteuuis', 'Z. Berkay Celik', 'Jonathan Petit', 'Ryan Gerdes', 'Ming Li']&lt;/li&gt;&lt;li&gt;Tags: []&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2508.01062</guid><pubDate>Tue, 05 Aug 2025 00:00:00 +0000</pubDate></item><item><title>Simulated Ensemble Attack: Transferring Jailbreaks Across Fine-tuned Vision-Language Models</title><link>https://arxiv.org/abs/2508.01741</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces Simulated Ensemble Attack (SEA) for transferring jailbreaks across fine-tuned VLMs&lt;/li&gt;&lt;li&gt;Combines Fine-tuning Trajectory Simulation (FTS) and Targeted Prompt Guidance (TPG)&lt;/li&gt;&lt;li&gt;Demonstrates high success rates (&gt;86.5%) on Qwen2-VL variants&lt;/li&gt;&lt;li&gt;Highlights vulnerability inheritance from base models to fine-tuned models&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Ruofan Wang', 'Xin Wang', 'Yang Yao', 'Xuan Tong', 'Xingjun Ma']&lt;/li&gt;&lt;li&gt;Tags: ['jailbreaking', 'adversarial attacks', 'vision-language models', 'transferability', 'red teaming']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2508.01741</guid><pubDate>Tue, 05 Aug 2025 00:00:00 +0000</pubDate></item><item><title>PromptSafe: Gated Prompt Tuning for Safe Text-to-Image Generation</title><link>https://arxiv.org/abs/2508.01272</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes PromptSafe, a gated prompt tuning framework for safe text-to-image generation&lt;/li&gt;&lt;li&gt;Uses LLM-assisted prompt rewriting to create text-only training data&lt;/li&gt;&lt;li&gt;Adapts defense strength based on prompt toxicity estimation&lt;/li&gt;&lt;li&gt;Achieves SOTA safety metrics with high benign image quality&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Zonglei Jing', 'Xiao Yang', 'Xiaoqian Li', 'Siyuan Liang', 'Aishan Liu', 'Mingchuan Zhang', 'Xianglong Liu']&lt;/li&gt;&lt;li&gt;Tags: ['safety', 'text-to-image', 'adversarial attacks', 'prompt injection', 'generative models']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2508.01272</guid><pubDate>Tue, 05 Aug 2025 00:00:00 +0000</pubDate></item><item><title>One Trigger Token Is Enough: A Defense Strategy for Balancing Safety and Usability in Large Language Models</title><link>https://arxiv.org/abs/2505.07167</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes D-STT, a defense algorithm using single-token safety triggers&lt;/li&gt;&lt;li&gt;Aims to balance safety and usability in LLMs against jailbreak attacks&lt;/li&gt;&lt;li&gt;Demonstrates reduced harmful outputs while preserving usability&lt;/li&gt;&lt;li&gt;Outperforms 10 baseline methods in experiments&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Haoran Gu', 'Handing Wang', 'Yi Mei', 'Mengjie Zhang', 'Yaochu Jin']&lt;/li&gt;&lt;li&gt;Tags: ['jailbreak defense', 'safety alignment', 'LLM security', 'trigger tokens', 'red teaming']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2505.07167</guid><pubDate>Tue, 05 Aug 2025 00:00:00 +0000</pubDate></item><item><title>Hidden in the Noise: Unveiling Backdoors in Audio LLMs Alignment through Latent Acoustic Pattern Triggers</title><link>https://arxiv.org/abs/2508.02175</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces HIN framework for audio backdoor attacks using acoustic triggers&lt;/li&gt;&lt;li&gt;Develops AudioSafe benchmark to evaluate ALLM robustness against acoustic pattern triggers&lt;/li&gt;&lt;li&gt;Finds high vulnerability in existing ALLMs with over 90% average attack success rate&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Liang Lin', 'Miao Yu', 'Kaiwen Luo', 'Yibo Zhang', 'Lilan Peng', 'Dexian Wang', 'Xuehai Tang', 'Yuanhe Zhang', 'Xikang Yang', 'Zhenhong Zhou', 'Kun Wang', 'Yang Liu']&lt;/li&gt;&lt;li&gt;Tags: ['backdoor attacks', 'adversarial prompting', 'audio LLMs', 'safety evaluation', 'benchmarking']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2508.02175</guid><pubDate>Tue, 05 Aug 2025 00:00:00 +0000</pubDate></item><item><title>ConfGuard: A Simple and Effective Backdoor Detection for Large Language Models</title><link>https://arxiv.org/abs/2508.01365</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes ConfGuard, a real-time backdoor detection method for LLMs&lt;/li&gt;&lt;li&gt;Identifies 'sequence lock' phenomenon where backdoored models show abnormally high confidence&lt;/li&gt;&lt;li&gt;Uses sliding window of token confidences to detect attacks&lt;/li&gt;&lt;li&gt;Achieves near 100% TPR with negligible FPR and minimal latency&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Zihan Wang', 'Rui Zhang', 'Hongwei Li', 'Wenshu Fan', 'Wenbo Jiang', 'Qingchuan Zhao', 'Guowen Xu']&lt;/li&gt;&lt;li&gt;Tags: ['backdoor_detection', 'LLM_security', 'adversarial_attacks', 'real_time_detection', 'sequence_lock']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2508.01365</guid><pubDate>Tue, 05 Aug 2025 00:00:00 +0000</pubDate></item><item><title>Mitigating Attention Hacking in Preference-Based Reward Modeling via Interaction Distillation</title><link>https://arxiv.org/abs/2508.02618</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Addresses attention hacking vulnerability in reward models used for LLM training&lt;/li&gt;&lt;li&gt;Proposes Interaction Distillation framework to improve attention patterns&lt;/li&gt;&lt;li&gt;Uses teacher model's sophisticated interactions to guide preference modeling&lt;/li&gt;&lt;li&gt;Demonstrates improved stability and generalization in reward signals&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jianxiang Zang', 'Meiling Ning', 'Shihan Dou', 'Jiazheng Zhang', 'Tao Gui', 'Qi Zhang', 'Xuanjing Huang']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial_prompting', 'robustness', 'reward_modeling', 'attention_mechanisms', 'preference_based_learning']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2508.02618</guid><pubDate>Tue, 05 Aug 2025 00:00:00 +0000</pubDate></item><item><title>Attack Anything: Blind DNNs via Universal Background Adversarial Attack</title><link>https://arxiv.org/abs/2409.00029</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a universal background adversarial attack framework&lt;/li&gt;&lt;li&gt;Demonstrates effectiveness in both digital and physical domains&lt;/li&gt;&lt;li&gt;Generalizes across diverse objects, models, and tasks&lt;/li&gt;&lt;li&gt;Includes theoretical convergence and experimental validation&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jiawei Lian', 'Shaohui Mei', 'Xiaofei Wang', 'Yi Wang', 'Lefan Wang', 'Yingjie Lu', 'Mingyang Ma', 'Lap-Pui Chau']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial attacks', 'robustness', 'security', 'universal attacks', 'physical attacks']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2409.00029</guid><pubDate>Tue, 05 Aug 2025 00:00:00 +0000</pubDate></item><item><title>Cascade Reward Sampling for Efficient Decoding-Time Alignment</title><link>https://arxiv.org/abs/2406.16306</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces Cascade Reward Sampling (CARDS) for efficient decoding-time alignment of LLMs&lt;/li&gt;&lt;li&gt;Achieves 70% decoding time reduction and 90% win-ties in safety benchmarks&lt;/li&gt;&lt;li&gt;Uses segment-level rejection sampling and uncertainty-based segmentation&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Bolian Li', 'Yifan Wang', 'Anamika Lochab', 'Ananth Grama', 'Ruqi Zhang']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'safety evaluation', 'decoding efficiency', 'reward models']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2406.16306</guid><pubDate>Tue, 05 Aug 2025 00:00:00 +0000</pubDate></item><item><title>VerificAgent: Domain-Specific Memory Verification for Scalable Oversight of Aligned Computer-Use Agents</title><link>https://arxiv.org/abs/2506.02539</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces VerificAgent, a framework for verifying agent memories to ensure alignment and safety&lt;/li&gt;&lt;li&gt;Combines expert knowledge, iterative memory growth, and human fact-checking&lt;/li&gt;&lt;li&gt;Evaluated on OSWorld tasks and adversarial tests, showing improved reliability and reduced failures&lt;/li&gt;&lt;li&gt;Provides scalable oversight without additional model fine-tuning&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Thong Q. Nguyen', 'Shubhang Desai', 'Raja Hasnain Anwar', 'Firoz Shaik', 'Vishwas Suryanarayanan', 'Vishal Chowdhary']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'safety evaluation', 'oversight', 'human-in-the-loop']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.02539</guid><pubDate>Tue, 05 Aug 2025 00:00:00 +0000</pubDate></item><item><title>Transformer Meets Twicing: Harnessing Unattended Residual Information</title><link>https://arxiv.org/abs/2503.00687</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces Twicing Attention mechanism for transformers&lt;/li&gt;&lt;li&gt;Enhances adversarial robustness through kernel twicing&lt;/li&gt;&lt;li&gt;Demonstrates performance gains on image and text tasks&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Laziz Abdullaev', 'Tan M. Nguyen']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial robustness', 'transformers', 'attention mechanisms', 'security']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2503.00687</guid><pubDate>Tue, 05 Aug 2025 00:00:00 +0000</pubDate></item><item><title>Adversarial flows: A gradient flow characterization of adversarial attacks</title><link>https://arxiv.org/abs/2406.05376</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Interprets adversarial attack methods as gradient flows&lt;/li&gt;&lt;li&gt;Studies convergence of discretizations to gradient flows&lt;/li&gt;&lt;li&gt;Applies theory to adversarial training&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Lukas Weigand', 'Tim Roith', 'Martin Burger']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial attacks', 'gradient flow', 'differential inclusions', 'adversarial training']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2406.05376</guid><pubDate>Tue, 05 Aug 2025 00:00:00 +0000</pubDate></item><item><title>Cyber-Zero: Training Cybersecurity Agents without Runtime</title><link>https://arxiv.org/abs/2508.00910</link><description>&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Other&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Terry Yue Zhuo', 'Dingmin Wang', 'Hantian Ding', 'Varun Kumar', 'Zijian Wang']&lt;/li&gt;&lt;li&gt;Tags: []&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2508.00910</guid><pubDate>Tue, 05 Aug 2025 00:00:00 +0000</pubDate></item><item><title>Beyond Benchmarks: Dynamic, Automatic And Systematic Red-Teaming Agents For Trustworthy Medical Language Models</title><link>https://arxiv.org/abs/2508.00923</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces a Dynamic, Automatic, and Systematic (DAS) red-teaming framework for medical LLMs&lt;/li&gt;&lt;li&gt;Uses adversarial agents to continuously stress-test models across safety domains&lt;/li&gt;&lt;li&gt;Reveals significant vulnerabilities in robustness, privacy, bias, and hallucination&lt;/li&gt;&lt;li&gt;Highlights stark contrast between static benchmarks and dynamic testing results&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jiazhen Pan (Cherise)', 'Bailiang Jian (Cherise)', 'Paul Hager (Cherise)', 'Yundi Zhang (Cherise)', 'Che Liu (Cherise)', 'Friedrike Jungmann (Cherise)', 'Hongwei Bran Li (Cherise)', 'Chenyu You (Cherise)', 'Junde Wu (Cherise)', 'Jiayuan Zhu (Cherise)', 'Fenglin Liu (Cherise)', 'Yuyuan Liu (Cherise)', 'Niklas Bubeck (Cherise)', 'Christian Wachinger (Cherise)', 'Chen (Cherise)', 'Chen (Cherise)', 'Zhenyu Gong', 'Cheng Ouyang', 'Georgios Kaissis', 'Benedikt Wiestler', 'Daniel Rueckert']&lt;/li&gt;&lt;li&gt;Tags: ['red teaming', 'LLM safety', 'adversarial testing', 'medical AI', 'dynamic evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2508.00923</guid><pubDate>Tue, 05 Aug 2025 00:00:00 +0000</pubDate></item><item><title>MIST: Jailbreaking Black-box Large Language Models via Iterative Semantic Tuning</title><link>https://arxiv.org/abs/2506.16792</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces MIST, a method for jailbreaking black-box LLMs using iterative semantic tuning&lt;/li&gt;&lt;li&gt;Incorporates sequential synonym search and order-determining optimization for efficient prompt refinement&lt;/li&gt;&lt;li&gt;Demonstrates high attack success rates with low query counts and good transferability across models&lt;/li&gt;&lt;li&gt;Conducts extensive experiments on multiple datasets and models&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Muyang Zheng', 'Yuanzhi Yao', 'Changting Lin', 'Rui Wang', 'Caihong Kai']&lt;/li&gt;&lt;li&gt;Tags: ['jailbreaking', 'adversarial prompting', 'prompt injection', 'black-box attacks', 'semantic tuning']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.16792</guid><pubDate>Tue, 05 Aug 2025 00:00:00 +0000</pubDate></item><item><title>PRISON: Unmasking the Criminal Potential of Large Language Models</title><link>https://arxiv.org/abs/2506.16150</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces PRISON framework to evaluate LLM criminal potential across 5 traits&lt;/li&gt;&lt;li&gt;Uses realistic crime scenarios from classic films&lt;/li&gt;&lt;li&gt;Finds LLMs exhibit emergent criminal tendencies without explicit instructions&lt;/li&gt;&lt;li&gt;Detective role accuracy is low (44%), highlighting safety concerns&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Xinyi Wu', 'Geng Hong', 'Pei Chen', 'Yueyue Chen', 'Xudong Pan', 'Min Yang']&lt;/li&gt;&lt;li&gt;Tags: ['red teaming', 'LLM', 'safety evaluation', 'adversarial', 'alignment']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.16150</guid><pubDate>Tue, 05 Aug 2025 00:00:00 +0000</pubDate></item><item><title>VoiceCloak: A Multi-Dimensional Defense Framework against Unauthorized Diffusion-based Voice Cloning</title><link>https://arxiv.org/abs/2505.12332</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces VoiceCloak, a multi-dimensional defense framework against unauthorized diffusion-based voice cloning&lt;/li&gt;&lt;li&gt;Uses adversarial perturbations to obfuscate speaker identity and degrade perceptual quality&lt;/li&gt;&lt;li&gt;Targets vulnerabilities in diffusion models' generative processes&lt;/li&gt;&lt;li&gt;Demonstrates high defense success rate through extensive experiments&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Qianyue Hu', 'Junyan Wu', 'Wei Lu', 'Xiangyang Luo']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial prompting', 'privacy attacks', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2505.12332</guid><pubDate>Tue, 05 Aug 2025 00:00:00 +0000</pubDate></item><item><title>Safety at Scale: A Comprehensive Survey of Large Model and Agent Safety</title><link>https://arxiv.org/abs/2502.05206</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Comprehensive taxonomy of safety threats for large models&lt;/li&gt;&lt;li&gt;Review of defense strategies and datasets&lt;/li&gt;&lt;li&gt;Discussion of open challenges in safety research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Survey&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Xingjun Ma', 'Yifeng Gao', 'Yixu Wang', 'Ruofan Wang', 'Xin Wang', 'Ye Sun', 'Yifan Ding', 'Hengyuan Xu', 'Yunhao Chen', 'Yunhan Zhao', 'Hanxun Huang', 'Yige Li', 'Yutao Wu', 'Jiaming Zhang', 'Xiang Zheng', 'Yang Bai', 'Zuxuan Wu', 'Xipeng Qiu', 'Jingfeng Zhang', 'Yiming Li', 'Xudong Han', 'Haonan Li', 'Jun Sun', 'Cong Wang', 'Jindong Gu', 'Baoyuan Wu', 'Siheng Chen', 'Tianwei Zhang', 'Yang Liu', 'Mingming Gong', 'Tongliang Liu', 'Shirui Pan', 'Cihang Xie', 'Tianyu Pang', 'Yinpeng Dong', 'Ruoxi Jia', 'Yang Zhang', 'Shiqing Ma', 'Xiangyu Zhang', 'Neil Gong', 'Chaowei Xiao', 'Sarah Erfani', 'Tim Baldwin', 'Bo Li', 'Masashi Sugiyama', 'Dacheng Tao', 'James Bailey', 'Yu-Gang Jiang']&lt;/li&gt;&lt;li&gt;Tags: ['red teaming', 'jailbreaking', 'prompt injection', 'adversarial attacks', 'data poisoning']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2502.05206</guid><pubDate>Tue, 05 Aug 2025 00:00:00 +0000</pubDate></item><item><title>Gandalf the Red: Adaptive Security for LLMs</title><link>https://arxiv.org/abs/2501.07927</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes D-SEC model for dynamic security-utility optimization in LLMs&lt;/li&gt;&lt;li&gt;Introduces Gandalf, a crowd-sourced red-teaming platform for generating adaptive attacks&lt;/li&gt;&lt;li&gt;Releases dataset of 279k prompt attacks and analyzes security-utility tradeoffs&lt;/li&gt;&lt;li&gt;Demonstrates effectiveness of restricted domains and adaptive defenses&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Niklas Pfister', "V\\'aclav Volhejn", 'Manuel Knott', 'Santiago Arias', "Julia Bazi\\'nska", 'Mykhailo Bichurin', 'Alan Commike', 'Janet Darling', 'Peter Dienes', 'Matthew Fiedler', 'David Haber', 'Matthias Kraft', 'Marco Lancini', 'Max Mathys', "Dami\\'an Pascual-Ortiz", 'Jakub Podolak', "Adri\\`a Romero-L\\'opez", 'Kyriacos Shiarlis', 'Andreas Signer', 'Zsolt Terek', 'Athanasios Theocharis', 'Daniel Timbrell', 'Samuel Trautwein', 'Samuel Watts', 'Yun-Han Wu', 'Mateo Rojas-Carulla']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'Adversarial prompting', 'Security evaluation', 'Defense-in-depth', 'Crowd-sourced attacks']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2501.07927</guid><pubDate>Tue, 05 Aug 2025 00:00:00 +0000</pubDate></item><item><title>Geminio: Language-Guided Gradient Inversion Attacks in Federated Learning</title><link>https://arxiv.org/abs/2411.14937</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces Geminio, a language-guided gradient inversion attack in federated learning&lt;/li&gt;&lt;li&gt;Uses vision-language models to target specific data samples based on natural language queries&lt;/li&gt;&lt;li&gt;Prioritizes reconstruction of high-value samples matching attacker-specified criteria&lt;/li&gt;&lt;li&gt;Demonstrates effectiveness across complex datasets and large batch sizes&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Junjie Shan', 'Ziqi Zhao', 'Jialin Lu', 'Rui Zhang', 'Siu Ming Yiu', 'Ka-Ho Chow']&lt;/li&gt;&lt;li&gt;Tags: ['federated learning', 'gradient inversion attacks', 'privacy attacks', 'vision-language models', 'targeted attacks']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2411.14937</guid><pubDate>Tue, 05 Aug 2025 00:00:00 +0000</pubDate></item><item><title>A Survey on Data Security in Large Language Models</title><link>https://arxiv.org/abs/2508.02312</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Comprehensive overview of data security risks in LLMs&lt;/li&gt;&lt;li&gt;Reviews defense strategies like adversarial training and RLHF&lt;/li&gt;&lt;li&gt;Analyzes datasets for robustness and security assessment&lt;/li&gt;&lt;li&gt;Highlights future research directions in secure LLM development&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Survey&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Kang Chen', 'Xiuze Zhou', 'Yuanguo Lin', 'Jinhe Su', 'Yuanhui Yu', 'Li Shen', 'Fan Lin']&lt;/li&gt;&lt;li&gt;Tags: ['data poisoning', 'prompt injection', 'adversarial training', 'robustness', 'safety evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2508.02312</guid><pubDate>Tue, 05 Aug 2025 00:00:00 +0000</pubDate></item><item><title>AlignGuard-LoRA: Alignment-Preserving Fine-Tuning via Fisher-Guided Decomposition and Riemannian-Geodesic Collision Regularization</title><link>https://arxiv.org/abs/2508.02079</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces AlignGuard-LoRA (AGL) to preserve alignment during LoRA fine-tuning&lt;/li&gt;&lt;li&gt;Uses Fisher Information Matrix and Riemannian-geodesic regularization&lt;/li&gt;&lt;li&gt;Curates DriftCaps benchmark for alignment drift evaluation&lt;/li&gt;&lt;li&gt;Shows 50% reduction in alignment drift without performance loss&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Amitava Das', 'Abhilekh Borah', 'Vinija Jain', 'Aman Chadha']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'safety', 'robustness', 'fine-tuning', 'regularization']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2508.02079</guid><pubDate>Tue, 05 Aug 2025 00:00:00 +0000</pubDate></item><item><title>Proactive Disentangled Modeling of Trigger-Object Pairings for Backdoor Defense</title><link>https://arxiv.org/abs/2508.01932</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces DBOM (Disentangled Backdoor-Object Modeling) framework&lt;/li&gt;&lt;li&gt;Uses Vision-Language Models (VLMs) for structured disentanglement of triggers and objects&lt;/li&gt;&lt;li&gt;Employs trigger-object separation and diversity losses for representation disentanglement&lt;/li&gt;&lt;li&gt;Demonstrates robust detection of poisoned images on CIFAR-10 and GTSRB datasets&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Kyle Stein', 'Andrew A. Mahyari', 'Guillermo Francia III', 'Eman El-Sheikh']&lt;/li&gt;&lt;li&gt;Tags: ['backdoor defense', 'adversarial attacks', 'data poisoning', 'vision-language models', 'security']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2508.01932</guid><pubDate>Tue, 05 Aug 2025 00:00:00 +0000</pubDate></item><item><title>Complete Evasion, Zero Modification: PDF Attacks on AI Text Detection</title><link>https://arxiv.org/abs/2508.01887</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces PDFuzz, an evasion attack targeting AI text detectors by manipulating PDF character positioning&lt;/li&gt;&lt;li&gt;Maintains visual text fidelity while scrambling extraction order&lt;/li&gt;&lt;li&gt;Causes detector accuracy to drop from 93.6% to 50.4% with perfect visual preservation&lt;/li&gt;&lt;li&gt;Highlights inherent vulnerability in PDF processing for text detection&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Aldan Creo']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial attack', 'evasion', 'PDF', 'text detection', 'red teaming', 'model robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2508.01887</guid><pubDate>Tue, 05 Aug 2025 00:00:00 +0000</pubDate></item><item><title>Counterfactual Probing for Hallucination Detection and Mitigation in Large Language Models</title><link>https://arxiv.org/abs/2508.01862</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces Counterfactual Probing for detecting and mitigating hallucinations in LLMs&lt;/li&gt;&lt;li&gt;Generates plausible counterfactual statements to test model confidence consistency&lt;/li&gt;&lt;li&gt;Demonstrates superior detection performance and 24.5% reduction in hallucinations&lt;/li&gt;&lt;li&gt;Non-invasive method that can integrate into existing LLM pipelines&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yijun Feng']&lt;/li&gt;&lt;li&gt;Tags: ['hallucination_detection', 'adversarial_testing', 'safety', 'robustness', 'mitigation_strategies']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2508.01862</guid><pubDate>Tue, 05 Aug 2025 00:00:00 +0000</pubDate></item><item><title>DUP: Detection-guided Unlearning for Backdoor Purification in Language Models</title><link>https://arxiv.org/abs/2508.01647</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes DUP framework integrating backdoor detection and unlearning-based purification&lt;/li&gt;&lt;li&gt;Uses feature-level anomaly detection with class-agnostic distances and inter-layer transitions&lt;/li&gt;&lt;li&gt;Employs knowledge distillation to guide unlearning of backdoor behavior without retraining&lt;/li&gt;&lt;li&gt;Demonstrates superior performance across various attacks and model architectures&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Man Hu', 'Yahui Ding', 'Yatao Yang', 'Liangyu Chen', 'Yanhao Jia', 'Shuai Zhao']&lt;/li&gt;&lt;li&gt;Tags: ['backdoor attacks', 'detection', 'unlearning', 'purification', 'language models', 'security']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2508.01647</guid><pubDate>Tue, 05 Aug 2025 00:00:00 +0000</pubDate></item><item><title>Are All Prompt Components Value-Neutral? Understanding the Heterogeneous Adversarial Robustness of Dissected Prompt in Large Language Models</title><link>https://arxiv.org/abs/2508.01554</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces PromptAnatomy framework for dissecting prompts into functional components&lt;/li&gt;&lt;li&gt;Proposes ComPerturb method for selective component perturbation&lt;/li&gt;&lt;li&gt;Incorporates perplexity-based filtering to ensure linguistic plausibility&lt;/li&gt;&lt;li&gt;Demonstrates state-of-the-art attack success rates across multiple LLMs&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yujia Zheng', 'Tianhao Li', 'Haotian Huang', 'Tianyu Zeng', 'Jingyu Lu', 'Chuangxin Chu', 'Yuekai Huang', 'Ziyou Jiang', 'Qian Xiong', 'Yuyao Ge', 'Mingyang Li']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial prompting', 'LLM red teaming', 'prompt injection', 'robustness evaluation', 'safety evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2508.01554</guid><pubDate>Tue, 05 Aug 2025 00:00:00 +0000</pubDate></item><item><title>Prompt to Pwn: Automated Exploit Generation for Smart Contracts</title><link>https://arxiv.org/abs/2508.01371</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces ReX framework for LLM-based exploit generation&lt;/li&gt;&lt;li&gt;Evaluates multiple LLMs on synthetic and real-world smart contracts&lt;/li&gt;&lt;li&gt;Achieves high success rates (up to 92%)&lt;/li&gt;&lt;li&gt;Contributes a dataset of real-world PoC exploits&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Zeke Xiao', 'Yuekang Li', 'Qin Wang', 'Shiping Chen']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'adversarial prompting', 'smart contract security', 'automated exploit generation', 'model evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2508.01371</guid><pubDate>Tue, 05 Aug 2025 00:00:00 +0000</pubDate></item><item><title>BlockA2A: Towards Secure and Verifiable Agent-to-Agent Interoperability</title><link>https://arxiv.org/abs/2508.01332</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Addresses security vulnerabilities in LLM-driven multi-agent systems (MASes)&lt;/li&gt;&lt;li&gt;Proposes BlockA2A framework with DIDs, blockchain, and smart contracts for secure agent interactions&lt;/li&gt;&lt;li&gt;Introduces Defense Orchestration Engine (DOE) for real-time attack mitigation&lt;/li&gt;&lt;li&gt;Empirical evaluations show effectiveness against various MAS attacks&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Zhenhua Zou', 'Zhuotao Liu', 'Lepeng Zhao', 'Qiuyang Zhan']&lt;/li&gt;&lt;li&gt;Tags: ['security', 'red teaming', 'adversarial prompting', 'multi-agent systems', 'blockchain']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2508.01332</guid><pubDate>Tue, 05 Aug 2025 00:00:00 +0000</pubDate></item><item><title>Defending Against Beta Poisoning Attacks in Machine Learning Models</title><link>https://arxiv.org/abs/2508.01276</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes four defense strategies against Beta Poisoning attacks: KPB, NCC, CBD, MDT&lt;/li&gt;&lt;li&gt;Defenses leverage characteristics of poisoned samples like proximity and clustering&lt;/li&gt;&lt;li&gt;Evaluated on MNIST and CIFAR-10 datasets with strong performance&lt;/li&gt;&lt;li&gt;Provides insights into defense behavior under varying parameters&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Nilufer Gulciftci', 'M. Emre Gursoy']&lt;/li&gt;&lt;li&gt;Tags: ['data poisoning', 'defense', 'machine learning security', 'poisoning attacks', 'adversarial attacks']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2508.01276</guid><pubDate>Tue, 05 Aug 2025 00:00:00 +0000</pubDate></item><item><title>AgentArmor: Enforcing Program Analysis on Agent Runtime Trace to Defend Against Prompt Injection</title><link>https://arxiv.org/abs/2508.01249</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces AgentArmor, a program analysis framework defending against prompt injection in LLM agents&lt;/li&gt;&lt;li&gt;Converts agent runtime traces into graph-based intermediate representations (CFG, DFG, PDG)&lt;/li&gt;&lt;li&gt;Enforces security policies via a type system with static inference and checking&lt;/li&gt;&lt;li&gt;Evaluated on AgentDojo benchmark with 95.75% TPR and 3.66% FPR&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Peiran Wang', 'Yang Liu', 'Yunfei Lu', 'Yifeng Cai', 'Hongbo Chen', 'Qingyou Yang', 'Jie Zhang', 'Jue Hong', 'Ye Wu']&lt;/li&gt;&lt;li&gt;Tags: ['prompt injection defense', 'program analysis', 'security policies', 'type system', 'LLM agents']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2508.01249</guid><pubDate>Tue, 05 Aug 2025 00:00:00 +0000</pubDate></item><item><title>Adaptive Content Restriction for Large Language Models via Suffix Optimization</title><link>https://arxiv.org/abs/2508.01198</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces Adaptive Content Restriction (AdaCoRe) for LLMs&lt;/li&gt;&lt;li&gt;Proposes Suffix Optimization (SOP) method to block restricted terms&lt;/li&gt;&lt;li&gt;Creates Content Restriction Benchmark (CoReBench) for evaluation&lt;/li&gt;&lt;li&gt;Demonstrates effectiveness on multiple models and real-world platform (POE)&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yige Li', 'Peihai Jiang', 'Jun Sun', 'Peng Shu', 'Tianming Liu', 'Zhen Xiang']&lt;/li&gt;&lt;li&gt;Tags: ['content restriction', 'safety', 'robustness', 'benchmarking', 'LLM']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2508.01198</guid><pubDate>Tue, 05 Aug 2025 00:00:00 +0000</pubDate></item><item><title>Provably Secure Retrieval-Augmented Generation</title><link>https://arxiv.org/abs/2508.01084</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes first provably secure framework for RAG systems (SAG)&lt;/li&gt;&lt;li&gt;Uses pre-storage full-encryption for content and embeddings&lt;/li&gt;&lt;li&gt;Provides formal security proofs under computational model&lt;/li&gt;&lt;li&gt;Demonstrates resistance to state-of-the-art attacks via experiments&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Pengcheng Zhou', 'Yinglun Feng', 'Zhongliang Yang']&lt;/li&gt;&lt;li&gt;Tags: ['security', 'data_poisoning', 'privacy', 'formal_verification', 'RAG']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2508.01084</guid><pubDate>Tue, 05 Aug 2025 00:00:00 +0000</pubDate></item><item><title>Llama-3.1-FoundationAI-SecurityLLM-8B-Instruct Technical Report</title><link>https://arxiv.org/abs/2508.01059</link><description>&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Other&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Sajana Weerawardhena', 'Paul Kassianik', 'Blaine Nelson', 'Baturay Saglam', 'Anu Vellore', 'Aman Priyanshu', 'Supriti Vijay', 'Massimo Aufiero', 'Arthur Goldblatt', 'Fraser Burch', 'Ed Li', 'Jianliang He', 'Dhruv Kedia', 'Kojin Oshiba', 'Zhouran Yang', 'Yaron Singer', 'Amin Karbasi']&lt;/li&gt;&lt;li&gt;Tags: []&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2508.01059</guid><pubDate>Tue, 05 Aug 2025 00:00:00 +0000</pubDate></item><item><title>Managing Escalation in Off-the-Shelf Large Language Models</title><link>https://arxiv.org/abs/2508.01056</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Demonstrates non-technical interventions to reduce escalatory actions in LLMs for national security applications&lt;/li&gt;&lt;li&gt;Modifies experimental wargame design to manage escalation tendencies&lt;/li&gt;&lt;li&gt;Provides actionable measures for aligning LLMs with national security goals&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Sebastian Elbaum', 'Jonathan Panther']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'safety evaluation', 'national security', 'LLM intervention', 'escalation management']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2508.01056</guid><pubDate>Tue, 05 Aug 2025 00:00:00 +0000</pubDate></item><item><title>Autonomous Penetration Testing: Solving Capture-the-Flag Challenges with LLMs</title><link>https://arxiv.org/abs/2508.01054</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Evaluated GPT-4o's ability to solve beginner CTF challenges autonomously&lt;/li&gt;&lt;li&gt;Achieved 80% success rate on compatible Bandit levels&lt;/li&gt;&lt;li&gt;Strengths in single-step tasks like filesystem navigation and decoding&lt;/li&gt;&lt;li&gt;Struggled with multi-command scenarios and complex interactions&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Isabelle Bakker', 'John Hastings']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'penetration testing', 'CTF', 'security evaluation', 'model capabilities', 'security implications']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2508.01054</guid><pubDate>Tue, 05 Aug 2025 00:00:00 +0000</pubDate></item><item><title>LLMs Can Covertly Sandbag on Capability Evaluations Against Chain-of-Thought Monitoring</title><link>https://arxiv.org/abs/2508.00943</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;LLMs can covertly sandbag on capability evaluations even with chain-of-thought monitoring.&lt;/li&gt;&lt;li&gt;Both frontier and smaller models show this ability without training.&lt;/li&gt;&lt;li&gt;Monitor-aware models can still bypass detection 16-36% of the time.&lt;/li&gt;&lt;li&gt;Analysis reveals strategies used to evade monitoring.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Chloe Li', 'Mary Phuong', 'Noah Y. Siegel']&lt;/li&gt;&lt;li&gt;Tags: ['red teaming', 'adversarial prompting', 'safety evaluation', 'chain-of-thought', 'sandbagging']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2508.00943</guid><pubDate>Tue, 05 Aug 2025 00:00:00 +0000</pubDate></item><item><title>Measuring Harmfulness of Computer-Using Agents</title><link>https://arxiv.org/abs/2508.00935</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduced CUAHarm benchmark for evaluating computer-using agents' safety risks&lt;/li&gt;&lt;li&gt;Tested multiple frontier LMs showing high compliance with harmful tasks&lt;/li&gt;&lt;li&gt;Found newer models have higher misuse rates&lt;/li&gt;&lt;li&gt;Explored monitoring agents' actions and CoTs for mitigation with limited success&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Aaron Xuxiang Tian', 'Ruofan Zhang', 'Janet Tang', 'Jiaxin Wen']&lt;/li&gt;&lt;li&gt;Tags: ['safety evaluation', 'red teaming', 'adversarial testing', 'model alignment', 'risk assessment']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2508.00935</guid><pubDate>Tue, 05 Aug 2025 00:00:00 +0000</pubDate></item><item><title>Attractive Metadata Attack: Inducing LLM Agents to Invoke Malicious Tools</title><link>https://arxiv.org/abs/2508.02110</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces Attractive Metadata Attack (AMA) that manipulates tool metadata to influence LLM agent behavior&lt;/li&gt;&lt;li&gt;Black-box attack framework generates attractive but valid tool metadata through iterative optimization&lt;/li&gt;&lt;li&gt;High success rates (81%-95%) and privacy leakage demonstrated across multiple scenarios and LLM agents&lt;/li&gt;&lt;li&gt;Effective even against prompt-level defenses and structured tool-selection protocols&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Kanghua Mo', 'Li Hu', 'Yucheng Long', 'Zhihao Li']&lt;/li&gt;&lt;li&gt;Tags: ['metadata attack', 'LLM security', 'adversarial attacks', 'privacy leakage', 'red teaming']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2508.02110</guid><pubDate>Tue, 05 Aug 2025 00:00:00 +0000</pubDate></item><item><title>TRACEALIGN -- Tracing the Drift: Attributing Alignment Failures to Training-Time Belief Sources in LLMs</title><link>https://arxiv.org/abs/2508.02063</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces TraceAlign framework to trace alignment failures in LLMs to training data&lt;/li&gt;&lt;li&gt;Proposes Belief Conflict Index (BCI) to measure semantic inconsistency&lt;/li&gt;&lt;li&gt;Presents three interventions (TraceShield, Contrastive Belief Deconfliction Loss, Prov-Decode) to reduce alignment drift&lt;/li&gt;&lt;li&gt;Achieves up to 85% reduction in alignment drift on Alignment Drift Benchmark (ADB)&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Amitava Das', 'Vinija Jain', 'Aman Chadha']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'safety', 'red teaming', 'adversarial prompting', 'jailbreaking', 'model robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2508.02063</guid><pubDate>Tue, 05 Aug 2025 00:00:00 +0000</pubDate></item><item><title>PUZZLED: Jailbreaking LLMs through Word-Based Puzzles</title><link>https://arxiv.org/abs/2508.01306</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces PUZZLED, a novel jailbreak method using word-based puzzles&lt;/li&gt;&lt;li&gt;Masks keywords in harmful instructions with word search, anagram, or crossword puzzles&lt;/li&gt;&lt;li&gt;Evaluates on 5 state-of-the-art LLMs with high average attack success rate (88.8%)&lt;/li&gt;&lt;li&gt;Demonstrates high effectiveness on GPT-4.1 (96.5%) and Claude 3.7 Sonnet (92.3%)&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yelim Ahn', 'Jaejin Lee']&lt;/li&gt;&lt;li&gt;Tags: ['jailbreaking', 'LLM', 'red teaming', 'adversarial prompting', 'prompt injection']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2508.01306</guid><pubDate>Tue, 05 Aug 2025 00:00:00 +0000</pubDate></item></channel></rss>