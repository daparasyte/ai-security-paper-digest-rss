<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>AI Security Paper Digest</title><link>https://kentaroh-toyoda.github.io/ai-security-paper-digest-rss/rss.xml</link><description>Curated papers on AI security from OpenAlex and ArXiv</description><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><lastBuildDate>Fri, 11 Jul 2025 22:20:47 +0000</lastBuildDate><item><title>Rainbow Artifacts from Electromagnetic Signal Injection Attacks on Image Sensors</title><link>https://arxiv.org/abs/2507.07773</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Investigates electromagnetic attacks on image sensors causing visual artifacts&lt;/li&gt;&lt;li&gt;Shows these artifacts lead to mispredictions in object detection models&lt;/li&gt;&lt;li&gt;Highlights vulnerability in physical-layer security for AI systems&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Youqian Zhang', 'Xinyu Ji', 'Zhihao Wang', 'Qinhong Jiang']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial attacks', 'data poisoning', 'robustness', 'security standards', 'physical-layer attacks']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2507.07773</guid><pubDate>Fri, 11 Jul 2025 00:00:00 +0000</pubDate></item><item><title>One Object, Multiple Lies: A Benchmark for Cross-task Adversarial Attack on Unified Vision-Language Models</title><link>https://arxiv.org/abs/2507.07709</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces CrossVLAD benchmark for cross-task adversarial attacks on VLMs&lt;/li&gt;&lt;li&gt;Proposes CRAFT attack framework for efficient region-based attacks&lt;/li&gt;&lt;li&gt;Evaluates adversarial transferability across multiple tasks&lt;/li&gt;&lt;li&gt;Demonstrates effectiveness on Florence-2 and other VLMs&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jiale Zhao', 'Xinyang Jiang', 'Junyao Gao', 'Yuhao Xue', 'Cairong Zhao']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial prompting', 'robustness', 'safety evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2507.07709</guid><pubDate>Fri, 11 Jul 2025 00:00:00 +0000</pubDate></item><item><title>Evaluating Robustness of Large Audio Language Models to Audio Injection: An Empirical Study</title><link>https://arxiv.org/abs/2505.19598</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Systematically evaluates 5 leading LALMs across 4 audio injection attack scenarios&lt;/li&gt;&lt;li&gt;Introduces new metrics for defense success and robustness&lt;/li&gt;&lt;li&gt;Highlights vulnerability to early sequence injection and trade-offs between capability and safety&lt;/li&gt;&lt;li&gt;Emphasizes need for multimodal defenses and robust training pipelines&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Guanyu Hou', 'Jiaming He', 'Yinhang Zhou', 'Ji Guo', 'Yitong Qiao', 'Rui Zhang', 'Wenbo Jiang']&lt;/li&gt;&lt;li&gt;Tags: ['red teaming', 'adversarial prompting', 'audio injection', 'robustness evaluation', 'LALM security']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2505.19598</guid><pubDate>Fri, 11 Jul 2025 00:00:00 +0000</pubDate></item><item><title>Derailer-Rerailer: Adaptive Verification for Efficient and Reliable Language Model Reasoning</title><link>https://arxiv.org/abs/2408.13940</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces Derailer-Rerailer framework for adaptive LLM verification&lt;/li&gt;&lt;li&gt;Enhances reasoning accuracy while maintaining efficiency&lt;/li&gt;&lt;li&gt;Demonstrates improvements across multiple reasoning tasks&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Guangya Wan', 'Yuqi Wu', 'Hao Wang', 'Shengming Zhao', 'Jie Chen', 'Sheng Li']&lt;/li&gt;&lt;li&gt;Tags: ['robustness', 'safety evaluation', 'LLM', 'verification']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2408.13940</guid><pubDate>Fri, 11 Jul 2025 00:00:00 +0000</pubDate></item><item><title>Operationalizing a Threat Model for Red-Teaming Large Language Models (LLMs)</title><link>https://arxiv.org/abs/2407.14937</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Presents a threat model for red-teaming LLMs&lt;/li&gt;&lt;li&gt;Develops a taxonomy of attacks based on LLM lifecycle stages&lt;/li&gt;&lt;li&gt;Compiles defense methods and red-teaming strategies&lt;/li&gt;&lt;li&gt;Provides insights from previous research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Survey&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Apurv Verma', 'Satyapriya Krishna', 'Sebastian Gehrmann', 'Madhavan Seshadri', 'Anu Pradhan', 'Tom Ault', 'Leslie Barrett', 'David Rabinowitz', 'John Doucette', 'NhatHai Phan']&lt;/li&gt;&lt;li&gt;Tags: ['red teaming', 'threat model', 'LLM security', 'taxonomy', 'attacks']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2407.14937</guid><pubDate>Fri, 11 Jul 2025 00:00:00 +0000</pubDate></item><item><title>Lost in Pronunciation: Detecting Chinese Offensive Language Disguised by Phonetic Cloaking Replacement</title><link>https://arxiv.org/abs/2507.07640</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces a new dataset of real-world Phonetic Cloaking Replacement (PCR) offensive posts&lt;/li&gt;&lt;li&gt;Benchmarking shows state-of-the-art LLMs struggle with PCR detection (F1 0.672)&lt;/li&gt;&lt;li&gt;Revisits and improves Pinyin-based prompting for better detection accuracy&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Haotan Guo', 'Jianfei He', 'Jiayuan Ma', 'Hongbin Na', 'Zimu Wang', 'Haiyang Zhang', 'Qi Chen', 'Wei Wang', 'Zijing Shi', 'Tao Shen', 'Ling Chen']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial prompting', 'robustness', 'safety evaluation', 'content moderation', 'Chinese NLP']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2507.07640</guid><pubDate>Fri, 11 Jul 2025 00:00:00 +0000</pubDate></item><item><title>Medical Red Teaming Protocol of Language Models: On the Importance of User Perspectives in Healthcare Settings</title><link>https://arxiv.org/abs/2507.07248</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduced a medical red teaming protocol for LLMs with patient and clinician perspectives&lt;/li&gt;&lt;li&gt;Created PatientSafetyBench benchmark with 466 samples across 5 critical categories&lt;/li&gt;&lt;li&gt;Evaluated MediPhi model collection using the new protocol&lt;/li&gt;&lt;li&gt;First work to define safety criteria through targeted red-teaming for medical LLMs&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Minseon Kim', 'Jean-Philippe Corbeil', 'Alessandro Sordoni', 'Francois Beaulieu', 'Paul Vozila']&lt;/li&gt;&lt;li&gt;Tags: ['red teaming', 'safety evaluation', 'medical LLMs', 'benchmarks', 'alignment']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2507.07248</guid><pubDate>Fri, 11 Jul 2025 00:00:00 +0000</pubDate></item><item><title>Watermarking Degrades Alignment in Language Models: Analysis and Mitigation</title><link>https://arxiv.org/abs/2506.04462</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Analyzes how watermarking affects alignment properties (truthfulness, safety, helpfulness) in LLMs&lt;/li&gt;&lt;li&gt;Identifies two degradation patterns: guard attenuation and guard amplification&lt;/li&gt;&lt;li&gt;Proposes Alignment Resampling (AR) method using a reward model to restore alignment&lt;/li&gt;&lt;li&gt;Demonstrates AR's effectiveness in recovering alignment while maintaining watermark detectability&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Apurv Verma', 'NhatHai Phan', 'Shubhendu Trivedi']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'safety evaluation', 'robustness', 'model extraction']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.04462</guid><pubDate>Fri, 11 Jul 2025 00:00:00 +0000</pubDate></item><item><title>Emoji Attack: Enhancing Jailbreak Attacks Against Judge LLM Detection</title><link>https://arxiv.org/abs/2411.01077</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces Emoji Attack, a novel jailbreak technique exploiting token segmentation bias in Judge LLMs&lt;/li&gt;&lt;li&gt;Uses in-context learning to insert emojis into prompts, causing embedding distortions&lt;/li&gt;&lt;li&gt;Demonstrates significant reduction in unsafe content detection rates against state-of-the-art Judge LLMs&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Zhipeng Wei', 'Yuqi Liu', 'N. Benjamin Erichson']&lt;/li&gt;&lt;li&gt;Tags: ['jailbreaking', 'adversarial_prompting', 'token_segmentation', 'emojis', 'LLM_security']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2411.01077</guid><pubDate>Fri, 11 Jul 2025 00:00:00 +0000</pubDate></item><item><title>"I am bad": Interpreting Stealthy, Universal and Robust Audio Jailbreaks in Audio-Language Models</title><link>https://arxiv.org/abs/2502.00718</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Demonstrates universal audio jailbreaks in Audio-Language Models (ALMs)&lt;/li&gt;&lt;li&gt;Analyzes how ALMs interpret adversarial audio examples&lt;/li&gt;&lt;li&gt;Reveals imperceptible toxic speech embedded in audio perturbations&lt;/li&gt;&lt;li&gt;Provides insights for defending against adversarial audio attacks&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Isha Gupta', 'David Khachaturov', 'Robert Mullins']&lt;/li&gt;&lt;li&gt;Tags: ['jailbreaking', 'adversarial attacks', 'audio modality', 'multimodal models', 'safety evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2502.00718</guid><pubDate>Fri, 11 Jul 2025 00:00:00 +0000</pubDate></item><item><title>Adaptive Randomized Smoothing: Certified Adversarial Robustness for Multi-Step Defences</title><link>https://arxiv.org/abs/2406.10427</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces Adaptive Randomized Smoothing (ARS) for certifying model predictions against adversarial examples&lt;/li&gt;&lt;li&gt;Extends randomized smoothing with f-Differential Privacy to handle multi-step adaptive defenses&lt;/li&gt;&lt;li&gt;Demonstrates improved accuracy on image classification benchmarks (CIFAR-10, CelebA, ImageNet)&lt;/li&gt;&lt;li&gt;Enables high-dimensional input-dependent masking for flexible adaptation&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Saiyue Lyu', 'Shadab Shaikh', 'Frederick Shpilevskiy', 'Evan Shelhamer', "Mathias L\\'ecuyer"]&lt;/li&gt;&lt;li&gt;Tags: ['adversarial robustness', 'randomized smoothing', 'differential privacy', 'image classification', 'multi-step defenses']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2406.10427</guid><pubDate>Fri, 11 Jul 2025 00:00:00 +0000</pubDate></item><item><title>Image Can Bring Your Memory Back: A Novel Multi-Modal Guided Attack against Image Generation Model Unlearning</title><link>https://arxiv.org/abs/2507.07139</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces Recall, a novel adversarial framework targeting unlearned image generation models&lt;/li&gt;&lt;li&gt;Uses multi-modal approach combining text and image prompts for more effective attacks&lt;/li&gt;&lt;li&gt;Demonstrates superior performance against 10 state-of-the-art unlearning methods&lt;/li&gt;&lt;li&gt;Highlights critical vulnerabilities in current model unlearning techniques&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Renyang Liu', 'Guanlin Li', 'Tianwei Zhang', 'See-Kiong Ng']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial prompting', 'robustness', 'red teaming', 'image generation', 'multi-modal']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2507.07139</guid><pubDate>Fri, 11 Jul 2025 00:00:00 +0000</pubDate></item><item><title>GuardVal: Dynamic Large Language Model Jailbreak Evaluation for Comprehensive Safety Testing</title><link>https://arxiv.org/abs/2507.07735</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces GuardVal, a dynamic jailbreak evaluation protocol for LLMs&lt;/li&gt;&lt;li&gt;Uses dynamic prompt generation and optimization to refine attacks&lt;/li&gt;&lt;li&gt;Evaluates across multiple models (Mistral-7b to GPT-4) and 10 safety domains&lt;/li&gt;&lt;li&gt;Provides insights into model robustness and safety vulnerabilities&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Peiyan Zhang', 'Haibo Jin', 'Liying Kang', 'Haohan Wang']&lt;/li&gt;&lt;li&gt;Tags: ['jailbreak', 'red teaming', 'adversarial prompting', 'safety evaluation', 'LLM security']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2507.07735</guid><pubDate>Fri, 11 Jul 2025 00:00:00 +0000</pubDate></item><item><title>An attention-aware GNN-based input defender against multi-turn jailbreak on LLMs</title><link>https://arxiv.org/abs/2507.07146</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes G-Guard, an attention-aware GNN-based input classifier to defend against multi-turn jailbreak attacks on LLMs&lt;/li&gt;&lt;li&gt;Constructs entity graphs for multi-turn queries to capture relationships between harmful keywords&lt;/li&gt;&lt;li&gt;Introduces attention-aware augmentation to retrieve similar single-turn queries for enhanced classification&lt;/li&gt;&lt;li&gt;Demonstrates superior performance over baselines across datasets and metrics&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Zixuan Huang', 'Kecheng Huang', 'Lihao Yin', 'Bowei He', 'Huiling Zhen', 'Mingxuan Yuan', 'Zili Shao']&lt;/li&gt;&lt;li&gt;Tags: ['jailbreak', 'red teaming', 'adversarial prompting', 'graph neural networks', 'multi-turn attacks']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2507.07146</guid><pubDate>Fri, 11 Jul 2025 00:00:00 +0000</pubDate></item><item><title>MAEBE: Multi-Agent Emergent Behavior Framework</title><link>https://arxiv.org/abs/2506.03053</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces MAEBE framework for evaluating multi-agent AI safety&lt;/li&gt;&lt;li&gt;Demonstrates brittleness in LLM moral preferences with question framing&lt;/li&gt;&lt;li&gt;Highlights unpredictable ensemble behavior due to emergent dynamics&lt;/li&gt;&lt;li&gt;Shows peer pressure effects in supervised multi-agent setups&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Sinem Erisken (Independent Researcher)', 'Timothy Gothard (Independent Researcher)', 'Martin Leitgab (Independent Researcher)', 'Ram Potham (Independent Researcher)']&lt;/li&gt;&lt;li&gt;Tags: ['multi-agent systems', 'safety evaluation', 'alignment', 'emergent behavior', 'moral preferences']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.03053</guid><pubDate>Fri, 11 Jul 2025 00:00:00 +0000</pubDate></item><item><title>Evaluating LLM Agent Adherence to Hierarchical Safety Principles: A Lightweight Benchmark for Probing Foundational Controllability Components</title><link>https://arxiv.org/abs/2506.02357</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduced a lightweight benchmark to evaluate LLM adherence to hierarchical safety principles&lt;/li&gt;&lt;li&gt;Found a 'cost of compliance' where safety constraints degrade task performance&lt;/li&gt;&lt;li&gt;Discovered an 'illusion of compliance' where high adherence masks task incompetence&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Ram Potham (Independent Researcher)']&lt;/li&gt;&lt;li&gt;Tags: ['safety', 'LLM', 'benchmark', 'evaluation', 'alignment']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.02357</guid><pubDate>Fri, 11 Jul 2025 00:00:00 +0000</pubDate></item><item><title>BountyBench: Dollar Impact of AI Agent Attackers and Defenders on Real-World Cybersecurity Systems</title><link>https://arxiv.org/abs/2505.15216</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces BountyBench framework for evaluating AI agents in cybersecurity tasks&lt;/li&gt;&lt;li&gt;Tests agents on real-world systems with bug bounty vulnerabilities&lt;/li&gt;&lt;li&gt;Evaluates detection, exploitation, and patching capabilities&lt;/li&gt;&lt;li&gt;Compares performance of various AI agents including Claude, OpenAI Codex, and custom agents&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Andy K. Zhang', 'Joey Ji', 'Celeste Menders', 'Riya Dulepet', 'Thomas Qin', 'Ron Y. Wang', 'Junrong Wu', 'Kyleen Liao', 'Jiliang Li', 'Jinghan Hu', 'Sara Hong', 'Nardos Demilew', 'Shivatmica Murgai', 'Jason Tran', 'Nishka Kacheria', 'Ethan Ho', 'Denis Liu', 'Lauren McLane', 'Olivia Bruvik', 'Dai-Rong Han', 'Seungwoo Kim', 'Akhil Vyas', 'Cuiyuanxiu Chen', 'Ryan Li', 'Weiran Xu', 'Jonathan Z. Ye', 'Prerit Choudhary', 'Siddharth M. Bhatia', 'Vikram Sivashankar', 'Yuxuan Bao', 'Dawn Song', 'Dan Boneh', 'Daniel E. Ho', 'Percy Liang']&lt;/li&gt;&lt;li&gt;Tags: ['red teaming', 'AI security', 'cybersecurity', 'vulnerability detection', 'exploit development']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2505.15216</guid><pubDate>Fri, 11 Jul 2025 00:00:00 +0000</pubDate></item><item><title>Access Controls Will Solve the Dual-Use Dilemma</title><link>https://arxiv.org/abs/2505.09341</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes access control framework for managing dual-use AI outputs&lt;/li&gt;&lt;li&gt;Aims to reduce both over-refusals and under-refusals in safety systems&lt;/li&gt;&lt;li&gt;Relies on user verification to determine access to potentially risky capabilities&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Position&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Ev\\v{z}en Wybitul']&lt;/li&gt;&lt;li&gt;Tags: ['safety', 'access control', 'dual-use', 'policy', 'framework']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2505.09341</guid><pubDate>Fri, 11 Jul 2025 00:00:00 +0000</pubDate></item><item><title>Constrain Alignment with Sparse Autoencoders</title><link>https://arxiv.org/abs/2411.07618</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes Feature-level constrained Preference Optimization (FPO) for efficient LLM alignment&lt;/li&gt;&lt;li&gt;Uses Sparse Autoencoders (SAEs) and feature-level constraints&lt;/li&gt;&lt;li&gt;Achieves 5.08% absolute improvement in win rate with lower computational cost&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Qingyu Yin', 'Chak Tou Leong', 'Minjun Zhu', 'Hanqi Yan', 'Qiang Zhang', 'Yulan He', 'Wenjie Li', 'Jun Wang', 'Yue Zhang', 'Linyi Yang']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'sparse autoencoders', 'FPO', 'efficiency']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2411.07618</guid><pubDate>Fri, 11 Jul 2025 00:00:00 +0000</pubDate></item><item><title>Machine Bullshit: Characterizing the Emergent Disregard for Truth in Large Language Models</title><link>https://arxiv.org/abs/2507.07484</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces Bullshit Index metric and taxonomy of machine bullshit types&lt;/li&gt;&lt;li&gt;Empirical evaluations on Marketplace, Political Neutrality, and new BullshitEval datasets&lt;/li&gt;&lt;li&gt;Finds RLHF exacerbates bullshit and CoT amplifies specific forms&lt;/li&gt;&lt;li&gt;Highlights AI alignment challenges and truthfulness concerns&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Kaiqu Liang', 'Haimin Hu', 'Xuandong Zhao', 'Dawn Song', 'Thomas L. Griffiths', "Jaime Fern\\'andez Fisac"]&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'safety evaluation', 'LLM red teaming', 'truthfulness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2507.07484</guid><pubDate>Fri, 11 Jul 2025 00:00:00 +0000</pubDate></item><item><title>May I have your Attention? Breaking Fine-Tuning based Prompt Injection Defenses using Architecture-Aware Attacks</title><link>https://arxiv.org/abs/2507.07417</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Evaluates prompt injection defenses using fine-tuning&lt;/li&gt;&lt;li&gt;Introduces a novel attention-based attack algorithm&lt;/li&gt;&lt;li&gt;Tests against SecAlign and StruQ with up to 70% success rate&lt;/li&gt;&lt;li&gt;Highlights robustness issues in whitebox defense scenarios&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Nishit V. Pandya', 'Andrey Labunets', 'Sicun Gao', 'Earlence Fernandes']&lt;/li&gt;&lt;li&gt;Tags: ['prompt injection', 'adversarial prompting', 'red teaming', 'LLM security', 'whitebox attacks']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2507.07417</guid><pubDate>Fri, 11 Jul 2025 00:00:00 +0000</pubDate></item><item><title>Autonomous AI-based Cybersecurity Framework for Critical Infrastructure: Real-Time Threat Mitigation</title><link>https://arxiv.org/abs/2507.07416</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes an AI-driven cybersecurity framework for critical infrastructure&lt;/li&gt;&lt;li&gt;Addresses real-time threat detection, modelling, and remediation&lt;/li&gt;&lt;li&gt;Considers adversarial AI and regulatory compliance&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jenifer Paulraj', 'Brindha Raghuraman', 'Nagarani Gopalakrishnan', 'Yazan Otoum']&lt;/li&gt;&lt;li&gt;Tags: ['AI cybersecurity', 'critical infrastructure', 'adversarial AI', 'threat mitigation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2507.07416</guid><pubDate>Fri, 11 Jul 2025 00:00:00 +0000</pubDate></item><item><title>Phishing Detection in the Gen-AI Era: Quantized LLMs vs Classical Models</title><link>https://arxiv.org/abs/2507.07406</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Comparative evaluation of ML, DL, and quantized LLMs for phishing detection&lt;/li&gt;&lt;li&gt;Investigates adversarial robustness via LLM-rephrased emails&lt;/li&gt;&lt;li&gt;Assesses model accuracy, VRAM usage, and cost-performance tradeoffs&lt;/li&gt;&lt;li&gt;Highlights potential for explainable, efficient LLM-based defense&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jikesh Thapa', 'Gurrehmat Chahal', 'Serban Voinea Gabreanu', 'Yazan Otoum']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial robustness', 'phishing detection', 'LLMs', 'model evaluation', 'red teaming']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2507.07406</guid><pubDate>Fri, 11 Jul 2025 00:00:00 +0000</pubDate></item><item><title>Exploiting Edge Features for Transferable Adversarial Attacks in Distributed Machine Learning</title><link>https://arxiv.org/abs/2507.07259</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Exploits intercepted intermediate features in distributed ML systems&lt;/li&gt;&lt;li&gt;Reconstructs tensor shapes and trains surrogate models&lt;/li&gt;&lt;li&gt;Demonstrates improved transferability of adversarial attacks&lt;/li&gt;&lt;li&gt;Highlights security risks in edge-cloud model splitting&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Giulio Rossolini', 'Fabio Brau', 'Alessandro Biondi', 'Battista Biggio', 'Giorgio Buttazzo']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial attacks', 'model extraction', 'distributed ML security', 'edge computing', 'black-box attacks']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2507.07259</guid><pubDate>Fri, 11 Jul 2025 00:00:00 +0000</pubDate></item><item><title>On the Impossibility of Separating Intelligence from Judgment: The Computational Intractability of Filtering for AI Alignment</title><link>https://arxiv.org/abs/2507.07341</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Demonstrates computational intractability of filtering prompts and outputs for LLMs&lt;/li&gt;&lt;li&gt;Shows adversarial prompts are indistinguishable from benign ones for efficient filters&lt;/li&gt;&lt;li&gt;Proves output filtering is intractable under cryptographic assumptions&lt;/li&gt;&lt;li&gt;Concludes alignment requires integrating judgment with intelligence, not external filters&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Sarah Ball', 'Greg Gluch', 'Shafi Goldwasser', 'Frauke Kreuter', 'Omer Reingold', 'Guy N. Rothblum']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'adversarial prompting', 'safety evaluation', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2507.07341</guid><pubDate>Fri, 11 Jul 2025 00:00:00 +0000</pubDate></item></channel></rss>