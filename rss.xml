<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>AI Security Paper Digest</title><link>https://kentaroh-toyoda.github.io/ai-security-paper-digest-rss/rss.xml</link><description>Curated papers on AI security from OpenAlex and ArXiv</description><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><lastBuildDate>Thu, 10 Jul 2025 22:19:57 +0000</lastBuildDate><item><title>Towards Adversarial Robustness via Debiased High-Confidence Logit Alignment</title><link>https://arxiv.org/abs/2408.06079</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Addresses vulnerability of DNNs to adversarial examples by proposing Debiased High-Confidence Adversarial Training (DHAT)&lt;/li&gt;&lt;li&gt;DHAT aligns adversarial logits with debiased high-confidence logits and enhances foreground logit orthogonality&lt;/li&gt;&lt;li&gt;Achieves state-of-the-art robustness on CIFAR and ImageNet-1K benchmarks while improving generalization&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Kejia Zhang', 'Juanjuan Weng', 'Shaozi Li', 'Zhiming Luo']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial', 'robustness', 'security', 'debiasing', 'image']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2408.06079</guid><pubDate>Thu, 10 Jul 2025 00:00:00 +0000</pubDate></item><item><title>GuidedBench: Measuring and Mitigating the Evaluation Discrepancies of In-the-wild LLM Jailbreak Methods</title><link>https://arxiv.org/abs/2502.16903</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces GuidedBench, a benchmark for evaluating LLM jailbreak methods with case-specific criteria&lt;/li&gt;&lt;li&gt;Highlights significant evaluation discrepancies in existing jailbreak studies due to lack of nuanced criteria&lt;/li&gt;&lt;li&gt;Demonstrates GuidedEval reduces inter-evaluator variance by 76.03%&lt;/li&gt;&lt;li&gt;Advocates for case-by-case evaluation paradigm to improve accuracy and comparability&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Ruixuan Huang', 'Xunguang Wang', 'Zongjie Li', 'Daoyuan Wu', 'Shuai Wang']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'jailbreaking', 'evaluation', 'benchmarking', 'security standards']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2502.16903</guid><pubDate>Thu, 10 Jul 2025 00:00:00 +0000</pubDate></item><item><title>Single Word Change is All You Need: Designing Attacks and Defenses for Text Classifiers</title><link>https://arxiv.org/abs/2401.17196</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces a metric (œÅ) to measure robustness against single-word perturbations&lt;/li&gt;&lt;li&gt;Presents SP-Attack for generating adversarial examples with minimal changes&lt;/li&gt;&lt;li&gt;Proposes SP-Defense using data augmentation to improve robustness&lt;/li&gt;&lt;li&gt;Demonstrates significant improvements in classifier robustness and attack resistance&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Lei Xu', 'Sarah Alnegheimish', 'Laure Berti-Equille', 'Alfredo Cuesta-Infante', 'Kalyan Veeramachaneni']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial attacks', 'robustness', 'safety evaluation', 'data augmentation', 'text classification']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2401.17196</guid><pubDate>Thu, 10 Jul 2025 00:00:00 +0000</pubDate></item><item><title>On the Robustness of Verbal Confidence of LLMs in Adversarial Attacks</title><link>https://arxiv.org/abs/2507.06489</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces a novel framework for attacking verbal confidence in LLMs using perturbation and jailbreak methods&lt;/li&gt;&lt;li&gt;Demonstrates significant vulnerabilities in current confidence elicitation methods across various prompts, model sizes, and domains&lt;/li&gt;&lt;li&gt;Finds existing defense techniques are largely ineffective or counterproductive&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Stephen Obadinma', 'Xiaodan Zhu']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial attacks', 'jailbreaking', 'LLM confidence', 'red teaming', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2507.06489</guid><pubDate>Thu, 10 Jul 2025 00:00:00 +0000</pubDate></item><item><title>Reward Models Can Improve Themselves: Reward-Guided Adversarial Failure Mode Discovery for Robust Reward Modeling</title><link>https://arxiv.org/abs/2507.06419</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces REFORM, a self-improving reward modeling framework&lt;/li&gt;&lt;li&gt;Uses reward-guided controlled decoding to discover failure modes&lt;/li&gt;&lt;li&gt;Augments training data with adversarial examples to enhance robustness&lt;/li&gt;&lt;li&gt;Demonstrates improved robustness on HH and PKU Beavertails datasets&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Pankayaraj Pathmanathan', 'Furong Huang']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial prompting', 'robustness', 'safety evaluation', 'model extraction']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2507.06419</guid><pubDate>Thu, 10 Jul 2025 00:00:00 +0000</pubDate></item><item><title>Protecting Classifiers From Attacks</title><link>https://arxiv.org/abs/2004.08705</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Presents a Bayesian decision theoretic framework for protecting classifiers against adversarial attacks&lt;/li&gt;&lt;li&gt;Incorporates adversarial risk analysis to handle uncertainty about attacker behavior&lt;/li&gt;&lt;li&gt;Proposes methods for both operational and training-time robustification&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Victor Gallego', 'Roi Naveiro', 'Alberto Redondo', 'David Rios Insua', 'Fabrizio Ruggeri']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial attacks', 'robustness', 'Bayesian methods', 'adversarial risk analysis', 'classifier security']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2004.08705</guid><pubDate>Thu, 10 Jul 2025 00:00:00 +0000</pubDate></item><item><title>LoRAShield: Data-Free Editing Alignment for Secure Personalized LoRA Sharing</title><link>https://arxiv.org/abs/2507.07056</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces LoRAShield, a data-free editing framework for securing LoRA models against misuse&lt;/li&gt;&lt;li&gt;Uses adversarial optimization and semantic augmentation to realign LoRA weight subspaces&lt;/li&gt;&lt;li&gt;Demonstrates high effectiveness in blocking malicious content generation while preserving benign functionality&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jiahao Chen', 'junhao li', 'Yiming Wang', 'Zhe Ma', 'Yi Jiang', 'Chunyi Zhou', 'Qingming Li', 'Tianyu Du', 'Shouling Ji']&lt;/li&gt;&lt;li&gt;Tags: ['red teaming', 'alignment', 'robustness', 'adversarial prompting', 'model security']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2507.07056</guid><pubDate>Thu, 10 Jul 2025 00:00:00 +0000</pubDate></item><item><title>Robust and Safe Traffic Sign Recognition using N-version with Weighted Voting</title><link>https://arxiv.org/abs/2507.06907</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes N-version machine learning (NVML) framework with safety-aware weighted soft voting for traffic sign recognition&lt;/li&gt;&lt;li&gt;Utilizes Failure Mode and Effects Analysis (FMEA) to dynamically assign safety weights to ensemble outputs&lt;/li&gt;&lt;li&gt;Evaluated robustness against FGSM and PGD adversarial attacks&lt;/li&gt;&lt;li&gt;Demonstrates significant improvements in robustness and safety under adversarial conditions&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Linyun Gao', 'Qiang Wen', 'Fumio Machida']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial attacks', 'robustness', 'safety evaluation', 'ensemble methods']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2507.06907</guid><pubDate>Thu, 10 Jul 2025 00:00:00 +0000</pubDate></item><item><title>Saffron-1: Safety Inference Scaling</title><link>https://arxiv.org/abs/2506.06444</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces SAFFRON, a novel inference scaling paradigm for LLM safety assurance&lt;/li&gt;&lt;li&gt;Addresses exploration-efficiency dilemma in safety-critical inference scaling&lt;/li&gt;&lt;li&gt;Proposes multifurcation reward model (MRM) with partial supervision training&lt;/li&gt;&lt;li&gt;Releases Saffron-1 model and Safety4M dataset for research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Ruizhong Qiu', 'Gaotang Li', 'Tianxin Wei', 'Jingrui He', 'Hanghang Tong']&lt;/li&gt;&lt;li&gt;Tags: ['LLM safety', 'red teaming', 'jailbreaking', 'adversarial prompting', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.06444</guid><pubDate>Thu, 10 Jul 2025 00:00:00 +0000</pubDate></item><item><title>Real AI Agents with Fake Memories: Fatal Context Manipulation Attacks on Web3 Agents</title><link>https://arxiv.org/abs/2503.16248</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces context manipulation attacks on Web3 AI agents&lt;/li&gt;&lt;li&gt;Showcases memory injection vulnerability leading to unauthorized asset transfers&lt;/li&gt;&lt;li&gt;Presents CrAIBench benchmark with 500+ attack test cases&lt;/li&gt;&lt;li&gt;Evaluates defenses against prompt and memory injection attacks&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Atharv Singh Patlan', 'Peiyao Sheng', 'S. Ashwin Hebbar', 'Prateek Mittal', 'Pramod Viswanath']&lt;/li&gt;&lt;li&gt;Tags: ['red teaming', 'prompt injection', 'adversarial prompting', 'Web3 security', 'memory injection']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2503.16248</guid><pubDate>Thu, 10 Jul 2025 00:00:00 +0000</pubDate></item><item><title>Breaking PEFT Limitations: Leveraging Weak-to-Strong Knowledge Transfer for Backdoor Attacks in LLMs</title><link>https://arxiv.org/abs/2409.17946</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a novel backdoor attack method (FAKD) for LLMs using PEFT&lt;/li&gt;&lt;li&gt;Utilizes a teacher model poisoned via FPFT to transfer knowledge to the student model&lt;/li&gt;&lt;li&gt;Achieves near 100% success rates in backdoor attacks on PEFT models&lt;/li&gt;&lt;li&gt;Addresses limitations of existing PEFT-based backdoor attacks&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Shuai Zhao', 'Leilei Gan', 'Zhongliang Guo', 'Xiaobao Wu', 'Yanhao Jia', 'Luwei Xiao', 'Cong-Duy Nguyen', 'Luu Anh Tuan']&lt;/li&gt;&lt;li&gt;Tags: ['backdoor attacks', 'data poisoning', 'PEFT', 'knowledge distillation', 'LLM security']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2409.17946</guid><pubDate>Thu, 10 Jul 2025 00:00:00 +0000</pubDate></item><item><title>Can adversarial attacks by large language models be attributed?</title><link>https://arxiv.org/abs/2411.08003</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Analyzes the identifiability of LLMs in adversarial settings using formal language theory&lt;/li&gt;&lt;li&gt;Shows that certain classes of LLMs are fundamentally non-identifiable from outputs alone&lt;/li&gt;&lt;li&gt;Demonstrates exponential growth in plausible model origins, making attribution infeasible&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Manuel Cebrian', 'Andres Abeliuk', 'Jan Arne Telle']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial attacks', 'model attribution', 'security', 'theoretical analysis', 'empirical analysis']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2411.08003</guid><pubDate>Thu, 10 Jul 2025 00:00:00 +0000</pubDate></item><item><title>VisualTrap: A Stealthy Backdoor Attack on GUI Agents via Visual Grounding Manipulation</title><link>https://arxiv.org/abs/2507.06899</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces VisualTrap, a stealthy backdoor attack on GUI agents via visual grounding manipulation&lt;/li&gt;&lt;li&gt;Injects poisoned data during pre-training to hijack visual grounding with invisible triggers&lt;/li&gt;&lt;li&gt;Attack generalizes across environments and persists after clean fine-tuning&lt;/li&gt;&lt;li&gt;Highlights urgent security risks in GUI agents powered by LVLMs&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Ziang Ye', 'Yang Zhang', 'Wentao Shi', 'Xiaoyu You', 'Fuli Feng', 'Tat-Seng Chua']&lt;/li&gt;&lt;li&gt;Tags: ['backdoor attack', 'data poisoning', 'visual grounding', 'GUI agents', 'stealthy attacks', 'LVLM', 'pre-training', 'cross-environment generalization']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2507.06899</guid><pubDate>Thu, 10 Jul 2025 00:00:00 +0000</pubDate></item><item><title>IAP: Invisible Adversarial Patch Attack through Perceptibility-Aware Localization and Perturbation Optimization</title><link>https://arxiv.org/abs/2507.06856</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces IAP, a novel adversarial patch attack framework&lt;/li&gt;&lt;li&gt;Uses perceptibility-aware localization and perturbation optimization&lt;/li&gt;&lt;li&gt;Achieves high invisibility and bypasses state-of-the-art defenses&lt;/li&gt;&lt;li&gt;Validated across multiple image benchmarks and model architectures&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Subrat Kishore Dutta', 'Xiao Zhang']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial attacks', 'security', 'computer vision', 'stealth', 'perturbation optimization']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2507.06856</guid><pubDate>Thu, 10 Jul 2025 00:00:00 +0000</pubDate></item><item><title>The Dark Side of LLMs Agent-based Attacks for Complete Computer Takeover</title><link>https://arxiv.org/abs/2507.06850</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Demonstrates three attack vectors against LLM agents: direct prompt injection, RAG backdoor attacks, and inter-agent trust exploitation&lt;/li&gt;&lt;li&gt;Evaluates 17 state-of-the-art LLMs including GPT-4o, Claude-4, and Gemini-2.5&lt;/li&gt;&lt;li&gt;Reveals high vulnerability rates across models, with 82.4% compromised via inter-agent trust&lt;/li&gt;&lt;li&gt;Highlights critical security flaws in multi-agent systems and calls for increased research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Matteo Lupinacci', 'Francesco Aurelio Pironti', 'Francesco Blefari', 'Francesco Romeo', 'Luigi Arena', 'Angelo Furfaro']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'prompt injection', 'adversarial prompting', 'security evaluation', 'multi-agent systems']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2507.06850</guid><pubDate>Thu, 10 Jul 2025 00:00:00 +0000</pubDate></item><item><title>Foundation Model Self-Play: Open-Ended Strategy Innovation via Foundation Models</title><link>https://arxiv.org/abs/2507.06466</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces Foundation-Model Self-Play (FMSP) for strategy innovation&lt;/li&gt;&lt;li&gt;Evaluates FMSP in Gandalf AI safety simulation for jailbreaking LLM defenses&lt;/li&gt;&lt;li&gt;Successfully red-teams through multiple defense levels and patches vulnerabilities&lt;/li&gt;&lt;li&gt;Combines diversity and quality in policy discovery&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Aaron Dharna', 'Cong Lu', 'Jeff Clune']&lt;/li&gt;&lt;li&gt;Tags: ['red teaming', 'jailbreaking', 'adversarial prompting', 'AI safety', 'self-play']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2507.06466</guid><pubDate>Thu, 10 Jul 2025 00:00:00 +0000</pubDate></item><item><title>Bridging AI and Software Security: A Comparative Vulnerability Assessment of LLM Agent Deployment Paradigms</title><link>https://arxiv.org/abs/2507.06323</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Comparative security evaluation of Function Calling and MCP LLM agent paradigms&lt;/li&gt;&lt;li&gt;Tested 3,250 attack scenarios including prompt injection and software vulnerabilities&lt;/li&gt;&lt;li&gt;Found significant differences in attack success rates between paradigms&lt;/li&gt;&lt;li&gt;Demonstrated that architectural choices reshape threat landscapes&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Tarek Gasmi', 'Ramzi Guesmi', 'Ines Belhadj', 'Jihene Bennaceur']&lt;/li&gt;&lt;li&gt;Tags: ['red teaming', 'prompt injection', 'security evaluation', 'LLM agents', 'threat modeling']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2507.06323</guid><pubDate>Thu, 10 Jul 2025 00:00:00 +0000</pubDate></item><item><title>The bitter lesson of misuse detection</title><link>https://arxiv.org/abs/2507.06282</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduced BELLS benchmark for evaluating LLM supervision systems&lt;/li&gt;&lt;li&gt;Revealed poor performance of specialized supervisors against diverse jailbreaks&lt;/li&gt;&lt;li&gt;Found generalist LLMs outperform market supervisors in misuse detection&lt;/li&gt;&lt;li&gt;Highlighted need for general capabilities in misuse detection&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Hadrien Mariaccia', 'Charbel-Rapha\\"el Segerie', 'Diego Dorn']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'jailbreaking', 'adversarial prompting', 'safety evaluation', 'supervision systems']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2507.06282</guid><pubDate>Thu, 10 Jul 2025 00:00:00 +0000</pubDate></item><item><title>Enhancing LLM Watermark Resilience Against Both Scrubbing and Spoofing Attacks</title><link>https://arxiv.org/abs/2507.06274</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces SEEK watermarking for LLMs to enhance resilience against scrubbing and spoofing attacks&lt;/li&gt;&lt;li&gt;Breaks the trade-off between scrubbing and spoofing resistance through equivalent texture keys&lt;/li&gt;&lt;li&gt;Achieves significant robustness gains in experiments across multiple datasets&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Huanming Shen', 'Baizhou Huang', 'Xiaojun Wan']&lt;/li&gt;&lt;li&gt;Tags: ['watermarking', 'scrubbing', 'spoofing', 'LLM security', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2507.06274</guid><pubDate>Thu, 10 Jul 2025 00:00:00 +0000</pubDate></item><item><title>Attacker's Noise Can Manipulate Your Audio-based LLM in the Real World</title><link>https://arxiv.org/abs/2507.06256</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Demonstrates adversarial audio perturbations can manipulate audio-based LLMs&lt;/li&gt;&lt;li&gt;Shows real-world impact through airborne noise affecting multiple users&lt;/li&gt;&lt;li&gt;Tests transferrability of attacks and potential defenses&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Vinu Sankar Sadasivan', 'Soheil Feizi', 'Rajiv Mathews', 'Lun Wang']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial_prompting', 'audio_attacks', 'real_world_scenarios', 'model_robustness', 'red_teaming']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2507.06256</guid><pubDate>Thu, 10 Jul 2025 00:00:00 +0000</pubDate></item><item><title>Emergent misalignment as prompt sensitivity: A research note</title><link>https://arxiv.org/abs/2507.06253</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Insecure models show high sensitivity to prompt nudges in refusal and free-form question settings&lt;/li&gt;&lt;li&gt;Asking models to be 'evil' increases misaligned responses, while 'HHH' reduces them&lt;/li&gt;&lt;li&gt;Insecure models change responses when user disagrees in factual recall tasks&lt;/li&gt;&lt;li&gt;Models rate perceived misalignment of prompts, correlating with actual misaligned responses&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Tim Wyse', 'Twm Stone', 'Anna Soligo', 'Daniel Tan']&lt;/li&gt;&lt;li&gt;Tags: ['prompt_injection', 'adversarial_prompting', 'model_alignment', 'red_team', 'emergent_misalignment']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2507.06253</guid><pubDate>Thu, 10 Jul 2025 00:00:00 +0000</pubDate></item><item><title>False Alarms, Real Damage: Adversarial Attacks Using LLM-based Models on Text-based Cyber Threat Intelligence Systems</title><link>https://arxiv.org/abs/2507.06252</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Investigates adversarial attacks on cyber threat intelligence (CTI) pipelines using LLM-based models&lt;/li&gt;&lt;li&gt;Focuses on evasion, flooding, and poisoning attacks through fake text generation&lt;/li&gt;&lt;li&gt;Demonstrates how adversarial text can mislead classifiers and disrupt system functionality&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Samaneh Shafee', 'Alysson Bessani', 'Pedro M. Ferreira']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial attacks', 'data poisoning', 'security', 'LLM', 'red teaming', 'text']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2507.06252</guid><pubDate>Thu, 10 Jul 2025 00:00:00 +0000</pubDate></item></channel></rss>