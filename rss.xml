<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>AI Security Paper Digest</title><link>https://kentaroh-toyoda.github.io/ai-security-paper-digest-rss/rss.xml</link><description>Curated papers on AI security from ArXiv</description><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><lastBuildDate>Fri, 12 Sep 2025 22:17:34 +0000</lastBuildDate><item><title>TESSER: Transfer-Enhancing Adversarial Attacks from Vision Transformers via Spectral and Semantic Regularization</title><link>https://arxiv.org/abs/2505.19613</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces TESSER, a novel adversarial attack framework enhancing transferability from Vision Transformers&lt;/li&gt;&lt;li&gt;Employs Feature-Sensitive Gradient Scaling (FSGS) and Spectral Smoothness Regularization (SSR)&lt;/li&gt;&lt;li&gt;Achieves +10.9% higher ASR on CNNs and +7.2% on ViTs compared to state-of-the-art&lt;/li&gt;&lt;li&gt;Significantly improves robustness against defended models with 53.55% ASR on adversarially trained CNNs&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Amira Guesmi', 'Bassem Ouni', 'Muhammad Shafique']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial attacks', 'transferability', 'vision transformers', 'CNNs', 'spectral regularization', 'black-box attacks', 'model robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2505.19613</guid><pubDate>Fri, 12 Sep 2025 00:00:00 +0000</pubDate></item><item><title>AdvReal: Physical Adversarial Patch Generation Framework for Security Evaluation of Object Detection Systems</title><link>https://arxiv.org/abs/2505.16402</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposed AdvReal framework generates physical adversarial patches for 2D and 3D object detection&lt;/li&gt;&lt;li&gt;Achieves 70.13% attack success rate on YOLOv12 in physical scenarios&lt;/li&gt;&lt;li&gt;Demonstrates robustness across multiple viewpoints, distances, and lighting conditions&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yuanhao Huang', 'Yilong Ren', 'Jinlei Wang', 'Lujia Huo', 'Xuesong Bai', 'Jinchuan Zhang', 'Haiyan Yu']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial examples', 'security evaluation', 'robustness', 'object detection']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2505.16402</guid><pubDate>Fri, 12 Sep 2025 00:00:00 +0000</pubDate></item><item><title>IDEATOR: Jailbreaking and Benchmarking Large Vision-Language Models Using Themselves</title><link>https://arxiv.org/abs/2411.00827</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes IDEATOR, a method for jailbreaking VLMs using the models themselves to generate malicious image-text pairs&lt;/li&gt;&lt;li&gt;Achieves high attack success rates (ASR) across multiple VLMs including MiniGPT-4, LLaVA, InstructBLIP, and Chameleon&lt;/li&gt;&lt;li&gt;Introduces VLJailbreakBench, a safety benchmark with 3,654 multimodal jailbreak samples&lt;/li&gt;&lt;li&gt;Evaluates 11 VLMs revealing significant safety gaps (e.g., 46.31% ASR on GPT-4o, 19.65% on Claude-3.5-Sonnet)&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Ruofan Wang', 'Juncheng Li', 'Yixu Wang', 'Bo Wang', 'Xiaosen Wang', 'Yan Teng', 'Yingchun Wang', 'Xingjun Ma', 'Yu-Gang Jiang']&lt;/li&gt;&lt;li&gt;Tags: ['jailbreaking', 'red teaming', 'vision-language models', 'safety benchmark', 'adversarial prompting']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2411.00827</guid><pubDate>Fri, 12 Sep 2025 00:00:00 +0000</pubDate></item><item><title>ASTPrompter: Preference-Aligned Automated Language Model Red-Teaming to Generate Low-Perplexity Unsafe Prompts</title><link>https://arxiv.org/abs/2407.09447</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces ASTPrompter, a new red-teaming method for LLMs focusing on low-perplexity unsafe prompts&lt;/li&gt;&lt;li&gt;Uses contrastive preference learning to balance attack success rate (ASR) and perplexity&lt;/li&gt;&lt;li&gt;Achieves higher ASR with lower perplexity compared to existing methods&lt;/li&gt;&lt;li&gt;Demonstrates transferability across multiple LLMs in black- and white-box settings&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Amelia F. Hardy', 'Houjun Liu', 'Allie Griffith', 'Bernard Lange', 'Duncan Eddy', 'Mykel J. Kochenderfer']&lt;/li&gt;&lt;li&gt;Tags: ['red teaming', 'adversarial prompting', 'LLM security', 'perplexity', 'contrastive learning']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2407.09447</guid><pubDate>Fri, 12 Sep 2025 00:00:00 +0000</pubDate></item><item><title>Steering MoE LLMs via Expert (De)Activation</title><link>https://arxiv.org/abs/2509.09660</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces SteerMoE framework for controlling MoE LLM behaviors via expert (de)activation&lt;/li&gt;&lt;li&gt;Detects experts with distinct activation patterns linked to specific behaviors&lt;/li&gt;&lt;li&gt;Demonstrates both safety improvements (+20%) and adversarial attacks (-41% safety, -100% with jailbreaks)&lt;/li&gt;&lt;li&gt;Evaluated across 11 benchmarks and 6 LLMs&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Mohsen Fayyaz', 'Ali Modarressi', 'Hanieh Deilamsalehy', 'Franck Dernoncourt', 'Ryan Rossi', 'Trung Bui', 'Hinrich Sch\\"utze', 'Nanyun Peng']&lt;/li&gt;&lt;li&gt;Tags: ['MoE', 'LLM steering', 'Adversarial attacks', 'Safety', 'Red teaming', 'Jailbreaking', 'Alignment']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.09660</guid><pubDate>Fri, 12 Sep 2025 00:00:00 +0000</pubDate></item><item><title>MetaRAG: Metamorphic Testing for Hallucination Detection in RAG Systems</title><link>https://arxiv.org/abs/2509.09360</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces MetaRAG, a metamorphic testing framework for hallucination detection in RAG systems&lt;/li&gt;&lt;li&gt;Operates in real-time, unsupervised, black-box setting without ground truth or model access&lt;/li&gt;&lt;li&gt;Localizes unsupported claims at specific factoid spans for identity-aware AI&lt;/li&gt;&lt;li&gt;Evaluated on proprietary enterprise dataset for trustworthy deployment&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Channdeth Sok', 'David Luz', 'Yacine Haddam']&lt;/li&gt;&lt;li&gt;Tags: ['safety evaluation', 'hallucination detection', 'RAG systems', 'metamorphic testing', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.09360</guid><pubDate>Fri, 12 Sep 2025 00:00:00 +0000</pubDate></item><item><title>Improving LLM Safety and Helpfulness using SFT and DPO: A Study on OPT-350M</title><link>https://arxiv.org/abs/2509.09055</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Investigates SFT, DPO, and combined alignment techniques on OPT-350M&lt;/li&gt;&lt;li&gt;Introduces HmR, HpR, and CAS metrics for safety evaluation&lt;/li&gt;&lt;li&gt;Finds SFT+DPO outperforms individual methods&lt;/li&gt;&lt;li&gt;Highlights challenges with noisy data and resource constraints&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Piyush Pant']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'safety evaluation', 'SFT', 'DPO', 'LLM']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.09055</guid><pubDate>Fri, 12 Sep 2025 00:00:00 +0000</pubDate></item><item><title>ACE: A Security Architecture for LLM-Integrated App Systems</title><link>https://arxiv.org/abs/2504.20984</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Identifies new attacks on LLM-integrated app systems affecting planning and execution integrity&lt;/li&gt;&lt;li&gt;Proposes ACE architecture with abstract/concrete planning phases and secure execution&lt;/li&gt;&lt;li&gt;Verifies security via static analysis and enforces data/capability barriers&lt;/li&gt;&lt;li&gt;Evaluated against prompt injection benchmarks and custom attacks&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Evan Li', 'Tushin Mallick', 'Evan Rose', 'William Robertson', 'Alina Oprea', 'Cristina Nita-Rotaru']&lt;/li&gt;&lt;li&gt;Tags: ['LLM security', 'red teaming', 'prompt injection', 'security architecture', 'privacy attacks', 'integrity', 'availability']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2504.20984</guid><pubDate>Fri, 12 Sep 2025 00:00:00 +0000</pubDate></item><item><title>On the Relationship Between Adversarial Robustness and Decision Region in Deep Neural Networks</title><link>https://arxiv.org/abs/2207.03400</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces Populated Region Set (PRS) as a measure of adversarial robustness&lt;/li&gt;&lt;li&gt;Shows low PRS ratio correlates with better adversarial robustness&lt;/li&gt;&lt;li&gt;Proposes PRS regularizer to improve robustness without adversarial training&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Seongjin Park', 'Haedong Jeong', 'Tair Djanibekov', 'Giyoung Jeon', 'Jinseok Seol', 'Jaesik Choi']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial robustness', 'deep neural networks', 'security metrics']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2207.03400</guid><pubDate>Fri, 12 Sep 2025 00:00:00 +0000</pubDate></item><item><title>Steering MoE LLMs via Expert (De)Activation</title><link>https://arxiv.org/abs/2509.09660</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces SteerMoE framework to detect and control behavior-linked experts in MoE LLMs&lt;/li&gt;&lt;li&gt;Demonstrates ability to improve safety (+20%) and faithfulness (+27%) by deactivating specific experts&lt;/li&gt;&lt;li&gt;Adversarial attack mode can drop safety by -41% alone and -100% when combined with existing jailbreak methods&lt;/li&gt;&lt;li&gt;Highlights new dimension of alignment faking within expert subnetworks&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Mohsen Fayyaz', 'Ali Modarressi', 'Hanieh Deilamsalehy', 'Franck Dernoncourt', 'Ryan Rossi', 'Trung Bui', 'Hinrich Sch\\"utze', 'Nanyun Peng']&lt;/li&gt;&lt;li&gt;Tags: ['MoE', 'adversarial attack', 'jailbreak', 'safety', 'red teaming']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.09660</guid><pubDate>Fri, 12 Sep 2025 00:00:00 +0000</pubDate></item><item><title>Improving LLM Safety and Helpfulness using SFT and DPO: A Study on OPT-350M</title><link>https://arxiv.org/abs/2509.09055</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Investigates SFT, DPO, and combined alignment techniques on OPT-350M&lt;/li&gt;&lt;li&gt;Introduces HmR, HpR, and CAS metrics for safety evaluation&lt;/li&gt;&lt;li&gt;Finds SFT+DPO outperforms individual methods&lt;/li&gt;&lt;li&gt;Highlights challenges with noisy data and resource constraints&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Piyush Pant']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'safety evaluation', 'SFT', 'DPO', 'LLM']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.09055</guid><pubDate>Fri, 12 Sep 2025 00:00:00 +0000</pubDate></item><item><title>IDEATOR: Jailbreaking and Benchmarking Large Vision-Language Models Using Themselves</title><link>https://arxiv.org/abs/2411.00827</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes IDEATOR, a method for jailbreaking VLMs using the models themselves to generate malicious image-text pairs&lt;/li&gt;&lt;li&gt;Achieves high attack success rates (ASR) across multiple VLMs including MiniGPT-4, LLaVA, InstructBLIP, and Chameleon&lt;/li&gt;&lt;li&gt;Introduces VLJailbreakBench, a safety benchmark with 3,654 multimodal jailbreak samples&lt;/li&gt;&lt;li&gt;Evaluates 11 VLMs revealing significant safety gaps (e.g., 46.31% ASR on GPT-4o, 19.65% on Claude-3.5-Sonnet)&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Ruofan Wang', 'Juncheng Li', 'Yixu Wang', 'Bo Wang', 'Xiaosen Wang', 'Yan Teng', 'Yingchun Wang', 'Xingjun Ma', 'Yu-Gang Jiang']&lt;/li&gt;&lt;li&gt;Tags: ['jailbreaking', 'red teaming', 'vision-language models', 'safety benchmark', 'adversarial prompting']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2411.00827</guid><pubDate>Fri, 12 Sep 2025 00:00:00 +0000</pubDate></item><item><title>Algorithmic Collusion by Large Language Models</title><link>https://arxiv.org/abs/2404.00806</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;LLM-based pricing agents autonomously collude in oligopoly settings&lt;/li&gt;&lt;li&gt;Prompt variations significantly affect collusion levels&lt;/li&gt;&lt;li&gt;Off-path analysis reveals price-war concerns contributing to supracompetitive pricing&lt;/li&gt;&lt;li&gt;Results extend to auction settings with implications for AI regulation&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Sara Fish', 'Yannai A. Gonczarowski', 'Ran I. Shorrer']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'safety evaluation', 'prompt injection', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2404.00806</guid><pubDate>Fri, 12 Sep 2025 00:00:00 +0000</pubDate></item><item><title>Character-Level Perturbations Disrupt LLM Watermarks</title><link>https://arxiv.org/abs/2509.09112</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces character-level perturbation attacks on LLM watermarks&lt;/li&gt;&lt;li&gt;Proposes GA-based guided removal attacks under limited query access&lt;/li&gt;&lt;li&gt;Demonstrates high effectiveness of character-level attacks under restrictive threat models&lt;/li&gt;&lt;li&gt;Highlights adversarial dilemma and adaptive compound attack strategy&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Zhaoxi Zhang', 'Xiaomei Zhang', 'Yanjun Zhang', 'He Zhang', 'Shirui Pan', 'Bo Liu', 'Asif Qumer Gill', 'Leo Yu Zhang']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial attacks', 'watermarking', 'red teaming', 'LLM security', 'tokenization vulnerability']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.09112</guid><pubDate>Fri, 12 Sep 2025 00:00:00 +0000</pubDate></item><item><title>Improving LLM Safety and Helpfulness using SFT and DPO: A Study on OPT-350M</title><link>https://arxiv.org/abs/2509.09055</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Investigates SFT, DPO, and combined alignment techniques on OPT-350M&lt;/li&gt;&lt;li&gt;Introduces HmR, HpR, and CAS metrics for safety evaluation&lt;/li&gt;&lt;li&gt;Finds SFT+DPO outperforms individual methods&lt;/li&gt;&lt;li&gt;Highlights challenges with noisy data and resource constraints&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Piyush Pant']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'safety evaluation', 'SFT', 'DPO', 'LLM']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.09055</guid><pubDate>Fri, 12 Sep 2025 00:00:00 +0000</pubDate></item></channel></rss>