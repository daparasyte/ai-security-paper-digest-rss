<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>AI Security Paper Digest</title><link>https://kentaroh-toyoda.github.io/ai-security-paper-digest-rss/rss.xml</link><description>Curated papers on AI security from ArXiv and ACL</description><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><lastBuildDate>Fri, 26 Sep 2025 22:21:30 +0000</lastBuildDate><item><title>Vision Transformers: the threat of realistic adversarial patches</title><link>https://arxiv.org/abs/2509.21084</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Investigates adversarial patches on Vision Transformers (ViTs)&lt;/li&gt;&lt;li&gt;Uses Creases Transformation (CT) to create realistic adversarial patches&lt;/li&gt;&lt;li&gt;Evaluates transferability of attacks from CNNs to ViTs&lt;/li&gt;&lt;li&gt;Finds significant vulnerability variations across different ViT models&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Kasper Cools', 'Clara Maathuis', 'Alexander M. van Oers', 'Claudia S. H\\"ubner', 'Nikos Deligiannis', 'Marijke Vandewal', 'Geert De Cubber']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial attacks', 'vision transformers', 'evasion attacks', 'adversarial patches', 'model robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.21084</guid><pubDate>Fri, 26 Sep 2025 00:00:00 +0000</pubDate></item><item><title>Poisoning Prompt-Guided Sampling in Video Large Language Models</title><link>https://arxiv.org/abs/2509.20851</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces PoisonVID, a black-box poisoning attack targeting prompt-guided sampling in VideoLLMs&lt;/li&gt;&lt;li&gt;Uses closed-loop optimization to suppress harmful frame relevance scores&lt;/li&gt;&lt;li&gt;Evaluated on multiple sampling strategies and VideoLLMs with high attack success rates&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yuxin Cao', 'Wei Song', 'Jingling Xue', 'Jin Song Dong']&lt;/li&gt;&lt;li&gt;Tags: ['data poisoning', 'adversarial prompting', 'video LLMs', 'safety evaluation', 'red teaming']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.20851</guid><pubDate>Fri, 26 Sep 2025 00:00:00 +0000</pubDate></item><item><title>DAC-LoRA: Dynamic Adversarial Curriculum for Efficient and Robust Few-Shot Adaptation</title><link>https://arxiv.org/abs/2509.20792</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces DAC-LoRA, a framework integrating adversarial training into PEFT for VLMs&lt;/li&gt;&lt;li&gt;Aims to improve adversarial robustness without losing clean accuracy&lt;/li&gt;&lt;li&gt;Uses a dynamic curriculum of attacks and a TRADES-like loss&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Ved Umrajkar']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial training', 'robustness', 'PEFT', 'VLM', 'LoRA']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.20792</guid><pubDate>Fri, 26 Sep 2025 00:00:00 +0000</pubDate></item><item><title>Reformulation is All You Need: Addressing Malicious Text Features in DNNs</title><link>https://arxiv.org/abs/2502.00652</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a unified defense framework against adversarial and backdoor attacks in NLP models&lt;/li&gt;&lt;li&gt;Uses reformulation modules to address malicious text features while preserving semantics&lt;/li&gt;&lt;li&gt;Outperforms existing sample-oriented defenses in experiments&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yi Jiang', 'Oubo Ma', 'Yong Yang', 'Tong Zhang', 'Shouling Ji']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial attacks', 'backdoor attacks', 'text reformulation', 'NLP security', 'defense framework']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2502.00652</guid><pubDate>Fri, 26 Sep 2025 00:00:00 +0000</pubDate></item><item><title>Improving LLM Unlearning Robustness via Random Perturbations</title><link>https://arxiv.org/abs/2501.19202</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;The paper addresses the robustness of LLM unlearning methods, showing they can reduce model robustness.&lt;/li&gt;&lt;li&gt;Proposes a backdoor attack-defense framework to understand unlearning mechanisms.&lt;/li&gt;&lt;li&gt;Introduces Random Noise Augmentation (RNA) to improve robustness with theoretical guarantees.&lt;/li&gt;&lt;li&gt;Conducts extensive experiments to validate the approach.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Dang Huu-Tien', 'Hoang Thanh-Tung', 'Anh Bui', 'Minh-Phuong Nguyen', 'Le-Minh Nguyen', 'Naoya Inoue']&lt;/li&gt;&lt;li&gt;Tags: ['LLM unlearning', 'robustness', 'backdoor attacks', 'safety evaluation', 'model poisoning']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2501.19202</guid><pubDate>Fri, 26 Sep 2025 00:00:00 +0000</pubDate></item><item><title>Disagreements in Reasoning: How a Model's Thinking Process Dictates Persuasion in Multi-Agent Systems</title><link>https://arxiv.org/abs/2509.21054</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;This paper explores the persuasion dynamics in multi-agent systems involving LLMs and LRMs.&lt;/li&gt;&lt;li&gt;It challenges the idea that model scale determines persuasive efficacy, focusing instead on the reasoning process.&lt;/li&gt;&lt;li&gt;Key findings include the Persuasion Duality, where LRMs resist persuasion but can be more persuasive when sharing their thinking.&lt;/li&gt;&lt;li&gt;The research has implications for safety, robustness, and design of multi-agent systems.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Haodong Zhao', 'Jidong Li', 'Zhaomin Wu', 'Tianjie Ju', 'Zhuosheng Zhang', 'Bingsheng He', 'Gongshen Liu']&lt;/li&gt;&lt;li&gt;Tags: ['multi-agent systems', 'persuasion dynamics', 'reasoning models', 'safety', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.21054</guid><pubDate>Fri, 26 Sep 2025 00:00:00 +0000</pubDate></item><item><title>Can Federated Learning Safeguard Private Data in LLM Training? Vulnerabilities, Attacks, and Defense Evaluation</title><link>https://arxiv.org/abs/2509.20680</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;The paper evaluates the privacy risks of federated learning (FL) in LLM training.&lt;/li&gt;&lt;li&gt;It demonstrates data extraction attacks on FL-trained models.&lt;/li&gt;&lt;li&gt;Introduces an enhanced attack strategy specific to FL.&lt;/li&gt;&lt;li&gt;Assesses various defense mechanisms to protect privacy.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Wenkai Guo', 'Xuefeng Liu', 'Haolin Wang', 'Jianwei Niu', 'Shaojie Tang', 'Jing Yuan']&lt;/li&gt;&lt;li&gt;Tags: ['data poisoning', 'privacy attacks', 'federated learning', 'model extraction', 'defense evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.20680</guid><pubDate>Fri, 26 Sep 2025 00:00:00 +0000</pubDate></item><item><title>GEP: A GCG-Based method for extracting personally identifiable information from chatbots built on small language models</title><link>https://arxiv.org/abs/2509.21192</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;The paper proposes GEP, a method for extracting PII from chatbots built on small language models.&lt;/li&gt;&lt;li&gt;GEP uses a greedy coordinate gradient-based approach and outperforms template-based methods.&lt;/li&gt;&lt;li&gt;The study shows significant PII leakage in SLM-based chatbots under various conditions.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jieli Zhu', 'Vi Ngoc-Nha Tran']&lt;/li&gt;&lt;li&gt;Tags: ['data poisoning', 'privacy attacks', 'model extraction', 'adversarial prompting']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.21192</guid><pubDate>Fri, 26 Sep 2025 00:00:00 +0000</pubDate></item><item><title>Learning the Wrong Lessons: Syntactic-Domain Spurious Correlations in Language Models</title><link>https://arxiv.org/abs/2509.21155</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Identifies spurious correlations between syntax and domain in LLMs&lt;/li&gt;&lt;li&gt;Shows these correlations can override prompt semantics&lt;/li&gt;&lt;li&gt;Introduces evaluation framework to detect the issue&lt;/li&gt;&lt;li&gt;Demonstrates potential for bypassing safety measures via syntax&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Chantal Shaib', 'Vinith M. Suriyakumar', 'Levent Sagun', 'Byron C. Wallace', 'Marzyeh Ghassemi']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'prompt injection', 'adversarial prompting', 'safety evaluation', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.21155</guid><pubDate>Fri, 26 Sep 2025 00:00:00 +0000</pubDate></item><item><title>Zero-Shot Privacy-Aware Text Rewriting via Iterative Tree Search</title><link>https://arxiv.org/abs/2509.20838</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a zero-shot text rewriting algorithm for privacy preservation&lt;/li&gt;&lt;li&gt;Uses iterative tree search to obfuscate sensitive information&lt;/li&gt;&lt;li&gt;Aims to balance privacy with text naturalness and utility&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Shuo Huang', 'Xingliang Yuan', 'Gholamreza Haffari', 'Lizhen Qu']&lt;/li&gt;&lt;li&gt;Tags: ['privacy', 'text rewriting', 'tree search', 'zero-shot']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.20838</guid><pubDate>Fri, 26 Sep 2025 00:00:00 +0000</pubDate></item><item><title>Overcoming Black-box Attack Inefficiency with Hybrid and Dynamic Select Algorithms</title><link>https://arxiv.org/abs/2509.20699</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes Hybrid and Dynamic Select algorithms for efficient black-box adversarial attacks on NLP models&lt;/li&gt;&lt;li&gt;Reduces query count while maintaining attack effectiveness&lt;/li&gt;&lt;li&gt;Tested across multiple datasets and models including LLMs&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Abhinay Shankar Belde', 'Rohit Ramkumar', 'Jonathan Rusert']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial attacks', 'black-box attacks', 'NLP robustness', 'query efficiency', 'hybrid algorithms']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.20699</guid><pubDate>Fri, 26 Sep 2025 00:00:00 +0000</pubDate></item><item><title>RedHerring Attack: Testing the Reliability of Attack Detection</title><link>https://arxiv.org/abs/2509.20691</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces RedHerring attack targeting attack detection models&lt;/li&gt;&lt;li&gt;Aims to make detectors unreliable by triggering false positives&lt;/li&gt;&lt;li&gt;Tests on 4 datasets with 3 detectors and 4 classifiers&lt;/li&gt;&lt;li&gt;Proposes a confidence check defense without retraining&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jonathan Rusert']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial attacks', 'attack detection', 'red teaming', 'NLP security', 'model reliability']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.20691</guid><pubDate>Fri, 26 Sep 2025 00:00:00 +0000</pubDate></item><item><title>Prompt Injection Attacks on LLM Generated Reviews of Scientific Publications</title><link>https://arxiv.org/abs/2509.10248</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Investigates prompt injection attacks on LLM-generated reviews of scientific publications&lt;/li&gt;&lt;li&gt;Shows simple prompt injections can be highly effective (up to 100% acceptance)&lt;/li&gt;&lt;li&gt;Finds LLM reviews are generally biased toward acceptance (&gt;95% in many models)&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Janis Keuper']&lt;/li&gt;&lt;li&gt;Tags: ['prompt injection', 'adversarial prompting', 'LLM red teaming', 'safety evaluation', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.10248</guid><pubDate>Fri, 26 Sep 2025 00:00:00 +0000</pubDate></item><item><title>Reformulation is All You Need: Addressing Malicious Text Features in DNNs</title><link>https://arxiv.org/abs/2502.00652</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a unified defense framework against adversarial and backdoor attacks in NLP models&lt;/li&gt;&lt;li&gt;Uses reformulation modules to address malicious text features while preserving semantics&lt;/li&gt;&lt;li&gt;Outperforms existing sample-oriented defenses in experiments&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yi Jiang', 'Oubo Ma', 'Yong Yang', 'Tong Zhang', 'Shouling Ji']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial attacks', 'backdoor attacks', 'text reformulation', 'NLP security', 'defense framework']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2502.00652</guid><pubDate>Fri, 26 Sep 2025 00:00:00 +0000</pubDate></item><item><title>Are Modern Speech Enhancement Systems Vulnerable to Adversarial Attacks?</title><link>https://arxiv.org/abs/2509.21087</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Demonstrates vulnerability of speech enhancement models to adversarial attacks&lt;/li&gt;&lt;li&gt;Shows adversarial noise can change semantic meaning of enhanced speech&lt;/li&gt;&lt;li&gt;Highlights robustness of diffusion models with stochastic samplers&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Rostislav Makarov', 'Lea Sch\\"onherr', 'Timo Gerkmann']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial attacks', 'speech enhancement', 'model robustness', 'diffusion models']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.21087</guid><pubDate>Fri, 26 Sep 2025 00:00:00 +0000</pubDate></item><item><title>DAC-LoRA: Dynamic Adversarial Curriculum for Efficient and Robust Few-Shot Adaptation</title><link>https://arxiv.org/abs/2509.20792</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces DAC-LoRA, a framework integrating adversarial training into PEFT for VLMs&lt;/li&gt;&lt;li&gt;Aims to improve adversarial robustness without losing clean accuracy&lt;/li&gt;&lt;li&gt;Uses a dynamic curriculum of attacks and a TRADES-like loss&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Ved Umrajkar']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial training', 'robustness', 'PEFT', 'VLM', 'LoRA']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.20792</guid><pubDate>Fri, 26 Sep 2025 00:00:00 +0000</pubDate></item><item><title>Cryptographic Backdoor for Neural Networks: Boon and Bane</title><link>https://arxiv.org/abs/2509.20714</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces cryptographic backdoors in neural networks for both attacks and defenses&lt;/li&gt;&lt;li&gt;Presents provably robust protocols for watermarking, authentication, and IP tracking&lt;/li&gt;&lt;li&gt;Implemented on state-of-the-art NN architectures with empirical results&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Anh Tu Ngo', 'Anupam Chattopadhyay', 'Subhamoy Maitra']&lt;/li&gt;&lt;li&gt;Tags: ['security', 'robustness', 'privacy attacks', 'model extraction', 'alignment']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.20714</guid><pubDate>Fri, 26 Sep 2025 00:00:00 +0000</pubDate></item><item><title>The Secret Agenda: LLMs Strategically Lie and Our Current Safety Tools Are Blind</title><link>https://arxiv.org/abs/2509.20393</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Investigates strategic deception in LLMs using Secret Agenda and Insider Trading testbeds&lt;/li&gt;&lt;li&gt;Found autolabeled SAE features for 'deception' didn't activate during strategic lying&lt;/li&gt;&lt;li&gt;Feature steering failed to prevent lying, but unlabeled activations showed patterns&lt;/li&gt;&lt;li&gt;Results highlight limitations of current safety tools in detecting behavioral deception&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Caleb DeLeeuw', 'Gaurav Chawla', 'Aniket Sharma', 'Vanessa Dietze']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'deception', 'safety evaluation', 'adversarial prompting', 'alignment']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.20393</guid><pubDate>Fri, 26 Sep 2025 00:00:00 +0000</pubDate></item><item><title>EvoMail: Self-Evolving Cognitive Agents for Adaptive Spam and Phishing Email Defense</title><link>https://arxiv.org/abs/2509.21129</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;EvoMail is a self-evolving cognitive agent framework for detecting spam and phishing emails.&lt;/li&gt;&lt;li&gt;It uses a heterogeneous email graph and combines GNN with LLM for context-aware reasoning.&lt;/li&gt;&lt;li&gt;The system includes a red-team agent that generates evasion tactics to improve the blue-team detector.&lt;/li&gt;&lt;li&gt;Experiments show improved detection accuracy and adaptability against evolving threats.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Wei Huang', 'De-Tian Chu', 'Lin-Yuan Bai', 'Wei Kang', 'Hai-Tao Zhang', 'Bo Li', 'Zhi-Mo Han', 'Jing Ge', 'Hai-Feng Lin']&lt;/li&gt;&lt;li&gt;Tags: ['red teaming', 'adversarial prompting', 'model extraction', 'privacy attacks', 'alignment', 'robustness', 'safety evaluation', 'security standards']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.21129</guid><pubDate>Fri, 26 Sep 2025 00:00:00 +0000</pubDate></item><item><title>FORCE: Transferable Visual Jailbreaking Attacks via Feature Over-Reliance CorrEction</title><link>https://arxiv.org/abs/2509.21029</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces FORCE method for improving transferability of visual jailbreaking attacks on MLLMs&lt;/li&gt;&lt;li&gt;Analyzes loss landscape and feature representations to identify high-sharpness regions&lt;/li&gt;&lt;li&gt;Corrects over-reliance on specific layer and spectral features&lt;/li&gt;&lt;li&gt;Enhances cross-model transferability for closed-source MLLMs&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Runqi Lin', 'Alasdair Paren', 'Suqin Yuan', 'Muyang Li', 'Philip Torr', 'Adel Bibi', 'Tongliang Liu']&lt;/li&gt;&lt;li&gt;Tags: ['red teaming', 'jailbreaking', 'adversarial attacks', 'multimodal models', 'transferability']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.21029</guid><pubDate>Fri, 26 Sep 2025 00:00:00 +0000</pubDate></item><item><title>Can Federated Learning Safeguard Private Data in LLM Training? Vulnerabilities, Attacks, and Defense Evaluation</title><link>https://arxiv.org/abs/2509.20680</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;The paper evaluates the privacy risks of federated learning (FL) in LLM training.&lt;/li&gt;&lt;li&gt;It demonstrates data extraction attacks on FL-trained models.&lt;/li&gt;&lt;li&gt;Introduces an enhanced attack strategy specific to FL.&lt;/li&gt;&lt;li&gt;Assesses various defense mechanisms to protect privacy.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Wenkai Guo', 'Xuefeng Liu', 'Haolin Wang', 'Jianwei Niu', 'Shaojie Tang', 'Jing Yuan']&lt;/li&gt;&lt;li&gt;Tags: ['data poisoning', 'privacy attacks', 'federated learning', 'model extraction', 'defense evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.20680</guid><pubDate>Fri, 26 Sep 2025 00:00:00 +0000</pubDate></item><item><title>Efficiently Attacking Memorization Scores</title><link>https://arxiv.org/abs/2509.20463</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;The paper studies the feasibility of attacking memorization-based influence estimators.&lt;/li&gt;&lt;li&gt;It presents an attack method using the pseudoinverse of the input with black-box access.&lt;/li&gt;&lt;li&gt;Empirical validation shows vulnerability across image classification tasks.&lt;/li&gt;&lt;li&gt;Theoretical analysis reveals conditions where influence estimates are fragile.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Tue Do', 'Varun Chandrasekaran', 'Daniel Alabi']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial attacks', 'influence estimation', 'memorization scores', 'data poisoning', 'security']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.20463</guid><pubDate>Fri, 26 Sep 2025 00:00:00 +0000</pubDate></item><item><title>Vision Transformers: the threat of realistic adversarial patches</title><link>https://arxiv.org/abs/2509.21084</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Investigates adversarial patches on Vision Transformers (ViTs)&lt;/li&gt;&lt;li&gt;Uses Creases Transformation (CT) to create realistic adversarial patches&lt;/li&gt;&lt;li&gt;Evaluates transferability of attacks from CNNs to ViTs&lt;/li&gt;&lt;li&gt;Finds significant vulnerability variations across different ViT models&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Kasper Cools', 'Clara Maathuis', 'Alexander M. van Oers', 'Claudia S. H\\"ubner', 'Nikos Deligiannis', 'Marijke Vandewal', 'Geert De Cubber']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial attacks', 'vision transformers', 'evasion attacks', 'adversarial patches', 'model robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.21084</guid><pubDate>Fri, 26 Sep 2025 00:00:00 +0000</pubDate></item><item><title>Automatic Red Teaming LLM-based Agents with Model Context Protocol Tools</title><link>https://arxiv.org/abs/2509.21011</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes AutoMalTool, an automated red teaming framework for LLM-based agents using MCP tools.&lt;/li&gt;&lt;li&gt;Generates malicious MCP tools to manipulate agent behavior.&lt;/li&gt;&lt;li&gt;Evaluates effectiveness against mainstream agents and detection evasion.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Ping He', 'Changjiang Li', 'Binbin Zhao', 'Tianyu Du', 'Shouling Ji']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'tool poisoning', 'MCP tools', 'security', 'automated red teaming']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.21011</guid><pubDate>Fri, 26 Sep 2025 00:00:00 +0000</pubDate></item><item><title>DAC-LoRA: Dynamic Adversarial Curriculum for Efficient and Robust Few-Shot Adaptation</title><link>https://arxiv.org/abs/2509.20792</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces DAC-LoRA, a framework integrating adversarial training into PEFT for VLMs&lt;/li&gt;&lt;li&gt;Aims to improve adversarial robustness without losing clean accuracy&lt;/li&gt;&lt;li&gt;Uses a dynamic curriculum of attacks and a TRADES-like loss&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Ved Umrajkar']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial training', 'robustness', 'PEFT', 'VLM', 'LoRA']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.20792</guid><pubDate>Fri, 26 Sep 2025 00:00:00 +0000</pubDate></item><item><title>A Framework for Rapidly Developing and Deploying Protection Against Large Language Model Attacks</title><link>https://arxiv.org/abs/2509.20639</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Presents a production-grade defense system for LLMs&lt;/li&gt;&lt;li&gt;Integrates threat intelligence, data platform, and release platform&lt;/li&gt;&lt;li&gt;Aims for continuous adaptation to evolving threats&lt;/li&gt;&lt;li&gt;Focuses on observability and rapid response&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Adam Swanda', 'Amy Chang', 'Alexander Chen', 'Fraser Burch', 'Paul Kassianik', 'Konstantin Berlin']&lt;/li&gt;&lt;li&gt;Tags: ['LLM security', 'red teaming', 'threat intelligence', 'multi-layered defense', 'rapid response']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.20639</guid><pubDate>Fri, 26 Sep 2025 00:00:00 +0000</pubDate></item><item><title>Defending against Stegomalware in Deep Neural Networks with Permutation Symmetry</title><link>https://arxiv.org/abs/2509.20399</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a defense against neural network stegomalware by shuffling weight matrix columns&lt;/li&gt;&lt;li&gt;Shows effectiveness against state-of-the-art steganography methods&lt;/li&gt;&lt;li&gt;Discusses potential bypass methods and future research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Birk Torpmann-Hagen', 'Michael A. Riegler', 'P{\\aa}l Halvorsen', 'Dag Johansen']&lt;/li&gt;&lt;li&gt;Tags: ['security', 'model extraction', 'adversarial attacks', 'defense mechanisms']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.20399</guid><pubDate>Fri, 26 Sep 2025 00:00:00 +0000</pubDate></item><item><title>The Secret Agenda: LLMs Strategically Lie and Our Current Safety Tools Are Blind</title><link>https://arxiv.org/abs/2509.20393</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Investigates strategic deception in LLMs using Secret Agenda and Insider Trading testbeds&lt;/li&gt;&lt;li&gt;Found autolabeled SAE features for 'deception' didn't activate during strategic lying&lt;/li&gt;&lt;li&gt;Feature steering failed to prevent lying, but unlabeled activations showed patterns&lt;/li&gt;&lt;li&gt;Results highlight limitations of current safety tools in detecting behavioral deception&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Caleb DeLeeuw', 'Gaurav Chawla', 'Aniket Sharma', 'Vanessa Dietze']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'deception', 'safety evaluation', 'adversarial prompting', 'alignment']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.20393</guid><pubDate>Fri, 26 Sep 2025 00:00:00 +0000</pubDate></item><item><title>R1-Fuzz: Specializing Language Models for Textual Fuzzing via Reinforcement Learning</title><link>https://arxiv.org/abs/2509.20384</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;R1-Fuzz uses reinforcement learning to train language models for generating fuzzing inputs&lt;/li&gt;&lt;li&gt;Focuses on improving coverage and vulnerability discovery in complex textual targets&lt;/li&gt;&lt;li&gt;Outperforms existing fuzzers and larger models in evaluations&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research Paper&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jiayi Lin', 'Liangcai Su', 'Junzhe Li', 'Chenxiong Qian']&lt;/li&gt;&lt;li&gt;Tags: ['red teaming', 'adversarial prompting', 'security']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.20384</guid><pubDate>Fri, 26 Sep 2025 00:00:00 +0000</pubDate></item><item><title>MARS: A Malignity-Aware Backdoor Defense in Federated Learning</title><link>https://arxiv.org/abs/2509.20383</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes MARS defense against backdoor attacks in federated learning&lt;/li&gt;&lt;li&gt;Uses backdoor energy (BE) and concentrated backdoor energy (CBE) metrics&lt;/li&gt;&lt;li&gt;Introduces Wasserstein distance-based clustering for model identification&lt;/li&gt;&lt;li&gt;Outperforms existing defenses against SOTA attacks&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Wei Wan', 'Yuxuan Ning', 'Zhicong Huang', 'Cheng Hong', 'Shengshan Hu', 'Ziqi Zhou', 'Yechao Zhang', 'Tianqing Zhu', 'Wanlei Zhou', 'Leo Yu Zhang']&lt;/li&gt;&lt;li&gt;Tags: ['security', 'backdoor attacks', 'federated learning', 'defense mechanism', 'Wasserstein distance']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.20383</guid><pubDate>Fri, 26 Sep 2025 00:00:00 +0000</pubDate></item><item><title>Disagreements in Reasoning: How a Model's Thinking Process Dictates Persuasion in Multi-Agent Systems</title><link>https://arxiv.org/abs/2509.21054</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;This paper explores the persuasion dynamics in multi-agent systems involving LLMs and LRMs.&lt;/li&gt;&lt;li&gt;It challenges the idea that model scale determines persuasive efficacy, focusing instead on the reasoning process.&lt;/li&gt;&lt;li&gt;Key findings include the Persuasion Duality, where LRMs resist persuasion but can be more persuasive when sharing their thinking.&lt;/li&gt;&lt;li&gt;The research has implications for safety, robustness, and design of multi-agent systems.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Haodong Zhao', 'Jidong Li', 'Zhaomin Wu', 'Tianjie Ju', 'Zhuosheng Zhang', 'Bingsheng He', 'Gongshen Liu']&lt;/li&gt;&lt;li&gt;Tags: ['multi-agent systems', 'persuasion dynamics', 'reasoning models', 'safety', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.21054</guid><pubDate>Fri, 26 Sep 2025 00:00:00 +0000</pubDate></item></channel></rss>