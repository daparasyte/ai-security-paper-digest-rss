<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>AI Security Paper Digest</title><link>https://kentaroh-toyoda.github.io/ai-security-paper-digest-rss/rss.xml</link><description>Curated papers on AI security from ArXiv and ACL</description><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><lastBuildDate>Wed, 15 Oct 2025 23:42:04 +0000</lastBuildDate><item><title>Tracing Back the Malicious Clients in Poisoning Attacks to Federated Learning</title><link>https://arxiv.org/abs/2407.07221</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes FLForensics for tracing malicious clients in federated learning poisoning attacks&lt;/li&gt;&lt;li&gt;Complements existing training defenses by post-attack analysis&lt;/li&gt;&lt;li&gt;Theoretically and empirically validated on benchmark datasets&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yuqi Jia', 'Minghong Fang', 'Hongbin Liu', 'Jinghuai Zhang', 'Neil Zhenqiang Gong']&lt;/li&gt;&lt;li&gt;Tags: ['data poisoning', 'federated learning', 'security', 'forensics']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2407.07221</guid><pubDate>Wed, 15 Oct 2025 00:00:00 +0000</pubDate></item><item><title>COMAL: A Convergent Meta-Algorithm for Aligning LLMs with General Preferences</title><link>https://arxiv.org/abs/2410.23223</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes COMAL, a meta-algorithm for aligning LLMs with general preferences using a game-theoretic framework&lt;/li&gt;&lt;li&gt;Aims to find the Nash equilibrium policy to ensure a 50% win rate against any competing policy&lt;/li&gt;&lt;li&gt;Provides theoretical convergence guarantees and empirical evaluations on synthetic and real datasets&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yixin Liu', 'Argyris Oikonomou', 'Weiqiang Zheng', 'Yang Cai', 'Arman Cohan']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'game theory', 'Nash equilibrium', 'preference learning', 'reinforcement learning']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2410.23223</guid><pubDate>Wed, 15 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Are Large Reasoning Models Interruptible?</title><link>https://arxiv.org/abs/2510.11713</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Evaluates LRM robustness under interruptions and dynamic context&lt;/li&gt;&lt;li&gt;Performance drops up to 60% with late changes&lt;/li&gt;&lt;li&gt;Identifies failure modes like reasoning leakage, panic, self-doubt&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Tsung-Han Wu', 'Mihran Miroyan', 'David M. Chan', 'Trevor Darrell', 'Narges Norouzi', 'Joseph E. Gonzalez']&lt;/li&gt;&lt;li&gt;Tags: ['robustness', 'safety evaluation', 'adversarial prompting', 'jailbreaking']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.11713</guid><pubDate>Wed, 15 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Simulating and Understanding Deceptive Behaviors in Long-Horizon Interactions</title><link>https://arxiv.org/abs/2510.03999</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces a simulation framework for evaluating deception in LLMs over long-horizon interactions&lt;/li&gt;&lt;li&gt;Uses a multi-agent system with performer, supervisor, and auditor agents&lt;/li&gt;&lt;li&gt;Tests 11 models and finds deception increases with pressure and erodes trust&lt;/li&gt;&lt;li&gt;Identifies strategies like concealment, equivocation, and falsification&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yang Xu', 'Xuanming Zhang', 'Samuel Yeh', 'Jwala Dhamala', 'Ousmane Dia', 'Rahul Gupta', 'Sharon Li']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'deception', 'long-horizon interactions', 'multi-agent systems', 'trust evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.03999</guid><pubDate>Wed, 15 Oct 2025 00:00:00 +0000</pubDate></item><item><title>The Silent Judge: Unacknowledged Shortcut Bias in LLM-as-a-Judge</title><link>https://arxiv.org/abs/2509.26072</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;The paper studies biases in LLMs used as judges, showing they rely on shortcuts like recency and provenance instead of content quality.&lt;/li&gt;&lt;li&gt;It uses ELI5 and LitBench datasets with GPT-4o and Gemini-2.5-Flash models.&lt;/li&gt;&lt;li&gt;The models show strong recency bias and provenance hierarchy, with GPT-4o more affected.&lt;/li&gt;&lt;li&gt;Justifications rarely mention the injected cues, leading to unreliable evaluations.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research Paper&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Arash Marioriyad', 'Mohammad Hossein Rohban', 'Mahdieh Soleymani Baghshah']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'safety evaluation', 'adversarial prompting', 'model evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.26072</guid><pubDate>Wed, 15 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Cognition-of-Thought Elicits Social-Aligned Reasoning in Large Language Models</title><link>https://arxiv.org/abs/2509.23441</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces Cognition-of-Thought (CooT), a decoding-time framework for LLMs&lt;/li&gt;&lt;li&gt;Uses a cognitive Perceiver to monitor and correct misalignments&lt;/li&gt;&lt;li&gt;Improves safety and social reasoning without retraining&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Xuanming Zhang', 'Yuxuan Chen', 'Samuel Yeh', 'Sharon Li']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'safety', 'red teaming', 'jailbreaking']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.23441</guid><pubDate>Wed, 15 Oct 2025 00:00:00 +0000</pubDate></item><item><title>LUMINA: Detecting Hallucinations in RAG System with Context-Knowledge Signals</title><link>https://arxiv.org/abs/2509.21875</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes LUMINA framework for detecting hallucinations in RAG systems&lt;/li&gt;&lt;li&gt;Uses context-knowledge signals via distributional distance and token evolution tracking&lt;/li&gt;&lt;li&gt;Validates with statistical framework&lt;/li&gt;&lt;li&gt;Outperforms prior methods on HalluRAG benchmarks&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Samuel Yeh', 'Sharon Li', 'Tanwi Mallick']&lt;/li&gt;&lt;li&gt;Tags: ['RAG', 'hallucination detection', 'context utilization', 'internal knowledge']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.21875</guid><pubDate>Wed, 15 Oct 2025 00:00:00 +0000</pubDate></item><item><title>SAFER: Probing Safety in Reward Models with Sparse Autoencoder</title><link>https://arxiv.org/abs/2507.00665</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces SAFER, a framework using Sparse Autoencoders to interpret and improve reward models in RLHF&lt;/li&gt;&lt;li&gt;Focuses on uncovering interpretable features in reward models to enhance safety&lt;/li&gt;&lt;li&gt;Applies SAFER to safety datasets and demonstrates targeted data poisoning/denoising&lt;/li&gt;&lt;li&gt;Aims to audit and refine reward models for better alignment and safety&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Sihang Li', 'Wei Shi', 'Ziyuan Xie', 'Tao Liang', 'Guojun Ma', 'Xiang Wang']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'safety evaluation', 'model extraction', 'data poisoning', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2507.00665</guid><pubDate>Wed, 15 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Cross-Modal Safety Alignment: Is textual unlearning all you need?</title><link>https://arxiv.org/abs/2406.02575</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;The paper explores cross-modal safety alignment in VLMs by focusing on textual unlearning.&lt;/li&gt;&lt;li&gt;It shows that textual unlearning can reduce ASR for both text and vision-text attacks.&lt;/li&gt;&lt;li&gt;Multi-modal dataset unlearning is less effective and more computationally expensive.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Trishna Chakraborty', 'Erfan Shayegani', 'Zikui Cai', 'Nael Abu-Ghazaleh', 'M. Salman Asif', 'Yue Dong', 'Amit K. Roy-Chowdhury', 'Chengyu Song']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'safety evaluation', 'alignment', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2406.02575</guid><pubDate>Wed, 15 Oct 2025 00:00:00 +0000</pubDate></item><item><title>When "Competency" in Reasoning Opens the Door to Vulnerability: Jailbreaking LLMs via Novel Complex Ciphers</title><link>https://arxiv.org/abs/2402.10601</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces ACE and LACE jailbreaking techniques using custom ciphers&lt;/li&gt;&lt;li&gt;Develops CipherBench benchmark for evaluating cipher decoding&lt;/li&gt;&lt;li&gt;Finds higher reasoning ability leads to increased vulnerability&lt;/li&gt;&lt;li&gt;Demonstrates LACE success rates up to 72% on gpt-oss-20b&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Divij Handa', 'Zehua Zhang', 'Amir Saeidi', 'Shrinidhi Kumbhar', 'Md Nayem Uddin', 'Aswin RRV', 'Chitta Baral']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'jailbreaking', 'adversarial prompting', 'safety evaluation', 'benchmarking']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2402.10601</guid><pubDate>Wed, 15 Oct 2025 00:00:00 +0000</pubDate></item><item><title>HackWorld: Evaluating Computer-Use Agents on Exploiting Web Application Vulnerabilities</title><link>https://arxiv.org/abs/2510.12200</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;HackWorld evaluates computer-use agents (CUAs) on their ability to exploit web application vulnerabilities through visual interaction.&lt;/li&gt;&lt;li&gt;The framework includes 36 real-world applications with various vulnerabilities.&lt;/li&gt;&lt;li&gt;CUAs show low exploitation rates and cybersecurity awareness, highlighting the need for improvement.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Xiaoxue Ren', 'Penghao Jiang', 'Kaixin Li', 'Zhiyong Huang', 'Xiaoning Du', 'Jiaojiao Jiang', 'Zhenchang Xing', 'Jiamou Sun', 'Terry Yue Zhuo']&lt;/li&gt;&lt;li&gt;Tags: ['red teaming', 'adversarial prompting', 'security evaluation', 'web application security', 'agent evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.12200</guid><pubDate>Wed, 15 Oct 2025 00:00:00 +0000</pubDate></item><item><title>ThinkPilot: Steering Reasoning Models via Automated Think-prefixes Optimization</title><link>https://arxiv.org/abs/2510.12063</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces ThinkPilot, a framework for optimizing LRM reasoning via think-prefixes&lt;/li&gt;&lt;li&gt;Significantly improves safety by reducing StrongREJECT scores&lt;/li&gt;&lt;li&gt;Enhances instruction following and reasoning efficiency&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Sunzhu Li', 'Zhiyu Lin', 'Shuling Yang', 'Jiale Zhao', 'Wei Chen']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'safety evaluation', 'red teaming', 'instruction following']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.12063</guid><pubDate>Wed, 15 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Deep Research Brings Deeper Harm</title><link>https://arxiv.org/abs/2510.11851</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;The paper introduces two new jailbreak strategies for Deep Research (DR) agents built on LLMs: Plan Injection and Intent Hijack.&lt;/li&gt;&lt;li&gt;Experiments show that DR agents are more vulnerable to generating harmful content compared to standalone LLMs.&lt;/li&gt;&lt;li&gt;The findings highlight the need for better alignment techniques specific to DR agents.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Shuo Chen', 'Zonggen Li', 'Zhen Han', 'Bailan He', 'Tong Liu', 'Haokun Chen', 'Georg Groh', 'Philip Torr', 'Volker Tresp', 'Jindong Gu']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'jailbreaking', 'adversarial prompting', 'alignment', 'safety evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.11851</guid><pubDate>Wed, 15 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Don't Walk the Line: Boundary Guidance for Filtered Generation</title><link>https://arxiv.org/abs/2510.11834</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes Boundary Guidance, a reinforcement learning method to steer generative models away from safety classifier boundaries.&lt;/li&gt;&lt;li&gt;Aims to reduce false positives and negatives by avoiding the decision margin.&lt;/li&gt;&lt;li&gt;Evaluated on jailbreak and ambiguous prompts using LLM-as-a-Judge metrics.&lt;/li&gt;&lt;li&gt;Shows improvements in safety and utility compared to existing filtering strategies.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Sarah Ball', 'Andreas Haupt']&lt;/li&gt;&lt;li&gt;Tags: ['red teaming', 'safety evaluation', 'robustness', 'jailbreaking', 'adversarial prompting']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.11834</guid><pubDate>Wed, 15 Oct 2025 00:00:00 +0000</pubDate></item><item><title>When Personalization Tricks Detectors: The Feature-Inversion Trap in Machine-Generated Text Detection</title><link>https://arxiv.org/abs/2510.12476</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces a benchmark for personalized machine-generated text detection&lt;/li&gt;&lt;li&gt;Identifies the feature-inversion trap causing performance drops in detectors&lt;/li&gt;&lt;li&gt;Proposes a method to predict detector performance changes in personalized settings&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Lang Gao', 'Xuhui Li', 'Chenxi Wang', 'Mingzhe Li', 'Wei Liu', 'Zirui Song', 'Jinghui Zhang', 'Rui Yan', 'Preslav Nakov', 'Xiuying Chen']&lt;/li&gt;&lt;li&gt;Tags: ['red teaming', 'adversarial prompting', 'model extraction', 'privacy attacks', 'alignment', 'robustness', 'safety evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.12476</guid><pubDate>Wed, 15 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Shallow Robustness, Deep Vulnerabilities: Multi-Turn Evaluation of Medical LLMs</title><link>https://arxiv.org/abs/2510.12255</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces MedQA-Followup framework for multi-turn evaluation of medical LLMs&lt;/li&gt;&lt;li&gt;Assesses shallow vs deep robustness under different intervention types&lt;/li&gt;&lt;li&gt;Finds significant vulnerabilities in multi-turn settings with accuracy drops&lt;/li&gt;&lt;li&gt;Highlights model-specific behaviors under repeated interventions&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Blazej Manczak', 'Eric Lin', 'Francisco Eiras', "James O' Neill", 'Vaikkunth Mugunthan']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'robustness', 'safety evaluation', 'medical applications', 'multi-turn interactions']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.12255</guid><pubDate>Wed, 15 Oct 2025 00:00:00 +0000</pubDate></item><item><title>SafeMT: Multi-turn Safety for Multimodal Language Models</title><link>https://arxiv.org/abs/2510.12133</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces SafeMT benchmark for multi-turn safety in multimodal LLMs&lt;/li&gt;&lt;li&gt;Proposes Safety Index (SI) for evaluating safety during conversations&lt;/li&gt;&lt;li&gt;Finds that attack success rate increases with more turns in dialogues&lt;/li&gt;&lt;li&gt;Proposes a dialogue safety moderator to detect malicious intent&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Han Zhu', 'Juntao Dai', 'Jiaming Ji', 'Haoran Li', 'Chengkun Cai', 'Pengcheng Wen', 'Chi-Min Chan', 'Boyuan Chen', 'Yaodong Yang', 'Sirui Han', 'Yike Guo']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'jailbreaking', 'safety evaluation', 'multimodal', 'dialogue safety']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.12133</guid><pubDate>Wed, 15 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Reasoning Model Unlearning: Forgetting Traces, Not Just Answers, While Preserving Reasoning Skills</title><link>https://arxiv.org/abs/2506.12963</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces R²MU, a method to suppress sensitive information in reasoning traces of large reasoning models (LRMs)&lt;/li&gt;&lt;li&gt;Addresses the issue where conventional unlearning fails to remove sensitive data from intermediate reasoning steps&lt;/li&gt;&lt;li&gt;Evaluates on models like DeepSeek-R1-Distill-LLaMA-8B and DeepSeek-R1-Distill-Qwen-14B&lt;/li&gt;&lt;li&gt;Aims to preserve reasoning skills while enhancing safety&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Changsheng Wang', 'Chongyu Fan', 'Yihua Zhang', 'Jinghan Jia', 'Dennis Wei', 'Parikshit Ram', 'Nathalie Baracaldo', 'Sijia Liu']&lt;/li&gt;&lt;li&gt;Tags: ['data_poisoning', 'privacy_attacks', 'safety_evaluation', 'alignment', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.12963</guid><pubDate>Wed, 15 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Noise Injection Systemically Degrades Large Language Model Safety Guardrails</title><link>https://arxiv.org/abs/2505.13500</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Investigates robustness of safety fine-tuning in LLMs by injecting Gaussian noise&lt;/li&gt;&lt;li&gt;Finds that noise increases harmful output rates by up to 27%&lt;/li&gt;&lt;li&gt;Deeper safety fine-tuning doesn't provide extra protection&lt;/li&gt;&lt;li&gt;Chain-of-thought reasoning remains intact&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Prithviraj Singh Shahani', 'Kaveh Eskandari Miandoab', 'Matthias Scheutz']&lt;/li&gt;&lt;li&gt;Tags: ['LLM safety', 'robustness', 'adversarial attacks', 'safety evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2505.13500</guid><pubDate>Wed, 15 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Can LLMs Handle WebShell Detection? Overcoming Detection Challenges with Behavioral Function-Aware Framework</title><link>https://arxiv.org/abs/2504.13811</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Evaluates LLMs for WebShell detection&lt;/li&gt;&lt;li&gt;Introduces BFAD framework to improve LLM performance&lt;/li&gt;&lt;li&gt;Shows significant F1 score improvement with BFAD&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Feijiang Han', 'Jiaming Zhang', 'Chuyi Deng', 'Jianheng Tang', 'Yunhuai Liu']&lt;/li&gt;&lt;li&gt;Tags: ['LLM', 'WebShell', 'detection', 'framework', 'security']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2504.13811</guid><pubDate>Wed, 15 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Learning from Reference Answers: Versatile Language Model Alignment without Binary Human Preference Data</title><link>https://arxiv.org/abs/2504.09895</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces RefAlign, an alignment algorithm using reference answers instead of binary preferences&lt;/li&gt;&lt;li&gt;Utilizes metrics like BERTScore for reward&lt;/li&gt;&lt;li&gt;Applies to safety and confidence alignment&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Shuai Zhao', 'Yunqiu Xu', 'Linchao Zhu', 'Yi Yang']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'safety', 'confidence', 'reward modeling', 'reference answers']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2504.09895</guid><pubDate>Wed, 15 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Debate, Deliberate, Decide (D3): A Cost-Aware Adversarial Framework for Reliable and Interpretable LLM Evaluation</title><link>https://arxiv.org/abs/2410.04663</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces D3 framework for adversarial evaluation of LLMs&lt;/li&gt;&lt;li&gt;Uses structured debate among agents to improve evaluation reliability&lt;/li&gt;&lt;li&gt;Includes theoretical analysis and experimental validation&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Chaithanya Bandi', 'Abir Harrasse']&lt;/li&gt;&lt;li&gt;Tags: ['LLM evaluation', 'adversarial framework', 'reliability', 'interpretability', 'cost-aware']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2410.04663</guid><pubDate>Wed, 15 Oct 2025 00:00:00 +0000</pubDate></item><item><title>When Vision Fails: Text Attacks Against ViT and OCR</title><link>https://arxiv.org/abs/2306.07033</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;The paper demonstrates Unicode-based adversarial attacks against OCR and ViT models using combining characters.&lt;/li&gt;&lt;li&gt;The attacks are generated via a genetic algorithm in a black-box setting.&lt;/li&gt;&lt;li&gt;The study includes real-world tests against models from Meta, Microsoft, IBM, and Google.&lt;/li&gt;&lt;li&gt;A user study confirms human comprehension is unaffected.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Nicholas Boucher', 'Jenny Blessing', 'Ilia Shumailov', 'Ross Anderson', 'Nicolas Papernot']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial attacks', 'text manipulation', 'OCR', 'ViT', 'Unicode']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2306.07033</guid><pubDate>Wed, 15 Oct 2025 00:00:00 +0000</pubDate></item><item><title>On Surjectivity of Neural Networks: Can you elicit any behavior from your model?</title><link>https://arxiv.org/abs/2508.19445</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;The paper studies the surjectivity of neural networks, which relates to whether any output can be generated by some input.&lt;/li&gt;&lt;li&gt;It proves that common neural network components like pre-layer normalization and linear-attention modules are almost always surjective.&lt;/li&gt;&lt;li&gt;This implies that generative models like GPT-style transformers and diffusion models can generate any output, including harmful content.&lt;/li&gt;&lt;li&gt;The findings highlight inherent vulnerabilities to adversarial attacks such as jailbreaking.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Haozhe Jiang', 'Nika Haghtalab']&lt;/li&gt;&lt;li&gt;Tags: ['surjectivity', 'neural networks', 'generative models', 'jailbreak vulnerabilities', 'adversarial attacks']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2508.19445</guid><pubDate>Wed, 15 Oct 2025 00:00:00 +0000</pubDate></item><item><title>SAFEx: Analyzing Vulnerabilities of MoE-Based LLMs via Stable Safety-critical Expert Identification</title><link>https://arxiv.org/abs/2506.17368</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces SAFEx framework to analyze MoE-based LLM safety vulnerabilities&lt;/li&gt;&lt;li&gt;Identifies safety-critical experts in HCDG and HRCG groups&lt;/li&gt;&lt;li&gt;Demonstrates targeted expert masking and adaptation for safety improvements&lt;/li&gt;&lt;li&gt;Shows significant safety behavior concentration in specific experts&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Zhenglin Lai', 'Mengyao Liao', 'Bingzhe Wu', 'Dong Xu', 'Zebin Zhao', 'Zhihang Yuan', 'Chao Fan', 'Jianqiang Li']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'alignment', 'safety evaluation', 'model extraction', 'adversarial prompting']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.17368</guid><pubDate>Wed, 15 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Ignition Phase : Standard Training for Fast Adversarial Robustness</title><link>https://arxiv.org/abs/2506.15685</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces Adversarial Evolution Training (AET)&lt;/li&gt;&lt;li&gt;Prepends ERM phase to AT for better feature representations&lt;/li&gt;&lt;li&gt;Improves robustness and clean accuracy&lt;/li&gt;&lt;li&gt;Reduces training costs&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Wang Yu-Hang', 'Liu ying', 'Fang liang', 'Wang Xuelin', 'Junkang Guo', 'Shiwei Li', 'Lei Gao', 'Jian Liu', 'Wenfei Yin']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial training', 'robustness', 'training efficiency']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.15685</guid><pubDate>Wed, 15 Oct 2025 00:00:00 +0000</pubDate></item><item><title>VLMGuard-R1: Proactive Safety Alignment for VLMs via Reasoning-Driven Prompt Optimization</title><link>https://arxiv.org/abs/2504.12661</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces VLMGuard-R1, a framework for safety alignment in VLMs using reasoning-driven prompt optimization&lt;/li&gt;&lt;li&gt;Employs a three-stage reasoning pipeline to generate a dataset for training the prompt rewriter&lt;/li&gt;&lt;li&gt;Achieves significant safety improvements across multiple VLMs and benchmarks&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Menglan Chen', 'Xianghe Pang', 'Jingjing Dong', 'WenHao Wang', 'Yaxin Du', 'Siheng Chen']&lt;/li&gt;&lt;li&gt;Tags: ['VLM safety', 'prompt optimization', 'reasoning pipeline', 'safety evaluation', 'multimodal']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2504.12661</guid><pubDate>Wed, 15 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Are Large Reasoning Models Interruptible?</title><link>https://arxiv.org/abs/2510.11713</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Evaluates LRM robustness under interruptions and dynamic context&lt;/li&gt;&lt;li&gt;Performance drops up to 60% with late updates&lt;/li&gt;&lt;li&gt;Identifies failure modes like reasoning leakage, panic, self-doubt&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research Paper&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Tsung-Han Wu', 'Mihran Miroyan', 'David M. Chan', 'Trevor Darrell', 'Narges Norouzi', 'Joseph E. Gonzalez']&lt;/li&gt;&lt;li&gt;Tags: ['robustness', 'safety evaluation', 'adversarial prompting', 'jailbreaking']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.11713</guid><pubDate>Wed, 15 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Adversarial Robustness in One-Stage Learning-to-Defer</title><link>https://arxiv.org/abs/2510.10988</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces adversarial robustness framework for one-stage Learning-to-Defer (L2D)&lt;/li&gt;&lt;li&gt;Covers classification and regression&lt;/li&gt;&lt;li&gt;Proposes cost-sensitive adversarial surrogate losses&lt;/li&gt;&lt;li&gt;Establishes theoretical guarantees and shows improved robustness&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yannis Montreuil', 'Letian Yu', 'Axel Carlier', 'Lai Xing Ng', 'Wei Tsang Ooi']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial robustness', 'learning-to-defer', 'security', 'theoretical guarantees']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.10988</guid><pubDate>Wed, 15 Oct 2025 00:00:00 +0000</pubDate></item><item><title>RefusalBench: Generative Evaluation of Selective Refusal in Grounded Language Models</title><link>https://arxiv.org/abs/2510.10390</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces RefusalBench for evaluating selective refusal in RAG systems&lt;/li&gt;&lt;li&gt;Highlights models' struggles with refusal accuracy and confidence&lt;/li&gt;&lt;li&gt;Uses generative methodology with controlled perturbations&lt;/li&gt;&lt;li&gt;Releases two benchmarks for single and multi-document tasks&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Aashiq Muhamed', 'Leonardo F. R. Ribeiro', 'Markus Dreyer', 'Virginia Smith', 'Mona T. Diab']&lt;/li&gt;&lt;li&gt;Tags: ['safety evaluation', 'red teaming', 'alignment', 'robustness', 'benchmarks']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.10390</guid><pubDate>Wed, 15 Oct 2025 00:00:00 +0000</pubDate></item><item><title>ArtPerception: ASCII Art-based Jailbreak on LLMs with Recognition Pre-test</title><link>https://arxiv.org/abs/2510.10281</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces ArtPerception, a black-box jailbreak framework using ASCII art for LLMs&lt;/li&gt;&lt;li&gt;Two-phase approach: pre-test for ASCII art recognition parameters and one-shot attack&lt;/li&gt;&lt;li&gt;Proposes Modified Levenshtein Distance (MLD) metric&lt;/li&gt;&lt;li&gt;Tested on SOTA models and commercial models like GPT-4o, Claude, DeepSeek&lt;/li&gt;&lt;li&gt;Validates against defenses like LLaMA Guard and Azure content filters&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Guan-Yan Yang', 'Tzu-Yu Cheng', 'Ya-Wen Teng', 'Farn Wanga', 'Kuo-Hui Yeh']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'jailbreaking', 'ASCII art', 'Modified Levenshtein Distance', 'model extraction']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.10281</guid><pubDate>Wed, 15 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Pharmacist: Safety Alignment Data Curation for Large Language Models against Harmful Fine-tuning</title><link>https://arxiv.org/abs/2510.10085</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Pharmacist is a data curation method for safety alignment in LLMs&lt;/li&gt;&lt;li&gt;Improves defense against harmful fine-tuning by selecting high-quality safety data&lt;/li&gt;&lt;li&gt;Enhances existing defense methods like RepNoise and T-Vaccine&lt;/li&gt;&lt;li&gt;Reduces training time while improving performance&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Guozhi Liu', 'Qi Mu', 'Tiansheng Huang', 'Xinhua Wang', 'Li Shen', 'Weiwei Lin', 'Zhang Li']&lt;/li&gt;&lt;li&gt;Tags: ['safety alignment', 'data curation', 'harmful fine-tuning', 'defense enhancement', 'training efficiency']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.10085</guid><pubDate>Wed, 15 Oct 2025 00:00:00 +0000</pubDate></item><item><title>All Code, No Thought: Current Language Models Struggle to Reason in Ciphered Language</title><link>https://arxiv.org/abs/2510.09714</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;The paper tests models' ability to reason in ciphered text, which could be used to evade CoT monitoring.&lt;/li&gt;&lt;li&gt;Models show comprehension but struggle with reasoning in lesser-known ciphers.&lt;/li&gt;&lt;li&gt;Ciphered reasoning capability correlates with cipher prevalence in pretraining data.&lt;/li&gt;&lt;li&gt;Scaling laws indicate slow improvement with more fine-tuning data.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Shiyuan Guo', 'Henry Sleight', 'Fabien Roger']&lt;/li&gt;&lt;li&gt;Tags: ['red teaming', 'adversarial prompting', 'safety evaluation', 'alignment', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.09714</guid><pubDate>Wed, 15 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Rounding-Guided Backdoor Injection in Deep Learning Model Quantization</title><link>https://arxiv.org/abs/2510.09647</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces QuRA, a backdoor attack exploiting model quantization&lt;/li&gt;&lt;li&gt;Uses weight selection and rounding optimization to embed malicious behavior&lt;/li&gt;&lt;li&gt;Achieves high attack success with minimal performance impact&lt;/li&gt;&lt;li&gt;Bypasses existing backdoor defenses&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Xiangxiang Chen', 'Peixin Zhang', 'Jun Sun', 'Wenhai Wang', 'Jingyi Wang']&lt;/li&gt;&lt;li&gt;Tags: ['model extraction', 'data poisoning', 'privacy attacks', 'alignment', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.09647</guid><pubDate>Wed, 15 Oct 2025 00:00:00 +0000</pubDate></item><item><title>ENIGMA: The Geometry of Reasoning and Alignment in Large-Language Models</title><link>https://arxiv.org/abs/2510.11278</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;ENIGMA is a new approach to LLM training focusing on reasoning, alignment, and robustness&lt;/li&gt;&lt;li&gt;Uses GRPO, SAMI-style InfoNCE, and Sinkhorn regularizer&lt;/li&gt;&lt;li&gt;Introduces Sufficiency Index for principle selection&lt;/li&gt;&lt;li&gt;Experiments show improved performance and training dynamics&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Gareth Seneque', 'Lap-Hang Ho', 'Nafise Erfanian Saeedi', 'Jeffrey Molendijk', 'Ariel Kupermann', 'Tim Elson']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'robustness', 'safety evaluation', 'model extraction', 'privacy attacks']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.11278</guid><pubDate>Wed, 15 Oct 2025 00:00:00 +0000</pubDate></item><item><title>The Easy Path to Robustness: Coreset Selection using Sample Hardness</title><link>https://arxiv.org/abs/2510.11018</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes EasyCore, a coreset selection algorithm using sample hardness (AIGN) to improve adversarial robustness&lt;/li&gt;&lt;li&gt;Shows significant adversarial accuracy improvements over existing methods&lt;/li&gt;&lt;li&gt;Model-agnostic approach applicable to standard and adversarial training&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Pranav Ramesh', 'Arjun Roy', 'Deepak Ravikumar', 'Kaushik Roy', 'Gopalakrishnan Srinivasan']&lt;/li&gt;&lt;li&gt;Tags: ['robustness', 'adversarial training', 'coreset selection', 'data-centric']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.11018</guid><pubDate>Wed, 15 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Catch-Only-One: Non-Transferable Examples for Model-Specific Authorization</title><link>https://arxiv.org/abs/2510.10982</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces non-transferable examples (NEs) for model-specific authorization&lt;/li&gt;&lt;li&gt;Uses input recoding in a model-specific subspace to preserve utility while blocking unauthorized models&lt;/li&gt;&lt;li&gt;Provides formal bounds and empirical results showing effectiveness&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Zihan Wang', 'Zhiyong Ma', 'Zhongkui Ma', 'Shuofeng Liu', 'Akide Liu', 'Derui Wang', 'Minhui Xue', 'Guangdong Bai']&lt;/li&gt;&lt;li&gt;Tags: ['security', 'data protection', 'model specificity', 'authorization']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.10982</guid><pubDate>Wed, 15 Oct 2025 00:00:00 +0000</pubDate></item><item><title>ImpMIA: Leveraging Implicit Bias for Membership Inference Attack under Realistic Scenarios</title><link>https://arxiv.org/abs/2510.10625</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces ImpMIA, a white-box membership inference attack leveraging neural network implicit bias&lt;/li&gt;&lt;li&gt;Uses KKT conditions to identify training samples by gradient reconstruction&lt;/li&gt;&lt;li&gt;Achieves state-of-the-art performance in realistic settings with model weights access&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yuval Golbari', 'Navve Wasserman', 'Gal Vardi', 'Michal Irani']&lt;/li&gt;&lt;li&gt;Tags: ['privacy attacks', 'membership inference', 'white-box attack', 'implicit bias', 'neural networks']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.10625</guid><pubDate>Wed, 15 Oct 2025 00:00:00 +0000</pubDate></item><item><title>SGM: A Statistical Godel Machine for Risk-Controlled Recursive Self-Modification</title><link>https://arxiv.org/abs/2510.10232</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces Statistical Godel Machine (SGM) for safe recursive self-modification&lt;/li&gt;&lt;li&gt;Uses statistical confidence tests instead of formal proofs&lt;/li&gt;&lt;li&gt;Validates safety in ML and RL benchmarks&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Xuening Wu', 'Shenqin Yin', 'Yanlan Kang', 'Xinhang Zhang', 'Qianya Xu', 'Zeping Chen', 'Wenqiang Zhang']&lt;/li&gt;&lt;li&gt;Tags: ['safety', 'self-modification', 'statistical testing', 'risk control']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.10232</guid><pubDate>Wed, 15 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Tight Robustness Certificates and Wasserstein Distributional Attacks for Deep Neural Networks</title><link>https://arxiv.org/abs/2510.10000</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces a primal approach and exact Lipschitz certificate for Wasserstein distributionally robust optimization (WDRO)&lt;/li&gt;&lt;li&gt;Proposes a novel Wasserstein distributional attack (WDA) for constructing worst-case distributions&lt;/li&gt;&lt;li&gt;Leverages piecewise-affine structure of ReLU networks for exact WDRO characterization&lt;/li&gt;&lt;li&gt;Demonstrates competitive robust accuracy and tighter certificates&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Bach C. Le', 'Tung V. Dao', 'Binh T. Nguyen', 'Hong T. M. Chu']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial robustness', 'distributional robustness', 'Wasserstein distance', 'deep neural networks', 'certified robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.10000</guid><pubDate>Wed, 15 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Building a Foundational Guardrail for General Agentic Systems via Synthetic Data</title><link>https://arxiv.org/abs/2510.09781</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces AuraGen for generating synthetic data with labeled risks for pre-execution safety&lt;/li&gt;&lt;li&gt;Proposes Safiron, a foundational guardrail combining a cross-planner adapter and guardian model&lt;/li&gt;&lt;li&gt;Releases Pre-Exec Bench benchmark for evaluating pre-execution safety measures&lt;/li&gt;&lt;li&gt;Addresses data, model, and evaluation gaps in current safety research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yue Huang', 'Hang Hua', 'Yujun Zhou', 'Pengcheng Jing', 'Manish Nagireddy', 'Inkit Padhi', 'Greta Dolcetti', 'Zhangchen Xu', 'Subhajit Chaudhury', 'Ambrish Rawat', 'Liubov Nedoshivina', 'Pin-Yu Chen', 'Prasanna Sattigeri', 'Xiangliang Zhang']&lt;/li&gt;&lt;li&gt;Tags: ['safety evaluation', 'data poisoning', 'alignment', 'robustness', 'model extraction']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.09781</guid><pubDate>Wed, 15 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Group-Adaptive Adversarial Learning for Robust Fake News Detection Against Malicious Comments</title><link>https://arxiv.org/abs/2510.09712</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Evaluates fake news detectors against adversarial comments&lt;/li&gt;&lt;li&gt;Introduces group-adaptive adversarial training&lt;/li&gt;&lt;li&gt;Uses LLMs to generate category-specific attacks&lt;/li&gt;&lt;li&gt;Applies adaptive sampling for robustness&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Zhao Tong', 'Chunlin Gong', 'Yimeng Gu', 'Haichao Shi', 'Qiang Liu', 'Shu Wu', 'Xiao-Yu Zhang']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial training', 'fake news detection', 'LLM red teaming', 'robustness', 'security']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.09712</guid><pubDate>Wed, 15 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Kelp: A Streaming Safeguard for Large Models via Latent Dynamics-Guided Risk Detection</title><link>https://arxiv.org/abs/2510.09694</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes Kelp, a streaming safeguard framework for LMs using latent dynamics-guided risk detection&lt;/li&gt;&lt;li&gt;Introduces SLD head for real-time risk prediction during generation&lt;/li&gt;&lt;li&gt;Uses ATC loss to enforce temporal consistency in harm predictions&lt;/li&gt;&lt;li&gt;Presents StreamGuardBench for evaluating streaming guardrails in text and vision-language tasks&lt;/li&gt;&lt;li&gt;Shows improved performance over post-hoc guardrails with low latency&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Xiaodan Li', 'Mengjie Wu', 'Yao Zhu', 'Yunna Lv', 'YueFeng Chen', 'Cen Chen', 'Jianmei Guo', 'Hui Xue']&lt;/li&gt;&lt;li&gt;Tags: ['safety', 'guardrails', 'real-time detection', 'streaming', 'benchmarking']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.09694</guid><pubDate>Wed, 15 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Malice in Agentland: Down the Rabbit Hole of Backdoors in the AI Supply Chain</title><link>https://arxiv.org/abs/2510.05159</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;The paper explores backdoor vulnerabilities in the AI supply chain when fine-tuning agents on their own interaction data.&lt;/li&gt;&lt;li&gt;Three threat models are presented: direct data poisoning, environmental poisoning, and supply chain poisoning.&lt;/li&gt;&lt;li&gt;Results show that even 2% poisoned data can lead to successful backdoor triggers, and existing defenses are insufficient.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ["L\\'eo Boisvert", 'Abhay Puri', 'Chandra Kiran Reddy Evuru', 'Nicolas Chapados', 'Quentin Cappart', 'Alexandre Lacoste', 'Krishnamurthy Dj Dvijotham', 'Alexandre Drouin']&lt;/li&gt;&lt;li&gt;Tags: ['data poisoning', 'backdoor attacks', 'AI supply chain security', 'adversarial training', 'security evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.05159</guid><pubDate>Wed, 15 Oct 2025 00:00:00 +0000</pubDate></item><item><title>SAFER: Probing Safety in Reward Models with Sparse Autoencoder</title><link>https://arxiv.org/abs/2507.00665</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces SAFER, a framework using Sparse Autoencoders to interpret and improve reward models in RLHF&lt;/li&gt;&lt;li&gt;Focuses on uncovering interpretable features in reward models to enhance safety&lt;/li&gt;&lt;li&gt;Applies SAFER to safety datasets and demonstrates targeted data poisoning/denoising&lt;/li&gt;&lt;li&gt;Aims to audit and refine reward models for better alignment and safety&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Sihang Li', 'Wei Shi', 'Ziyuan Xie', 'Tao Liang', 'Guojun Ma', 'Xiang Wang']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'safety evaluation', 'model extraction', 'data poisoning', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2507.00665</guid><pubDate>Wed, 15 Oct 2025 00:00:00 +0000</pubDate></item><item><title>AgentBreeder: Mitigating the AI Safety Risks of Multi-Agent Scaffolds via Self-Improvement</title><link>https://arxiv.org/abs/2502.00757</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces AgentBreeder, a framework for self-improving multi-agent scaffolds&lt;/li&gt;&lt;li&gt;Evaluates safety and capability improvements through blue and red modes&lt;/li&gt;&lt;li&gt;Highlights risks of multi-agent systems and provides mitigation strategies&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['J Rosser', 'Jakob Foerster']&lt;/li&gt;&lt;li&gt;Tags: ['multi-agent systems', 'safety evaluation', 'red teaming', 'evolutionary algorithms', 'scaffold optimization']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2502.00757</guid><pubDate>Wed, 15 Oct 2025 00:00:00 +0000</pubDate></item><item><title>GraphRAG under Fire</title><link>https://arxiv.org/abs/2501.14050</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;GraphRAG is a retrieval-augmented generation model using knowledge graphs.&lt;/li&gt;&lt;li&gt;The paper explores GraphRAG's vulnerability to poisoning attacks.&lt;/li&gt;&lt;li&gt;Introduces GragPoison, a novel attack exploiting shared relations in the knowledge graph.&lt;/li&gt;&lt;li&gt;Evaluates the attack's effectiveness and scalability, suggesting defensive measures.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jiacheng Liang', 'Yuhui Wang', 'Changjiang Li', 'Rongyi Zhu', 'Tanqiu Jiang', 'Neil Gong', 'Ting Wang']&lt;/li&gt;&lt;li&gt;Tags: ['data poisoning', 'adversarial attacks', 'red teaming', 'security evaluation', 'model robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2501.14050</guid><pubDate>Wed, 15 Oct 2025 00:00:00 +0000</pubDate></item><item><title>COMAL: A Convergent Meta-Algorithm for Aligning LLMs with General Preferences</title><link>https://arxiv.org/abs/2410.23223</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes COMAL, a meta-algorithm for aligning LLMs with general preferences using a game-theoretic framework&lt;/li&gt;&lt;li&gt;Aims to find the Nash equilibrium policy to ensure a 50% win rate against any competing policy&lt;/li&gt;&lt;li&gt;Provides theoretical convergence guarantees and empirical evaluations on synthetic and real datasets&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yixin Liu', 'Argyris Oikonomou', 'Weiqiang Zheng', 'Yang Cai', 'Arman Cohan']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'game theory', 'Nash equilibrium', 'preference learning', 'reinforcement learning']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2410.23223</guid><pubDate>Wed, 15 Oct 2025 00:00:00 +0000</pubDate></item><item><title>When "Competency" in Reasoning Opens the Door to Vulnerability: Jailbreaking LLMs via Novel Complex Ciphers</title><link>https://arxiv.org/abs/2402.10601</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces ACE and LACE jailbreaking techniques using custom ciphers&lt;/li&gt;&lt;li&gt;Develops CipherBench benchmark for evaluating cipher decoding&lt;/li&gt;&lt;li&gt;Finds higher reasoning ability leads to increased vulnerability&lt;/li&gt;&lt;li&gt;Demonstrates LACE success rates up to 72% on gpt-oss-20b&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Divij Handa', 'Zehua Zhang', 'Amir Saeidi', 'Shrinidhi Kumbhar', 'Md Nayem Uddin', 'Aswin RRV', 'Chitta Baral']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'jailbreaking', 'adversarial prompting', 'safety evaluation', 'benchmarking']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2402.10601</guid><pubDate>Wed, 15 Oct 2025 00:00:00 +0000</pubDate></item><item><title>When Personalization Tricks Detectors: The Feature-Inversion Trap in Machine-Generated Text Detection</title><link>https://arxiv.org/abs/2510.12476</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces a benchmark for personalized machine-generated text detection&lt;/li&gt;&lt;li&gt;Identifies the feature-inversion trap causing performance drops in detectors&lt;/li&gt;&lt;li&gt;Proposes a method to predict detector performance changes in personalized settings&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Lang Gao', 'Xuhui Li', 'Chenxi Wang', 'Mingzhe Li', 'Wei Liu', 'Zirui Song', 'Jinghui Zhang', 'Rui Yan', 'Preslav Nakov', 'Xiuying Chen']&lt;/li&gt;&lt;li&gt;Tags: ['red teaming', 'adversarial prompting', 'model extraction', 'privacy attacks', 'alignment', 'robustness', 'safety evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.12476</guid><pubDate>Wed, 15 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Shallow Robustness, Deep Vulnerabilities: Multi-Turn Evaluation of Medical LLMs</title><link>https://arxiv.org/abs/2510.12255</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces MedQA-Followup framework for multi-turn evaluation of medical LLMs&lt;/li&gt;&lt;li&gt;Assesses shallow vs deep robustness under different intervention types&lt;/li&gt;&lt;li&gt;Finds significant vulnerabilities in multi-turn settings with accuracy drops&lt;/li&gt;&lt;li&gt;Highlights model-specific behaviors under repeated interventions&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Blazej Manczak', 'Eric Lin', 'Francisco Eiras', "James O' Neill", 'Vaikkunth Mugunthan']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'robustness', 'safety evaluation', 'medical applications', 'multi-turn interactions']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.12255</guid><pubDate>Wed, 15 Oct 2025 00:00:00 +0000</pubDate></item><item><title>PromptLocate: Localizing Prompt Injection Attacks</title><link>https://arxiv.org/abs/2510.12252</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes PromptLocate, a method to localize prompt injection attacks in LLMs&lt;/li&gt;&lt;li&gt;Involves splitting data into segments, identifying injected instructions and data&lt;/li&gt;&lt;li&gt;Tested on 16 attacks with accurate localization&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yuqi Jia', 'Yupei Liu', 'Zedian Shao', 'Jinyuan Jia', 'Neil Gong']&lt;/li&gt;&lt;li&gt;Tags: ['red teaming', 'prompt injection', 'adversarial prompting', 'safety evaluation', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.12252</guid><pubDate>Wed, 15 Oct 2025 00:00:00 +0000</pubDate></item><item><title>SafeMT: Multi-turn Safety for Multimodal Language Models</title><link>https://arxiv.org/abs/2510.12133</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces SafeMT benchmark for multi-turn safety in multimodal LLMs&lt;/li&gt;&lt;li&gt;Proposes Safety Index (SI) for evaluating safety during conversations&lt;/li&gt;&lt;li&gt;Finds that attack success rate increases with more turns in dialogues&lt;/li&gt;&lt;li&gt;Proposes a dialogue safety moderator to detect malicious intent&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Han Zhu', 'Juntao Dai', 'Jiaming Ji', 'Haoran Li', 'Chengkun Cai', 'Pengcheng Wen', 'Chi-Min Chan', 'Boyuan Chen', 'Yaodong Yang', 'Sirui Han', 'Yike Guo']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'jailbreaking', 'safety evaluation', 'multimodal', 'dialogue safety']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.12133</guid><pubDate>Wed, 15 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Countermind: A Multi-Layered Security Architecture for Large Language Models</title><link>https://arxiv.org/abs/2510.11837</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes Countermind, a multi-layered security architecture for LLMs&lt;/li&gt;&lt;li&gt;Focuses on proactive defenses against prompt injection and jailbreaking&lt;/li&gt;&lt;li&gt;Introduces Semantic Boundary Logic, Parameter-Space Restriction, and Secure Core&lt;/li&gt;&lt;li&gt;Aims to reduce attack surface and mitigate semantic drift&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Dominik Schwarz']&lt;/li&gt;&lt;li&gt;Tags: ['LLM security', 'red teaming', 'jailbreaking', 'prompt injection', 'adversarial prompting', 'model extraction', 'alignment', 'robustness', 'safety evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.11837</guid><pubDate>Wed, 15 Oct 2025 00:00:00 +0000</pubDate></item><item><title>BlackIce: A Containerized Red Teaming Toolkit for AI Security Testing</title><link>https://arxiv.org/abs/2510.11823</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces BlackIce, a containerized red teaming toolkit for AI security testing&lt;/li&gt;&lt;li&gt;Bundles 14 open-source tools for LLM and ML model assessments&lt;/li&gt;&lt;li&gt;Aims to lower barriers to entry and standardize AI red teaming environments&lt;/li&gt;&lt;li&gt;Supports reproducible and modular testing via Docker&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Caelin Kaplan', 'Alexander Warnecke', 'Neil Archibald']&lt;/li&gt;&lt;li&gt;Tags: ['red teaming', 'containerized toolkit', 'AI security', 'LLM', 'ML', 'reproducibility', 'modular architecture']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.11823</guid><pubDate>Wed, 15 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Towards Robust Artificial Intelligence: Self-Supervised Learning Approach for Out-of-Distribution Detection</title><link>https://arxiv.org/abs/2510.12713</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a self-supervised learning approach for OOD detection&lt;/li&gt;&lt;li&gt;Aims to improve AI robustness&lt;/li&gt;&lt;li&gt;Uses graph-theoretical techniques&lt;/li&gt;&lt;li&gt;Achieves high AUROC&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Wissam Salhab', 'Darine Ameyed', 'Hamid Mcheick', 'Fehmi Jaafar']&lt;/li&gt;&lt;li&gt;Tags: ['robustness', 'self-supervised learning', 'out-of-distribution detection', 'graph theory']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.12713</guid><pubDate>Wed, 15 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Evaluating and Mitigating LLM-as-a-judge Bias in Communication Systems</title><link>https://arxiv.org/abs/2510.12462</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Investigates judgment biases in LLM-as-a-judge models (GPT-Judge and JudgeLM)&lt;/li&gt;&lt;li&gt;Evaluates robustness to biased inputs and impact of fine-tuning on biased data&lt;/li&gt;&lt;li&gt;Proposes mitigation strategies for fair AI judging&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jiaxin Gao', 'Chen Chen', 'Yanwen Jia', 'Xueluan Gong', 'Kwok-Yan Lam', 'Qian Wang']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'safety evaluation', 'bias', 'robustness', 'data poisoning']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.12462</guid><pubDate>Wed, 15 Oct 2025 00:00:00 +0000</pubDate></item><item><title>ThinkPilot: Steering Reasoning Models via Automated Think-prefixes Optimization</title><link>https://arxiv.org/abs/2510.12063</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces ThinkPilot, a framework for optimizing LRM reasoning via think-prefixes&lt;/li&gt;&lt;li&gt;Significantly improves safety by reducing StrongREJECT scores&lt;/li&gt;&lt;li&gt;Enhances instruction following and reasoning efficiency&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Sunzhu Li', 'Zhiyu Lin', 'Shuling Yang', 'Jiale Zhao', 'Wei Chen']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'safety evaluation', 'red teaming', 'instruction following']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.12063</guid><pubDate>Wed, 15 Oct 2025 00:00:00 +0000</pubDate></item></channel></rss>