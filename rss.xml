<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>AI Security Paper Digest</title><link>https://kentaroh-toyoda.github.io/ai-security-paper-digest-rss/rss.xml</link><description>Curated papers on AI security from OpenAlex and ArXiv</description><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><lastBuildDate>Tue, 10 Jun 2025 22:43:52 +0000</lastBuildDate><item><title>LLM-attacker: Enhancing Closed-loop Adversarial Scenario Generation for Autonomous Driving with Large Language Models</title><link>https://arxiv.org/abs/2501.15850</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes LLM-attacker, a framework using large language models to generate adversarial scenarios for autonomous driving systems (ADS).&lt;/li&gt;&lt;li&gt;Employs multiple LLM agents to identify and optimize adversarial participants in traffic scenarios.&lt;/li&gt;&lt;li&gt;Uses a closed-loop process where generated adversarial scenarios are iteratively refined based on ADS performance.&lt;/li&gt;&lt;li&gt;Demonstrates that training with these adversarial scenarios significantly improves the safety and robustness of ADS.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research Paper&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Quality Assessment&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Clarity: 4/5&lt;/li&gt;&lt;li&gt;Novelty: 5/5&lt;/li&gt;&lt;li&gt;Significance: 4/5&lt;/li&gt;&lt;li&gt;Try-worthiness: Yes&lt;/li&gt;&lt;li&gt;Justification: The paper is clearly written, with a well-structured abstract that outlines the motivation, challenges, proposed method, and results. The use of large language models (LLMs) for adversarial scenario generation in autonomous driving is a novel approach, especially the closed-loop framework with coordinated LLM agents. While the paper is very recent and has no citations yet, the problem addressed is highly significant for the safety of autonomous vehicles, and the reported results (halving collision rates) are promising. The method appears worth implementing or experimenting with, especially for researchers in autonomous driving safety and adversarial testing. No code repository is provided in the metadata or abstract.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: Yuewen Mei, Tong Nie, Jian Sun, Ye Tian&lt;/li&gt;&lt;li&gt;Tags: adversarial scenario generation, autonomous driving safety, LLM red teaming, robustness testing&lt;/li&gt;&lt;li&gt;Relevance Score: 5/5&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2501.15850</guid><pubDate>Tue, 10 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Black-Box Forgery Attacks on Semantic Watermarks for Diffusion Models</title><link>https://arxiv.org/abs/2412.03283</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Analyzes security vulnerabilities in semantic watermarking techniques for latent diffusion models.&lt;/li&gt;&lt;li&gt;Proposes two black-box forgery attacks that can imprint or remove watermarks using unrelated models.&lt;/li&gt;&lt;li&gt;Demonstrates that attackers can forge or remove semantic watermarks with minimal information (a single reference image).&lt;/li&gt;&lt;li&gt;Findings challenge the reliability of semantic watermarks for content attribution and detection in generative models.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research Paper&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Quality Assessment&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Clarity: 5/5&lt;/li&gt;&lt;li&gt;Novelty: 5/5&lt;/li&gt;&lt;li&gt;Significance: 4/5&lt;/li&gt;&lt;li&gt;Try-worthiness: Yes&lt;/li&gt;&lt;li&gt;Justification: The abstract is very clearly written, concisely explaining the motivation, methods, and implications of the work. The paper addresses a novel and timely security vulnerability in semantic watermarking for diffusion models, introducing two new attack strategies that work even in black-box and cross-architecture settings. This is highly relevant as semantic watermarking is a recent and popular approach for content attribution in generative AI. The significance is high given the growing deployment of such watermarking schemes, though the impact will depend on further peer review and adoption. The work is worth experimenting with, especially for researchers and practitioners concerned with AI-generated content attribution and security. No code repository is provided in the metadata.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: Andreas M\"uller, Denis Lukovnikov, Jonas Thietke, Asja Fischer, Erwin Quiring&lt;/li&gt;&lt;li&gt;Tags: watermarking attacks, diffusion models, AI security, forgery attacks, content attribution&lt;/li&gt;&lt;li&gt;Relevance Score: 5/5&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2412.03283</guid><pubDate>Tue, 10 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Towards Black-Box Membership Inference Attack for Diffusion Models</title><link>https://arxiv.org/abs/2405.20771</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a novel black-box membership inference attack targeting diffusion models, requiring only access to the image-to-image variation API.&lt;/li&gt;&lt;li&gt;Demonstrates that the attack can identify whether a specific image was used in the training set of diffusion models without internal model access.&lt;/li&gt;&lt;li&gt;Validates the method on DDIM, Stable Diffusion, and Diffusion Transformer architectures, showing superior performance over previous approaches.&lt;/li&gt;&lt;li&gt;Addresses privacy and copyright concerns related to AI-generated art by exposing potential vulnerabilities in diffusion models.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research Paper&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Quality Assessment&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Clarity: 4/5&lt;/li&gt;&lt;li&gt;Novelty: 5/5&lt;/li&gt;&lt;li&gt;Significance: 4/5&lt;/li&gt;&lt;li&gt;Try-worthiness: Yes&lt;/li&gt;&lt;li&gt;Justification: The abstract is well-written and clearly explains the motivation, problem, and proposed solution, though some technical details are necessarily omitted. The work is highly novel, introducing a black-box membership inference attack for diffusion models that does not require access to internal U-nets, which is a significant step forward given the proprietary nature of many deployed models. The significance is high due to the relevance of copyright and data privacy in generative AI, and the method's applicability to popular architectures like Stable Diffusion and Diffusion Transformers. While the paper is very recent and has no citations yet, its topic and claimed performance improvements make it worth experimenting with, especially for researchers or practitioners concerned with model privacy and security. No code repository is provided in the metadata.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: Jingwei Li, Jing Dong, Tianxing He, Jingzhao Zhang&lt;/li&gt;&lt;li&gt;Tags: membership inference, privacy attacks, diffusion models, black-box attacks, AI security&lt;/li&gt;&lt;li&gt;Relevance Score: 5/5&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2405.20771</guid><pubDate>Tue, 10 Jun 2025 00:00:00 +0000</pubDate></item><item><title>RAID: A Dataset for Testing the Adversarial Robustness of AI-Generated Image Detectors</title><link>https://arxiv.org/abs/2506.03988</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces RAID, a large dataset of adversarial examples targeting AI-generated image detectors.&lt;/li&gt;&lt;li&gt;Demonstrates that current state-of-the-art detectors are vulnerable to adversarial attacks.&lt;/li&gt;&lt;li&gt;Provides tools and data to facilitate robust evaluation of AI-generated image detectors under adversarial conditions.&lt;/li&gt;&lt;li&gt;Highlights the need for more robust detection methods to counter fraud and disinformation risks.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research Paper&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Quality Assessment&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Clarity: 5/5&lt;/li&gt;&lt;li&gt;Novelty: 4/5&lt;/li&gt;&lt;li&gt;Significance: 4/5&lt;/li&gt;&lt;li&gt;Try-worthiness: Yes&lt;/li&gt;&lt;li&gt;Justification: The abstract is very clear, well-structured, and communicates the motivation, methodology, and contributions of the work effectively. The novelty is high, as the paper introduces a new large-scale dataset (RAID) specifically for testing adversarial robustness of AI-generated image detectors, which addresses a gap in current evaluation practices. The significance is strong given the urgent need for robust detection methods in the face of increasingly convincing AI-generated images, though the impact is yet to be measured due to the paper's recency and preprint status. The dataset and code are openly released, making it highly try-worthy for researchers and practitioners interested in adversarial robustness and AI-generated content detection.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: Hicham Eddoubi, Jonas Ricker, Federico Cocchi, Lorenzo Baraldi, Angelo Sotgiu, Maura Pintor, Marcella Cornia, Lorenzo Baraldi, Asja Fischer, Rita Cucchiara, Battista Biggio&lt;/li&gt;&lt;li&gt;Tags: adversarial robustness, AI-generated image detection, dataset, security evaluation, adversarial attacks&lt;/li&gt;&lt;li&gt;Relevance Score: 5/5&lt;/li&gt;&lt;li&gt;Code Repository: &lt;a href='https://github.com/pralab/RAID'&gt;https://github.com/pralab/RAID&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.03988</guid><pubDate>Tue, 10 Jun 2025 00:00:00 +0000</pubDate></item><item><title>CAPAA: Classifier-Agnostic Projector-Based Adversarial Attack</title><link>https://arxiv.org/abs/2506.00978</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a new projector-based adversarial attack (CAPAA) that works across multiple classifiers and varying camera poses.&lt;/li&gt;&lt;li&gt;Introduces a classifier-agnostic adversarial loss and optimization framework to aggregate gradients from multiple classifiers.&lt;/li&gt;&lt;li&gt;Develops an attention-based gradient weighting mechanism to enhance attack robustness and stealthiness.&lt;/li&gt;&lt;li&gt;Demonstrates through experiments that CAPAA outperforms existing projector-based adversarial attack methods.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research Paper&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Quality Assessment&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Clarity: 4/5&lt;/li&gt;&lt;li&gt;Novelty: 5/5&lt;/li&gt;&lt;li&gt;Significance: 4/5&lt;/li&gt;&lt;li&gt;Try-worthiness: Yes&lt;/li&gt;&lt;li&gt;Justification: The abstract is well-written and clearly explains the motivation, contributions, and results of the paper, though some technical details are dense. The work is highly novel, introducing a classifier-agnostic projector-based adversarial attack that addresses limitations of prior methods (single classifier, fixed camera pose) and proposes an attention-based gradient weighting mechanism. The significance is high given the practical implications for privacy and robustness in real-world systems, though the impact is yet to be seen due to its recent release and preprint status. The availability of code and promising experimental results make this paper worth trying for researchers and practitioners in adversarial machine learning.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: Zhan Li, Mingyu Zhao, Xin Dong, Haibin Ling, Bingyao Huang&lt;/li&gt;&lt;li&gt;Tags: adversarial attacks, robustness, physical attacks, classifier-agnostic, AI security&lt;/li&gt;&lt;li&gt;Relevance Score: 5/5&lt;/li&gt;&lt;li&gt;Code Repository: &lt;a href='https://github.com/ZhanLiQxQ/CAPAA'&gt;https://github.com/ZhanLiQxQ/CAPAA&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.00978</guid><pubDate>Tue, 10 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Robustifying Vision-Language Models via Dynamic Token Reweighting</title><link>https://arxiv.org/abs/2505.17132</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes DTR, a novel inference-time defense mechanism for vision-language models (VLMs) against jailbreak attacks.&lt;/li&gt;&lt;li&gt;Introduces dynamic token reweighting by optimizing key-value caches to mitigate adversarial visual inputs.&lt;/li&gt;&lt;li&gt;Demonstrates improved robustness against multimodal jailbreak attacks without sacrificing benign task performance.&lt;/li&gt;&lt;li&gt;Presents the first use of KV cache optimization for enhancing safety in multimodal foundation models.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research Paper&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Quality Assessment&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Clarity: 4/5&lt;/li&gt;&lt;li&gt;Novelty: 5/5&lt;/li&gt;&lt;li&gt;Significance: 4/5&lt;/li&gt;&lt;li&gt;Try-worthiness: Yes&lt;/li&gt;&lt;li&gt;Justification: The abstract is well-written and clearly explains the motivation, method (DTR), and contributions, though some technical terms (e.g., KV caches) may require background knowledge. The approach—dynamic token reweighting via KV cache optimization for defending against multimodal jailbreak attacks—appears novel, especially as it claims to be the first to apply this technique for safety in VLMs. The significance is high given the growing concern over VLM safety and the lack of robust defenses, though the paper is very new and only on arXiv, so impact is yet to be seen. The method is described as inference-time, efficient, and outperforming prior work, making it worth trying for practitioners and researchers in the field. No code repository is provided in the metadata.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: Tanqiu Jiang, Jiacheng Liang, Rongyi Zhu, Jiawei Zhou, Fenglong Ma, Ting Wang&lt;/li&gt;&lt;li&gt;Tags: jailbreak defense, vision-language models, adversarial robustness, AI safety, multimodal security&lt;/li&gt;&lt;li&gt;Relevance Score: 5/5&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2505.17132</guid><pubDate>Tue, 10 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Diffusion-based Adversarial Purification from the Perspective of the Frequency Domain</title><link>https://arxiv.org/abs/2505.01267</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a diffusion-based method for purifying adversarial examples in images by leveraging frequency domain analysis.&lt;/li&gt;&lt;li&gt;Finds that adversarial perturbations cause more damage at higher frequencies in both amplitude and phase spectra.&lt;/li&gt;&lt;li&gt;Introduces a technique to selectively preserve low-frequency components during purification, reducing semantic loss.&lt;/li&gt;&lt;li&gt;Demonstrates through experiments that the proposed method outperforms existing adversarial defense techniques.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research Paper&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Quality Assessment&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Clarity: 4/5&lt;/li&gt;&lt;li&gt;Novelty: 5/5&lt;/li&gt;&lt;li&gt;Significance: 4/5&lt;/li&gt;&lt;li&gt;Try-worthiness: Yes&lt;/li&gt;&lt;li&gt;Justification: The abstract is well-written and clearly explains both the motivation and the proposed method, though some technical details may require domain knowledge. The approach is novel, leveraging frequency domain analysis for adversarial purification in diffusion models, which is a fresh perspective compared to prior pixel-domain methods. While the paper is very new and has no citations yet, the problem addressed is significant in adversarial robustness, and the claimed empirical improvements suggest potential impact. The method appears practical and worth experimenting with, especially for researchers in adversarial defenses and diffusion models.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: Gaozheng Pei, Ke Ma, Yingfei Sun, Qianqian Xu, Qingming Huang&lt;/li&gt;&lt;li&gt;Tags: adversarial robustness, adversarial defense, image purification, frequency domain analysis&lt;/li&gt;&lt;li&gt;Relevance Score: 5/5&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2505.01267</guid><pubDate>Tue, 10 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Gungnir: Exploiting Stylistic Features in Images for Backdoor Attacks on Diffusion Models</title><link>https://arxiv.org/abs/2502.20650</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces Gungnir, a novel backdoor attack method targeting diffusion models using stylistic features as triggers.&lt;/li&gt;&lt;li&gt;Demonstrates that these style-based triggers are perceptually indistinguishable from clean images, evading both manual and automated detection.&lt;/li&gt;&lt;li&gt;Proposes new techniques (Reconstructing-Adversarial Noise and Short-Term Timesteps-Retention) to enhance the stealth and effectiveness of the attack.&lt;/li&gt;&lt;li&gt;Shows experimentally that Gungnir bypasses existing defense mechanisms, achieving a 0 backdoor detection rate.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research Paper&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Quality Assessment&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Clarity: 4/5&lt;/li&gt;&lt;li&gt;Novelty: 5/5&lt;/li&gt;&lt;li&gt;Significance: 4/5&lt;/li&gt;&lt;li&gt;Try-worthiness: Yes&lt;/li&gt;&lt;li&gt;Justification: The abstract is well-written and clearly explains the motivation, method, and results, though some technical terms (e.g., RAN, STTR) are not fully explained in the abstract. The work is highly novel, introducing the use of stylistic features as backdoor triggers in diffusion models, which has not been previously explored. The significance is high given the growing importance of diffusion models and the demonstrated ability to bypass existing defenses, though the paper is very new and only on arXiv, so its impact is not yet established. The method appears robust and practical, with code available, making it worth trying for researchers in security and generative models.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: Yu Pan, Jiahao Chen, Bingrong Dai, Lin Wang, Yi Du, Jiao Liu&lt;/li&gt;&lt;li&gt;Tags: backdoor attacks, diffusion models, adversarial machine learning, AI security, robustness&lt;/li&gt;&lt;li&gt;Relevance Score: 5/5&lt;/li&gt;&lt;li&gt;Code Repository: &lt;a href='https://github.com/paoche11/Gungnir'&gt;https://github.com/paoche11/Gungnir&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2502.20650</guid><pubDate>Tue, 10 Jun 2025 00:00:00 +0000</pubDate></item><item><title>EgoNormia: Benchmarking Physical Social Norm Understanding</title><link>https://arxiv.org/abs/2502.20490</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces EGONORMIA, a benchmark dataset for evaluating physical and social norm understanding in vision-language models (VLMs).&lt;/li&gt;&lt;li&gt;Covers norm categories including safety and privacy, which are directly relevant to AI safety concerns in real-world agents.&lt;/li&gt;&lt;li&gt;Finds that current VLMs perform poorly on norm understanding, highlighting potential safety and privacy risks.&lt;/li&gt;&lt;li&gt;Explores methods to improve normative reasoning, such as retrieval-augmented generation (RAG).&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: MohammadHossein Rezaei, Yicheng Fu, Phil Cuvin, Caleb Ziems, Yanzhe Zhang, Hao Zhu, Diyi Yang&lt;/li&gt;&lt;li&gt;Tags: AI safety, normative reasoning, privacy, vision-language models, benchmarking&lt;/li&gt;&lt;li&gt;Relevance Score: 4/5&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2502.20490</guid><pubDate>Tue, 10 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Synthetic Face Datasets Generation via Latent Space Exploration from Brownian Identity Diffusion</title><link>https://arxiv.org/abs/2405.00228</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes new algorithms for generating synthetic face datasets using latent space exploration inspired by Brownian motion.&lt;/li&gt;&lt;li&gt;Addresses privacy concerns by generating synthetic data to replace or complement real face data in training recognition models.&lt;/li&gt;&lt;li&gt;Benchmarks the generated datasets for diversity and performance, showing competitive results with state-of-the-art methods.&lt;/li&gt;&lt;li&gt;Explicitly discusses prevention of data leakage and memorization, aiming for responsible and privacy-preserving dataset creation.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research Paper&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: David Geissb\"uhler, Hatef Otroshi Shahreza, S\'ebastien Marcel&lt;/li&gt;&lt;li&gt;Tags: synthetic data, privacy, face recognition, data leakage prevention, diffusion models&lt;/li&gt;&lt;li&gt;Relevance Score: 3/5&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2405.00228</guid><pubDate>Tue, 10 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Certified Human Trajectory Prediction</title><link>https://arxiv.org/abs/2403.13778</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a certification approach for human trajectory prediction models to guarantee robustness against adversarial examples and noisy inputs.&lt;/li&gt;&lt;li&gt;Addresses challenges specific to trajectory prediction, such as unbounded outputs and multi-modality.&lt;/li&gt;&lt;li&gt;Introduces a diffusion-based trajectory denoiser to mitigate performance drops due to certification.&lt;/li&gt;&lt;li&gt;Presents new certified performance metrics and demonstrates improved robustness and accuracy through experiments.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research Paper&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Quality Assessment&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Clarity: 4/5&lt;/li&gt;&lt;li&gt;Novelty: 5/5&lt;/li&gt;&lt;li&gt;Significance: 4/5&lt;/li&gt;&lt;li&gt;Try-worthiness: Yes&lt;/li&gt;&lt;li&gt;Justification: The abstract is well-written and clearly outlines the motivation, challenges, and contributions of the work, though some technical details are necessarily omitted at the abstract level. The novelty is high: the paper proposes a certification approach for human trajectory prediction, addressing robustness guarantees, which is a significant step beyond heuristic or empirical methods. The use of diffusion-based denoising and new certified performance metrics further adds to its originality. While the paper is very recent and has no citations yet, the problem addressed is important for autonomous vehicles and safety-critical systems, making the work significant. The availability of code and the practical focus on robustness make this paper worth trying for researchers and practitioners in trajectory prediction and autonomous systems.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: Mohammadhossein Bahari, Saeed Saadatnejad, Amirhossein Askari Farsangi, Seyed-Mohsen Moosavi-Dezfooli, Alexandre Alahi&lt;/li&gt;&lt;li&gt;Tags: robustness, adversarial examples, certification, trajectory prediction, AI safety&lt;/li&gt;&lt;li&gt;Relevance Score: 4/5&lt;/li&gt;&lt;li&gt;Code Repository: &lt;a href='https://s-attack.github.io/'&gt;https://s-attack.github.io/&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2403.13778</guid><pubDate>Tue, 10 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Flattery in Motion: Benchmarking and Analyzing Sycophancy in Video-LLMs</title><link>https://arxiv.org/abs/2506.07180</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces VISE, a benchmark for evaluating sycophantic behavior in Video-LLMs when user input contradicts visual evidence.&lt;/li&gt;&lt;li&gt;Analyzes how Video-LLMs respond to misleading prompts and identifies types and patterns of sycophancy.&lt;/li&gt;&lt;li&gt;Explores mitigation strategies, such as key-frame selection, to reduce sycophantic bias and improve factual consistency.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Quality Assessment&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Clarity: 5/5&lt;/li&gt;&lt;li&gt;Novelty: 5/5&lt;/li&gt;&lt;li&gt;Significance: 4/5&lt;/li&gt;&lt;li&gt;Try-worthiness: Yes&lt;/li&gt;&lt;li&gt;Justification: The paper is very clearly written, with a well-structured abstract that defines the problem (sycophancy in Video-LLMs), the gap in current research, and the proposed solution (the VISE benchmark). The work is highly novel, as it is the first to systematically benchmark and analyze sycophancy specifically in Video-LLMs, extending linguistic perspectives into the visual domain. The significance is high given the growing importance of Video-LLMs and the critical need for trustworthy multimodal reasoning, though as a very recent preprint, it has not yet accumulated citations or peer-reviewed recognition. The introduction of a new benchmark and a mitigation strategy makes it worth trying for researchers in the field. No code repository is provided in the metadata.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: Wenrui Zhou, Shu Yang, Qingsong Yang, Zikun Guo, Lijie Hu, Di Wang&lt;/li&gt;&lt;li&gt;Tags: AI safety, alignment, harmful outputs, benchmarking, multimodal AI&lt;/li&gt;&lt;li&gt;Relevance Score: 4/5&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.07180</guid><pubDate>Tue, 10 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Rewriting the Budget: A General Framework for Black-Box Attacks Under Cost Asymmetry</title><link>https://arxiv.org/abs/2506.06933</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a general framework for decision-based black-box adversarial attacks under asymmetric query costs.&lt;/li&gt;&lt;li&gt;Introduces new algorithms (Asymmetric Search and Asymmetric Gradient Estimation) to minimize attack costs in scenarios where different queries have different associated costs.&lt;/li&gt;&lt;li&gt;Demonstrates that the proposed methods can be integrated into existing black-box attack strategies with minimal changes.&lt;/li&gt;&lt;li&gt;Provides both theoretical analysis and empirical results showing significant improvements in attack efficiency and effectiveness.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research Paper&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Quality Assessment&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Clarity: 5/5&lt;/li&gt;&lt;li&gt;Novelty: 5/5&lt;/li&gt;&lt;li&gt;Significance: 4/5&lt;/li&gt;&lt;li&gt;Try-worthiness: Yes&lt;/li&gt;&lt;li&gt;Justification: The abstract is very clearly written, outlining the motivation, problem, proposed methods, and results in a structured way. The work is highly novel, addressing the underexplored area of black-box adversarial attacks under asymmetric query costs, and introducing new algorithmic components (AS and AGREST) that can be integrated into existing attacks. While the paper is a preprint and has no citations yet (which is expected given its recency), the problem is practically significant, especially for real-world systems like content moderation. The reported improvements (up to 40% reduction in cost) and the generality of the framework make it worth trying for researchers and practitioners in adversarial ML. No code repository is provided in the metadata.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: Mahdi Salmani, Alireza Abdollahpoorrostam, Seyed-Mohsen Moosavi-Dezfooli&lt;/li&gt;&lt;li&gt;Tags: black-box attacks, adversarial examples, cost asymmetry, AI security, robustness&lt;/li&gt;&lt;li&gt;Relevance Score: 5/5&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.06933</guid><pubDate>Tue, 10 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Video Unlearning via Low-Rank Refusal Vector</title><link>https://arxiv.org/abs/2506.07891</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces a novel unlearning technique for video diffusion models to suppress generation of harmful or illegal content.&lt;/li&gt;&lt;li&gt;Uses a low-rank refusal vector derived from prompt pairs to neutralize unsafe concepts while preserving model quality.&lt;/li&gt;&lt;li&gt;The method embeds the refusal direction into model weights, making it more robust against adversarial bypass than input-output filters.&lt;/li&gt;&lt;li&gt;Demonstrates effectiveness in neutralizing explicit, violent, and copyright-infringing content without retraining or original data access.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research Paper&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Quality Assessment&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Clarity: 5/5&lt;/li&gt;&lt;li&gt;Novelty: 5/5&lt;/li&gt;&lt;li&gt;Significance: 4/5&lt;/li&gt;&lt;li&gt;Try-worthiness: Yes&lt;/li&gt;&lt;li&gt;Justification: The abstract is very well-written, clearly outlining the motivation, method, and contributions of the work. The proposed 'refusal vector' and low-rank factorization approach for unlearning in video diffusion models is novel, especially as it claims to be the first technique tailored for this domain. The significance is high due to the urgent need for safety mechanisms in generative video models, though the impact is yet to be seen given the paper's recency and preprint status. The method's practicality (no retraining, no original data required, robust to adversarial bypass) makes it highly try-worthy for researchers and practitioners concerned with model safety. The project page is provided, but no direct code repository link is given in the metadata.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: Simone Facchiano, Stefano Saravalle, Matteo Migliarini, Edoardo De Matteis, Alessio Sampieri, Andrea Pilzer, Emanuele Rodol\`a, Indro Spinelli, Luca Franco, Fabio Galasso&lt;/li&gt;&lt;li&gt;Tags: AI safety, harmful content suppression, robustness, adversarial robustness, video diffusion models&lt;/li&gt;&lt;li&gt;Relevance Score: 5/5&lt;/li&gt;&lt;li&gt;Code Repository: &lt;a href='https://www.pinlab.org/video-unlearning'&gt;https://www.pinlab.org/video-unlearning&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.07891</guid><pubDate>Tue, 10 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Explore the vulnerability of black-box models via diffusion models</title><link>https://arxiv.org/abs/2506.07590</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces a novel attack using diffusion models to generate synthetic images for training substitute models.&lt;/li&gt;&lt;li&gt;Demonstrates model extraction and transfer-based adversarial attacks on black-box classification models.&lt;/li&gt;&lt;li&gt;Achieves high attack success rates with minimal queries, outperforming state-of-the-art methods.&lt;/li&gt;&lt;li&gt;Highlights significant security and privacy risks posed by diffusion models in black-box settings.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research Paper&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Quality Assessment&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Clarity: 4/5&lt;/li&gt;&lt;li&gt;Novelty: 5/5&lt;/li&gt;&lt;li&gt;Significance: 4/5&lt;/li&gt;&lt;li&gt;Try-worthiness: Yes&lt;/li&gt;&lt;li&gt;Justification: The abstract is well-written, clearly outlining the motivation, methodology, and results of the work. The paper introduces a novel security threat by leveraging diffusion models for model extraction and adversarial attacks, which is a timely and original contribution given the rise of diffusion models. The reported improvements over state-of-the-art methods and high attack success rates across multiple benchmarks indicate significant practical impact. Although the paper is a preprint and has no citations yet, its relevance and strong results make it worth experimenting with, especially for researchers in AI security and privacy. No code repository is provided in the metadata.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: Jiacheng Shi, Yanfu Zhang, Huajie Shao, Ashley Gao&lt;/li&gt;&lt;li&gt;Tags: model extraction, adversarial attacks, diffusion models, black-box security, AI security&lt;/li&gt;&lt;li&gt;Relevance Score: 5/5&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.07590</guid><pubDate>Tue, 10 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Uncertainty-o: One Model-agnostic Framework for Unveiling Uncertainty in Large Multimodal Models</title><link>https://arxiv.org/abs/2506.07575</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a model-agnostic framework (Uncertainty-o) for evaluating and quantifying uncertainty in Large Multimodal Models (LMMs).&lt;/li&gt;&lt;li&gt;Explores multimodal prompt perturbations to empirically reveal uncertainty in LMMs.&lt;/li&gt;&lt;li&gt;Demonstrates applications of uncertainty estimation for tasks such as hallucination detection and mitigation, and uncertainty-aware reasoning.&lt;/li&gt;&lt;li&gt;Evaluates the framework across multiple benchmarks and LMMs, both open- and closed-source.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research Paper&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Quality Assessment&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Clarity: 4/5&lt;/li&gt;&lt;li&gt;Novelty: 5/5&lt;/li&gt;&lt;li&gt;Significance: 4/5&lt;/li&gt;&lt;li&gt;Try-worthiness: Yes&lt;/li&gt;&lt;li&gt;Justification: The abstract is well-written and clearly outlines the motivation, challenges, and contributions of the work. The focus on uncertainty estimation in large multimodal models (LMMs) is highly relevant and novel, especially given the current interest in robust AI systems. The proposed framework, Uncertainty-o, claims to be model-agnostic and applicable across various modalities and architectures, which is a significant advancement. The empirical evaluation across 18 benchmarks and 10 LMMs further adds to its significance. Although the paper is very recent and has no citations yet, its potential impact on tasks like hallucination detection and uncertainty-aware reasoning makes it worth trying. No code repository is provided in the metadata.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: Ruiyang Zhang, Hu Zhang, Hao Fei, Zhedong Zheng&lt;/li&gt;&lt;li&gt;Tags: uncertainty estimation, robustness, hallucination detection, multimodal models, AI safety&lt;/li&gt;&lt;li&gt;Relevance Score: 3/5&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.07575</guid><pubDate>Tue, 10 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Synthesize Privacy-Preserving High-Resolution Images via Private Textual Intermediaries</title><link>https://arxiv.org/abs/2506.07555</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a novel method (SPTI) for generating high-resolution, differentially private synthetic images using private textual intermediaries.&lt;/li&gt;&lt;li&gt;Shifts the privacy challenge from the image domain to the text domain by summarizing images into text, applying DP text generation, and reconstructing images.&lt;/li&gt;&lt;li&gt;Demonstrates significant improvements in image quality (FID scores) over previous differentially private image synthesis methods.&lt;/li&gt;&lt;li&gt;Enables privacy-preserving sharing and analysis of sensitive visual data without model training, using only inference with off-the-shelf models.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research Paper&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Quality Assessment&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Clarity: 5/5&lt;/li&gt;&lt;li&gt;Novelty: 5/5&lt;/li&gt;&lt;li&gt;Significance: 4/5&lt;/li&gt;&lt;li&gt;Try-worthiness: Yes&lt;/li&gt;&lt;li&gt;Justification: The abstract is very clearly written, outlining the motivation, method, and results in a concise manner. The approach—using private textual intermediaries to synthesize high-resolution, differentially private images—is novel and creative, shifting the privacy challenge from the image to the text domain. The reported improvements in FID scores over existing methods on standard datasets (LSUN Bedroom, CelebA HQ) suggest significant practical impact. While the paper is very new and has no citations yet, the results and method are promising and likely to be of interest to both privacy and generative modeling communities. The method's lack of training requirements and compatibility with off-the-shelf models further increase its try-worthiness. No code repository is provided in the metadata.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: Haoxiang Wang, Zinan Lin, Da Yu, Huishuai Zhang&lt;/li&gt;&lt;li&gt;Tags: differential privacy, privacy-preserving AI, synthetic data generation, image synthesis&lt;/li&gt;&lt;li&gt;Relevance Score: 4/5&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.07555</guid><pubDate>Tue, 10 Jun 2025 00:00:00 +0000</pubDate></item><item><title>MrM: Black-Box Membership Inference Attacks against Multimodal RAG Systems</title><link>https://arxiv.org/abs/2506.07399</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces MrM, the first black-box membership inference attack (MIA) framework targeting multimodal retrieval-augmented generation (RAG) systems.&lt;/li&gt;&lt;li&gt;Focuses on privacy vulnerabilities in multimodal RAG systems, specifically exploiting both visual and textual modalities.&lt;/li&gt;&lt;li&gt;Proposes a novel object-aware data perturbation and counterfact-informed mask selection strategy to enhance attack efficacy.&lt;/li&gt;&lt;li&gt;Demonstrates strong attack performance and robustness against adaptive defenses on commercial vision-language models.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research Paper&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Quality Assessment&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Clarity: 4/5&lt;/li&gt;&lt;li&gt;Novelty: 5/5&lt;/li&gt;&lt;li&gt;Significance: 4/5&lt;/li&gt;&lt;li&gt;Try-worthiness: Yes&lt;/li&gt;&lt;li&gt;Justification: The abstract is well-written and clearly explains the motivation, methodology, and results of the paper, though it is somewhat dense with technical terms. The work is highly novel, being the first to propose a black-box membership inference attack specifically targeting multimodal RAG systems, addressing a gap in the literature where most prior work focused only on textual modalities. The significance is high given the increasing deployment of multimodal RAG systems and the privacy risks involved, though the impact is yet to be seen due to the paper's recency and preprint status. The method is evaluated on strong commercial models and shows robust results, making it worth trying for researchers or practitioners interested in privacy and security of AI systems. No code repository is provided in the metadata.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: Peiru Yang, Jinhua Yin, Haoran Zheng, Xueying Bai, Huili Wang, Yufei Sun, Xintian Li, Shangguang Wang, Yongfeng Huang, Tao Qi&lt;/li&gt;&lt;li&gt;Tags: membership inference, privacy attacks, multimodal RAG, vision-language models, black-box attacks&lt;/li&gt;&lt;li&gt;Relevance Score: 5/5&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.07399</guid><pubDate>Tue, 10 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Backdoor Attack on Vision Language Models with Stealthy Semantic Manipulation</title><link>https://arxiv.org/abs/2506.07214</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces BadSem, a novel backdoor attack on Vision Language Models (VLMs) using stealthy semantic manipulation via cross-modal mismatches.&lt;/li&gt;&lt;li&gt;Demonstrates the attack's effectiveness with high attack success rates (ASR) and transferability across models and datasets.&lt;/li&gt;&lt;li&gt;Analyzes model behavior under attack using attention visualization, showing focus on semantically sensitive regions.&lt;/li&gt;&lt;li&gt;Evaluates defense strategies (system prompt and supervised fine-tuning) and finds them ineffective against the proposed attack.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research Paper&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: Zhiyuan Zhong, Zhen Sun, Yepang Liu, Xinlei He, Guanhong Tao&lt;/li&gt;&lt;li&gt;Tags: backdoor attacks, vision language models, data poisoning, AI security, adversarial attacks&lt;/li&gt;&lt;li&gt;Relevance Score: 5/5&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.07214</guid><pubDate>Tue, 10 Jun 2025 00:00:00 +0000</pubDate></item><item><title>D2R: dual regularization loss with collaborative adversarial generation for model robustness</title><link>https://arxiv.org/abs/2506.07056</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a dual regularization loss (D2R Loss) and collaborative adversarial generation (CAG) strategy to improve model robustness.&lt;/li&gt;&lt;li&gt;Addresses limitations in current adversarial defense methods, specifically in loss function guidance and adversarial sample generation.&lt;/li&gt;&lt;li&gt;Demonstrates improved robustness of deep neural networks against adversarial attacks through extensive experiments on standard benchmarks.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research Paper&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Quality Assessment&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Clarity: 4/5&lt;/li&gt;&lt;li&gt;Novelty: 4/5&lt;/li&gt;&lt;li&gt;Significance: 3/5&lt;/li&gt;&lt;li&gt;Try-worthiness: Yes&lt;/li&gt;&lt;li&gt;Justification: The abstract is well-written and clearly outlines the motivation, proposed methods (D2R loss and CAG), and experimental validation. The dual regularization loss and collaborative adversarial generation appear to be novel contributions, addressing specific limitations in existing adversarial robustness methods. The use of strong benchmarks (CIFAR-10, CIFAR-100, Tiny ImageNet) and popular architectures (WideResNet34-10, PreActResNet18) adds credibility, though the significance is limited by the paper's very recent release and lack of citations. As a preprint, it has not yet undergone peer review, but the described approach seems promising and worth experimenting with for researchers in adversarial robustness.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: Zhenyu Liu, Huizhi Liang, Rajiv Ranjan, Zhanxing Zhu, Vaclav Snasel, Varun Ojha&lt;/li&gt;&lt;li&gt;Tags: adversarial robustness, adversarial training, model robustness, deep learning security&lt;/li&gt;&lt;li&gt;Relevance Score: 5/5&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.07056</guid><pubDate>Tue, 10 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Interpretable and Reliable Detection of AI-Generated Images via Grounded Reasoning in MLLMs</title><link>https://arxiv.org/abs/2506.07045</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a method for interpretable and robust detection of AI-generated (synthetic) images using fine-tuned Multi-modal Large Language Models (MLLMs).&lt;/li&gt;&lt;li&gt;Introduces a new dataset of AI-generated images annotated with bounding boxes and captions to highlight synthesis artifacts.&lt;/li&gt;&lt;li&gt;Focuses on improving both detection accuracy and the ability to provide human-understandable explanations for detection decisions.&lt;/li&gt;&lt;li&gt;Addresses the challenge of aligning model reasoning with human visual interpretation to reduce hallucinations and improve reliability.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research Paper&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Quality Assessment&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Clarity: 5/5&lt;/li&gt;&lt;li&gt;Novelty: 4/5&lt;/li&gt;&lt;li&gt;Significance: 4/5&lt;/li&gt;&lt;li&gt;Try-worthiness: Yes&lt;/li&gt;&lt;li&gt;Justification: The abstract is well-written, clearly outlining the motivation, methodology, and contributions of the paper. The work is novel in its use of fine-tuned MLLMs for interpretable detection of AI-generated images, especially with the introduction of a new dataset annotated for grounded reasoning. The significance is high given the growing importance of detecting AI-generated content and the focus on interpretability, though the impact is yet to be seen due to its recent release and preprint status. The approach is promising and worth experimenting with, especially for researchers interested in explainable AI and image forensics. No code repository is provided in the metadata.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: Yikun Ji, Hong Yan, Jun Lan, Huijia Zhu, Weiqiang Wang, Qi Fan, Liqing Zhang, Jianfu Zhang&lt;/li&gt;&lt;li&gt;Tags: AI-generated content detection, model robustness, interpretability, MLLMs, forgery detection&lt;/li&gt;&lt;li&gt;Relevance Score: 4/5&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.07045</guid><pubDate>Tue, 10 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Boosting Adversarial Transferability via Commonality-Oriented Gradient Optimization</title><link>https://arxiv.org/abs/2506.06992</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a new method (COGO) to improve the transferability of adversarial examples against Vision Transformers (ViTs).&lt;/li&gt;&lt;li&gt;Introduces techniques to enhance perturbations common across surrogate models and suppress those unique to individual models.&lt;/li&gt;&lt;li&gt;Demonstrates that the method significantly increases the success rate of black-box adversarial attacks compared to existing approaches.&lt;/li&gt;&lt;li&gt;Provides insights into the vulnerabilities and robustness of ViTs in adversarial settings.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research Paper&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: Yanting Gao, Yepeng Liu, Junming Liu, Qi Zhang, Hongyun Zhang, Duoqian Miao, Cairong Zhao&lt;/li&gt;&lt;li&gt;Tags: adversarial examples, transferability, robustness, vision transformers, black-box attacks&lt;/li&gt;&lt;li&gt;Relevance Score: 5/5&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.06992</guid><pubDate>Tue, 10 Jun 2025 00:00:00 +0000</pubDate></item><item><title>KNN-Defense: Defense against 3D Adversarial Point Clouds using Nearest-Neighbor Search</title><link>https://arxiv.org/abs/2506.06906</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes KNN-Defense, a method to defend 3D point cloud classifiers against adversarial attacks such as point dropping, shifting, and adding.&lt;/li&gt;&lt;li&gt;Utilizes nearest-neighbor search in feature space to restore perturbed inputs based on semantic similarity.&lt;/li&gt;&lt;li&gt;Demonstrates significant improvements in robustness against various attack types on multiple 3D vision models.&lt;/li&gt;&lt;li&gt;Method is lightweight, computationally efficient, and suitable for real-time applications.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research Paper&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Quality Assessment&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Clarity: 5/5&lt;/li&gt;&lt;li&gt;Novelty: 4/5&lt;/li&gt;&lt;li&gt;Significance: 4/5&lt;/li&gt;&lt;li&gt;Try-worthiness: Yes&lt;/li&gt;&lt;li&gt;Justification: The abstract is well-written, clearly outlining the problem, the proposed method (KNN-Defense), and its empirical results. The approach is novel in the context of 3D adversarial defense, leveraging nearest-neighbor search in feature space rather than traditional geometric reconstruction or uniformity enforcement. The reported improvements, especially under point-dropping attacks, are substantial and relevant for the field. While the paper is a recent arXiv preprint and has no citations yet, the results and the availability of open-source code make it significant and worth trying for researchers and practitioners in 3D vision and adversarial robustness.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: Nima Jamali, Matina Mahdizadeh Sani, Hanieh Naderi, Shohreh Kasaei&lt;/li&gt;&lt;li&gt;Tags: adversarial robustness, 3D point clouds, defense mechanisms, deep learning security&lt;/li&gt;&lt;li&gt;Relevance Score: 5/5&lt;/li&gt;&lt;li&gt;Code Repository: &lt;a href='https://github.com/nimajam41/3d-knn-defense'&gt;https://github.com/nimajam41/3d-knn-defense&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.06906</guid><pubDate>Tue, 10 Jun 2025 00:00:00 +0000</pubDate></item><item><title>LitMAS: A Lightweight and Generalized Multi-Modal Anti-Spoofing Framework for Biometric Security</title><link>https://arxiv.org/abs/2506.06759</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes LitMAS, a lightweight and generalized anti-spoofing framework for multiple biometric modalities (speech, face, iris, fingerprint).&lt;/li&gt;&lt;li&gt;Introduces a Modality-Aligned Concentration Loss to improve spoof detection and cross-modal consistency.&lt;/li&gt;&lt;li&gt;Demonstrates superior performance and efficiency compared to state-of-the-art methods, making it suitable for edge deployment.&lt;/li&gt;&lt;li&gt;Addresses the security vulnerability of biometric systems to spoofing attacks.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research Paper&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Quality Assessment&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Clarity: 5/5&lt;/li&gt;&lt;li&gt;Novelty: 4/5&lt;/li&gt;&lt;li&gt;Significance: 4/5&lt;/li&gt;&lt;li&gt;Try-worthiness: Yes&lt;/li&gt;&lt;li&gt;Justification: The abstract is well-written, clearly outlining the motivation, proposed method (LitMAS), and its advantages. The focus on a lightweight, generalized anti-spoofing framework across multiple biometric modalities is novel, as most prior work is modality-specific. The introduction of a new loss function (Modality-Aligned Concentration Loss) and strong results across seven datasets further support its novelty and significance. While the paper is a recent arXiv preprint and has no citations yet, its reported performance and efficiency make it a promising candidate for experimentation, especially for edge deployment scenarios. The availability of code and trained models increases its practical value.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: Nidheesh Gorthi, Kartik Thakral, Rishabh Ranjan, Richa Singh, Mayank Vatsa&lt;/li&gt;&lt;li&gt;Tags: biometric security, anti-spoofing, adversarial attacks, robustness, multi-modal AI&lt;/li&gt;&lt;li&gt;Relevance Score: 4/5&lt;/li&gt;&lt;li&gt;Code Repository: &lt;a href='https://github.com/IAB-IITJ/LitMAS'&gt;https://github.com/IAB-IITJ/LitMAS&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.06759</guid><pubDate>Tue, 10 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Securing Traffic Sign Recognition Systems in Autonomous Vehicles</title><link>https://arxiv.org/abs/2506.06563</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Investigates the vulnerability of DNN-based traffic sign recognition systems to data poisoning attacks.&lt;/li&gt;&lt;li&gt;Proposes a data augmentation-based training method to mitigate the impact of error-minimizing (poisoning) attacks.&lt;/li&gt;&lt;li&gt;Demonstrates the effectiveness of the mitigation scheme and a detection model for identifying poisoned data.&lt;/li&gt;&lt;li&gt;Highlights the importance of robust training methods for securing AI systems in autonomous vehicles.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research Paper&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Quality Assessment&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Clarity: 4/5&lt;/li&gt;&lt;li&gt;Novelty: 4/5&lt;/li&gt;&lt;li&gt;Significance: 3/5&lt;/li&gt;&lt;li&gt;Try-worthiness: Yes&lt;/li&gt;&lt;li&gt;Justification: The abstract is well-written and clearly outlines the motivation, methodology, and results, making the paper easy to understand (Clarity: 4). The work addresses the robustness of DNNs in traffic sign recognition against error-minimizing data poisoning attacks, and proposes a novel data augmentation-based mitigation method, which appears to be a fresh approach in this specific context (Novelty: 4). While the topic is significant for autonomous vehicles, the paper is a recent arXiv preprint with no citations yet and has not been peer-reviewed, which slightly limits its immediate impact (Significance: 3). The strong reported results and practical relevance make it worth trying for researchers or practitioners in the field (Try-worthiness: true). No code repository is provided in the metadata or abstract.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: Thushari Hapuarachchi, Long Dang, Kaiqi Xiong&lt;/li&gt;&lt;li&gt;Tags: data poisoning, robustness, adversarial attacks, AI security, autonomous vehicles&lt;/li&gt;&lt;li&gt;Relevance Score: 5/5&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.06563</guid><pubDate>Tue, 10 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Exploring Adversarial Watermarking in Transformer-Based Models: Transferability and Robustness Against Defense Mechanism for Medical Images</title><link>https://arxiv.org/abs/2506.06389</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Investigates the vulnerability of Vision Transformers (ViTs) to adversarial watermarking attacks in medical image analysis.&lt;/li&gt;&lt;li&gt;Explores the transferability of adversarial attacks from ViTs to CNNs.&lt;/li&gt;&lt;li&gt;Evaluates the effectiveness of adversarial training as a defense mechanism against such attacks.&lt;/li&gt;&lt;li&gt;Finds that ViTs are significantly more vulnerable to adversarial watermarking, but adversarial training can substantially mitigate this vulnerability.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research Paper&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Quality Assessment&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Clarity: 3/5&lt;/li&gt;&lt;li&gt;Novelty: 4/5&lt;/li&gt;&lt;li&gt;Significance: 3/5&lt;/li&gt;&lt;li&gt;Try-worthiness: Yes&lt;/li&gt;&lt;li&gt;Justification: The abstract is somewhat clear but contains grammatical errors and awkward phrasing, which affects readability (Clarity: 3). The topic—adversarial watermarking in Vision Transformers (ViTs) for medical images, with a focus on transferability and robustness—is relatively novel, especially in the medical imaging context (Novelty: 4). The significance is moderate: while the problem is important and ViTs are state-of-the-art, the paper is a preprint with no citations yet and is not peer-reviewed (Significance: 3). The results (notably, a large drop in accuracy and recovery via adversarial training) are promising and actionable, making the paper worth trying for researchers interested in adversarial robustness in medical imaging (Try-worthiness: true). No code repository is provided.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: Rifat Sadik, Tanvir Rahman, Arpan Bhattacharjee, Bikash Chandra Halder, Ismail Hossain&lt;/li&gt;&lt;li&gt;Tags: adversarial attacks, robustness, transformer models, adversarial training, medical imaging security&lt;/li&gt;&lt;li&gt;Relevance Score: 5/5&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.06389</guid><pubDate>Tue, 10 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Distillation Robustifies Unlearning</title><link>https://arxiv.org/abs/2506.06278</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Investigates the robustness of current LLM unlearning methods and finds them vulnerable to reversion via finetuning.&lt;/li&gt;&lt;li&gt;Proposes a new method, UNDO, which uses distillation into a partially noised model to robustify unlearning.&lt;/li&gt;&lt;li&gt;Demonstrates that UNDO achieves robustness comparable to retraining from scratch, but with significantly reduced compute and data requirements.&lt;/li&gt;&lt;li&gt;Validates the method on synthetic tasks and the Weapons of Mass Destruction Proxy (WMDP) benchmark, showing improved removal of undesired capabilities.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research Paper&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Quality Assessment&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Clarity: 5/5&lt;/li&gt;&lt;li&gt;Novelty: 5/5&lt;/li&gt;&lt;li&gt;Significance: 4/5&lt;/li&gt;&lt;li&gt;Try-worthiness: Yes&lt;/li&gt;&lt;li&gt;Justification: The abstract is very clearly written, concisely explaining the problem (lack of robustness in LLM unlearning), the key insight (distillation robustifies unlearning), and the proposed method (UNDO). The work appears highly novel, introducing a new approach to robust unlearning and establishing a new Pareto frontier on relevant tasks. While the paper is very recent and has no citations yet, the problem is significant for both research and practical deployment of LLMs, and the results claim substantial improvements in robustness and efficiency. The method is likely worth trying for practitioners or researchers interested in model unlearning or safe deployment. No code repository is provided in the metadata.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: Bruce W. Lee, Addie Foote, Alex Infanger, Leni Shor, Harish Kamath, Jacob Goldman-Wetzler, Bryce Woodworth, Alex Cloud, Alexander Matt Turner&lt;/li&gt;&lt;li&gt;Tags: AI unlearning, robustness, capability removal, LLM safety, distillation&lt;/li&gt;&lt;li&gt;Relevance Score: 5/5&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.06278</guid><pubDate>Tue, 10 Jun 2025 00:00:00 +0000</pubDate></item><item><title>A Red Teaming Roadmap Towards System-Level Safety</title><link>https://arxiv.org/abs/2506.05376</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Critiques current LLM red teaming research for not prioritizing system-level safety and realistic threat models.&lt;/li&gt;&lt;li&gt;Argues that red teaming should focus on clear product safety specifications and real-world attacker scenarios.&lt;/li&gt;&lt;li&gt;Proposes a roadmap for advancing red teaming research towards system-level safety, including leveraging AI for threat mitigation in deployment.&lt;/li&gt;&lt;li&gt;Highlights the need for red teaming to evolve in response to new threats posed by rapidly advancing AI systems.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Position Paper&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Quality Assessment&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Clarity: 4/5&lt;/li&gt;&lt;li&gt;Novelty: 4/5&lt;/li&gt;&lt;li&gt;Significance: 3/5&lt;/li&gt;&lt;li&gt;Try-worthiness: No&lt;/li&gt;&lt;li&gt;Justification: The abstract is well-written and clearly articulates the authors' position on the current state and future direction of red teaming research for LLM safety, meriting a high clarity score. The novelty is above average, as the paper critiques current research priorities and proposes a shift towards system-level safety and realistic threat models, which is a timely and relevant perspective. However, as a position or roadmap paper rather than an empirical or technical contribution, its immediate significance is moderate, especially given its preprint status and lack of citations (which is expected for a new paper). The paper does not present a new method or system to implement, so it is not directly try-worthy for experimentation. No code repository is provided.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: Zifan Wang, Christina Q. Knight, Jeremy Kritz, Willow E. Primack, Julian Michael&lt;/li&gt;&lt;li&gt;Tags: LLM red teaming, system-level safety, AI security, threat modeling, AI safety&lt;/li&gt;&lt;li&gt;Relevance Score: 5/5&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.05376</guid><pubDate>Tue, 10 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Large Language Models Often Know When They Are Being Evaluated</title><link>https://arxiv.org/abs/2505.23836</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Investigates whether large language models (LLMs) can detect when they are being evaluated versus deployed in real-world settings.&lt;/li&gt;&lt;li&gt;Finds that frontier LLMs demonstrate significant 'evaluation awareness', potentially compromising the reliability of AI safety and robustness benchmarks.&lt;/li&gt;&lt;li&gt;Shows that both humans and AI models are better at detecting evaluation contexts in agentic (task-based) settings than in chat settings.&lt;/li&gt;&lt;li&gt;Recommends monitoring evaluation awareness as a capability in future models due to its implications for AI safety, robustness, and governance.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research Paper&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Quality Assessment&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Clarity: 5/5&lt;/li&gt;&lt;li&gt;Novelty: 5/5&lt;/li&gt;&lt;li&gt;Significance: 4/5&lt;/li&gt;&lt;li&gt;Try-worthiness: Yes&lt;/li&gt;&lt;li&gt;Justification: The abstract is exceptionally clear, well-structured, and communicates the motivation, methodology, and findings concisely. The concept of 'evaluation awareness' in large language models is novel and addresses a critical, underexplored issue in AI evaluation and deployment. The work is significant as it highlights a potential flaw in current benchmarking practices, which could have major implications for AI safety, governance, and real-world reliability. While the paper is very recent and thus has no citations yet, its relevance and the breadth of its benchmark suggest it will be influential. The methodology appears robust and the findings are actionable, making it worth implementing or experimenting with, especially for those interested in AI evaluation or safety. No code repository is provided in the metadata.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: Joe Needham, Giles Edkins, Govind Pimpale, Henning Bartsch, Marius Hobbhahn&lt;/li&gt;&lt;li&gt;Tags: AI safety, evaluation robustness, model behavior, AI governance&lt;/li&gt;&lt;li&gt;Relevance Score: 5/5&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2505.23836</guid><pubDate>Tue, 10 Jun 2025 00:00:00 +0000</pubDate></item><item><title>When Two LLMs Debate, Both Think They'll Win</title><link>https://arxiv.org/abs/2505.19184</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Evaluates LLMs' ability to calibrate confidence in adversarial, multi-turn debate settings.&lt;/li&gt;&lt;li&gt;Finds systematic overconfidence and inability to update beliefs rationally during debates.&lt;/li&gt;&lt;li&gt;Highlights misalignment between private reasoning and public confidence, raising concerns about faithfulness and reliability.&lt;/li&gt;&lt;li&gt;Suggests risks for deploying LLMs in agentic roles without robust safety checks.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research Paper&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Quality Assessment&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Clarity: 5/5&lt;/li&gt;&lt;li&gt;Novelty: 5/5&lt;/li&gt;&lt;li&gt;Significance: 4/5&lt;/li&gt;&lt;li&gt;Try-worthiness: Yes&lt;/li&gt;&lt;li&gt;Justification: The abstract is exceptionally clear, outlining the motivation, methodology, and key findings in a structured and accessible manner. The work is highly novel, as it investigates LLM calibration in a dynamic, adversarial debate setting—a scenario not widely explored in prior literature, especially with the multi-turn and zero-sum structure. The significance is high given the increasing deployment of LLMs in agentic roles, though the impact is yet to be fully realized due to the paper's recency and preprint status. The findings highlight critical limitations in LLM self-assessment and belief updating, which are important for both research and practical deployment. The availability of code further increases the paper's try-worthiness, making it feasible for others to replicate or extend the experiments.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: Pradyumna Shyama Prasad, Minh Nhat Nguyen&lt;/li&gt;&lt;li&gt;Tags: AI safety, LLM alignment, robustness, risk assessment&lt;/li&gt;&lt;li&gt;Relevance Score: 4/5&lt;/li&gt;&lt;li&gt;Code Repository: &lt;a href='https://github.com/pradyuprasad/llms_overconfidence'&gt;https://github.com/pradyuprasad/llms_overconfidence&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2505.19184</guid><pubDate>Tue, 10 Jun 2025 00:00:00 +0000</pubDate></item><item><title>EVADE: Multimodal Benchmark for Evasive Content Detection in E-Commerce Applications</title><link>https://arxiv.org/abs/2505.17654</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces EVADE, a multimodal benchmark for detecting evasive (policy-evading) content in e-commerce using LLMs and VLMs.&lt;/li&gt;&lt;li&gt;Focuses on content that superficially complies with policies but covertly conveys prohibited claims, highlighting a real-world adversarial challenge.&lt;/li&gt;&lt;li&gt;Benchmarks 26 mainstream models, revealing significant gaps in current models' ability to detect evasive content.&lt;/li&gt;&lt;li&gt;Aims to improve the safety and transparency of automated content moderation systems by providing a rigorous evaluation standard.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Quality Assessment&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Clarity: 5/5&lt;/li&gt;&lt;li&gt;Novelty: 5/5&lt;/li&gt;&lt;li&gt;Significance: 4/5&lt;/li&gt;&lt;li&gt;Try-worthiness: Yes&lt;/li&gt;&lt;li&gt;Justification: The abstract is very well-written, clearly outlining the motivation, contributions, and findings of the work. The focus on evasive content detection in e-commerce, especially with a multimodal (text and image) and Chinese-language benchmark, is highly novel and addresses a real-world gap not covered by existing benchmarks. The dataset is expert-curated and covers challenging categories, and the benchmark includes both fine-grained and long-context reasoning tasks. While the paper is very recent and thus has no citations yet, its release of a new, rigorous standard and strong baselines for a pressing problem in content moderation makes it significant. The public release of the dataset (on HuggingFace) further increases its try-worthiness for researchers and practitioners interested in robust content moderation. No code repository is mentioned, but the dataset is available at https://huggingface.co/datasets/koenshen/EVADE-Bench.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: Ancheng Xu, Zhihao Yang, Jingpeng Li, Guanghu Yuan, Longze Chen, Liang Yan, Jiehui Zhou, Zhen Qin, Hengyun Chang, Hamid Alinejad-Rokny, Bo Zheng, Min Yang&lt;/li&gt;&lt;li&gt;Tags: adversarial content, content moderation, robustness evaluation, AI safety, multimodal models&lt;/li&gt;&lt;li&gt;Relevance Score: 5/5&lt;/li&gt;&lt;li&gt;Code Repository: &lt;a href='https://huggingface.co/datasets/koenshen/EVADE-Bench'&gt;https://huggingface.co/datasets/koenshen/EVADE-Bench&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2505.17654</guid><pubDate>Tue, 10 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Pel, A Programming Language for Orchestrating AI Agents</title><link>https://arxiv.org/abs/2505.13453</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces Pel, a new programming language designed for orchestrating LLM-based AI agents.&lt;/li&gt;&lt;li&gt;Emphasizes security and fine-grained capability control at the syntax level, reducing the need for complex sandboxing.&lt;/li&gt;&lt;li&gt;Aims to provide a robust and secure framework for controlling agent actions and inter-agent communication.&lt;/li&gt;&lt;li&gt;Highlights features that enhance safety and reliability in agentic AI systems, such as constrained grammar and automated error correction.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research Paper&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: Behnam Mohammadi&lt;/li&gt;&lt;li&gt;Tags: AI agent orchestration, security by design, capability control, safe AI frameworks&lt;/li&gt;&lt;li&gt;Relevance Score: 4/5&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2505.13453</guid><pubDate>Tue, 10 Jun 2025 00:00:00 +0000</pubDate></item><item><title>MARVEL: Multi-Agent RTL Vulnerability Extraction using Large Language Models</title><link>https://arxiv.org/abs/2505.11963</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes MARVEL, a multi-agent LLM framework for extracting security vulnerabilities from RTL (Register Transfer Level) hardware designs.&lt;/li&gt;&lt;li&gt;Framework mimics the cognitive process of a designer searching for security bugs, using supervisor and executor agents.&lt;/li&gt;&lt;li&gt;Executor agents utilize various tools (formal verification, linters, simulation, LLM-based detection, static analysis) to identify vulnerabilities.&lt;/li&gt;&lt;li&gt;Demonstrates effectiveness by identifying security issues in a real-world SoC design (OpenTitan).&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research Paper&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: Luca Collini, Baleegh Ahmad, Joey Ah-kiow, Ramesh Karri&lt;/li&gt;&lt;li&gt;Tags: AI security, hardware security, LLM red teaming, vulnerability detection, multi-agent systems&lt;/li&gt;&lt;li&gt;Relevance Score: 5/5&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2505.11963</guid><pubDate>Tue, 10 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Learning Pareto-Optimal Rewards from Noisy Preferences: A Framework for Multi-Objective Inverse Reinforcement Learning</title><link>https://arxiv.org/abs/2505.11864</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a framework for Multi-Objective Inverse Reinforcement Learning (MO-IRL) that models human preferences as vector-valued rewards.&lt;/li&gt;&lt;li&gt;Addresses the challenge of aligning AI agent behavior with complex, multi-faceted human values rather than reducing them to a single scalar reward.&lt;/li&gt;&lt;li&gt;Provides theoretical guarantees, including sample complexity bounds and a regret formulation for suboptimality in multi-objective settings.&lt;/li&gt;&lt;li&gt;Introduces a provably convergent algorithm for policy optimization based on preference-inferred reward structures.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research Paper&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Quality Assessment&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Clarity: 4/5&lt;/li&gt;&lt;li&gt;Novelty: 5/5&lt;/li&gt;&lt;li&gt;Significance: 4/5&lt;/li&gt;&lt;li&gt;Try-worthiness: Yes&lt;/li&gt;&lt;li&gt;Justification: The abstract is well-written and clearly communicates the motivation, contributions, and theoretical advances of the paper, though it is somewhat dense with technical terms. The work is highly novel, introducing a new theoretical framework for multi-objective inverse reinforcement learning (MO-IRL) that models human preferences as vector-valued rewards and addresses Pareto-optimality under noisy feedback—an underexplored area. The significance is high given the importance of alignment in generative agents and the lack of existing principled approaches for multi-objective preference learning, though the impact is yet to be seen due to its recency and preprint status. The proposed algorithm and theoretical guarantees make it worth trying for researchers in IRL, alignment, and multi-objective RL. No code repository is provided.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: Kalyan Cherukuri, Aarav Lala&lt;/li&gt;&lt;li&gt;Tags: AI alignment, multi-objective reinforcement learning, human values, reward learning&lt;/li&gt;&lt;li&gt;Relevance Score: 4/5&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2505.11864</guid><pubDate>Tue, 10 Jun 2025 00:00:00 +0000</pubDate></item><item><title>A Comprehensive Survey in LLM(-Agent) Full Stack Safety: Data, Training and Deployment</title><link>https://arxiv.org/abs/2504.15585</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Presents a comprehensive survey of safety and security issues across the entire lifecycle of Large Language Models (LLMs), from data preparation to deployment and commercialization.&lt;/li&gt;&lt;li&gt;Covers a wide range of topics including data safety, training vulnerabilities, deployment risks, and alignment techniques.&lt;/li&gt;&lt;li&gt;Analyzes over 800 papers to provide an exhaustive overview of current research and organizes security issues systematically.&lt;/li&gt;&lt;li&gt;Identifies future research directions in areas such as data generation safety, model editing, and LLM-based agent systems.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Survey/Review&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Quality Assessment&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Clarity: 5/5&lt;/li&gt;&lt;li&gt;Novelty: 4/5&lt;/li&gt;&lt;li&gt;Significance: 4/5&lt;/li&gt;&lt;li&gt;Try-worthiness: Yes&lt;/li&gt;&lt;li&gt;Justification: The paper is very clearly written, with a well-structured abstract that outlines its scope and contributions. Its novelty lies in being the first survey to systematically address 'full-stack' safety across the entire LLM lifecycle, rather than focusing on isolated phases. While it is a survey (not original research), its comprehensive coverage (800+ papers) and holistic perspective make it significant for both researchers and practitioners. The work is timely and relevant given the rapid deployment of LLMs and associated safety concerns. As a survey, it is not something to 'implement,' but it is highly worth reading and using as a reference or roadmap for future research and development. No code repository is provided, which is typical for survey papers.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: Kun Wang, Guibin Zhang, Zhenhong Zhou, Jiahao Wu, Miao Yu, Shiqian Zhao, Chenlong Yin, Jinhu Fu, Yibo Yan, Hanjun Luo, Liang Lin, Zhihao Xu, Haolang Lu, Xinye Cao, Xinyun Zhou, Weifei Jin, Fanci Meng, Shicheng Xu, Junyuan Mao, Yu Wang, Hao Wu, Minghe Wang, Fan Zhang, Junfeng Fang, Wenjie Qu, Yue Liu, Chengwei Liu, Yifan Zhang, Qiankun Li, Chongye Guo, Yalan Qin, Zhaoxin Fan, Kai Wang, Yi Ding, Donghai Hong, Jiaming Ji, Yingxin Lai, Zitong Yu, Xinfeng Li, Yifan Jiang, Yanhui Li, Xinyu Deng, Junlin Wu, Dongxia Wang, Yihao Huang, Yufei Guo, Jen-tse Huang, Qiufeng Wang, Xiaolong Jin, Wenxuan Wang, Dongrui Liu, Yanwei Yue, Wenke Huang, Guancheng Wan, Heng Chang, Tianlin Li, Yi Yu, Chenghao Li, Jiawei Li, Lei Bai, Jie Zhang, Qing Guo, Jingyi Wang, Tianlong Chen, Joey Tianyi Zhou, Xiaojun Jia, Weisong Sun, Cong Wu, Jing Chen, Xuming Hu, Yiming Li, Xiao Wang, Ningyu Zhang, Luu Anh Tuan, Guowen Xu, Jiaheng Zhang, Tianwei Zhang, Xingjun Ma, Jindong Gu, Liang Pang, Xiang Wang, Bo An, Jun Sun, Mohit Bansal, Shirui Pan, Lingjuan Lyu, Yuval Elovici, Bhavya Kailkhura, Yaodong Yang, Hongwei Li, Wenyuan Xu, Yizhou Sun, Wei Wang, Qing Li, Ke Tang, Yu-Gang Jiang, Felix Juefei-Xu, Hui Xiong, Xiaofeng Wang, Dacheng Tao, Philip S. Yu, Qingsong Wen, Yang Liu&lt;/li&gt;&lt;li&gt;Tags: LLM safety, AI security, alignment, lifecycle risk, survey&lt;/li&gt;&lt;li&gt;Relevance Score: 5/5&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2504.15585</guid><pubDate>Tue, 10 Jun 2025 00:00:00 +0000</pubDate></item><item><title>sudo rm -rf agentic_security</title><link>https://arxiv.org/abs/2503.20279</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces SUDO, a novel attack framework targeting LLM-based computer-use agents by bypassing refusal-trained safeguards.&lt;/li&gt;&lt;li&gt;Describes Detox2Tox, a mechanism that transforms harmful requests into benign ones and then reintroduces malicious content for execution.&lt;/li&gt;&lt;li&gt;Demonstrates high attack success rates against commercial agents like Claude for Computer Use, including iterative refinement to increase effectiveness.&lt;/li&gt;&lt;li&gt;Highlights urgent security vulnerabilities in real-world deployments of LLM agents and the need for improved, context-aware safeguards.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research Paper&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Quality Assessment&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Clarity: 4/5&lt;/li&gt;&lt;li&gt;Novelty: 5/5&lt;/li&gt;&lt;li&gt;Significance: 4/5&lt;/li&gt;&lt;li&gt;Try-worthiness: Yes&lt;/li&gt;&lt;li&gt;Justification: The abstract is well-written, clearly outlining the motivation, methodology, and results of the work. The paper introduces a novel attack framework (SUDO) targeting refusal-trained safeguards in commercial computer-use agents, which is a timely and important topic as LLMs are increasingly used in agentic roles. The iterative Detox2Tox mechanism and its demonstrated effectiveness against robust policy filters represent a significant advancement over conventional jailbreaks. The work is highly significant given the growing deployment of such agents, though its impact is not yet reflected in citations due to its recency. The attack success rates reported are substantial, making this paper important for both researchers and practitioners concerned with LLM security. The lack of a code repository is a minor drawback, but the paper is still worth experimenting with for those interested in agentic LLM security.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: Sejin Lee, Jian Kim, Haon Park, Ashkan Yousefpour, Sangyoon Yu, Min Song&lt;/li&gt;&lt;li&gt;Tags: LLM red teaming, adversarial prompting, security vulnerabilities, AI agent security, safeguard bypass&lt;/li&gt;&lt;li&gt;Relevance Score: 5/5&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2503.20279</guid><pubDate>Tue, 10 Jun 2025 00:00:00 +0000</pubDate></item><item><title>from Benign import Toxic: Jailbreaking the Language Model via Adversarial Metaphors</title><link>https://arxiv.org/abs/2503.00038</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces AVATAR, a novel adversarial metaphor-based framework for jailbreaking large language models (LLMs).&lt;/li&gt;&lt;li&gt;Demonstrates that inducing LLMs to transform benign metaphors into harmful content is an effective attack vector.&lt;/li&gt;&lt;li&gt;Shows that AVATAR achieves high attack success rates and is transferable across multiple advanced LLMs.&lt;/li&gt;&lt;li&gt;Highlights a new class of jailbreak attacks that exploit LLM reasoning over metaphorical content.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research Paper&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Quality Assessment&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Clarity: 4/5&lt;/li&gt;&lt;li&gt;Novelty: 5/5&lt;/li&gt;&lt;li&gt;Significance: 4/5&lt;/li&gt;&lt;li&gt;Try-worthiness: Yes&lt;/li&gt;&lt;li&gt;Justification: The abstract is generally well-written and clearly explains the motivation, method (AVATAR), and results, though some sentences are dense and could be clearer. The idea of using adversarial metaphors to jailbreak LLMs is novel and creative, representing a new attack vector not widely explored in prior work. The significance is high given the ongoing concern about LLM safety and the claim of state-of-the-art attack success rates, though the impact will depend on further peer review and adoption. As a very recent arXiv preprint, it has no citations yet, but the topic is timely and important. The method appears practical and worth experimenting with for researchers in LLM safety and adversarial attacks.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: Yu Yan, Sheng Sun, Zenghao Duan, Teli Liu, Min Liu, Zhiyi Yin, Jiangyu Lei, Qi Li&lt;/li&gt;&lt;li&gt;Tags: LLM jailbreaking, adversarial prompting, AI security, harmful content generation&lt;/li&gt;&lt;li&gt;Relevance Score: 5/5&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2503.00038</guid><pubDate>Tue, 10 Jun 2025 00:00:00 +0000</pubDate></item><item><title>EgoNormia: Benchmarking Physical Social Norm Understanding</title><link>https://arxiv.org/abs/2502.20490</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces EGONORMIA, a benchmark for evaluating physical and social norm understanding in vision-language models (VLMs).&lt;/li&gt;&lt;li&gt;Covers norm categories including safety and privacy, which are directly relevant to AI safety and security.&lt;/li&gt;&lt;li&gt;Finds that current VLMs perform poorly on norm understanding, highlighting potential safety and privacy risks in real-world deployment.&lt;/li&gt;&lt;li&gt;Explores methods to improve normative reasoning, such as retrieval-augmented generation.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: MohammadHossein Rezaei, Yicheng Fu, Phil Cuvin, Caleb Ziems, Yanzhe Zhang, Hao Zhu, Diyi Yang&lt;/li&gt;&lt;li&gt;Tags: AI safety, normative reasoning, privacy, safety evaluation, vision-language models&lt;/li&gt;&lt;li&gt;Relevance Score: 4/5&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2502.20490</guid><pubDate>Tue, 10 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Automated Capability Discovery via Foundation Model Self-Exploration</title><link>https://arxiv.org/abs/2502.07577</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces Automated Capability Discovery (ACD), a framework for automated, systematic evaluation of foundation models' capabilities and failures.&lt;/li&gt;&lt;li&gt;Uses foundation models to generate and evaluate open-ended tasks, reducing the need for extensive human effort in capability and risk assessment.&lt;/li&gt;&lt;li&gt;Demonstrates the method across multiple foundation models, uncovering diverse capabilities and failure modes.&lt;/li&gt;&lt;li&gt;Validates automated scoring with human surveys, showing high agreement and supporting the reliability of the approach.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research Paper&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: Cong Lu, Shengran Hu, Jeff Clune&lt;/li&gt;&lt;li&gt;Tags: AI safety evaluation, Automated risk assessment, Foundation model testing, Failure mode discovery&lt;/li&gt;&lt;li&gt;Relevance Score: 4/5&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2502.07577</guid><pubDate>Tue, 10 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Unsafe LLM-Based Search: Quantitative Analysis and Mitigation of Safety Risks in AI Web Search</title><link>https://arxiv.org/abs/2502.04951</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Quantitatively analyzes safety risks in AI-powered search engines (AIPSEs) that use large language models (LLMs), focusing on the dissemination of malicious or harmful content.&lt;/li&gt;&lt;li&gt;Defines threat models and risk types, and evaluates responses of seven production AIPSEs to various query types using real-world malicious URL datasets.&lt;/li&gt;&lt;li&gt;Demonstrates that AIPSEs can be deceived into quoting or citing malicious content, with case studies on document spoofing and phishing.&lt;/li&gt;&lt;li&gt;Proposes and evaluates an agent-based defense mechanism using a GPT-4.1-based content refinement tool and a URL detector, showing effective risk mitigation.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research Paper&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: Zeren Luo, Zifan Peng, Yule Liu, Zhen Sun, Mingchen Li, Jingyi Zheng, Xinlei He&lt;/li&gt;&lt;li&gt;Tags: LLM safety, AI-powered search, malicious content detection, phishing and spoofing, risk mitigation&lt;/li&gt;&lt;li&gt;Relevance Score: 5/5&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2502.04951</guid><pubDate>Tue, 10 Jun 2025 00:00:00 +0000</pubDate></item><item><title>GraphRAG under Fire</title><link>https://arxiv.org/abs/2501.14050</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Investigates the security vulnerabilities of GraphRAG, a retrieval-augmented generation system using knowledge graphs.&lt;/li&gt;&lt;li&gt;Introduces GragPoison, a novel poisoning attack exploiting shared relations in knowledge graphs to compromise multiple queries.&lt;/li&gt;&lt;li&gt;Demonstrates that GragPoison is highly effective and scalable, outperforming existing RAG poisoning attacks.&lt;/li&gt;&lt;li&gt;Explores potential defensive measures and discusses their limitations, highlighting areas for future research.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research Paper&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Quality Assessment&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Clarity: 5/5&lt;/li&gt;&lt;li&gt;Novelty: 5/5&lt;/li&gt;&lt;li&gt;Significance: 4/5&lt;/li&gt;&lt;li&gt;Try-worthiness: Yes&lt;/li&gt;&lt;li&gt;Justification: The abstract is well-written, clearly outlining the motivation, methodology, and findings. The work is novel, introducing a new attack (GragPoison) specifically targeting GraphRAG, a recent and advanced RAG variant. The identification of a security paradox and the development of new attack strategies demonstrate originality. While the paper is very recent and has no citations yet, its focus on the security of emerging RAG architectures is highly relevant and significant for both research and practical applications. The empirical results (high attack success rate and scalability) further enhance its importance. The lack of a code repository is a minor drawback, but the paper is still worth experimenting with, especially for those interested in RAG security.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: Jiacheng Liang, Yuhui Wang, Changjiang Li, Rongyi Zhu, Tanqiu Jiang, Neil Gong, Ting Wang&lt;/li&gt;&lt;li&gt;Tags: data poisoning, retrieval-augmented generation, knowledge graphs, AI security, attack and defense&lt;/li&gt;&lt;li&gt;Relevance Score: 5/5&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2501.14050</guid><pubDate>Tue, 10 Jun 2025 00:00:00 +0000</pubDate></item><item><title>On Adversarial Robustness of Language Models in Transfer Learning</title><link>https://arxiv.org/abs/2501.00066</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Investigates how transfer learning affects the adversarial robustness of large language models (LLMs).&lt;/li&gt;&lt;li&gt;Finds that transfer learning can increase vulnerability to adversarial attacks, despite improving standard performance.&lt;/li&gt;&lt;li&gt;Shows that larger models tend to be more resilient to adversarial attacks in transfer learning scenarios.&lt;/li&gt;&lt;li&gt;Highlights the importance of considering adversarial robustness when deploying LLMs in real-world applications.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research Paper&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Quality Assessment&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Clarity: 4/5&lt;/li&gt;&lt;li&gt;Novelty: 4/5&lt;/li&gt;&lt;li&gt;Significance: 4/5&lt;/li&gt;&lt;li&gt;Try-worthiness: Yes&lt;/li&gt;&lt;li&gt;Justification: The paper is clearly written, with a well-structured abstract that outlines the motivation, methodology, and key findings. The focus on adversarial robustness in transfer learning for LLMs is timely and relevant, especially as transfer learning is widely used but its security implications are less explored. The use of multiple datasets and architectures adds to the robustness of the study. While the venue is arXiv and the paper is very recent (hence no citations yet), the topic is significant for practitioners concerned with deploying robust language models. The insights about model size and robustness are actionable and worth further experimentation. However, no code repository is provided, which slightly limits immediate reproducibility.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: Bohdan Turbal, Anastasiia Mazur, Jiaxu Zhao, Mykola Pechenizkiy&lt;/li&gt;&lt;li&gt;Tags: adversarial robustness, transfer learning, language models, model security&lt;/li&gt;&lt;li&gt;Relevance Score: 5/5&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2501.00066</guid><pubDate>Tue, 10 Jun 2025 00:00:00 +0000</pubDate></item><item><title>SATA: A Paradigm for LLM Jailbreak via Simple Assistive Task Linkage</title><link>https://arxiv.org/abs/2412.15289</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces SATA, a new paradigm for jailbreaking large language models (LLMs) using simple assistive task linkage.&lt;/li&gt;&lt;li&gt;SATA masks harmful keywords in malicious queries and uses assistive tasks (e.g., masked language modeling) to encode and recover the masked content.&lt;/li&gt;&lt;li&gt;Demonstrates that SATA can effectively bypass LLM safety mechanisms and elicit harmful responses, outperforming existing jailbreak methods.&lt;/li&gt;&lt;li&gt;Provides extensive experimental results showing high attack success rates and harmfulness scores on benchmark datasets.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research Paper&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: Xiaoning Dong, Wenbo Hu, Wei Xu, Tianxing He&lt;/li&gt;&lt;li&gt;Tags: LLM jailbreak, adversarial prompting, AI security, prompt injection, red teaming&lt;/li&gt;&lt;li&gt;Relevance Score: 5/5&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2412.15289</guid><pubDate>Tue, 10 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Black-Box Forgery Attacks on Semantic Watermarks for Diffusion Models</title><link>https://arxiv.org/abs/2412.03283</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Analyzes security vulnerabilities in semantic watermarking techniques for latent diffusion models.&lt;/li&gt;&lt;li&gt;Proposes two black-box forgery attacks that can imprint or remove watermarks using unrelated models.&lt;/li&gt;&lt;li&gt;Demonstrates that attackers can forge or remove semantic watermarks with minimal information (a single reference image).&lt;/li&gt;&lt;li&gt;Raises concerns about the robustness and applicability of semantic watermarks for content attribution.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research Paper&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Quality Assessment&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Clarity: 5/5&lt;/li&gt;&lt;li&gt;Novelty: 4/5&lt;/li&gt;&lt;li&gt;Significance: 4/5&lt;/li&gt;&lt;li&gt;Try-worthiness: Yes&lt;/li&gt;&lt;li&gt;Justification: The abstract is well-written and clearly explains the motivation, methods, and findings of the paper, warranting a high clarity score. The work is novel in that it exposes fundamental vulnerabilities in recently proposed semantic watermarking techniques for diffusion models, an area of growing importance. The significance is high, as the findings question the security of watermarking methods that are being considered for content attribution in generative AI, though the impact will depend on further validation and community uptake. The attacks described are practical and could be valuable for both researchers and practitioners to understand and mitigate, making the paper worth experimenting with. No code repository is provided in the metadata.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: Andreas M\"uller, Denis Lukovnikov, Jonas Thietke, Asja Fischer, Erwin Quiring&lt;/li&gt;&lt;li&gt;Tags: watermarking attacks, diffusion models, AI security, forgery, content attribution&lt;/li&gt;&lt;li&gt;Relevance Score: 5/5&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2412.03283</guid><pubDate>Tue, 10 Jun 2025 00:00:00 +0000</pubDate></item><item><title>A Cognac Shot To Forget Bad Memories: Corrective Unlearning for Graph Neural Networks</title><link>https://arxiv.org/abs/2412.00789</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Addresses the problem of removing the effects of adversarial manipulations or incorrect data in trained Graph Neural Networks (GNNs) through corrective unlearning.&lt;/li&gt;&lt;li&gt;Proposes a new method, Cognac, that can effectively unlearn the impact of manipulated data even when only a small portion of the manipulation set is identified.&lt;/li&gt;&lt;li&gt;Demonstrates that Cognac outperforms existing unlearning methods and is more efficient than retraining from scratch.&lt;/li&gt;&lt;li&gt;Aims to help GNN developers mitigate harmful effects caused by adversarial or erroneous data post-training.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research Paper&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Quality Assessment&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Clarity: 5/5&lt;/li&gt;&lt;li&gt;Novelty: 5/5&lt;/li&gt;&lt;li&gt;Significance: 4/5&lt;/li&gt;&lt;li&gt;Try-worthiness: Yes&lt;/li&gt;&lt;li&gt;Justification: The abstract is well-written, clearly explaining the motivation, problem, and contributions. The work addresses the novel and timely problem of corrective unlearning in Graph Neural Networks (GNNs), which is important due to the non-i.i.d. nature of graph data and the propagation of adversarial or incorrect information. The proposed method, Cognac, claims significant improvements over existing methods, including the ability to unlearn with only partial knowledge of manipulated data and substantial efficiency gains. While the paper is very recent and has no citations yet, the topic is highly relevant, and the results (beating retraining and being 8x more efficient) are promising. The availability of code further increases its try-worthiness for practitioners and researchers.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: Varshita Kolipaka, Akshit Sinha, Debangan Mishra, Sumit Kumar, Arvindh Arun, Shashwat Goel, Ponnurangam Kumaraguru&lt;/li&gt;&lt;li&gt;Tags: graph neural networks, corrective unlearning, adversarial robustness, data poisoning, AI security&lt;/li&gt;&lt;li&gt;Relevance Score: 5/5&lt;/li&gt;&lt;li&gt;Code Repository: &lt;a href='https://github.com/cognac-gnn-unlearning/corrective-unlearning-for-gnns'&gt;https://github.com/cognac-gnn-unlearning/corrective-unlearning-for-gnns&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2412.00789</guid><pubDate>Tue, 10 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Epistemic Integrity in Large Language Models</title><link>https://arxiv.org/abs/2411.06528</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Addresses the problem of epistemic miscalibration in LLMs, where models express unwarranted confidence in their outputs.&lt;/li&gt;&lt;li&gt;Introduces a new human-labeled dataset and a novel method for measuring linguistic assertiveness in LLMs.&lt;/li&gt;&lt;li&gt;Demonstrates that current LLMs often present information with confidence that does not match their actual accuracy, posing risks of misleading users.&lt;/li&gt;&lt;li&gt;Proposes a framework for diagnosing and potentially correcting this miscalibration to improve AI trustworthiness.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research Paper&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Quality Assessment&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Clarity: 5/5&lt;/li&gt;&lt;li&gt;Novelty: 4/5&lt;/li&gt;&lt;li&gt;Significance: 4/5&lt;/li&gt;&lt;li&gt;Try-worthiness: Yes&lt;/li&gt;&lt;li&gt;Justification: The abstract is well-written, clearly outlining the problem of epistemic miscalibration in LLMs and the proposed contributions: a new dataset, a novel measurement method, and empirical validation. The focus on linguistic assertiveness versus internal certainty is a timely and important issue, and the claim of halving error rates over previous benchmarks suggests meaningful progress. While the venue is arXiv (preprint), the topic is highly relevant and the methodology appears robust, making it significant for both research and practical applications. The lack of citations is expected given the recency. The absence of a code repository is a drawback, but the paper is still worth experimenting with due to its potential impact.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: Bijean Ghafouri, Shahrad Mohammadzadeh, James Zhou, Pratheeksha Nair, Jacob-Junqi Tian, Hikaru Tsujimura, Mayank Goel, Sukanya Krishna, Reihaneh Rabbany, Jean-Fran\c{c}ois Godbout, Kellin Pelrine&lt;/li&gt;&lt;li&gt;Tags: AI safety, LLM alignment, epistemic calibration, risk assessment&lt;/li&gt;&lt;li&gt;Relevance Score: 4/5&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2411.06528</guid><pubDate>Tue, 10 Jun 2025 00:00:00 +0000</pubDate></item><item><title>SudoLM: Learning Access Control of Parametric Knowledge with Authorization Alignment</title><link>https://arxiv.org/abs/2410.14676</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes SudoLM, a framework for learning access control in LLMs via authorization alignment.&lt;/li&gt;&lt;li&gt;Allows differentiated access to parametric knowledge based on user credentials, using a SUDO key mechanism.&lt;/li&gt;&lt;li&gt;Addresses the limitation of one-size-fits-all alignment by enabling advanced users to access otherwise restricted knowledge.&lt;/li&gt;&lt;li&gt;Demonstrates effectiveness in controlling knowledge access while maintaining general utility through experiments.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research Paper&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Quality Assessment&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Clarity: 4/5&lt;/li&gt;&lt;li&gt;Novelty: 5/5&lt;/li&gt;&lt;li&gt;Significance: 4/5&lt;/li&gt;&lt;li&gt;Try-worthiness: Yes&lt;/li&gt;&lt;li&gt;Justification: The abstract is well-written and clearly explains the motivation, problem, and proposed solution (SudoLM). The idea of fine-grained access control over LLM parametric knowledge via authorization alignment is novel and addresses a real limitation in current alignment approaches. While the paper is very new and has no citations yet, the problem it tackles is significant for both research and practical deployment of LLMs, especially in sensitive or expert domains. The approach is promising and worth experimenting with, particularly for organizations or applications requiring differentiated access. No code repository is provided in the metadata.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: Qin Liu, Fei Wang, Chaowei Xiao, Muhao Chen&lt;/li&gt;&lt;li&gt;Tags: access control, authorization alignment, LLM security, AI safety, knowledge restriction&lt;/li&gt;&lt;li&gt;Relevance Score: 5/5&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2410.14676</guid><pubDate>Tue, 10 Jun 2025 00:00:00 +0000</pubDate></item><item><title>ASMA: An Adaptive Safety Margin Algorithm for Vision-Language Drone Navigation via Scene-Aware Control Barrier Functions</title><link>https://arxiv.org/abs/2409.10283</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes an Adaptive Safety Margin Algorithm (ASMA) for enhancing the safety of vision-language navigation (VLN) drones.&lt;/li&gt;&lt;li&gt;Integrates scene-aware Control Barrier Functions (CBFs) with Model Predictive Control (MPC) to enforce real-time safety constraints.&lt;/li&gt;&lt;li&gt;ASMA dynamically tracks moving objects and adjusts navigation to avoid hazards based on real-time observations.&lt;/li&gt;&lt;li&gt;Demonstrates significant improvements in safe navigation success rates in a simulated drone environment.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research Paper&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Quality Assessment&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Clarity: 4/5&lt;/li&gt;&lt;li&gt;Novelty: 5/5&lt;/li&gt;&lt;li&gt;Significance: 4/5&lt;/li&gt;&lt;li&gt;Try-worthiness: Yes&lt;/li&gt;&lt;li&gt;Justification: The abstract is well-written and clearly explains the motivation, methodology, and results of the work, though it is somewhat dense with technical terms. The proposed Adaptive Safety Margin Algorithm (ASMA) introduces a novel integration of scene-aware Control Barrier Functions (CBFs) with vision-language navigation for drones, which is a new direction in the field. The significance is high due to the practical safety improvements (64%-67% increase in success rates) and the real-world deployment on a drone platform, though the impact is limited by the paper's recency and preprint status. The approach is promising and worth experimenting with, especially for researchers in robotics, safety, and vision-language navigation. No code repository is provided in the metadata.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: Sourav Sanyal, Kaushik Roy&lt;/li&gt;&lt;li&gt;Tags: AI safety, robotics safety, control barrier functions, vision-language navigation, real-time hazard avoidance&lt;/li&gt;&lt;li&gt;Relevance Score: 4/5&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2409.10283</guid><pubDate>Tue, 10 Jun 2025 00:00:00 +0000</pubDate></item><item><title>HSF: Defending against Jailbreak Attacks with Hidden State Filtering</title><link>https://arxiv.org/abs/2409.03788</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a novel defense mechanism (Hidden State Filter) to proactively detect and block jailbreak attacks on LLMs before inference.&lt;/li&gt;&lt;li&gt;Leverages clustering patterns in LLM hidden state representations to distinguish between safe, harmful, and adversarial (jailbreak) prompts.&lt;/li&gt;&lt;li&gt;Demonstrates significant reduction in jailbreak attack success rates with minimal impact on benign queries and low computational overhead.&lt;/li&gt;&lt;li&gt;Benchmarks the approach on multiple LLMs and datasets, outperforming existing defense baselines.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research Paper&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Quality Assessment&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Clarity: 4/5&lt;/li&gt;&lt;li&gt;Novelty: 5/5&lt;/li&gt;&lt;li&gt;Significance: 4/5&lt;/li&gt;&lt;li&gt;Try-worthiness: Yes&lt;/li&gt;&lt;li&gt;Justification: The abstract is well-written and clearly explains the motivation, method, and results, though it is dense and could be more concise. The approach—using hidden state clustering for pre-inference filtering of jailbreak attacks—is novel and addresses a pressing problem in LLM safety. The significance is high given the growing threat of jailbreak attacks and the need for efficient, proactive defenses, though the paper is still a preprint and has not yet been peer-reviewed or cited. The method appears practical and is supported by experiments on multiple models and attacks, making it worth trying for those interested in LLM safety. The code and data are available, further increasing its try-worthiness.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: Cheng Qian, Hainan Zhang, Lei Sha, Zhiming Zheng&lt;/li&gt;&lt;li&gt;Tags: LLM jailbreak defense, adversarial prompting, hidden state analysis, AI safety, robustness&lt;/li&gt;&lt;li&gt;Relevance Score: 5/5&lt;/li&gt;&lt;li&gt;Code Repository: &lt;a href='https://anonymous.4open.science/r/Hidden-State-Filtering-8652/'&gt;https://anonymous.4open.science/r/Hidden-State-Filtering-8652/&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2409.03788</guid><pubDate>Tue, 10 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Knowledge-to-Jailbreak: Investigating Knowledge-driven Jailbreaking Attacks for Large Language Models</title><link>https://arxiv.org/abs/2406.11682</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces a new task: knowledge-to-jailbreak, focusing on generating jailbreaking attacks for LLMs using domain-specific knowledge.&lt;/li&gt;&lt;li&gt;Creates a large-scale dataset of knowledge-jailbreak pairs and fine-tunes a model to generate domain-relevant jailbreak prompts.&lt;/li&gt;&lt;li&gt;Demonstrates the effectiveness of the approach across 13 domains and 8 LLMs, showing that the generated jailbreaks are both effective and knowledge-relevant.&lt;/li&gt;&lt;li&gt;Shows that the method can generalize to out-of-domain knowledge, producing harmful jailbreaks comparable to those crafted by human experts.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research Paper&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Quality Assessment&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Clarity: 5/5&lt;/li&gt;&lt;li&gt;Novelty: 5/5&lt;/li&gt;&lt;li&gt;Significance: 4/5&lt;/li&gt;&lt;li&gt;Try-worthiness: Yes&lt;/li&gt;&lt;li&gt;Justification: The paper is very clearly written, with a well-structured abstract that explains the motivation, challenges, proposed task, methodology, and results. The work is highly novel, introducing the concept of knowledge-driven jailbreaking attacks tailored to domain-specific LLM safety, which is a new and important direction. While the paper is very recent and thus has no citations yet, the topic is highly significant given the increasing deployment of LLMs in sensitive domains. The release of a large dataset and code, as well as the demonstration of effectiveness across multiple domains and models, make this work highly try-worthy for researchers and practitioners interested in LLM safety and adversarial robustness.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: Shangqing Tu, Zhuoran Pan, Wenxuan Wang, Zhexin Zhang, Yuliang Sun, Jifan Yu, Hongning Wang, Lei Hou, Juanzi Li&lt;/li&gt;&lt;li&gt;Tags: LLM jailbreaking, adversarial prompting, domain-specific attacks, AI safety evaluation&lt;/li&gt;&lt;li&gt;Relevance Score: 5/5&lt;/li&gt;&lt;li&gt;Code Repository: &lt;a href='https://github.com/THU-KEG/Knowledge-to-Jailbreak/'&gt;https://github.com/THU-KEG/Knowledge-to-Jailbreak/&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2406.11682</guid><pubDate>Tue, 10 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Towards Black-Box Membership Inference Attack for Diffusion Models</title><link>https://arxiv.org/abs/2405.20771</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a novel black-box membership inference attack targeting diffusion models, requiring only access to image-to-image variation APIs.&lt;/li&gt;&lt;li&gt;Demonstrates that the attack can identify whether a specific image was used in the training set of proprietary diffusion models without internal model access.&lt;/li&gt;&lt;li&gt;Validates the method on DDIM, Stable Diffusion, and Diffusion Transformer architectures, showing improved performance over previous approaches.&lt;/li&gt;&lt;li&gt;Addresses privacy and security concerns related to AI-generated art and copyright.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research Paper&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Quality Assessment&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Clarity: 4/5&lt;/li&gt;&lt;li&gt;Novelty: 5/5&lt;/li&gt;&lt;li&gt;Significance: 4/5&lt;/li&gt;&lt;li&gt;Try-worthiness: Yes&lt;/li&gt;&lt;li&gt;Justification: The abstract is well-written and clearly outlines the motivation, problem, and proposed solution, though some technical details are dense. The work is highly novel, as it proposes a black-box membership inference attack for diffusion models that does not require access to internal U-nets, addressing a key limitation of prior work. The significance is high due to the relevance of copyright and data privacy in AI-generated art, and the method's applicability to popular models like Stable Diffusion and Diffusion Transformers. While the paper is very recent and has no citations yet, its topic and results suggest it will be impactful. The method appears practical and worth experimenting with, especially for researchers in AI security and privacy. No code repository is provided in the metadata.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: Jingwei Li, Jing Dong, Tianxing He, Jingzhao Zhang&lt;/li&gt;&lt;li&gt;Tags: membership inference, privacy attacks, diffusion models, black-box attacks, AI security&lt;/li&gt;&lt;li&gt;Relevance Score: 5/5&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2405.20771</guid><pubDate>Tue, 10 Jun 2025 00:00:00 +0000</pubDate></item><item><title>LLM4Vuln: A Unified Evaluation Framework for Decoupling and Enhancing LLMs' Vulnerability Reasoning</title><link>https://arxiv.org/abs/2401.16185</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces LLM4Vuln, a framework to evaluate and decouple LLMs' vulnerability reasoning from external aids.&lt;/li&gt;&lt;li&gt;Presents UniVul, a benchmark dataset for vulnerability reasoning in code across multiple programming languages.&lt;/li&gt;&lt;li&gt;Evaluates six LLMs on their ability to detect vulnerabilities, analyzing the impact of knowledge retrieval, context, and prompting.&lt;/li&gt;&lt;li&gt;Discovers 14 zero-day vulnerabilities in real-world bug bounty programs using the evaluated LLMs.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: Yuqiang Sun, Daoyuan Wu, Yue Xue, Han Liu, Wei Ma, Lyuye Zhang, Yang Liu, Yingjiu Li&lt;/li&gt;&lt;li&gt;Tags: LLM security, vulnerability detection, evaluation framework, AI safety, benchmarking&lt;/li&gt;&lt;li&gt;Relevance Score: 5/5&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2401.16185</guid><pubDate>Tue, 10 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Enhancing Adversarial Robustness with Conformal Prediction: A Framework for Guaranteed Model Reliability</title><link>https://arxiv.org/abs/2506.07804</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a new adversarial attack (OPSA) targeting conformal prediction frameworks to maximize model uncertainty.&lt;/li&gt;&lt;li&gt;Introduces OPSA-AT, a novel adversarial training defense that integrates conformal prediction principles.&lt;/li&gt;&lt;li&gt;Demonstrates improved robustness and reliability of deep learning models against adversarial attacks, especially in safety-critical applications.&lt;/li&gt;&lt;li&gt;Provides experimental evidence that the proposed methods outperform baseline defenses in terms of uncertainty and robustness.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research Paper&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Quality Assessment&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Clarity: 4/5&lt;/li&gt;&lt;li&gt;Novelty: 5/5&lt;/li&gt;&lt;li&gt;Significance: 4/5&lt;/li&gt;&lt;li&gt;Try-worthiness: Yes&lt;/li&gt;&lt;li&gt;Justification: The paper is clearly written, with a well-structured abstract that outlines the motivation, methods (OPSA attack and OPSA-AT defense), and results. The integration of conformal prediction with adversarial robustness is a novel approach, especially the design of an attack specifically targeting conformal prediction efficiency and a corresponding defense. While the paper is very recent and has no citations yet, the topic is highly significant for safety-critical applications, and the proposed methods appear to outperform baselines. The availability of code further increases its try-worthiness for practitioners and researchers interested in robust and reliable deep learning.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: Jie Bao, Chuangyin Dang, Rui Luo, Hanwei Zhang, Zhixin Zhou&lt;/li&gt;&lt;li&gt;Tags: adversarial robustness, conformal prediction, adversarial attacks, adversarial training, model reliability&lt;/li&gt;&lt;li&gt;Relevance Score: 5/5&lt;/li&gt;&lt;li&gt;Code Repository: &lt;a href='https://github.com/bjbbbb/Enhancing-Adversarial-Robustness-with-Conformal-Prediction'&gt;https://github.com/bjbbbb/Enhancing-Adversarial-Robustness-with-Conformal-Prediction&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.07804</guid><pubDate>Tue, 10 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Synthesize Privacy-Preserving High-Resolution Images via Private Textual Intermediaries</title><link>https://arxiv.org/abs/2506.07555</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a novel method (SPTI) for generating high-resolution, differentially private synthetic images using private textual intermediaries.&lt;/li&gt;&lt;li&gt;Shifts the privacy challenge from the image domain to the text domain by leveraging differentially private text generation.&lt;/li&gt;&lt;li&gt;Demonstrates significant improvements in image quality (FID scores) over previous differentially private image synthesis methods.&lt;/li&gt;&lt;li&gt;Enables privacy-preserving sharing and analysis of sensitive visual data without requiring model training.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research Paper&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Quality Assessment&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Clarity: 5/5&lt;/li&gt;&lt;li&gt;Novelty: 5/5&lt;/li&gt;&lt;li&gt;Significance: 4/5&lt;/li&gt;&lt;li&gt;Try-worthiness: Yes&lt;/li&gt;&lt;li&gt;Justification: The paper is very clearly written, with a well-structured abstract that explains the motivation, method, and results. The approach—using private textual intermediaries to synthesize high-resolution, differentially private images—is novel and creative, shifting the privacy challenge from the image to the text domain. The reported improvements in FID scores over prior methods on standard datasets indicate significant practical impact. While the paper is a preprint and has no citations yet due to its recency, the results and method suggest it is a significant contribution to privacy-preserving data synthesis. The method's lack of training requirements and compatibility with proprietary models further increase its practical value. No code repository is provided in the metadata.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: Haoxiang Wang, Zinan Lin, Da Yu, Huishuai Zhang&lt;/li&gt;&lt;li&gt;Tags: differential privacy, privacy-preserving AI, synthetic data generation, image synthesis&lt;/li&gt;&lt;li&gt;Relevance Score: 4/5&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.07555</guid><pubDate>Tue, 10 Jun 2025 00:00:00 +0000</pubDate></item><item><title>IntenTest: Stress Testing for Intent Integrity in API-Calling LLM Agents</title><link>https://arxiv.org/abs/2506.07524</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces IntenTest, a stress testing framework for evaluating intent integrity in API-calling LLM agents.&lt;/li&gt;&lt;li&gt;Focuses on uncovering cases where LLM agents misinterpret user intent, leading to unintended or potentially harmful actions.&lt;/li&gt;&lt;li&gt;Proposes semantic partitioning and targeted mutation strategies to systematically generate and test realistic tasks.&lt;/li&gt;&lt;li&gt;Demonstrates effectiveness in exposing intent integrity violations across a wide range of APIs and LLM models.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research Paper&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: Shiwei Feng, Xiangzhe Xu, Xuan Chen, Kaiyuan Zhang, Syed Yusuf Ahmed, Zian Su, Mingwei Zheng, Xiangyu Zhang&lt;/li&gt;&lt;li&gt;Tags: LLM safety, intent integrity, AI robustness, AI safety evaluation, stress testing&lt;/li&gt;&lt;li&gt;Relevance Score: 5/5&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.07524</guid><pubDate>Tue, 10 Jun 2025 00:00:00 +0000</pubDate></item><item><title>When Style Breaks Safety: Defending Language Models Against Superficial Style Alignment</title><link>https://arxiv.org/abs/2506.07452</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Investigates how superficial style alignment (e.g., formatting responses in specific ways) can increase LLM vulnerability to jailbreak attacks.&lt;/li&gt;&lt;li&gt;Evaluates 32 LLMs across seven jailbreak benchmarks, showing that style patterns in prompts inflate attack success rates.&lt;/li&gt;&lt;li&gt;Finds that fine-tuning with specific styles makes models more susceptible to jailbreaks using those styles.&lt;/li&gt;&lt;li&gt;Proposes SafeStyle, a defense strategy using safety training data augmented with style patterns, which improves LLM safety against such attacks.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research Paper&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Quality Assessment&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Clarity: 5/5&lt;/li&gt;&lt;li&gt;Novelty: 5/5&lt;/li&gt;&lt;li&gt;Significance: 4/5&lt;/li&gt;&lt;li&gt;Try-worthiness: Yes&lt;/li&gt;&lt;li&gt;Justification: The abstract is well-written, clearly outlining the motivation, methodology, findings, and proposed defense (SafeStyle). The work addresses a novel and timely issue: how stylistic prompts can undermine LLM safety, which has not been widely explored. The evaluation is thorough, covering 32 LLMs and multiple benchmarks, and the proposed defense is practical and empirically validated. While the paper is very recent and has no citations yet, the topic is highly relevant to current LLM deployment and safety concerns, making it significant. The approach is worth trying for practitioners concerned with LLM safety. No code repository is provided in the metadata.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: Yuxin Xiao, Sana Tonekaboni, Walter Gerych, Vinith Suriyakumar, Marzyeh Ghassemi&lt;/li&gt;&lt;li&gt;Tags: LLM red teaming, jailbreaking, adversarial prompting, alignment, defense strategies&lt;/li&gt;&lt;li&gt;Relevance Score: 5/5&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.07452</guid><pubDate>Tue, 10 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Well Begun is Half Done: Low-resource Preference Alignment by Weak-to-Strong Decoding</title><link>https://arxiv.org/abs/2506.07434</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a novel Weak-to-Strong Decoding (WSD) framework for improving preference alignment in LLMs, especially in low-resource settings.&lt;/li&gt;&lt;li&gt;Uses a small, well-aligned model to generate the initial part of a response, then switches to a larger base model to complete the output.&lt;/li&gt;&lt;li&gt;Introduces a new dataset (GenerAlign) and demonstrates that WSD enhances alignment without sacrificing performance on downstream tasks.&lt;/li&gt;&lt;li&gt;Includes experiments and analyses on alignment quality, efficiency, and the mechanisms behind WSD.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research Paper&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Quality Assessment&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Clarity: 4/5&lt;/li&gt;&lt;li&gt;Novelty: 5/5&lt;/li&gt;&lt;li&gt;Significance: 4/5&lt;/li&gt;&lt;li&gt;Try-worthiness: Yes&lt;/li&gt;&lt;li&gt;Justification: The paper is clearly written, with a well-structured abstract that explains the motivation, method (Weak-to-Strong Decoding), and contributions (including a new dataset and empirical results). The approach of using a small aligned model to guide the initial decoding of a larger base model is novel and addresses a key challenge in low-resource preference alignment. The significance is high given the ongoing importance of alignment in LLMs, though as a very recent arXiv preprint, it has not yet accumulated citations or peer-reviewed validation. The introduction of a new dataset (GenerAlign) and the claim of outperforming baselines make it worth trying for practitioners interested in LLM alignment. No code repository is provided in the metadata.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: Feifan Song, Shaohang Wei, Wen Luo, Yuxuan Fan, Tianyu Liu, Guoyin Wang, Houfeng Wang&lt;/li&gt;&lt;li&gt;Tags: alignment, LLM safety, preference alignment, harmful output prevention&lt;/li&gt;&lt;li&gt;Relevance Score: 4/5&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.07434</guid><pubDate>Tue, 10 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Evidential Spectrum-Aware Contrastive Learning for OOD Detection in Dynamic Graphs</title><link>https://arxiv.org/abs/2506.07417</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a novel method (EviSEC) for out-of-distribution (OOD) detection in dynamic graphs using evidential deep learning.&lt;/li&gt;&lt;li&gt;Addresses challenges of high bias/variance and score homogenization in OOD detection for dynamic graphs.&lt;/li&gt;&lt;li&gt;Introduces spectrum-aware augmentation to generate OOD approximations and improve detection performance.&lt;/li&gt;&lt;li&gt;Demonstrates effectiveness through experiments on real-world datasets, relevant for security-sensitive applications.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research Paper&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Quality Assessment&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Clarity: 4/5&lt;/li&gt;&lt;li&gt;Novelty: 5/5&lt;/li&gt;&lt;li&gt;Significance: 3/5&lt;/li&gt;&lt;li&gt;Try-worthiness: Yes&lt;/li&gt;&lt;li&gt;Justification: The paper is clearly written, with a well-structured abstract that outlines the motivation, challenges, and proposed solution (EviSEC) for OOD detection in dynamic graphs. The use of Evidential Deep Learning and spectrum-aware contrastive learning for this specific problem appears novel, especially as most prior work focuses on static graphs. The significance is moderate at this stage: while the problem is important and the approach promising, the paper is a recent arXiv preprint with no citations yet and has not been peer-reviewed or published in a top-tier venue. Nevertheless, the described method addresses real challenges and reports extensive experiments, making it worth trying for researchers in OOD detection or dynamic graph analysis.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: Nan Sun, Xixun Lin, Zhiheng Zhou, Yanmin Shang, Zhenlin Cheng, Yanan Cao&lt;/li&gt;&lt;li&gt;Tags: OOD detection, dynamic graphs, AI security, robustness, uncertainty estimation&lt;/li&gt;&lt;li&gt;Relevance Score: 4/5&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.07417</guid><pubDate>Tue, 10 Jun 2025 00:00:00 +0000</pubDate></item><item><title>MrM: Black-Box Membership Inference Attacks against Multimodal RAG Systems</title><link>https://arxiv.org/abs/2506.07399</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces MrM, the first black-box membership inference attack (MIA) framework targeting multimodal retrieval-augmented generation (RAG) systems.&lt;/li&gt;&lt;li&gt;Focuses on privacy vulnerabilities in vision-language models that integrate external knowledge databases, which may contain sensitive information.&lt;/li&gt;&lt;li&gt;Proposes a novel object-aware data perturbation and counterfactual mask selection strategy to enhance attack efficacy and bypass model self-knowledge.&lt;/li&gt;&lt;li&gt;Demonstrates strong attack performance and robustness against adaptive defenses on commercial visual-language models like GPT-4o and Gemini-2.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research Paper&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Quality Assessment&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Clarity: 4/5&lt;/li&gt;&lt;li&gt;Novelty: 5/5&lt;/li&gt;&lt;li&gt;Significance: 4/5&lt;/li&gt;&lt;li&gt;Try-worthiness: Yes&lt;/li&gt;&lt;li&gt;Justification: The abstract is well-written and clearly explains the motivation, methodology, and results, though it is somewhat dense with technical terms. The work is highly novel, being the first black-box membership inference attack (MIA) framework specifically targeting multimodal RAG systems, addressing a gap in the literature where visual modalities have been underexplored. The significance is high given the increasing deployment of multimodal RAG systems and the privacy risks involved, though the impact is yet to be fully measured due to the paper's recency and preprint status. The method is evaluated on strong commercial models and shows robust results, making it worth trying for researchers interested in privacy and security of AI systems. No code repository is provided in the metadata.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: Peiru Yang, Jinhua Yin, Haoran Zheng, Xueying Bai, Huili Wang, Yufei Sun, Xintian Li, Shangguang Wang, Yongfeng Huang, Tao Qi&lt;/li&gt;&lt;li&gt;Tags: membership inference, privacy attacks, multimodal RAG, vision-language models, black-box attacks&lt;/li&gt;&lt;li&gt;Relevance Score: 5/5&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.07399</guid><pubDate>Tue, 10 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Deepfake Technology Unveiled: The Commoditization of AI and Its Impact on Digital Trust</title><link>https://arxiv.org/abs/2506.07363</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Examines the commoditization of deepfake technology and its impact on digital trust and security.&lt;/li&gt;&lt;li&gt;Analyzes how accessible generative AI tools enable the creation of realistic deepfakes, lowering barriers for malicious use.&lt;/li&gt;&lt;li&gt;Discusses risks such as fraud, misinformation, and privacy erosion enabled by deepfakes.&lt;/li&gt;&lt;li&gt;Explores technical and ethical challenges in deepfake detection and mitigation, and calls for regulatory and collaborative responses.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Other&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: Claudiu Popa, Rex Pallath, Liam Cunningham, Hewad Tahiri, Abiram Kesavarajah, Tao Wu&lt;/li&gt;&lt;li&gt;Tags: deepfake, AI security, misinformation, privacy, regulation&lt;/li&gt;&lt;li&gt;Relevance Score: 4/5&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.07363</guid><pubDate>Tue, 10 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Distributed Risk-Sensitive Safety Filters for Uncertain Discrete-Time Systems</title><link>https://arxiv.org/abs/2506.07347</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a risk-sensitive safety filter for multi-agent systems with uncertain dynamics.&lt;/li&gt;&lt;li&gt;Utilizes control barrier functions and exponential risk operators to ensure robustness and safety.&lt;/li&gt;&lt;li&gt;Introduces distributed strategies for safety filtering, allowing agents to switch between worst-case and safe-policy-based approaches.&lt;/li&gt;&lt;li&gt;Demonstrates effectiveness through numerical evaluations in maintaining safety without excessive conservatism.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research Paper&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Quality Assessment&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Clarity: 4/5&lt;/li&gt;&lt;li&gt;Novelty: 5/5&lt;/li&gt;&lt;li&gt;Significance: 4/5&lt;/li&gt;&lt;li&gt;Try-worthiness: Yes&lt;/li&gt;&lt;li&gt;Justification: The abstract is well-written and clearly outlines the motivation, approach, and contributions, though some technical terms may require background knowledge. The work appears highly novel, introducing distributed risk-sensitive safety filters for uncertain multi-agent systems using value-function-based CBFs and exponential risk operators, which is a fresh approach in the field. The significance is high given the importance of safety in multi-agent systems and the challenge of distributed implementation, though the impact is yet to be seen due to its recent release and preprint status. The approach is worth trying for researchers in safe multi-agent control, especially those interested in risk-sensitive and distributed methods. No code repository is provided.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: Armin Lederer, Erfaun Noorani, Andreas Krause&lt;/li&gt;&lt;li&gt;Tags: AI safety, robustness, multi-agent systems, risk-sensitive control&lt;/li&gt;&lt;li&gt;Relevance Score: 4/5&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.07347</guid><pubDate>Tue, 10 Jun 2025 00:00:00 +0000</pubDate></item><item><title>JavelinGuard: Low-Cost Transformer Architectures for LLM Security</title><link>https://arxiv.org/abs/2506.07330</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces JavelinGuard, a suite of transformer-based architectures for detecting malicious intent in LLM interactions.&lt;/li&gt;&lt;li&gt;Focuses on low-cost, high-performance models optimized for production deployment and rapid inference.&lt;/li&gt;&lt;li&gt;Benchmarks models on nine adversarial datasets, including newly introduced JavelinBench, targeting LLM security threats.&lt;/li&gt;&lt;li&gt;Compares proposed architectures with existing guardrail models and large LLMs, highlighting superior cost-performance trade-offs.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research Paper&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Quality Assessment&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Clarity: 5/5&lt;/li&gt;&lt;li&gt;Novelty: 4/5&lt;/li&gt;&lt;li&gt;Significance: 4/5&lt;/li&gt;&lt;li&gt;Try-worthiness: Yes&lt;/li&gt;&lt;li&gt;Justification: The abstract is well-written, clearly outlining the motivation, methods, and results of the work. The paper introduces a suite of transformer-based architectures specifically optimized for LLM security, with a focus on low-cost, high-performance deployment, which is a timely and important topic. The systematic exploration of multiple architectures and benchmarking across diverse adversarial datasets, including a newly introduced benchmark, adds to the novelty. While the paper is very recent and has no citations yet, the significance is high due to the practical relevance and comprehensive evaluation. The comparison with both open-source guardrail models and large LLMs further strengthens its impact. The work appears highly try-worthy for practitioners interested in LLM security. No code repository link is provided in the metadata.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: Yash Datta, Sharath Rajasekar&lt;/li&gt;&lt;li&gt;Tags: LLM security, malicious intent detection, adversarial robustness, guardrails, benchmarking&lt;/li&gt;&lt;li&gt;Relevance Score: 5/5&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.07330</guid><pubDate>Tue, 10 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Reward Model Interpretability via Optimal and Pessimal Tokens</title><link>https://arxiv.org/abs/2506.07326</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a novel method for interpreting reward models by analyzing their responses to all possible single-token completions for value-laden prompts.&lt;/li&gt;&lt;li&gt;Finds significant heterogeneity and systematic biases in how different reward models encode human value judgments.&lt;/li&gt;&lt;li&gt;Highlights concerning biases toward certain identity groups and unintended consequences of harmlessness training.&lt;/li&gt;&lt;li&gt;Raises questions about the reliability and safety of using current reward models as proxies for human values in LLM alignment.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research Paper&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Quality Assessment&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Clarity: 5/5&lt;/li&gt;&lt;li&gt;Novelty: 5/5&lt;/li&gt;&lt;li&gt;Significance: 4/5&lt;/li&gt;&lt;li&gt;Try-worthiness: Yes&lt;/li&gt;&lt;li&gt;Justification: The abstract is exceptionally clear, outlining the motivation, methodology, and key findings in a concise manner. The approach—exhaustively analyzing reward model responses across the vocabulary space for interpretability—is novel and addresses an underexplored aspect of reward modeling. The findings (heterogeneity, asymmetry, prompt sensitivity, and frequency bias) are both surprising and important, challenging assumptions about reward model interchangeability and their use as proxies for human values. The significance is high, especially given the widespread use of reward models in aligning LLMs, though the lack of citations and preprint status slightly temper this. The work is highly try-worthy for researchers interested in model interpretability, alignment, and bias analysis. No code repository is provided in the metadata.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: Brian Christian, Hannah Rose Kirk, Jessica A. F. Thompson, Christopher Summerfield, Tsvetomira Dumbalska&lt;/li&gt;&lt;li&gt;Tags: reward modeling, AI alignment, model interpretability, bias and fairness, AI safety&lt;/li&gt;&lt;li&gt;Relevance Score: 4/5&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.07326</guid><pubDate>Tue, 10 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Sword and Shield: Uses and Strategies of LLMs in Navigating Disinformation</title><link>https://arxiv.org/abs/2506.07211</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Explores how Large Language Models (LLMs) can be used both to generate and to combat disinformation in online forums.&lt;/li&gt;&lt;li&gt;Simulates a communication game with Disinformers, Moderators, and Users leveraging LLMs to achieve their objectives.&lt;/li&gt;&lt;li&gt;Analyzes the strategies and effectiveness of LLMs in both spreading and detecting disinformation.&lt;/li&gt;&lt;li&gt;Discusses implications for LLM development and online platform design to mitigate risks of LLM-assisted disinformation.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research Paper&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Quality Assessment&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Clarity: 4/5&lt;/li&gt;&lt;li&gt;Novelty: 4/5&lt;/li&gt;&lt;li&gt;Significance: 3/5&lt;/li&gt;&lt;li&gt;Try-worthiness: Yes&lt;/li&gt;&lt;li&gt;Justification: The paper is clearly written, with a well-structured abstract that outlines the dual role of LLMs in both generating and combating disinformation. The use of a communication game inspired by Werewolf to simulate online forums and study LLM strategies is a novel and creative approach. While the venue is arXiv (preprint), the topic is highly relevant and timely, though the significance is moderate due to the early stage of publication and lack of citations. The study's findings could inform future LLM and platform design, making it worth experimenting with or building upon, especially for researchers in AI safety, disinformation, or online moderation.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: Gionnieve Lim, Bryan Chen Zhengyu Tan, Kellie Yu Hui Sim, Weiyan Shi, Ming Hui Chew, Ming Shan Hee, Roy Ka-Wei Lee, Simon T. Perrault, Kenny Tsu Wei Choo&lt;/li&gt;&lt;li&gt;Tags: LLM misuse, disinformation, AI safety, risk mitigation, content moderation&lt;/li&gt;&lt;li&gt;Relevance Score: 4/5&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.07211</guid><pubDate>Tue, 10 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Flattery in Motion: Benchmarking and Analyzing Sycophancy in Video-LLMs</title><link>https://arxiv.org/abs/2506.07180</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces VISE, a benchmark for evaluating sycophantic behavior in Video-LLMs when user input contradicts visual evidence.&lt;/li&gt;&lt;li&gt;Analyzes how Video-LLMs respond to misleading prompts and identifies different types of sycophancy in multimodal reasoning.&lt;/li&gt;&lt;li&gt;Explores mitigation strategies, such as key-frame selection, to reduce sycophantic bias and improve factual consistency.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Quality Assessment&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Clarity: 5/5&lt;/li&gt;&lt;li&gt;Novelty: 5/5&lt;/li&gt;&lt;li&gt;Significance: 4/5&lt;/li&gt;&lt;li&gt;Try-worthiness: Yes&lt;/li&gt;&lt;li&gt;Justification: The paper is very clearly written, with a well-structured abstract that outlines the motivation, gap, proposed solution (VISE benchmark), and a mitigation strategy. The focus on sycophancy in Video-LLMs is highly novel, as previous work has not systematically addressed this issue in the video-language domain. The introduction of a dedicated benchmark and analysis framework is likely to be significant for the community, especially as Video-LLMs become more prevalent. While the paper is very recent and has no citations yet, its topic and approach are timely and relevant. The proposed mitigation strategy (key-frame selection) also adds practical value. The lack of a code repository is a minor drawback, but the paper is still worth experimenting with, especially for researchers interested in multimodal model reliability and evaluation.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: Wenrui Zhou, Shu Yang, Qingsong Yang, Zikun Guo, Lijie Hu, Di Wang&lt;/li&gt;&lt;li&gt;Tags: AI safety, alignment, multimodal models, robustness, benchmarking&lt;/li&gt;&lt;li&gt;Relevance Score: 4/5&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.07180</guid><pubDate>Tue, 10 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Mind the Web: The Security of Web Use Agents</title><link>https://arxiv.org/abs/2506.07153</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Analyzes the security vulnerabilities of web-use agents powered by LLMs with extensive browser capabilities.&lt;/li&gt;&lt;li&gt;Demonstrates novel attack techniques, including task-aligned injection, that exploit agents via malicious web content.&lt;/li&gt;&lt;li&gt;Systematically evaluates multiple popular agents, showing high success rates for attacks compromising confidentiality, integrity, and availability.&lt;/li&gt;&lt;li&gt;Proposes mitigation strategies such as oversight mechanisms, execution constraints, and improved task-aware reasoning.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research Paper&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: Avishag Shapira, Parth Atulbhai Gandhi, Edan Habler, Oleg Brodt, Asaf Shabtai&lt;/li&gt;&lt;li&gt;Tags: LLM red teaming, prompt injection, AI security, web agent vulnerabilities, mitigation strategies&lt;/li&gt;&lt;li&gt;Relevance Score: 5/5&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.07153</guid><pubDate>Tue, 10 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Quality-Diversity Red-Teaming: Automated Generation of High-Quality and Diverse Attackers for Large Language Models</title><link>https://arxiv.org/abs/2506.07121</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces Quality-Diversity Red-Teaming (QDRT), a framework for generating diverse and high-quality adversarial prompts for LLMs.&lt;/li&gt;&lt;li&gt;Addresses limitations of previous red-teaming approaches, such as simplistic diversity metrics and reliance on single attacker models.&lt;/li&gt;&lt;li&gt;Implements behavior-conditioned training and a behavioral replay buffer to enhance diversity and effectiveness of attacks.&lt;/li&gt;&lt;li&gt;Demonstrates that QDRT produces more diverse and effective attacks across multiple LLMs, supporting improved safety evaluation.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research Paper&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Quality Assessment&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Clarity: 5/5&lt;/li&gt;&lt;li&gt;Novelty: 4/5&lt;/li&gt;&lt;li&gt;Significance: 4/5&lt;/li&gt;&lt;li&gt;Try-worthiness: Yes&lt;/li&gt;&lt;li&gt;Justification: The abstract is well-written, clearly outlining the motivation, limitations of prior work, and the proposed solution (QDRT). The approach appears novel in its use of behavior-conditioned training, a behavioral replay buffer, and multiple specialized attackers for red-teaming LLMs, addressing known shortcomings in diversity and coverage. While the paper is very recent and has no citations yet, the topic is highly significant given the importance of LLM safety and the broad applicability to current models (GPT-2, Llama-3, etc.). The method's empirical effectiveness and systematic approach make it worth experimenting with, especially for those working on LLM safety or adversarial robustness. No code repository is provided in the metadata.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: Ren-Jian Wang, Ke Xue, Zeyu Qin, Ziniu Li, Sheng Tang, Hao-Tian Li, Shengcai Liu, Chao Qian&lt;/li&gt;&lt;li&gt;Tags: LLM red teaming, adversarial prompting, AI safety evaluation, automated attack generation&lt;/li&gt;&lt;li&gt;Relevance Score: 5/5&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.07121</guid><pubDate>Tue, 10 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Dual-Priv Pruning : Efficient Differential Private Fine-Tuning in Multimodal Large Language Models</title><link>https://arxiv.org/abs/2506.07077</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes Dual-Priv Pruning, a framework for efficient differentially private fine-tuning in multimodal large language models (MLLMs).&lt;/li&gt;&lt;li&gt;Introduces two pruning mechanisms: visual token pruning and gradient-update pruning to reduce computational overhead and mitigate utility loss from DP noise.&lt;/li&gt;&lt;li&gt;Demonstrates competitive performance and improved memory efficiency compared to standard DP-SGD and zeroth-order methods.&lt;/li&gt;&lt;li&gt;First work to specifically address differential privacy fine-tuning in MLLMs.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research Paper&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: Qianshan Wei, Jiaqi Li, Zihan You, Yi Zhan, Kecen Li, Jialin Wu, Xinfeng Li Hengjun Liu, Yi Yu, Bin Cao, Yiwen Xu, Yang Liu, Guilin Qi&lt;/li&gt;&lt;li&gt;Tags: differential privacy, multimodal large language models, privacy-preserving machine learning, model fine-tuning, AI security&lt;/li&gt;&lt;li&gt;Relevance Score: 5/5&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.07077</guid><pubDate>Tue, 10 Jun 2025 00:00:00 +0000</pubDate></item><item><title>HauntAttack: When Attack Follows Reasoning as a Shadow</title><link>https://arxiv.org/abs/2506.07031</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces HauntAttack, a black-box attack framework that embeds harmful instructions into reasoning questions for Large Reasoning Models (LRMs).&lt;/li&gt;&lt;li&gt;Demonstrates how reasoning pathways can be manipulated to guide models toward generating unsafe outputs.&lt;/li&gt;&lt;li&gt;Presents comprehensive experiments revealing significant safety vulnerabilities in state-of-the-art LRMs.&lt;/li&gt;&lt;li&gt;Provides detailed analysis of model behaviors, types of harmful instructions, and output patterns, offering insights into LRM security.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research Paper&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Quality Assessment&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Clarity: 4/5&lt;/li&gt;&lt;li&gt;Novelty: 5/5&lt;/li&gt;&lt;li&gt;Significance: 4/5&lt;/li&gt;&lt;li&gt;Try-worthiness: Yes&lt;/li&gt;&lt;li&gt;Justification: The abstract is well-written and clearly explains the motivation, methodology, and findings of the paper, though some technical details are left for the full text. The concept of HauntAttack—embedding harmful instructions into reasoning questions to probe safety vulnerabilities in Large Reasoning Models (LRMs)—is novel and timely, especially as reasoning capabilities become a focus in LLM research. The significance is high given the growing deployment of LRMs and the critical importance of safety, though the paper is very new and not yet peer-reviewed or cited. The attack framework appears general and practical, making it worth experimenting with for both researchers and practitioners interested in LLM safety. No code repository is provided in the metadata.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: Jingyuan Ma, Rui Li, Zheng Li, Junfeng Liu, Lei Sha, Zhifang Sui&lt;/li&gt;&lt;li&gt;Tags: LLM red teaming, adversarial prompting, AI safety, harmful outputs, security vulnerabilities&lt;/li&gt;&lt;li&gt;Relevance Score: 5/5&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.07031</guid><pubDate>Tue, 10 Jun 2025 00:00:00 +0000</pubDate></item><item><title>AlphaSteer: Learning Refusal Steering with Principled Null-Space Constraint</title><link>https://arxiv.org/abs/2506.07022</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes AlphaSteer, a novel activation steering method to enhance LLM refusal of malicious/jailbreak prompts.&lt;/li&gt;&lt;li&gt;Balances safety and utility by learning separate steering vectors for benign and malicious prompts using null-space constraints and linear regression.&lt;/li&gt;&lt;li&gt;Addresses the trade-off between over-refusal (false positives) and effective refusal of harmful prompts.&lt;/li&gt;&lt;li&gt;Demonstrates improved safety against jailbreak attacks without degrading performance on benign tasks.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research Paper&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Quality Assessment&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Clarity: 4/5&lt;/li&gt;&lt;li&gt;Novelty: 4/5&lt;/li&gt;&lt;li&gt;Significance: 3/5&lt;/li&gt;&lt;li&gt;Try-worthiness: Yes&lt;/li&gt;&lt;li&gt;Justification: The abstract is well-written and clearly explains the motivation, problem, and proposed solution (AlphaSteer) for improving LLM refusal behavior. The method introduces a theoretically grounded approach with null-space constraints and linear regression, which appears novel compared to prior ad-hoc or less principled methods. While the paper is a preprint and very recent (hence no citations yet), the topic is highly relevant to LLM safety and the results claim significant improvements without utility loss. The availability of code further increases its try-worthiness for practitioners and researchers interested in LLM safety.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: Leheng Sheng, Changshuo Shen, Weixiang Zhao, Junfeng Fang, Xiaohao Liu, Zhenkai Liang, Xiang Wang, An Zhang, Tat-Seng Chua&lt;/li&gt;&lt;li&gt;Tags: LLM red teaming, jailbreak prevention, activation steering, AI safety, robustness&lt;/li&gt;&lt;li&gt;Relevance Score: 5/5&lt;/li&gt;&lt;li&gt;Code Repository: &lt;a href='https://github.com/AlphaLab-USTC/AlphaSteer'&gt;https://github.com/AlphaLab-USTC/AlphaSteer&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.07022</guid><pubDate>Tue, 10 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Auditing Black-Box LLM APIs with a Rank-Based Uniformity Test</title><link>https://arxiv.org/abs/2506.06975</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a rank-based uniformity test to audit black-box LLM APIs for behavioral equivalence to authentic models.&lt;/li&gt;&lt;li&gt;Addresses the detection of covert model substitutions, such as quantized or fine-tuned variants, which may impact safety and performance.&lt;/li&gt;&lt;li&gt;Demonstrates robustness against adversarial API providers who may attempt to evade detection.&lt;/li&gt;&lt;li&gt;Evaluates the method in scenarios including quantization, harmful fine-tuning, jailbreak prompts, and full model substitution.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research Paper&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Quality Assessment&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Clarity: 5/5&lt;/li&gt;&lt;li&gt;Novelty: 5/5&lt;/li&gt;&lt;li&gt;Significance: 4/5&lt;/li&gt;&lt;li&gt;Try-worthiness: Yes&lt;/li&gt;&lt;li&gt;Justification: The abstract is very clearly written, outlining the motivation, problem, and proposed solution in a concise manner. The work is highly novel, addressing the timely and underexplored issue of auditing black-box LLM APIs for model substitution or tampering, which is increasingly relevant as LLMs are deployed via opaque APIs. The proposed rank-based uniformity test appears to be a new approach, and the paper claims superior statistical power and robustness compared to prior methods. While the paper is very recent and has no citations yet, the significance is high due to the practical importance of model integrity in LLM deployments. The method's query efficiency and stealthiness make it particularly attractive for real-world use. Given these factors, the paper is worth implementing or experimenting with, especially for those concerned with LLM security and auditing. No code repository is provided in the metadata.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: Xiaoyuan Zhu, Yaowen Ye, Tianyi Qiu, Hanlin Zhu, Sijun Tan, Ajraf Mannan, Jonathan Michala, Raluca Ada Popa, Willie Neiswanger&lt;/li&gt;&lt;li&gt;Tags: LLM auditing, model substitution detection, AI security, robustness testing, black-box evaluation&lt;/li&gt;&lt;li&gt;Relevance Score: 5/5&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.06975</guid><pubDate>Tue, 10 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Rewriting the Budget: A General Framework for Black-Box Attacks Under Cost Asymmetry</title><link>https://arxiv.org/abs/2506.06933</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a general framework for decision-based black-box adversarial attacks under asymmetric query costs.&lt;/li&gt;&lt;li&gt;Introduces new algorithms (Asymmetric Search and Asymmetric Gradient Estimation) to minimize attack costs in scenarios where different queries incur different costs.&lt;/li&gt;&lt;li&gt;Demonstrates that the framework can be integrated into existing black-box attack methods with minimal changes.&lt;/li&gt;&lt;li&gt;Provides both theoretical analysis and empirical evaluation, showing significant improvements in attack efficiency and perturbation size.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research Paper&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Quality Assessment&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Clarity: 5/5&lt;/li&gt;&lt;li&gt;Novelty: 4/5&lt;/li&gt;&lt;li&gt;Significance: 4/5&lt;/li&gt;&lt;li&gt;Try-worthiness: Yes&lt;/li&gt;&lt;li&gt;Justification: The abstract is very well-written, clearly stating the motivation, problem, and contributions. The paper addresses a practical and underexplored aspect of black-box adversarial attacks: asymmetric query costs, which is a realistic scenario in many applications. The proposed framework (AS and AGREST) appears to be a novel extension to existing attack strategies, and the claim of up to 40% improvement in cost is significant. While the paper is a preprint and has no citations yet (expected for its age), the problem and solution are both timely and relevant, making it worth experimenting with, especially for researchers or practitioners interested in adversarial robustness or security. No code repository is provided in the metadata.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: Mahdi Salmani, Alireza Abdollahpoorrostam, Seyed-Mohsen Moosavi-Dezfooli&lt;/li&gt;&lt;li&gt;Tags: black-box attacks, adversarial examples, cost asymmetry, AI security, robustness&lt;/li&gt;&lt;li&gt;Relevance Score: 5/5&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.06933</guid><pubDate>Tue, 10 Jun 2025 00:00:00 +0000</pubDate></item><item><title>DivScore: Zero-Shot Detection of LLM-Generated Text in Specialized Domains</title><link>https://arxiv.org/abs/2506.06705</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes DivScore, a zero-shot detection framework for identifying LLM-generated text in specialized domains such as medicine and law.&lt;/li&gt;&lt;li&gt;Addresses the challenge of domain shift that causes existing detectors to fail in specialized contexts.&lt;/li&gt;&lt;li&gt;Introduces a domain-specific benchmark for evaluating LLM-generated text detection in high-stakes fields.&lt;/li&gt;&lt;li&gt;Demonstrates DivScore's superior robustness and performance, including in adversarial settings.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research Paper&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: Zhihui Chen, Kai He, Yucheng Huang, Yunxiao Zhu, Mengling Feng&lt;/li&gt;&lt;li&gt;Tags: LLM-generated text detection, adversarial robustness, domain-specific AI security, zero-shot detection&lt;/li&gt;&lt;li&gt;Relevance Score: 4/5&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.06705</guid><pubDate>Tue, 10 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Saffron-1: Towards an Inference Scaling Paradigm for LLM Safety Assurance</title><link>https://arxiv.org/abs/2506.06444</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Identifies limitations of current inference scaling techniques for LLM safety assurance, especially against jailbreak attacks.&lt;/li&gt;&lt;li&gt;Introduces SAFFRON, a novel inference scaling paradigm specifically designed for LLM safety.&lt;/li&gt;&lt;li&gt;Proposes a multifurcation reward model (MRM) and supporting techniques to improve efficiency and robustness in safety evaluation.&lt;/li&gt;&lt;li&gt;Releases a new safety reward dataset (Safety4M) and code to support further research in LLM safety.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research Paper&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Quality Assessment&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Clarity: 4/5&lt;/li&gt;&lt;li&gt;Novelty: 5/5&lt;/li&gt;&lt;li&gt;Significance: 4/5&lt;/li&gt;&lt;li&gt;Try-worthiness: Yes&lt;/li&gt;&lt;li&gt;Justification: The abstract is well-written and clearly outlines the motivation, challenges, and contributions of the paper, though some technical terms may require background knowledge. The work is highly novel, introducing the concept of inference scaling for LLM safety—a largely unexplored area—and proposing new methods such as the multifurcation reward model and Trie-based caching. The significance is high given the growing importance of LLM safety, though the paper is very recent and published as a preprint, so its impact is yet to be established. The public release of code, models, and data further increases its try-worthiness for researchers and practitioners interested in LLM safety.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: Ruizhong Qiu, Gaotang Li, Tianxin Wei, Jingrui He, Hanghang Tong&lt;/li&gt;&lt;li&gt;Tags: LLM safety, jailbreak attacks, inference scaling, reward modeling, safety evaluation&lt;/li&gt;&lt;li&gt;Relevance Score: 5/5&lt;/li&gt;&lt;li&gt;Code Repository: &lt;a href='https://github.com/q-rz/saffron'&gt;https://github.com/q-rz/saffron&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.06444</guid><pubDate>Tue, 10 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Benchmarking Misuse Mitigation Against Covert Adversaries</title><link>https://arxiv.org/abs/2506.06414</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces a benchmarking pipeline (BSD) for evaluating language model defenses against covert, stateful adversarial attacks.&lt;/li&gt;&lt;li&gt;Focuses on attacks where adversaries decompose harmful tasks into benign-seeming queries to bypass safeguards.&lt;/li&gt;&lt;li&gt;Curates new datasets to test model responses to such covert misuse strategies.&lt;/li&gt;&lt;li&gt;Evaluates the effectiveness of stateful defenses in mitigating these advanced misuse tactics.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Quality Assessment&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Clarity: 4/5&lt;/li&gt;&lt;li&gt;Novelty: 5/5&lt;/li&gt;&lt;li&gt;Significance: 4/5&lt;/li&gt;&lt;li&gt;Try-worthiness: Yes&lt;/li&gt;&lt;li&gt;Justification: The abstract is well-written and clearly explains the motivation, methodology, and findings of the paper, though some technical terms (e.g., 'stateful defenses', 'decomposition attacks') may require background knowledge. The work is highly novel, addressing a realistic and underexplored threat model (covert, multi-query misuse) and introducing a new benchmarking pipeline and datasets. Its significance is high, as it targets a critical gap in current LLM safety evaluations and proposes actionable countermeasures, though the impact is yet to be seen due to its recent publication and preprint status. The approach and datasets are likely to be valuable for researchers and practitioners, making the paper worth implementing or experimenting with. No code repository is provided in the metadata.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: Davis Brown, Mahdi Sabbaghi, Luze Sun, Alexander Robey, George J. Pappas, Eric Wong, Hamed Hassani&lt;/li&gt;&lt;li&gt;Tags: LLM red teaming, misuse mitigation, covert attacks, stateful defenses, benchmarking&lt;/li&gt;&lt;li&gt;Relevance Score: 5/5&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.06414</guid><pubDate>Tue, 10 Jun 2025 00:00:00 +0000</pubDate></item><item><title>HeavyWater and SimplexWater: Watermarking Low-Entropy Text Distributions</title><link>https://arxiv.org/abs/2506.06409</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes two new watermarking methods (HeavyWater and SimplexWater) for LLM-generated text, especially effective in low-entropy (near-deterministic) tasks like code generation.&lt;/li&gt;&lt;li&gt;Presents an optimization framework to maximize watermark detection while minimizing text distortion.&lt;/li&gt;&lt;li&gt;Benchmarks the proposed methods, demonstrating high detection accuracy with minimal impact on text quality.&lt;/li&gt;&lt;li&gt;Discusses theoretical connections between watermarking and coding theory.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research Paper&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Quality Assessment&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Clarity: 5/5&lt;/li&gt;&lt;li&gt;Novelty: 5/5&lt;/li&gt;&lt;li&gt;Significance: 4/5&lt;/li&gt;&lt;li&gt;Try-worthiness: Yes&lt;/li&gt;&lt;li&gt;Justification: The abstract is well-written, clearly outlining the motivation, challenges, and contributions of the paper. The work addresses a novel and important problem: watermarking LLM outputs in low-entropy (near-deterministic) settings such as code generation, which is a known challenge for existing watermarking methods. The introduction of two new watermarking schemes (HeavyWater and SimplexWater), along with an optimization framework and theoretical connections to coding theory, demonstrates strong novelty. The significance is high given the growing need for reliable LLM output attribution, though as a recent arXiv preprint, it has not yet accumulated citations or peer-reviewed validation. The presence of a public code repository increases the paper's try-worthiness for practitioners and researchers interested in LLM watermarking.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: Dor Tsur, Carol Xuan Long, Claudio Mayrink Verdun, Hsiang Hsu, Chen-Fu Chen, Haim Permuter, Sajani Vithana, Flavio P. Calmon&lt;/li&gt;&lt;li&gt;Tags: LLM watermarking, text provenance, AI misuse prevention, security evaluation, robustness&lt;/li&gt;&lt;li&gt;Relevance Score: 4/5&lt;/li&gt;&lt;li&gt;Code Repository: &lt;a href='https://github.com/DorTsur/HeavyWater_SimplexWater'&gt;https://github.com/DorTsur/HeavyWater_SimplexWater&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.06409</guid><pubDate>Tue, 10 Jun 2025 00:00:00 +0000</pubDate></item><item><title>TimeWak: Temporal Chained-Hashing Watermark for Time Series Data</title><link>https://arxiv.org/abs/2506.06407</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces TimeWak, a watermarking algorithm for synthetic multivariate time series data generated by diffusion models.&lt;/li&gt;&lt;li&gt;Addresses the challenge of embedding watermarks directly in real temporal-feature space, handling temporal dependencies and feature heterogeneity.&lt;/li&gt;&lt;li&gt;Proposes a temporal chained-hashing watermark and an epsilon-exact inversion method to ensure watermark detectability despite reconstruction errors.&lt;/li&gt;&lt;li&gt;Evaluates robustness of the watermark against post-editing attacks and demonstrates improvements over existing methods.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research Paper&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Quality Assessment&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Clarity: 4/5&lt;/li&gt;&lt;li&gt;Novelty: 5/5&lt;/li&gt;&lt;li&gt;Significance: 4/5&lt;/li&gt;&lt;li&gt;Try-worthiness: Yes&lt;/li&gt;&lt;li&gt;Justification: The abstract is well-written and clearly explains the motivation, challenges, and contributions of the work, though it is somewhat dense with technical terms. The paper introduces a novel watermarking method (TimeWak) specifically designed for multivariate time series diffusion models, addressing the incompatibility of latent-space watermarking with real-space generators—a clear gap in the literature. The significance is high given the growing importance of synthetic time series data in privacy-sensitive domains, and the reported improvements over baselines are substantial. As a recent arXiv preprint, it has no citations yet, but the technical contribution and experimental results make it worth trying for researchers or practitioners in synthetic data generation and watermarking. No code repository is provided in the metadata.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: Zhi Wen Soi, Chaoyi Zhu, Fouad Abiad, Aditya Shankar, Jeroen M. Galjaard, Huijuan Wang, Lydia Y. Chen&lt;/li&gt;&lt;li&gt;Tags: watermarking, synthetic data security, traceability, robustness, time series&lt;/li&gt;&lt;li&gt;Relevance Score: 4/5&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.06407</guid><pubDate>Tue, 10 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Unintended Harms of Value-Aligned LLMs: Psychological and Empirical Insights</title><link>https://arxiv.org/abs/2506.06404</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Identifies and analyzes specific safety risks associated with value-aligned LLMs.&lt;/li&gt;&lt;li&gt;Finds that value-aligned LLMs can be more prone to harmful behaviors compared to non-fine-tuned or other fine-tuned models.&lt;/li&gt;&lt;li&gt;Explores psychological principles explaining why value alignment can amplify harmful outcomes.&lt;/li&gt;&lt;li&gt;Proposes in-context alignment methods to mitigate safety risks in value-aligned LLMs.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research Paper&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Quality Assessment&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Clarity: 4/5&lt;/li&gt;&lt;li&gt;Novelty: 5/5&lt;/li&gt;&lt;li&gt;Significance: 4/5&lt;/li&gt;&lt;li&gt;Try-worthiness: Yes&lt;/li&gt;&lt;li&gt;Justification: The paper is clearly written, with a well-structured abstract that outlines the motivation, findings, and contributions. The focus on the unintended harms of value-aligned LLMs is highly novel, especially the empirical and psychological analysis of safety risks, which is a relatively unexplored area. The significance is high given the growing deployment of personalized LLMs and the urgent need to understand their safety implications, though the impact is yet to be seen due to the paper's recency and preprint status. The proposed in-context alignment methods and the empirical findings make this work worth experimenting with for researchers interested in LLM safety and alignment.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: Sooyung Choi, Jaehyeok Lee, Xiaoyuan Yi, Jing Yao, Xing Xie, JinYeong Bak&lt;/li&gt;&lt;li&gt;Tags: AI safety, value alignment, harmful outputs, LLM safety evaluation&lt;/li&gt;&lt;li&gt;Relevance Score: 5/5&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.06404</guid><pubDate>Tue, 10 Jun 2025 00:00:00 +0000</pubDate></item><item><title>From Rogue to Safe AI: The Role of Explicit Refusals in Aligning LLMs with International Humanitarian Law</title><link>https://arxiv.org/abs/2506.06391</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Evaluates leading LLMs on their ability to refuse prompts that violate International Humanitarian Law (IHL).&lt;/li&gt;&lt;li&gt;Assesses the clarity and helpfulness of refusal responses, focusing on how well models communicate legal and safety rationales.&lt;/li&gt;&lt;li&gt;Finds that standardized safety prompts improve refusal quality, but complex or technical prompts still expose vulnerabilities.&lt;/li&gt;&lt;li&gt;Proposes a benchmark for evaluating LLM compliance with IHL, contributing to safer and more transparent AI systems.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Quality Assessment&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Clarity: 5/5&lt;/li&gt;&lt;li&gt;Novelty: 4/5&lt;/li&gt;&lt;li&gt;Significance: 4/5&lt;/li&gt;&lt;li&gt;Try-worthiness: Yes&lt;/li&gt;&lt;li&gt;Justification: The paper is clearly written, with a well-structured abstract that outlines the motivation, methodology, and findings. The focus on explicit refusals in LLMs with respect to International Humanitarian Law (IHL) is a novel and timely angle, especially as AI systems are increasingly deployed in sensitive domains. The work is significant as it proposes a benchmark for IHL compliance and demonstrates practical interventions (e.g., safety prompts) that improve model behavior. While the paper is a preprint and has not yet accrued citations, its relevance and potential impact on AI safety and alignment are high. The lack of a code repository is a minor drawback, but the paper is still worth experimenting with, especially for those interested in AI safety, alignment, and legal compliance.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: John Mavi, Diana Teodora G\u{a}itan, Sergio Coronado&lt;/li&gt;&lt;li&gt;Tags: LLM refusal, AI safety, alignment, harmful output prevention, safety evaluation&lt;/li&gt;&lt;li&gt;Relevance Score: 5/5&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.06391</guid><pubDate>Tue, 10 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Detection Method for Prompt Injection by Integrating Pre-trained Model and Heuristic Feature Engineering</title><link>https://arxiv.org/abs/2506.06384</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a dual-channel detection framework (DMPI-PMHFE) for identifying prompt injection attacks in LLMs.&lt;/li&gt;&lt;li&gt;Combines a pretrained language model (DeBERTa-v3-base) with heuristic feature engineering to enhance detection.&lt;/li&gt;&lt;li&gt;Demonstrates improved performance over existing methods on benchmark datasets and real-world LLM deployments.&lt;/li&gt;&lt;li&gt;Addresses the trade-off between detection effectiveness and generalizability across different LLMs.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research Paper&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Quality Assessment&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Clarity: 4/5&lt;/li&gt;&lt;li&gt;Novelty: 4/5&lt;/li&gt;&lt;li&gt;Significance: 3/5&lt;/li&gt;&lt;li&gt;Try-worthiness: Yes&lt;/li&gt;&lt;li&gt;Justification: The paper is clearly written, with a well-structured abstract that explains the motivation, method, and results. The dual-channel approach—combining a pre-trained model (DeBERTa-v3-base) with heuristic feature engineering—appears to be a novel contribution in the context of prompt injection detection for LLMs. The significance is moderate: while prompt injection is an important and timely problem, the paper is a preprint and has not yet been peer-reviewed or cited, so its impact is not yet established. The reported results are promising, and the method is described in enough detail to warrant experimentation. However, no code repository is provided, which may hinder immediate adoption.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: Yi Ji, Runzhi Li, Baolei Mao&lt;/li&gt;&lt;li&gt;Tags: prompt injection, LLM security, adversarial prompting, attack detection&lt;/li&gt;&lt;li&gt;Relevance Score: 5/5&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.06384</guid><pubDate>Tue, 10 Jun 2025 00:00:00 +0000</pubDate></item><item><title>CPS-Guard: Framework for Dependability Assurance of AI- and LLM-Based Cyber-Physical Systems</title><link>https://arxiv.org/abs/2506.06381</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes CPS-Guard, a framework for dependability assurance in AI- and LLM-based cyber-physical systems.&lt;/li&gt;&lt;li&gt;Automates verification and validation using multi-role orchestration, including safety monitoring, security assessment, and fault injection.&lt;/li&gt;&lt;li&gt;Demonstrates effectiveness through a case study with an autonomous vehicle, detecting vulnerabilities and supporting adaptive recovery.&lt;/li&gt;&lt;li&gt;Focuses on rigorous evaluation of safety and security in AI-powered critical systems.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research Paper&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Quality Assessment&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Clarity: 5/5&lt;/li&gt;&lt;li&gt;Novelty: 4/5&lt;/li&gt;&lt;li&gt;Significance: 4/5&lt;/li&gt;&lt;li&gt;Try-worthiness: Yes&lt;/li&gt;&lt;li&gt;Justification: The abstract is well-written, clearly outlining the motivation, approach, and results of the CPS-Guard framework. The use of multi-role orchestration for automated assurance in AI-powered CPS appears novel, especially with dedicated agents for safety, security, and recovery. While the paper is very recent and published as a preprint on arXiv (so no citations yet), the topic is highly significant given the increasing reliance on AI in safety-critical CPS. The demonstration on an autonomous vehicle scenario adds practical relevance. The framework's structured approach to V&amp;V in AI/LLM-based CPS makes it worth experimenting with, especially for researchers or practitioners in dependable AI systems. No code repository is provided in the metadata.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: Trisanth Srinivasan, Santosh Patapati, Himani Musku, Idhant Gode, Aditya Arora, Samvit Bhattacharya, Abubakr Nazriev, Sanika Hirave, Zaryab Kanjiani, Srinjoy Ghose, Srinidhi Shetty&lt;/li&gt;&lt;li&gt;Tags: AI safety, AI security, cyber-physical systems, verification and validation, fault injection&lt;/li&gt;&lt;li&gt;Relevance Score: 5/5&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.06381</guid><pubDate>Tue, 10 Jun 2025 00:00:00 +0000</pubDate></item><item><title>How Malicious AI Swarms Can Threaten Democracy</title><link>https://arxiv.org/abs/2506.06299</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Discusses the emerging threat of coordinated malicious AI swarms capable of sophisticated disinformation and manipulation campaigns.&lt;/li&gt;&lt;li&gt;Highlights specific risks such as infiltration, evasion of detection, data contamination, and erosion of trust in democratic processes.&lt;/li&gt;&lt;li&gt;Proposes multi-layered defenses including platform-side detection, model-side safeguards, and system-level oversight.&lt;/li&gt;&lt;li&gt;Recommends technical and governance solutions such as swarm-detection dashboards, provenance authentication, and international monitoring.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Position Paper&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Quality Assessment&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Clarity: 5/5&lt;/li&gt;&lt;li&gt;Novelty: 4/5&lt;/li&gt;&lt;li&gt;Significance: 4/5&lt;/li&gt;&lt;li&gt;Try-worthiness: No&lt;/li&gt;&lt;li&gt;Justification: The paper is very clearly written, with a well-structured abstract that outlines both the threat (malicious AI swarms) and proposed responses. The concept of coordinated, persistent AI swarms as a new vector for disinformation is relatively novel and timely, especially given recent advances in AI. The significance is high due to the potential impact on democracy, but as a preprint with no citations yet, its influence is still to be determined. However, the paper appears to be more of a position or perspective piece rather than an empirical or technical contribution, and there is no code or implementation to try. Thus, it is not try-worthy in the sense of implementation or experimentation.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: Daniel Thilo Schroeder, Meeyoung Cha, Andrea Baronchelli, Nick Bostrom, Nicholas A. Christakis, David Garcia, Amit Goldenberg, Yara Kyrychenko, Kevin Leyton-Brown, Nina Lutz, Gary Marcus, Filippo Menczer, Gordon Pennycook, David G. Rand, Frank Schweitzer, Christopher Summerfield, Audrey Tang, Jay Van Bavel, Sander van der Linden, Dawn Song, Jonas R. Kunst&lt;/li&gt;&lt;li&gt;Tags: AI security, disinformation, AI governance, adversarial attacks, risk mitigation&lt;/li&gt;&lt;li&gt;Relevance Score: 5/5&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.06299</guid><pubDate>Tue, 10 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Pairwise Calibrated Rewards for Pluralistic Alignment</title><link>https://arxiv.org/abs/2506.06298</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a method to align AI models with diverse human preferences by learning a distribution over multiple reward functions.&lt;/li&gt;&lt;li&gt;Introduces the concept of pairwise calibration to ensure that model preferences reflect the diversity of annotator opinions.&lt;/li&gt;&lt;li&gt;Demonstrates empirically that their approach leads to improved calibration and more faithful representation of pluralistic values.&lt;/li&gt;&lt;li&gt;Addresses the challenge of collapsing minority perspectives in current alignment pipelines.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research Paper&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Quality Assessment&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Clarity: 5/5&lt;/li&gt;&lt;li&gt;Novelty: 5/5&lt;/li&gt;&lt;li&gt;Significance: 4/5&lt;/li&gt;&lt;li&gt;Try-worthiness: Yes&lt;/li&gt;&lt;li&gt;Justification: The abstract is clearly written, concisely explaining the motivation, method, and empirical validation. The work is highly novel, addressing the underexplored problem of pluralistic alignment by learning a distribution over reward functions from pairwise preferences without requiring annotator IDs or predefined groups. This approach is distinct from standard alignment pipelines that assume a single reward function. The significance is high given the importance of value pluralism in AI alignment, though the impact is not yet established due to the paper's recency and preprint status. The proposed method appears practical and is empirically validated, making it worth trying for researchers interested in alignment and preference modeling. No code repository is provided in the metadata.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: Daniel Halpern, Evi Micha, Ariel D. Procaccia, Itai Shapira&lt;/li&gt;&lt;li&gt;Tags: AI alignment, pluralistic alignment, reward modeling, human preferences, safety evaluation&lt;/li&gt;&lt;li&gt;Relevance Score: 4/5&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.06298</guid><pubDate>Tue, 10 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Disentangling AI Alignment: A Structured Taxonomy Beyond Safety and Ethics</title><link>https://arxiv.org/abs/2506.06286</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a structured taxonomy for understanding AI alignment, distinguishing between safety, ethicality, legality, and other aims.&lt;/li&gt;&lt;li&gt;Clarifies conceptual boundaries and interrelations among AI Safety, AI Alignment, and Machine Ethics.&lt;/li&gt;&lt;li&gt;Aims to provide clearer guidance for researchers by categorizing alignment along axes such as aim, scope, and constituency.&lt;/li&gt;&lt;li&gt;Facilitates interdisciplinary integration and practical application of alignment concepts.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Position Paper&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Quality Assessment&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Clarity: 5/5&lt;/li&gt;&lt;li&gt;Novelty: 4/5&lt;/li&gt;&lt;li&gt;Significance: 4/5&lt;/li&gt;&lt;li&gt;Try-worthiness: No&lt;/li&gt;&lt;li&gt;Justification: The abstract is well-written, clearly outlining the motivation, problem, and proposed contribution—a structured taxonomy for AI alignment that goes beyond the usual safety/ethics dichotomy. The work appears novel in its attempt to systematically disentangle and structure the conceptual space of AI alignment, introducing new axes (aim, scope, constituency) for analysis. While the significance is potentially high for researchers seeking conceptual clarity in AI alignment, the lack of citations is expected due to its recency. However, as a conceptual and taxonomical paper rather than an empirical or algorithmic one, it is not directly try-worthy in terms of implementation or experimentation. No code repository is provided or expected for this type of work.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: Kevin Baum&lt;/li&gt;&lt;li&gt;Tags: AI alignment, AI safety, taxonomy, alignment frameworks, AI governance&lt;/li&gt;&lt;li&gt;Relevance Score: 4/5&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.06286</guid><pubDate>Tue, 10 Jun 2025 00:00:00 +0000</pubDate></item><item><title>RSafe: Incentivizing proactive reasoning to build robust and adaptive LLM safeguards</title><link>https://arxiv.org/abs/2506.07736</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes RSafe, an adaptive safeguard for LLMs that uses guided reasoning to detect and block policy-violating content.&lt;/li&gt;&lt;li&gt;Addresses limitations of current guard models, particularly their reliance on curated datasets and poor performance on out-of-distribution threats like jailbreak attacks.&lt;/li&gt;&lt;li&gt;Introduces a two-stage training paradigm: guided reasoning for safety risk analysis and reinforced alignment using rule-based RL.&lt;/li&gt;&lt;li&gt;RSafe is designed to generalize to unseen or adversarial safety violation scenarios and can be tailored to user-specified safety policies.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research Paper&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Quality Assessment&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Clarity: 4/5&lt;/li&gt;&lt;li&gt;Novelty: 4/5&lt;/li&gt;&lt;li&gt;Significance: 3/5&lt;/li&gt;&lt;li&gt;Try-worthiness: Yes&lt;/li&gt;&lt;li&gt;Justification: The abstract is well-written and clearly outlines the motivation, methodology, and contributions of the paper. The proposed RSafe framework introduces a two-stage, reasoning-based approach to LLM safety, which appears novel compared to existing guard models that rely on static datasets. The ability to generalize to out-of-distribution threats and accept user-specified safety policies is particularly interesting. However, as this is a very recent arXiv preprint with no citations and not yet peer-reviewed, its significance is moderate for now. Given the importance of LLM safety and the described adaptive approach, the paper is worth experimenting with, especially for researchers or practitioners in AI safety.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: Jingnan Zheng, Xiangtian Ji, Yijun Lu, Chenhang Cui, Weixiang Zhao, Gelei Deng, Zhenkai Liang, An Zhang, Tat-Seng Chua&lt;/li&gt;&lt;li&gt;Tags: LLM safeguards, adversarial robustness, safety alignment, policy compliance, jailbreak prevention&lt;/li&gt;&lt;li&gt;Relevance Score: 5/5&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.07736</guid><pubDate>Tue, 10 Jun 2025 00:00:00 +0000</pubDate></item><item><title>SAFEFLOW: A Principled Protocol for Trustworthy and Transactional Autonomous Agent Systems</title><link>https://arxiv.org/abs/2506.07564</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces SAFEFLOW, a protocol-level framework for secure and trustworthy LLM/VLM-based autonomous agents.&lt;/li&gt;&lt;li&gt;Enforces fine-grained information flow control to track provenance, integrity, and confidentiality of data exchanged between agents and their environments.&lt;/li&gt;&lt;li&gt;Implements transactional execution, conflict resolution, and secure scheduling to ensure robustness and global consistency in multi-agent systems.&lt;/li&gt;&lt;li&gt;Presents SAFEFLOWBENCH, a benchmark suite for evaluating agent reliability and security under adversarial and concurrent conditions.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research Paper&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: Peiran Li, Xinkai Zou, Zhuohang Wu, Ruifeng Li, Shuo Xing, Hanwen Zheng, Zhikai Hu, Yuping Wang, Haoxi Li, Qin Yuan, Yingmo Zhang, Zhengzhong Tu&lt;/li&gt;&lt;li&gt;Tags: AI security, information flow control, multi-agent systems, robustness, secure autonomous agents&lt;/li&gt;&lt;li&gt;Relevance Score: 5/5&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.07564</guid><pubDate>Tue, 10 Jun 2025 00:00:00 +0000</pubDate></item><item><title>HeTa: Relation-wise Heterogeneous Graph Foundation Attack Model</title><link>https://arxiv.org/abs/2506.07428</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a novel attack model (HeTa) targeting Heterogeneous Graph Neural Networks (HGNNs).&lt;/li&gt;&lt;li&gt;Introduces a foundation surrogate model to identify and exploit shared vulnerability patterns across different HGNNs.&lt;/li&gt;&lt;li&gt;Demonstrates that the attack model can generalize and transfer perturbations across various HGNNs and adapt to new heterogeneous graphs.&lt;/li&gt;&lt;li&gt;Presents extensive experimental results showing the effectiveness and generalizability of the attack method.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research Paper&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Quality Assessment&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Clarity: 4/5&lt;/li&gt;&lt;li&gt;Novelty: 5/5&lt;/li&gt;&lt;li&gt;Significance: 4/5&lt;/li&gt;&lt;li&gt;Try-worthiness: Yes&lt;/li&gt;&lt;li&gt;Justification: The abstract is well-written and clearly outlines the motivation, problem, and proposed solution (HeTa). The novelty is high, as the paper introduces the concept of a 'foundation attack model' for heterogeneous graph neural networks (HGNNs), leveraging shared vulnerability patterns across models—a new direction inspired by foundation models in other domains. The significance is promising, given the growing importance of HGNNs and security concerns, though the impact is yet to be validated due to the paper's recency and preprint status. The approach appears practical and generalizable, making it worth experimenting with, especially for researchers in graph learning and adversarial robustness. No code repository is provided in the metadata.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: Yuling Wang, Zihui Chen, Pengfei Jiao, Xiao Wang&lt;/li&gt;&lt;li&gt;Tags: adversarial attacks, graph neural networks, robustness evaluation, AI security&lt;/li&gt;&lt;li&gt;Relevance Score: 5/5&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.07428</guid><pubDate>Tue, 10 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Boosting Vulnerability Detection of LLMs via Curriculum Preference Optimization with Synthetic Reasoning Data</title><link>https://arxiv.org/abs/2506.07390</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a new framework (ReVD) to improve LLM-based software vulnerability detection using synthetic reasoning data.&lt;/li&gt;&lt;li&gt;Introduces forward and backward reasoning processes to generate high-quality vulnerability reasoning data.&lt;/li&gt;&lt;li&gt;Utilizes triplet supervised fine-tuning and curriculum preference optimization to enhance LLM understanding of vulnerability patterns.&lt;/li&gt;&lt;li&gt;Demonstrates significant improvements in vulnerability detection accuracy on benchmark datasets.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research Paper&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Quality Assessment&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Clarity: 4/5&lt;/li&gt;&lt;li&gt;Novelty: 5/5&lt;/li&gt;&lt;li&gt;Significance: 4/5&lt;/li&gt;&lt;li&gt;Try-worthiness: Yes&lt;/li&gt;&lt;li&gt;Justification: The paper is clearly written, with a well-structured abstract that outlines the motivation, challenges, proposed method (ReVD), and results. The novelty is high, as it introduces curriculum preference optimization with synthetic reasoning data for LLM-based vulnerability detection—a relatively unexplored area. The significance is strong, given the reported substantial improvements (12.24%-22.77%) over state-of-the-art on established benchmarks, though the venue is arXiv (preprint) and the paper is very new, so peer review and citations are pending. The approach is worth trying, especially for researchers or practitioners in software security and LLMs, as the code and data are openly available.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: Xin-Cheng Wen, Yijun Yang, Cuiyun Gao, Yang Xiao, Deheng Ye&lt;/li&gt;&lt;li&gt;Tags: LLM security, vulnerability detection, adversarial robustness, security evaluation&lt;/li&gt;&lt;li&gt;Relevance Score: 5/5&lt;/li&gt;&lt;li&gt;Code Repository: &lt;a href='https://github.com/Xin-Cheng-Wen/PO4Vul'&gt;https://github.com/Xin-Cheng-Wen/PO4Vul&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.07390</guid><pubDate>Tue, 10 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Evaluating LLM-corrupted Crowdsourcing Data Without Ground Truth</title><link>https://arxiv.org/abs/2506.06991</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Addresses the security risk of LLM-generated responses corrupting crowdsourced data intended for human feedback.&lt;/li&gt;&lt;li&gt;Proposes a peer prediction mechanism to detect and mitigate LLM-assisted cheating in annotation tasks without requiring ground truth.&lt;/li&gt;&lt;li&gt;Introduces a training-free scoring method with theoretical guarantees, robust against LLM collusion among workers.&lt;/li&gt;&lt;li&gt;Empirically validates the approach on real-world crowdsourcing datasets, demonstrating effectiveness in detecting low-effort cheating.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research Paper&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Quality Assessment&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Clarity: 5/5&lt;/li&gt;&lt;li&gt;Novelty: 5/5&lt;/li&gt;&lt;li&gt;Significance: 4/5&lt;/li&gt;&lt;li&gt;Try-worthiness: Yes&lt;/li&gt;&lt;li&gt;Justification: The abstract is clearly written, with a well-defined problem statement and a concise summary of the proposed approach. The work is highly novel, addressing the emerging issue of LLM-generated responses corrupting crowdsourced data, especially in annotation tasks where traditional LLM detection methods are ineffective. The use of peer prediction without ground truth and the introduction of a training-free scoring mechanism with theoretical guarantees are innovative. While the paper is very recent and has no citations yet, the topic is timely and significant given the rapid adoption of LLMs in crowdsourcing. The method's empirical validation on real-world datasets and its robustness to LLM collusion make it worth trying for practitioners and researchers in crowdsourcing and AI safety. No code repository is provided in the metadata.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: Yichi Zhang, Jinlong Pang, Zhaowei Zhu, Yang Liu&lt;/li&gt;&lt;li&gt;Tags: LLM security, data poisoning, crowdsourcing integrity, adversarial detection&lt;/li&gt;&lt;li&gt;Relevance Score: 4/5&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.06991</guid><pubDate>Tue, 10 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Incorporating Failure of Machine Learning in Dynamic Probabilistic Safety Assurance</title><link>https://arxiv.org/abs/2506.06868</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a probabilistic safety assurance framework that explicitly models machine learning (ML) failures in safety-critical systems.&lt;/li&gt;&lt;li&gt;Integrates SafeML (for detecting distributional shifts and assigning confidence to ML components) with Bayesian Networks for causal safety analysis.&lt;/li&gt;&lt;li&gt;Demonstrates the approach on a simulated automotive platooning system, focusing on traffic sign recognition.&lt;/li&gt;&lt;li&gt;Highlights the importance of dynamically evaluating and adapting system safety in the presence of ML reasoning failures.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research Paper&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Quality Assessment&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Clarity: 4/5&lt;/li&gt;&lt;li&gt;Novelty: 4/5&lt;/li&gt;&lt;li&gt;Significance: 3/5&lt;/li&gt;&lt;li&gt;Try-worthiness: Yes&lt;/li&gt;&lt;li&gt;Justification: The paper is clearly written, with a well-structured abstract that explains the motivation, approach, and contributions. The integration of SafeML with Bayesian Networks for dynamic probabilistic safety assurance in ML-enabled safety-critical systems appears novel, especially in the context of explicit modeling of ML failures. While the work is a preprint and very recent (hence no citations yet), the topic is significant given the increasing deployment of ML in safety-critical domains. The demonstration on a simulated automotive platooning system adds practical relevance. The approach is worth experimenting with, especially for researchers or practitioners in safety assurance for ML systems. No code repository is provided in the metadata.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: Razieh Arshadizadeh, Mahmoud Asgari, Zeinab Khosravi, Yiannis Papadopoulos, Koorosh Aslansefat&lt;/li&gt;&lt;li&gt;Tags: AI safety, robustness, safety assurance, distributional shift, autonomous systems&lt;/li&gt;&lt;li&gt;Relevance Score: 5/5&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.06868</guid><pubDate>Tue, 10 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Will artificial agents pursue power by default?</title><link>https://arxiv.org/abs/2506.06352</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Formalizes the concepts of instrumental convergence and power-seeking in AI agents using a decision-theoretic framework.&lt;/li&gt;&lt;li&gt;Assesses the claim that sufficiently capable AI agents will pursue power as a convergent instrumental goal.&lt;/li&gt;&lt;li&gt;Discusses the predictive utility and limitations of the instrumental convergence thesis for advanced AI safety.&lt;/li&gt;&lt;li&gt;Highlights implications for catastrophic risks and alignment challenges in advanced AI systems.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research Paper&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Quality Assessment&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Clarity: 4/5&lt;/li&gt;&lt;li&gt;Novelty: 4/5&lt;/li&gt;&lt;li&gt;Significance: 3/5&lt;/li&gt;&lt;li&gt;Try-worthiness: No&lt;/li&gt;&lt;li&gt;Justification: The abstract is well-written and clearly explains the motivation, approach, and conclusions of the paper, earning a high clarity score. The work appears novel in its attempt to formalize the debate around instrumental convergence and power-seeking in AI using a decision-theoretic framework, which is a step beyond prior informal arguments. However, as a theoretical/philosophical contribution without empirical results or a concrete algorithm, its immediate significance and try-worthiness for implementation are limited. The significance is moderate, as the topic is important in AI safety, but the lack of empirical validation and the early stage (preprint, no citations yet) reduce its impact for now. No code repository is provided or expected for this type of work.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: Christian Tarsney&lt;/li&gt;&lt;li&gt;Tags: AI safety, instrumental convergence, power-seeking, alignment, risk assessment&lt;/li&gt;&lt;li&gt;Relevance Score: 5/5&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.06352</guid><pubDate>Tue, 10 Jun 2025 00:00:00 +0000</pubDate></item></channel></rss>