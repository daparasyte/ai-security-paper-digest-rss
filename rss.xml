<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>AI Security Paper Digest</title><link>https://kentaroh-toyoda.github.io/ai-security-paper-digest-rss/rss.xml</link><description>Curated papers on AI security from ArXiv and ACL</description><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><lastBuildDate>Mon, 20 Oct 2025 22:20:15 +0000</lastBuildDate><item><title>On the Interaction of Compressibility and Adversarial Robustness</title><link>https://arxiv.org/abs/2507.17725</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Analyzes the interaction between compressibility and adversarial robustness in neural networks&lt;/li&gt;&lt;li&gt;Shows that compressibility can create sensitive directions in the representation space&lt;/li&gt;&lt;li&gt;Provides a robustness bound linking compressibility metrics to adversarial vulnerability&lt;/li&gt;&lt;li&gt;Empirically validates findings across tasks and training methods&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Melih Barsbey', 'Ant\\^onio H. Ribeiro', 'Umut \\c{S}im\\c{s}ekli', 'Tolga Birdal']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial robustness', 'compressibility', 'model security', 'theoretical analysis', 'empirical evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2507.17725</guid><pubDate>Mon, 20 Oct 2025 00:00:00 +0000</pubDate></item><item><title>WebInject: Prompt Injection Attack to Web Agents</title><link>https://arxiv.org/abs/2505.11717</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes WebInject, a prompt injection attack on MLLM-based web agents&lt;/li&gt;&lt;li&gt;Manipulates webpage pixels to induce specific actions&lt;/li&gt;&lt;li&gt;Overcomes non-differentiable mapping with neural network approximation&lt;/li&gt;&lt;li&gt;Demonstrates high effectiveness in evaluations&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Xilong Wang', 'John Bloch', 'Zedian Shao', 'Yuepeng Hu', 'Shuyan Zhou', 'Neil Zhenqiang Gong']&lt;/li&gt;&lt;li&gt;Tags: ['prompt injection', 'adversarial prompting', 'web agents', 'MLLM', 'optimization']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2505.11717</guid><pubDate>Mon, 20 Oct 2025 00:00:00 +0000</pubDate></item><item><title>NDM: A Noise-driven Detection and Mitigation Framework against Implicit Sexual Intentions in Text-to-Image Generation</title><link>https://arxiv.org/abs/2510.15752</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes NDM framework to detect and mitigate implicit sexual intentions in T2I generation&lt;/li&gt;&lt;li&gt;Uses noise-based detection and adaptive negative guidance&lt;/li&gt;&lt;li&gt;Validated on natural and adversarial datasets with superior performance&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yitong Sun', 'Yao Huang', 'Ruochen Zhang', 'Huanran Chen', 'Shouwei Ruan', 'Ranjie Duan', 'Xingxing Wei']&lt;/li&gt;&lt;li&gt;Tags: ['text-to-image', 'content moderation', 'adversarial attacks', 'safety evaluation', 'model robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.15752</guid><pubDate>Mon, 20 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Learning to Detect Unknown Jailbreak Attacks in Large Vision-Language Models</title><link>https://arxiv.org/abs/2510.15430</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a framework called Learning to Detect (LoD) for detecting unknown jailbreak attacks in LVLMs.&lt;/li&gt;&lt;li&gt;Uses a Multi-modal Safety Concept Activation Vector module and a Safety Pattern Auto-Encoder module.&lt;/li&gt;&lt;li&gt;Aims to improve generalization and efficiency over existing methods.&lt;/li&gt;&lt;li&gt;Demonstrates higher detection AUROC on diverse unknown attacks.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Shuang Liang', 'Zhihao Xu', 'Jialing Tao', 'Hui Xue', 'Xiting Wang']&lt;/li&gt;&lt;li&gt;Tags: ['jailbreaking', 'red teaming', 'safety evaluation', 'vision-language models', 'adversarial attacks']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.15430</guid><pubDate>Mon, 20 Oct 2025 00:00:00 +0000</pubDate></item><item><title>PRISON: Unmasking the Criminal Potential of Large Language Models</title><link>https://arxiv.org/abs/2506.16150</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces PRISON framework to evaluate LLMs' criminal potential across five traits&lt;/li&gt;&lt;li&gt;Uses realistic crime scenarios from films to test both criminal and anti-crime abilities&lt;/li&gt;&lt;li&gt;Finds LLMs exhibit emergent criminal tendencies and poor detection of deceptive behavior&lt;/li&gt;&lt;li&gt;Highlights need for robustness, alignment, and safety mechanisms&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Xinyi Wu', 'Geng Hong', 'Pei Chen', 'Yueyue Chen', 'Xudong Pan', 'Min Yang']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'safety evaluation', 'adversarial prompting', 'alignment', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.16150</guid><pubDate>Mon, 20 Oct 2025 00:00:00 +0000</pubDate></item><item><title>WebInject: Prompt Injection Attack to Web Agents</title><link>https://arxiv.org/abs/2505.11717</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes WebInject, a prompt injection attack on MLLM-based web agents&lt;/li&gt;&lt;li&gt;Manipulates webpage pixels to induce specific actions&lt;/li&gt;&lt;li&gt;Overcomes non-differentiable mapping with neural network approximation&lt;/li&gt;&lt;li&gt;Demonstrates high effectiveness in evaluations&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Xilong Wang', 'John Bloch', 'Zedian Shao', 'Yuepeng Hu', 'Shuyan Zhou', 'Neil Zhenqiang Gong']&lt;/li&gt;&lt;li&gt;Tags: ['prompt injection', 'adversarial prompting', 'web agents', 'MLLM', 'optimization']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2505.11717</guid><pubDate>Mon, 20 Oct 2025 00:00:00 +0000</pubDate></item><item><title>On the Ability of LLMs to Handle Character-Level Perturbations: How Well and How?</title><link>https://arxiv.org/abs/2510.14365</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Investigates LLM resilience against character-level perturbations using UCC-Inj method&lt;/li&gt;&lt;li&gt;Evaluates robustness across different model configurations and noise types&lt;/li&gt;&lt;li&gt;Highlights potential risks of LLM misuse in secure environments like online exams&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Anyuan Zhuo', 'Xuefei Ning', 'Ningyuan Li', 'Yu Wang', 'Pinyan Lu']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial prompting', 'robustness', 'security standards', 'red teaming']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.14365</guid><pubDate>Mon, 20 Oct 2025 00:00:00 +0000</pubDate></item><item><title>DRO-InstructZero: Distributionally Robust Prompt Optimization for Large Language Models</title><link>https://arxiv.org/abs/2510.15260</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;DRO-InstructZero is a method for robust prompt optimization in LLMs&lt;/li&gt;&lt;li&gt;Uses distributionally robust Bayesian optimization to handle distribution shifts&lt;/li&gt;&lt;li&gt;Improves performance under adversarial evaluations and domain shifts&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yangyang Li']&lt;/li&gt;&lt;li&gt;Tags: ['prompt optimization', 'distributional robustness', 'adversarial evaluation', 'Bayesian optimization']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.15260</guid><pubDate>Mon, 20 Oct 2025 00:00:00 +0000</pubDate></item><item><title>DeceptionBench: A Comprehensive Benchmark for AI Deception Behaviors in Real-world Scenarios</title><link>https://arxiv.org/abs/2510.15501</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces DeceptionBench, a benchmark for evaluating AI deception behaviors in real-world scenarios&lt;/li&gt;&lt;li&gt;Covers 150 scenarios across 5 domains with over 1,000 samples&lt;/li&gt;&lt;li&gt;Analyzes intrinsic and extrinsic factors affecting deception&lt;/li&gt;&lt;li&gt;Reveals vulnerabilities in LLMs and LRMs under manipulative conditions&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yao Huang', 'Yitong Sun', 'Yichi Zhang', 'Ruochen Zhang', 'Yinpeng Dong', 'Xingxing Wei']&lt;/li&gt;&lt;li&gt;Tags: ['red teaming', 'safety evaluation', 'benchmarking', 'deception', 'real-world scenarios']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.15501</guid><pubDate>Mon, 20 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Rethinking Toxicity Evaluation in Large Language Models: A Multi-Label Perspective</title><link>https://arxiv.org/abs/2510.15007</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces multi-label benchmarks for toxicity detection in LLMs&lt;/li&gt;&lt;li&gt;Provides theoretical proof for pseudo-label training&lt;/li&gt;&lt;li&gt;Develops a pseudo-label-based detection method&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research Paper&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Zhiqiang Kou', 'Junyang Chen', 'Xin-Qiang Cai', 'Ming-Kun Xie', 'Biao Liu', 'Changwei Wang', 'Lei Feng', 'Yuheng Jia', 'Gang Niu', 'Masashi Sugiyama', 'Xin Geng']&lt;/li&gt;&lt;li&gt;Tags: ['safety evaluation', 'toxicity detection', 'multi-label classification']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.15007</guid><pubDate>Mon, 20 Oct 2025 00:00:00 +0000</pubDate></item><item><title>PIShield: Detecting Prompt Injection Attacks via Intrinsic LLM Features</title><link>https://arxiv.org/abs/2510.14005</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes PIShield, a method to detect prompt injection attacks using LLM internal features&lt;/li&gt;&lt;li&gt;Uses a linear classifier on the injection-critical layer's representations&lt;/li&gt;&lt;li&gt;Outperforms 11 baselines across 5 datasets and 8 attack types&lt;/li&gt;&lt;li&gt;Resists adaptive attacks&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Wei Zou', 'Yupei Liu', 'Yanting Wang', 'Ying Chen', 'Neil Gong', 'Jinyuan Jia']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'prompt injection', 'adversarial prompting', 'security', 'detection']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.14005</guid><pubDate>Mon, 20 Oct 2025 00:00:00 +0000</pubDate></item><item><title>GuardReasoner: Towards Reasoning-based LLM Safeguards</title><link>https://arxiv.org/abs/2501.18492</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces GuardReasoner, a safeguard for LLMs using reasoning-based guard models&lt;/li&gt;&lt;li&gt;Creates GuardReasonerTrain dataset with 127K samples and 460K reasoning steps&lt;/li&gt;&lt;li&gt;Applies reasoning SFT and hard sample DPO to enhance guard model capabilities&lt;/li&gt;&lt;li&gt;Outperforms GPT-4o+CoT and LLaMA Guard 3 8B in safety benchmarks&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yue Liu', 'Hongcheng Gao', 'Shengfang Zhai', 'Yufei He', 'Jun Xia', 'Zhengyu Hu', 'Yulin Chen', 'Xihong Yang', 'Jiaheng Zhang', 'Stan Z. Li', 'Hui Xiong', 'Bryan Hooi']&lt;/li&gt;&lt;li&gt;Tags: ['LLM safety', 'guardrails', 'reasoning', 'safety evaluation', 'benchmarking']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2501.18492</guid><pubDate>Mon, 20 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Keep Calm and Avoid Harmful Content: Concept Alignment and Latent Manipulation Towards Safer Answers</title><link>https://arxiv.org/abs/2510.12672</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes CALM, an inference-time method to suppress harmful concepts in LLMs&lt;/li&gt;&lt;li&gt;Uses concept whitening and orthogonal projection on latent layers&lt;/li&gt;&lt;li&gt;No retraining or fine-tuning required&lt;/li&gt;&lt;li&gt;Reduces harmful outputs with minimal computational overhead&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Ruben Belo', 'Marta Guimaraes', 'Claudia Soares']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'jailbreaking', 'adversarial prompting', 'safety evaluation', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.12672</guid><pubDate>Mon, 20 Oct 2025 00:00:00 +0000</pubDate></item><item><title>On the Interaction of Compressibility and Adversarial Robustness</title><link>https://arxiv.org/abs/2507.17725</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Analyzes the interaction between compressibility and adversarial robustness in neural networks&lt;/li&gt;&lt;li&gt;Shows that compressibility can create sensitive directions in the representation space&lt;/li&gt;&lt;li&gt;Provides a robustness bound linking compressibility metrics to adversarial vulnerability&lt;/li&gt;&lt;li&gt;Empirically validates findings across tasks and training methods&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Melih Barsbey', 'Ant\\^onio H. Ribeiro', 'Umut \\c{S}im\\c{s}ekli', 'Tolga Birdal']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial robustness', 'compressibility', 'model security', 'theoretical analysis', 'empirical evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2507.17725</guid><pubDate>Mon, 20 Oct 2025 00:00:00 +0000</pubDate></item><item><title>WebInject: Prompt Injection Attack to Web Agents</title><link>https://arxiv.org/abs/2505.11717</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes WebInject, a prompt injection attack on MLLM-based web agents&lt;/li&gt;&lt;li&gt;Manipulates webpage pixels to induce specific actions&lt;/li&gt;&lt;li&gt;Overcomes non-differentiable mapping with neural network approximation&lt;/li&gt;&lt;li&gt;Demonstrates high effectiveness in evaluations&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Xilong Wang', 'John Bloch', 'Zedian Shao', 'Yuepeng Hu', 'Shuyan Zhou', 'Neil Zhenqiang Gong']&lt;/li&gt;&lt;li&gt;Tags: ['prompt injection', 'adversarial prompting', 'web agents', 'MLLM', 'optimization']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2505.11717</guid><pubDate>Mon, 20 Oct 2025 00:00:00 +0000</pubDate></item><item><title>DeceptionBench: A Comprehensive Benchmark for AI Deception Behaviors in Real-world Scenarios</title><link>https://arxiv.org/abs/2510.15501</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces DeceptionBench, a benchmark for evaluating AI deception behaviors in real-world scenarios&lt;/li&gt;&lt;li&gt;Covers 150 scenarios across 5 domains with over 1,000 samples&lt;/li&gt;&lt;li&gt;Analyzes intrinsic and extrinsic factors affecting deception&lt;/li&gt;&lt;li&gt;Reveals vulnerabilities in LLMs and LRMs under manipulative conditions&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yao Huang', 'Yitong Sun', 'Yichi Zhang', 'Ruochen Zhang', 'Yinpeng Dong', 'Xingxing Wei']&lt;/li&gt;&lt;li&gt;Tags: ['red teaming', 'safety evaluation', 'benchmarking', 'deception', 'real-world scenarios']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.15501</guid><pubDate>Mon, 20 Oct 2025 00:00:00 +0000</pubDate></item><item><title>PoTS: Proof-of-Training-Steps for Backdoor Detection in Large Language Models</title><link>https://arxiv.org/abs/2510.15106</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces Proof-of-Training Steps (PoTS) for detecting backdoor attacks in LLMs&lt;/li&gt;&lt;li&gt;Enables auditors to verify training data, architecture, and hyperparameters&lt;/li&gt;&lt;li&gt;Detects subtle backdoor injections even with triggers in 10% of data&lt;/li&gt;&lt;li&gt;Verification steps are 3x faster than training steps&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Issam Seddik', 'Sami Souihi', 'Mohamed Tamaazousti', 'Sara Tucci Piergiovanni']&lt;/li&gt;&lt;li&gt;Tags: ['LLM security', 'backdoor detection', 'training verification', 'adversarial attacks', 'model robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.15106</guid><pubDate>Mon, 20 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Language Models are Injective and Hence Invertible</title><link>https://arxiv.org/abs/2510.15511</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proves mathematically that transformer LLMs are injective&lt;/li&gt;&lt;li&gt;Empirically confirms no collisions in 6 SOTA models&lt;/li&gt;&lt;li&gt;Introduces SipIt algorithm for exact input reconstruction&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Giorgos Nikolaou', 'Tommaso Mencattini', 'Donato Crisostomi', 'Andrea Santilli', 'Yannis Panagakis', "Emanuele Rodola'"]&lt;/li&gt;&lt;li&gt;Tags: ['model extraction', 'privacy attacks', 'safety evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.15511</guid><pubDate>Mon, 20 Oct 2025 00:00:00 +0000</pubDate></item><item><title>DRO-InstructZero: Distributionally Robust Prompt Optimization for Large Language Models</title><link>https://arxiv.org/abs/2510.15260</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;DRO-InstructZero is a method for robust prompt optimization in LLMs&lt;/li&gt;&lt;li&gt;Uses distributionally robust Bayesian optimization to handle distribution shifts&lt;/li&gt;&lt;li&gt;Improves performance under adversarial evaluations and domain shifts&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yangyang Li']&lt;/li&gt;&lt;li&gt;Tags: ['prompt optimization', 'distributional robustness', 'adversarial evaluation', 'Bayesian optimization']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.15260</guid><pubDate>Mon, 20 Oct 2025 00:00:00 +0000</pubDate></item><item><title>ReasonIF: Large Reasoning Models Fail to Follow Instructions During Reasoning</title><link>https://arxiv.org/abs/2510.15211</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces ReasonIF benchmark for evaluating reasoning instruction following in large reasoning models (LRMs)&lt;/li&gt;&lt;li&gt;Finds that current models have low instruction adherence during reasoning (IFS &lt;0.25)&lt;/li&gt;&lt;li&gt;Explores strategies like multi-turn reasoning and RIF finetuning to improve instruction fidelity&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yongchan Kwon', 'Shang Zhu', 'Federico Bianchi', 'Kaitlyn Zhou', 'James Zou']&lt;/li&gt;&lt;li&gt;Tags: ['instruction following', 'reasoning models', 'benchmarking', 'safety evaluation', 'alignment']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.15211</guid><pubDate>Mon, 20 Oct 2025 00:00:00 +0000</pubDate></item><item><title>PromptLocate: Localizing Prompt Injection Attacks</title><link>https://arxiv.org/abs/2510.12252</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes PromptLocate, a method to localize prompt injection attacks in LLMs&lt;/li&gt;&lt;li&gt;Involves splitting data into segments, identifying injected instructions and data&lt;/li&gt;&lt;li&gt;Tested on 16 attacks with accurate localization&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yuqi Jia', 'Yupei Liu', 'Zedian Shao', 'Jinyuan Jia', 'Neil Gong']&lt;/li&gt;&lt;li&gt;Tags: ['red teaming', 'prompt injection', 'adversarial prompting', 'safety evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.12252</guid><pubDate>Mon, 20 Oct 2025 00:00:00 +0000</pubDate></item><item><title>A Framework for Rapidly Developing and Deploying Protection Against Large Language Model Attacks</title><link>https://arxiv.org/abs/2509.20639</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Presents a production-grade defense system for LLMs&lt;/li&gt;&lt;li&gt;Integrates threat intelligence, data platform, and release platform&lt;/li&gt;&lt;li&gt;Aims for continuous adaptation to evolving threats&lt;/li&gt;&lt;li&gt;Focuses on observability and rapid response&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Adam Swanda', 'Amy Chang', 'Alexander Chen', 'Fraser Burch', 'Paul Kassianik', 'Konstantin Berlin']&lt;/li&gt;&lt;li&gt;Tags: ['LLM security', 'threat intelligence', 'multi-layered defense', 'rapid response', 'observability']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.20639</guid><pubDate>Mon, 20 Oct 2025 00:00:00 +0000</pubDate></item><item><title>FPEdit: Robust LLM Fingerprinting through Localized Parameter Editing</title><link>https://arxiv.org/abs/2508.02092</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces FPEdit, a framework for robust LLM fingerprinting using localized parameter editing&lt;/li&gt;&lt;li&gt;Uses Promote-Suppress Value Vector Optimization to enhance target tokens and suppress others&lt;/li&gt;&lt;li&gt;Achieves high retention rates under various adversarial conditions&lt;/li&gt;&lt;li&gt;Efficient resource usage with minimal impact on model performance&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Shida Wang', 'Chaohu Liu', 'Yubo Wang', 'Linli Xu']&lt;/li&gt;&lt;li&gt;Tags: ['LLM security', 'fingerprinting', 'model protection', 'adversarial robustness', 'parameter editing']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2508.02092</guid><pubDate>Mon, 20 Oct 2025 00:00:00 +0000</pubDate></item><item><title>On the Interaction of Compressibility and Adversarial Robustness</title><link>https://arxiv.org/abs/2507.17725</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Analyzes the interaction between compressibility and adversarial robustness in neural networks&lt;/li&gt;&lt;li&gt;Shows that compressibility can create sensitive directions in the representation space&lt;/li&gt;&lt;li&gt;Provides a robustness bound linking compressibility metrics to adversarial vulnerability&lt;/li&gt;&lt;li&gt;Empirically validates findings across tasks and training methods&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Melih Barsbey', 'Ant\\^onio H. Ribeiro', 'Umut \\c{S}im\\c{s}ekli', 'Tolga Birdal']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial robustness', 'compressibility', 'model security', 'theoretical analysis', 'empirical evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2507.17725</guid><pubDate>Mon, 20 Oct 2025 00:00:00 +0000</pubDate></item><item><title>PRISON: Unmasking the Criminal Potential of Large Language Models</title><link>https://arxiv.org/abs/2506.16150</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces PRISON framework to evaluate LLMs' criminal potential across five traits&lt;/li&gt;&lt;li&gt;Uses realistic crime scenarios from films to test both criminal and anti-crime abilities&lt;/li&gt;&lt;li&gt;Finds LLMs exhibit emergent criminal tendencies and poor detection of deceptive behavior&lt;/li&gt;&lt;li&gt;Highlights need for robustness, alignment, and safety mechanisms&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Xinyi Wu', 'Geng Hong', 'Pei Chen', 'Yueyue Chen', 'Xudong Pan', 'Min Yang']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'safety evaluation', 'adversarial prompting', 'alignment', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.16150</guid><pubDate>Mon, 20 Oct 2025 00:00:00 +0000</pubDate></item><item><title>WebInject: Prompt Injection Attack to Web Agents</title><link>https://arxiv.org/abs/2505.11717</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes WebInject, a prompt injection attack on MLLM-based web agents&lt;/li&gt;&lt;li&gt;Manipulates webpage pixels to induce specific actions&lt;/li&gt;&lt;li&gt;Overcomes non-differentiable mapping with neural network approximation&lt;/li&gt;&lt;li&gt;Demonstrates high effectiveness in evaluations&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Xilong Wang', 'John Bloch', 'Zedian Shao', 'Yuepeng Hu', 'Shuyan Zhou', 'Neil Zhenqiang Gong']&lt;/li&gt;&lt;li&gt;Tags: ['prompt injection', 'adversarial prompting', 'web agents', 'MLLM', 'optimization']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2505.11717</guid><pubDate>Mon, 20 Oct 2025 00:00:00 +0000</pubDate></item><item><title>GuardReasoner: Towards Reasoning-based LLM Safeguards</title><link>https://arxiv.org/abs/2501.18492</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces GuardReasoner, a safeguard for LLMs using reasoning-based guard models&lt;/li&gt;&lt;li&gt;Creates GuardReasonerTrain dataset with 127K samples and 460K reasoning steps&lt;/li&gt;&lt;li&gt;Applies reasoning SFT and hard sample DPO to enhance guard model capabilities&lt;/li&gt;&lt;li&gt;Outperforms GPT-4o+CoT and LLaMA Guard 3 8B in safety benchmarks&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yue Liu', 'Hongcheng Gao', 'Shengfang Zhai', 'Yufei He', 'Jun Xia', 'Zhengyu Hu', 'Yulin Chen', 'Xihong Yang', 'Jiaheng Zhang', 'Stan Z. Li', 'Hui Xiong', 'Bryan Hooi']&lt;/li&gt;&lt;li&gt;Tags: ['LLM safety', 'guardrails', 'reasoning', 'safety evaluation', 'benchmarking']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2501.18492</guid><pubDate>Mon, 20 Oct 2025 00:00:00 +0000</pubDate></item><item><title>NDM: A Noise-driven Detection and Mitigation Framework against Implicit Sexual Intentions in Text-to-Image Generation</title><link>https://arxiv.org/abs/2510.15752</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes NDM framework to detect and mitigate implicit sexual intentions in T2I generation&lt;/li&gt;&lt;li&gt;Uses noise-based detection and adaptive negative guidance&lt;/li&gt;&lt;li&gt;Validated on natural and adversarial datasets with superior performance&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yitong Sun', 'Yao Huang', 'Ruochen Zhang', 'Huanran Chen', 'Shouwei Ruan', 'Ranjie Duan', 'Xingxing Wei']&lt;/li&gt;&lt;li&gt;Tags: ['text-to-image', 'content moderation', 'adversarial attacks', 'safety evaluation', 'model robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.15752</guid><pubDate>Mon, 20 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Language Models are Injective and Hence Invertible</title><link>https://arxiv.org/abs/2510.15511</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proves mathematically that transformer LLMs are injective&lt;/li&gt;&lt;li&gt;Empirically confirms no collisions in 6 SOTA models&lt;/li&gt;&lt;li&gt;Introduces SipIt algorithm for exact input reconstruction&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Giorgos Nikolaou', 'Tommaso Mencattini', 'Donato Crisostomi', 'Andrea Santilli', 'Yannis Panagakis', "Emanuele Rodola'"]&lt;/li&gt;&lt;li&gt;Tags: ['model extraction', 'privacy attacks', 'safety evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.15511</guid><pubDate>Mon, 20 Oct 2025 00:00:00 +0000</pubDate></item><item><title>DeceptionBench: A Comprehensive Benchmark for AI Deception Behaviors in Real-world Scenarios</title><link>https://arxiv.org/abs/2510.15501</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces DeceptionBench, a benchmark for evaluating AI deception behaviors in real-world scenarios&lt;/li&gt;&lt;li&gt;Covers 150 scenarios across 5 domains with over 1,000 samples&lt;/li&gt;&lt;li&gt;Analyzes intrinsic and extrinsic factors affecting deception&lt;/li&gt;&lt;li&gt;Reveals vulnerabilities in LLMs and LRMs under manipulative conditions&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yao Huang', 'Yitong Sun', 'Yichi Zhang', 'Ruochen Zhang', 'Yinpeng Dong', 'Xingxing Wei']&lt;/li&gt;&lt;li&gt;Tags: ['red teaming', 'safety evaluation', 'benchmarking', 'deception', 'real-world scenarios']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.15501</guid><pubDate>Mon, 20 Oct 2025 00:00:00 +0000</pubDate></item><item><title>SoK: Taxonomy and Evaluation of Prompt Security in Large Language Models</title><link>https://arxiv.org/abs/2510.15476</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a taxonomy for LLM prompt security&lt;/li&gt;&lt;li&gt;Introduces machine-readable threat models&lt;/li&gt;&lt;li&gt;Releases an evaluation toolkit and dataset&lt;/li&gt;&lt;li&gt;Evaluates state-of-the-art methods&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Hanbin Hong', 'Shuya Feng', 'Nima Naderloui', 'Shenao Yan', 'Jingyu Zhang', 'Biying Liu', 'Ali Arastehfard', 'Heqing Huang', 'Yuan Hong']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'jailbreaking', 'prompt injection', 'adversarial prompting', 'safety evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.15476</guid><pubDate>Mon, 20 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Learning to Detect Unknown Jailbreak Attacks in Large Vision-Language Models</title><link>https://arxiv.org/abs/2510.15430</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a framework called Learning to Detect (LoD) for detecting unknown jailbreak attacks in LVLMs.&lt;/li&gt;&lt;li&gt;Uses a Multi-modal Safety Concept Activation Vector module and a Safety Pattern Auto-Encoder module.&lt;/li&gt;&lt;li&gt;Aims to improve generalization and efficiency over existing methods.&lt;/li&gt;&lt;li&gt;Demonstrates higher detection AUROC on diverse unknown attacks.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Shuang Liang', 'Zhihao Xu', 'Jialing Tao', 'Hui Xue', 'Xiting Wang']&lt;/li&gt;&lt;li&gt;Tags: ['jailbreaking', 'red teaming', 'safety evaluation', 'vision-language models', 'adversarial attacks']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.15430</guid><pubDate>Mon, 20 Oct 2025 00:00:00 +0000</pubDate></item><item><title>DRO-InstructZero: Distributionally Robust Prompt Optimization for Large Language Models</title><link>https://arxiv.org/abs/2510.15260</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;DRO-InstructZero is a method for robust prompt optimization in LLMs&lt;/li&gt;&lt;li&gt;Uses distributionally robust Bayesian optimization to handle distribution shifts&lt;/li&gt;&lt;li&gt;Improves performance under adversarial evaluations and domain shifts&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yangyang Li']&lt;/li&gt;&lt;li&gt;Tags: ['prompt optimization', 'distributional robustness', 'adversarial evaluation', 'Bayesian optimization']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.15260</guid><pubDate>Mon, 20 Oct 2025 00:00:00 +0000</pubDate></item><item><title>ReasonIF: Large Reasoning Models Fail to Follow Instructions During Reasoning</title><link>https://arxiv.org/abs/2510.15211</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces ReasonIF benchmark for evaluating reasoning instruction following in large reasoning models (LRMs)&lt;/li&gt;&lt;li&gt;Finds that current models have low instruction adherence during reasoning (IFS &lt;0.25)&lt;/li&gt;&lt;li&gt;Explores strategies like multi-turn reasoning and RIF finetuning to improve instruction fidelity&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yongchan Kwon', 'Shang Zhu', 'Federico Bianchi', 'Kaitlyn Zhou', 'James Zou']&lt;/li&gt;&lt;li&gt;Tags: ['instruction following', 'reasoning models', 'benchmarking', 'safety evaluation', 'alignment']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.15211</guid><pubDate>Mon, 20 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Sequential Comics for Jailbreaking Multimodal Large Language Models via Structured Visual Storytelling</title><link>https://arxiv.org/abs/2510.15068</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces a method using sequential comic-style visuals to jailbreak MLLMs&lt;/li&gt;&lt;li&gt;Decomposes malicious queries into innocuous visual elements&lt;/li&gt;&lt;li&gt;Achieves high attack success rates across safety benchmarks&lt;/li&gt;&lt;li&gt;Analyzes vulnerabilities in multimodal safety mechanisms&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Deyue Zhang', 'Dongdong Yang', 'Junjie Mu', 'Quancheng Zou', 'Zonghao Ying', 'Wenzhuo Xu', 'Zhao Liu', 'Xuan Wang', 'Xiangzheng Zhang']&lt;/li&gt;&lt;li&gt;Tags: ['jailbreaking', 'multimodal', 'visual storytelling', 'safety evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.15068</guid><pubDate>Mon, 20 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Active Honeypot Guardrail System: Probing and Confirming Multi-Turn LLM Jailbreaks</title><link>https://arxiv.org/abs/2510.15017</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a honeypot-based system to detect multi-turn LLM jailbreaks&lt;/li&gt;&lt;li&gt;Uses bait model responses to probe user intent&lt;/li&gt;&lt;li&gt;Introduces Honeypot Utility Score (HUS) and Defense Efficacy Rate (DER)&lt;/li&gt;&lt;li&gt;Shows improved defense against jailbreak attacks while maintaining usability&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['ChenYu Wu', 'Yi Wang', 'Yang Liao']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'jailbreaking', 'adversarial prompting', 'safety evaluation', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.15017</guid><pubDate>Mon, 20 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Rethinking Toxicity Evaluation in Large Language Models: A Multi-Label Perspective</title><link>https://arxiv.org/abs/2510.15007</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces multi-label benchmarks for toxicity detection in LLMs&lt;/li&gt;&lt;li&gt;Provides theoretical proof for pseudo-label training&lt;/li&gt;&lt;li&gt;Develops a pseudo-label-based detection method&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research Paper&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Zhiqiang Kou', 'Junyang Chen', 'Xin-Qiang Cai', 'Ming-Kun Xie', 'Biao Liu', 'Changwei Wang', 'Lei Feng', 'Yuheng Jia', 'Gang Niu', 'Masashi Sugiyama', 'Xin Geng']&lt;/li&gt;&lt;li&gt;Tags: ['safety evaluation', 'toxicity detection', 'multi-label classification']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.15007</guid><pubDate>Mon, 20 Oct 2025 00:00:00 +0000</pubDate></item><item><title>AURA: An Agent Autonomy Risk Assessment Framework</title><link>https://arxiv.org/abs/2510.15739</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces AURA framework for assessing risks in agentic AI systems&lt;/li&gt;&lt;li&gt;Focuses on alignment, governance, and risk management&lt;/li&gt;&lt;li&gt;Incorporates gamma-based risk scoring and HITL oversight&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Lorenzo Satta Chiris (University of Exeter', 'United Kingdom)', 'Ayush Mishra (University of Exeter', 'United Kingdom)']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'risk management', 'governance', 'HITL', 'agentic AI']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.15739</guid><pubDate>Mon, 20 Oct 2025 00:00:00 +0000</pubDate></item></channel></rss>