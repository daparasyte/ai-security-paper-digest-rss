<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>AI Security Paper Digest</title><link>https://kentaroh-toyoda.github.io/ai-security-paper-digest-rss/rss.xml</link><description>Curated papers on AI security from ArXiv and ACL</description><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><lastBuildDate>Wed, 01 Oct 2025 22:41:45 +0000</lastBuildDate><item><title>PUREVQ-GAN: Defending Data Poisoning Attacks through Vector-Quantized Bottlenecks</title><link>https://arxiv.org/abs/2509.25792</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces PureVQ-GAN to defend against data poisoning attacks&lt;/li&gt;&lt;li&gt;Uses Vector-Quantized VAE with GAN discriminator to destroy triggers&lt;/li&gt;&lt;li&gt;Highly effective on CIFAR-10 with low clean accuracy loss&lt;/li&gt;&lt;li&gt;Faster than diffusion-based methods&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Alexander Branch', 'Omead Pooladzandi', 'Radin Khosraviani', 'Sunay Gajanan Bhat', 'Jeffrey Jiang', 'Gregory Pottie']&lt;/li&gt;&lt;li&gt;Tags: ['data poisoning', 'defense mechanism', 'VQ-VAE', 'GAN', 'CIFAR-10']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.25792</guid><pubDate>Wed, 01 Oct 2025 00:00:00 +0000</pubDate></item><item><title>RADAR: A Risk-Aware Dynamic Multi-Agent Framework for LLM Safety Evaluation via Role-Specialized Collaboration</title><link>https://arxiv.org/abs/2509.25271</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces RADAR, a multi-agent framework for LLM safety evaluation&lt;/li&gt;&lt;li&gt;Decomposes risk into explicit, implicit, and non-risk subspaces&lt;/li&gt;&lt;li&gt;Uses role-specialized collaboration and dynamic updates&lt;/li&gt;&lt;li&gt;Validated with 800 challenging cases and public benchmarks&lt;/li&gt;&lt;li&gt;Shows significant improvement in risk identification accuracy&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Xiuyuan Chen', 'Jian Zhao', 'Yuchen Yuan', 'Tianle Zhang', 'Huilin Zhou', 'Zheng Zhu', 'Ping Hu', 'Linghe Kong', 'Chi Zhang', 'Weiran Huang', 'Xuelong Li']&lt;/li&gt;&lt;li&gt;Tags: ['LLM safety evaluation', 'multi-agent systems', 'risk concept space', 'dynamic updates', 'role-specialized collaboration']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.25271</guid><pubDate>Wed, 01 Oct 2025 00:00:00 +0000</pubDate></item><item><title>LLaVAShield: Safeguarding Multimodal Multi-Turn Dialogues in Vision-Language Models</title><link>https://arxiv.org/abs/2509.25896</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces LLaVAShield for safeguarding multimodal multi-turn dialogues in VLMs&lt;/li&gt;&lt;li&gt;Presents MMDS dataset with safety ratings and policy labels&lt;/li&gt;&lt;li&gt;Develops MCTS-based red-teaming framework to generate unsafe dialogues&lt;/li&gt;&lt;li&gt;Outperforms baselines in MMT content moderation tasks&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Guolei Huang', 'Qingzhi Peng', 'Gan Xu', 'Yuxuan Lu', 'Yongjun Shen']&lt;/li&gt;&lt;li&gt;Tags: ['red teaming', 'multimodal', 'multi-turn dialogues', 'safety evaluation', 'content moderation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.25896</guid><pubDate>Wed, 01 Oct 2025 00:00:00 +0000</pubDate></item><item><title>VISOR++: Universal Visual Inputs based Steering for Large Vision Language Models</title><link>https://arxiv.org/abs/2509.25533</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces VISOR++ for steering VLMs using optimized visual inputs&lt;/li&gt;&lt;li&gt;Demonstrates effectiveness on alignment tasks like refusal and sycophancy&lt;/li&gt;&lt;li&gt;Works with both open and closed-source models&lt;/li&gt;&lt;li&gt;Preserves performance on unrelated tasks&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Ravikumar Balakrishnan', 'Mansi Phute']&lt;/li&gt;&lt;li&gt;Tags: ['red teaming', 'alignment', 'safety evaluation', 'model steering', 'visual inputs']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.25533</guid><pubDate>Wed, 01 Oct 2025 00:00:00 +0000</pubDate></item><item><title>MaskSQL: Safeguarding Privacy for LLM-Based Text-to-SQL via Abstraction</title><link>https://arxiv.org/abs/2509.23459</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces MaskSQL, a text-to-SQL framework using abstraction to mask sensitive info in LLM prompts&lt;/li&gt;&lt;li&gt;Aims to balance privacy and utility for text-to-SQL tasks&lt;/li&gt;&lt;li&gt;Outperforms SLM-based models and approaches LLM performance&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Sepideh Abedini', 'Shubhankar Mohapatra', 'D. B. Emerson', 'Masoumeh Shafieinejad', 'Jesse C. Cresswell', 'Xi He']&lt;/li&gt;&lt;li&gt;Tags: ['privacy', 'text-to-sql', 'abstraction', 'data masking']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.23459</guid><pubDate>Wed, 01 Oct 2025 00:00:00 +0000</pubDate></item><item><title>AudioTrust: Benchmarking the Multifaceted Trustworthiness of Audio Large Language Models</title><link>https://arxiv.org/abs/2505.16211</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces AudioTrust framework for evaluating trustworthiness of Audio Large Language Models (ALLMs)&lt;/li&gt;&lt;li&gt;Addresses audio-specific risks like timbre, accent, background noise&lt;/li&gt;&lt;li&gt;Evaluates across six dimensions: fairness, hallucination, safety, privacy, robustness, authentication&lt;/li&gt;&lt;li&gt;Tests 14 state-of-the-art ALLMs with 26 sub-tasks using 4,420 audio samples&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Kai Li', 'Can Shen', 'Yile Liu', 'Jirui Han', 'Kelong Zheng', 'Xuechao Zou', 'Zhe Wang', 'Shun Zhang', 'Xingjian Du', 'Hanjun Luo', 'Yingbin Jin', 'Xinxin Xing', 'Ziyang Ma', 'Yue Liu', 'Yifan Zhang', 'Junfeng Fang', 'Kun Wang', 'Yibo Yan', 'Gelei Deng', 'Haoyang Li', 'Yiming Li', 'Xiaobin Zhuang', 'Tianlong Chen', 'Qingsong Wen', 'Tianwei Zhang', 'Yang Liu', 'Haibo Hu', 'Zhizheng Wu', 'Xiaolin Hu', 'Eng-Siong Chng', 'Wenyuan Xu', 'XiaoFeng Wang', 'Wei Dong', 'Xinfeng Li']&lt;/li&gt;&lt;li&gt;Tags: ['red teaming', 'safety evaluation', 'robustness', 'privacy attacks', 'data poisoning']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2505.16211</guid><pubDate>Wed, 01 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Watermark under Fire: A Robustness Evaluation of LLM Watermarking</title><link>https://arxiv.org/abs/2411.13425</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Systematizes existing LLM watermarkers and watermark removal attacks&lt;/li&gt;&lt;li&gt;Develops WaterPark, a unified evaluation platform&lt;/li&gt;&lt;li&gt;Conducts comprehensive assessment of watermarker robustness&lt;/li&gt;&lt;li&gt;Explores best practices for adversarial environments&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jiacheng Liang', 'Zian Wang', 'Lauren Hong', 'Shouling Ji', 'Ting Wang']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'adversarial prompting', 'robustness', 'safety evaluation', 'privacy attacks']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2411.13425</guid><pubDate>Wed, 01 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Wolf Hidden in Sheep's Conversations: Toward Harmless Data-Based Backdoor Attacks for Jailbreaking Large Language Models</title><link>https://arxiv.org/abs/2505.17601</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a clean-data backdoor attack for jailbreaking LLMs&lt;/li&gt;&lt;li&gt;Uses harmless QA pairs to overfit triggers to benign responses&lt;/li&gt;&lt;li&gt;Harmful responses generated in two stages&lt;/li&gt;&lt;li&gt;Employs gradient-based optimization for triggers&lt;/li&gt;&lt;li&gt;High ASR even with guardrails&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jiawei Kong', 'Hao Fang', 'Xiaochen Yang', 'Kuofeng Gao', 'Bin Chen', 'Shu-Tao Xia', 'Yaowei Wang', 'Min Zhang']&lt;/li&gt;&lt;li&gt;Tags: ['jailbreaking', 'backdoor attack', 'data poisoning', 'adversarial prompting', 'safety evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2505.17601</guid><pubDate>Wed, 01 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Safety is Not Only About Refusal: Reasoning-Enhanced Fine-tuning for Interpretable LLM Safety</title><link>https://arxiv.org/abs/2503.05021</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a reasoning-enhanced fine-tuning framework for LLM safety&lt;/li&gt;&lt;li&gt;Aims to address jailbreak attacks and improve context-aware decision-making&lt;/li&gt;&lt;li&gt;Trains models to generate explicit safe reasoning before responding&lt;/li&gt;&lt;li&gt;Emphasizes safety beyond refusal through adaptive and interpretable responses&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yuyou Zhang', 'Miao Li', 'William Han', 'Yihang Yao', 'Zhepeng Cen', 'Ding Zhao']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'jailbreaking', 'safety evaluation', 'alignment', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2503.05021</guid><pubDate>Wed, 01 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Answer, Refuse, or Guess? Investigating Risk-Aware Decision Making in Language Models</title><link>https://arxiv.org/abs/2503.01332</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;The paper evaluates how language models make risk-aware decisions, specifically whether to answer, refuse, or guess in different risk scenarios.&lt;/li&gt;&lt;li&gt;It identifies that LMs over-answer in high-risk and over-defer in low-risk settings.&lt;/li&gt;&lt;li&gt;A skill-decomposition method is proposed to improve decision policies.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research Paper&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Cheng-Kuang Wu', 'Zhi Rui Tam', 'Chieh-Yen Lin', 'Yun-Nung Chen', 'Hung-yi Lee']&lt;/li&gt;&lt;li&gt;Tags: ['safety evaluation', 'risk-aware decision making', 'model limitations']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2503.01332</guid><pubDate>Wed, 01 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Dagger Behind Smile: Fool LLMs with a Happy Ending Story</title><link>https://arxiv.org/abs/2501.13115</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces Happy Ending Attack (HEA) for jailbreaking LLMs using positive prompts&lt;/li&gt;&lt;li&gt;Achieves high success rates on models like GPT-4o, Llama3-70b, Gemini-pro&lt;/li&gt;&lt;li&gt;Requires only up to two turns to jailbreak&lt;/li&gt;&lt;li&gt;Provides quantitative explanations for the attack's effectiveness&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Xurui Song', 'Zhixin Xie', 'Shuo Huai', 'Jiayi Kong', 'Jun Luo']&lt;/li&gt;&lt;li&gt;Tags: ['jailbreaking', 'adversarial prompting', 'LLM security', 'red teaming']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2501.13115</guid><pubDate>Wed, 01 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Cut the Deadwood Out: Backdoor Purification via Guided Module Substitution</title><link>https://arxiv.org/abs/2412.20476</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes Guided Module Substitution (GMS) for backdoor purification in NLP models&lt;/li&gt;&lt;li&gt;Aims to address data poisoning attacks without retraining&lt;/li&gt;&lt;li&gt;Uses merging of victim and proxy models with a guided trade-off signal&lt;/li&gt;&lt;li&gt;Demonstrates robustness and transferability across attacks&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yao Tong', 'Weijun Li', 'Xuanli He', 'Haolan Zhan', 'Qiongkai Xu']&lt;/li&gt;&lt;li&gt;Tags: ['data poisoning', 'backdoor defense', 'model merging', 'NLP security', 'LLM robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2412.20476</guid><pubDate>Wed, 01 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Preemptive Detection and Correction of Misaligned Actions in LLM Agents</title><link>https://arxiv.org/abs/2407.11843</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces InferAct, a method using LLMs' Theory-of-Mind to detect misaligned actions before execution&lt;/li&gt;&lt;li&gt;Aims to prevent unintended critical actions in LLM agents&lt;/li&gt;&lt;li&gt;Shows 20% improvement in misalignment detection over baselines&lt;/li&gt;&lt;li&gt;Evaluates both detection and correction of misaligned actions&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Haishuo Fang', 'Xiaodan Zhu', 'Iryna Gurevych']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'safety', 'LLM', 'action detection', 'Theory-of-Mind']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2407.11843</guid><pubDate>Wed, 01 Oct 2025 00:00:00 +0000</pubDate></item><item><title>SeedPrints: Fingerprints Can Even Tell Which Seed Your Large Language Model Was Trained From</title><link>https://arxiv.org/abs/2509.26404</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces SeedPrints, a method for fingerprinting LLMs using random initialization biases&lt;/li&gt;&lt;li&gt;Enables seed-level distinguishability and lifecycle identity verification&lt;/li&gt;&lt;li&gt;Robust under domain shifts and parameter modifications&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yao Tong', 'Haonan Wang', 'Siquan Li', 'Kenji Kawaguchi', 'Tianyang Hu']&lt;/li&gt;&lt;li&gt;Tags: ['model extraction', 'privacy attacks', 'safety evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.26404</guid><pubDate>Wed, 01 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Your Agent May Misevolve: Emergent Risks in Self-evolving LLM Agents</title><link>https://arxiv.org/abs/2509.26354</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces the concept of 'Misevolution' in self-evolving LLM agents&lt;/li&gt;&lt;li&gt;Evaluates misevolution across model, memory, tool, and workflow pathways&lt;/li&gt;&lt;li&gt;Empirical findings show widespread risks in top-tier LLMs like Gemini-2.5-Pro&lt;/li&gt;&lt;li&gt;Highlights need for new safety paradigms and mitigation strategies&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Shuai Shao', 'Qihan Ren', 'Chen Qian', 'Boyi Wei', 'Dadi Guo', 'Jingyi Yang', 'Xinhao Song', 'Linfeng Zhang', 'Weinan Zhang', 'Dongrui Liu', 'Jing Shao']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'safety evaluation', 'alignment', 'robustness', 'emergent risks']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.26354</guid><pubDate>Wed, 01 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Rotation Control Unlearning: Quantifying and Controlling Continuous Unlearning for LLM with The Cognitive Rotation Space</title><link>https://arxiv.org/abs/2509.25743</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes Rotation Control Unlearning (RCU) for continuous unlearning in LLMs&lt;/li&gt;&lt;li&gt;Aims to mitigate security risks by removing influence of undesirable data&lt;/li&gt;&lt;li&gt;Introduces cognitive rotation space and orthogonal rotation axes regularization&lt;/li&gt;&lt;li&gt;Achieves SOTA performance without retained dataset&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Xiang Zhang', 'Kun Wei', 'Xu Yang', 'Chenghao Xu', 'Su Yan', 'Cheng Deng']&lt;/li&gt;&lt;li&gt;Tags: ['LLM', 'unlearning', 'security', 'privacy', 'model extraction']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.25743</guid><pubDate>Wed, 01 Oct 2025 00:00:00 +0000</pubDate></item><item><title>STAC: When Innocent Tools Form Dangerous Chains to Jailbreak LLM Agents</title><link>https://arxiv.org/abs/2509.25624</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces STAC, a multi-turn attack framework exploiting LLM agent tool use&lt;/li&gt;&lt;li&gt;Evaluates vulnerability of state-of-the-art agents like GPT-4.1 with high ASR&lt;/li&gt;&lt;li&gt;Proposes a reasoning-driven defense prompt to mitigate STAC attacks&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jing-Jing Li', 'Jianfeng He', 'Chao Shang', 'Devang Kulshreshtha', 'Xun Xian', 'Yi Zhang', 'Hang Su', 'Sandesh Swamy', 'Yanjun Qi']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'jailbreaking', 'adversarial prompting', 'security', 'safety evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.25624</guid><pubDate>Wed, 01 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Fingerprinting LLMs via Prompt Injection</title><link>https://arxiv.org/abs/2509.25448</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes LLMPrint, a detection framework using prompt injection to fingerprint LLMs&lt;/li&gt;&lt;li&gt;Aims to identify model provenance even after post-processing like quantization&lt;/li&gt;&lt;li&gt;Evaluates on multiple base models and their variants with high accuracy&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yuepeng Hu', 'Zhengyuan Jiang', 'Mengyuan Li', 'Osama Ahmed', 'Zhicong Huang', 'Cheng Hong', 'Neil Gong']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'prompt injection', 'model extraction', 'security', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.25448</guid><pubDate>Wed, 01 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Dive into the Agent Matrix: A Realistic Evaluation of Self-Replication Risk in LLM Agents</title><link>https://arxiv.org/abs/2509.25302</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Evaluates self-replication risk in LLM agents under realistic scenarios&lt;/li&gt;&lt;li&gt;Introduces Overuse Rate (OR) and Aggregate Overuse Count (AOC) metrics&lt;/li&gt;&lt;li&gt;Tests 21 models, finds over 50% have high risk scores under pressure&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Boxuan Zhang', 'Yi Yu', 'Jiaxuan Guo', 'Jing Shao']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'alignment', 'safety evaluation', 'robustness', 'self-replication risk']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.25302</guid><pubDate>Wed, 01 Oct 2025 00:00:00 +0000</pubDate></item><item><title>The Silent Judge: Unacknowledged Shortcut Bias in LLM-as-a-Judge</title><link>https://arxiv.org/abs/2509.26072</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;LLM judges exhibit shortcut biases based on injected cues like provenance and recency&lt;/li&gt;&lt;li&gt;Models favor newer responses and certain author identities without acknowledging these biases&lt;/li&gt;&lt;li&gt;Results show GPT-4o and Gemini-2.5-Flash have significant biases in evaluation tasks&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Arash Marioriyad', 'Mohammad Hossein Rohban', 'Mahdieh Soleymani Baghshah']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'safety evaluation', 'adversarial prompting', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.26072</guid><pubDate>Wed, 01 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Reliability Crisis of Reference-free Metrics for Grammatical Error Correction</title><link>https://arxiv.org/abs/2509.25961</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;The paper proposes adversarial attack strategies for reference-free metrics in GEC&lt;/li&gt;&lt;li&gt;Demonstrates that adversarial systems can outperform state-of-the-art&lt;/li&gt;&lt;li&gt;Highlights the need for more robust evaluation methods&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Takumi Goto', 'Yusuke Sakai', 'Taro Watanabe']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial prompting', 'red teaming', 'robustness', 'evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.25961</guid><pubDate>Wed, 01 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Cyclic Ablation: Testing Concept Localization against Functional Regeneration in AI</title><link>https://arxiv.org/abs/2509.25220</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces cyclic ablation method to test concept localization in LLMs&lt;/li&gt;&lt;li&gt;Aims to remove deception but finds functional regeneration&lt;/li&gt;&lt;li&gt;Highlights limitations of model editing for safety&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Eduard Kapelko']&lt;/li&gt;&lt;li&gt;Tags: ['safety', 'red teaming', 'alignment', 'robustness', 'model editing']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.25220</guid><pubDate>Wed, 01 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Scalable Fingerprinting of Large Language Models</title><link>https://arxiv.org/abs/2502.07760</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces Perinucleus sampling for scalable fingerprinting of LLMs&lt;/li&gt;&lt;li&gt;Can add 24,576 fingerprints to Llama-3.1-8B without utility loss&lt;/li&gt;&lt;li&gt;Addresses security risks and shows persistence after fine-tuning&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Anshul Nasery', 'Jonathan Hayase', 'Creston Brooks', 'Peiyao Sheng', 'Himanshu Tyagi', 'Pramod Viswanath', 'Sewoong Oh']&lt;/li&gt;&lt;li&gt;Tags: ['model extraction', 'privacy attacks', 'security']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2502.07760</guid><pubDate>Wed, 01 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Watermark under Fire: A Robustness Evaluation of LLM Watermarking</title><link>https://arxiv.org/abs/2411.13425</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Systematizes existing LLM watermarkers and watermark removal attacks&lt;/li&gt;&lt;li&gt;Develops WaterPark, a unified evaluation platform&lt;/li&gt;&lt;li&gt;Conducts comprehensive assessment of watermarker robustness&lt;/li&gt;&lt;li&gt;Explores best practices for adversarial environments&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jiacheng Liang', 'Zian Wang', 'Lauren Hong', 'Shouling Ji', 'Ting Wang']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'adversarial prompting', 'robustness', 'safety evaluation', 'privacy attacks']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2411.13425</guid><pubDate>Wed, 01 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Beyond Sharp Minima: Robust LLM Unlearning via Feedback-Guided Multi-Point Optimization</title><link>https://arxiv.org/abs/2509.20230</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes StableUN, a framework for robust LLM unlearning&lt;/li&gt;&lt;li&gt;Addresses vulnerability of sharp minima leading to relearning attacks&lt;/li&gt;&lt;li&gt;Uses feedback-guided multi-point optimization to find stable parameters&lt;/li&gt;&lt;li&gt;Demonstrates improved robustness against relearning and jailbreaking attacks&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Wenhan Wu', 'Zheyuan Liu', 'Chongyang Gao', 'Ren Wang', 'Kaize Ding']&lt;/li&gt;&lt;li&gt;Tags: ['LLM unlearning', 'Robustness', 'Security', 'Relearning attacks', 'Jailbreaking attacks']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.20230</guid><pubDate>Wed, 01 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Inducing Uncertainty on Open-Weight Models for Test-Time Privacy in Image Recognition</title><link>https://arxiv.org/abs/2509.11625</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a method to induce uncertainty on protected instances in open-weight models to enhance test-time privacy&lt;/li&gt;&lt;li&gt;Uses a Pareto optimal objective to balance privacy and utility&lt;/li&gt;&lt;li&gt;Provides a certifiable approximation algorithm with privacy guarantees&lt;/li&gt;&lt;li&gt;Empirically shows significant uncertainty increase with minimal accuracy loss&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Muhammad H. Ashiq', 'Peter Triantafillou', 'Hung Yun Tseng', 'Grigoris G. Chrysos']&lt;/li&gt;&lt;li&gt;Tags: ['privacy attacks', 'robustness', 'safety evaluation', 'security standards']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.11625</guid><pubDate>Wed, 01 Oct 2025 00:00:00 +0000</pubDate></item><item><title>AutoRAN: Automated Hijacking of Safety Reasoning in Large Reasoning Models</title><link>https://arxiv.org/abs/2505.10846</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces AutoRAN, a framework for automating the hijacking of safety reasoning in large reasoning models (LRMs).&lt;/li&gt;&lt;li&gt;Uses a weaker model to simulate execution reasoning and iteratively refine attacks.&lt;/li&gt;&lt;li&gt;Evaluates against models like GPT-o3/o4-mini and Gemini-2.5-Flash with high success rates.&lt;/li&gt;&lt;li&gt;Highlights the vulnerability of reasoning transparency and the need for new defenses.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jiacheng Liang', 'Tanqiu Jiang', 'Yuhui Wang', 'Rongyi Zhu', 'Fenglong Ma', 'Ting Wang']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'jailbreaking', 'adversarial prompting', 'safety evaluation', 'alignment']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2505.10846</guid><pubDate>Wed, 01 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Detecting Instruction Fine-tuning Attacks on Language Models using Influence Function</title><link>https://arxiv.org/abs/2504.09026</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Detects instruction fine-tuning attacks on LLMs using influence functions&lt;/li&gt;&lt;li&gt;Compares influence distributions before/after sentiment inversion to find poison examples&lt;/li&gt;&lt;li&gt;Works on sentiment and math tasks, restores performance by removing 1% of data&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jiawei Li']&lt;/li&gt;&lt;li&gt;Tags: ['data poisoning', 'influence functions', 'sentiment inversion', 'poison detection', 'LLM security']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2504.09026</guid><pubDate>Wed, 01 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Beyond Next Token Probabilities: Learnable, Fast Detection of Hallucinations and Data Contamination on LLM Output Distributions</title><link>https://arxiv.org/abs/2503.14043</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a method to detect hallucinations and data contamination in LLM outputs using the full sequence of next-token probability distributions.&lt;/li&gt;&lt;li&gt;Introduces LOS-Net, an attention-based architecture that processes the LLM Output Signature (LOS) for detection.&lt;/li&gt;&lt;li&gt;Achieves better performance than existing methods while maintaining low latency and transferability.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Guy Bar-Shalom', 'Fabrizio Frasca', 'Derek Lim', 'Yoav Gelberg', 'Yftah Ziser', 'Ran El-Yaniv', 'Gal Chechik', 'Haggai Maron']&lt;/li&gt;&lt;li&gt;Tags: ['safety evaluation', 'robustness', 'data poisoning', 'privacy attacks', 'alignment']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2503.14043</guid><pubDate>Wed, 01 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Model Extraction Attacks Revisited</title><link>https://arxiv.org/abs/2312.05386</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Revisits model extraction attacks on MLaaS platforms&lt;/li&gt;&lt;li&gt;Analyzes vulnerability evolution over time&lt;/li&gt;&lt;li&gt;Challenges previous findings and suggests improvements&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jiacheng Liang', 'Ren Pang', 'Changjiang Li', 'Ting Wang']&lt;/li&gt;&lt;li&gt;Tags: ['model extraction', 'MLaaS', 'security', 'vulnerability', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2312.05386</guid><pubDate>Wed, 01 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Are Robust LLM Fingerprints Adversarially Robust?</title><link>https://arxiv.org/abs/2509.26598</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;The paper evaluates the adversarial robustness of LLM fingerprinting schemes.&lt;/li&gt;&lt;li&gt;It identifies vulnerabilities and develops attacks that bypass authentication.&lt;/li&gt;&lt;li&gt;The study covers ten recent fingerprinting methods and shows their weaknesses.&lt;/li&gt;&lt;li&gt;Recommendations are provided for improving adversarial robustness.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Anshul Nasery', 'Edoardo Contente', 'Alkin Kaz', 'Pramod Viswanath', 'Sewoong Oh']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial attacks', 'model fingerprinting', 'robustness', 'security', 'red teaming']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.26598</guid><pubDate>Wed, 01 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Fairness Testing in Retrieval-Augmented Generation: How Small Perturbations Reveal Bias in Small Language Models</title><link>https://arxiv.org/abs/2509.26584</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;This paper addresses fairness and security issues in Retrieval-Augmented Generation (RAG) using Small Language Models (SLMs).&lt;/li&gt;&lt;li&gt;It tests for demographic biases through metamorphic testing and finds significant issues, especially with racial cues.&lt;/li&gt;&lt;li&gt;The study highlights the need for careful curation of the retrieval component to prevent bias amplification.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Matheus Vinicius da Silva de Oliveira', 'Jonathan de Andrade Silva', 'Awdren de Lima Fontao']&lt;/li&gt;&lt;li&gt;Tags: ['fairness', 'bias', 'RAG', 'metamorphic testing', 'demographic cues']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.26584</guid><pubDate>Wed, 01 Oct 2025 00:00:00 +0000</pubDate></item><item><title>DeepProv: Behavioral Characterization and Repair of Neural Networks via Inference Provenance Graph Analysis</title><link>https://arxiv.org/abs/2509.26562</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces DeepProv for characterizing and repairing DNN behavior using Inference Provenance Graphs (IPGs)&lt;/li&gt;&lt;li&gt;Focuses on adversarial robustness with significant improvements in accuracy&lt;/li&gt;&lt;li&gt;Potential applications in privacy and fairness&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Firas Ben Hmida', 'Abderrahmen Amich', 'Ata Kaboudi', 'Birhanu Eshete']&lt;/li&gt;&lt;li&gt;Tags: ['robustness', 'adversarial', 'privacy', 'fairness', 'repair']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.26562</guid><pubDate>Wed, 01 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Your Agent May Misevolve: Emergent Risks in Self-evolving LLM Agents</title><link>https://arxiv.org/abs/2509.26354</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces the concept of 'Misevolution' in self-evolving LLM agents&lt;/li&gt;&lt;li&gt;Evaluates misevolution across model, memory, tool, and workflow pathways&lt;/li&gt;&lt;li&gt;Empirical findings show widespread risks in top-tier LLMs like Gemini-2.5-Pro&lt;/li&gt;&lt;li&gt;Highlights need for new safety paradigms and mitigation strategies&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Shuai Shao', 'Qihan Ren', 'Chen Qian', 'Boyi Wei', 'Dadi Guo', 'Jingyi Yang', 'Xinhao Song', 'Linfeng Zhang', 'Weinan Zhang', 'Dongrui Liu', 'Jing Shao']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'safety evaluation', 'alignment', 'robustness', 'emergent risks']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.26354</guid><pubDate>Wed, 01 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Better Privilege Separation for Agents by Restricting Data Types</title><link>https://arxiv.org/abs/2509.25926</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes type-directed privilege separation for LLMs to prevent prompt injection attacks&lt;/li&gt;&lt;li&gt;Restricts LLM interaction with third-party data by converting untrusted content to curated data types&lt;/li&gt;&lt;li&gt;Evaluates across case studies showing high utility while preventing attacks&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Dennis Jacob', 'Emad Alghamdi', 'Zhanhao Hu', 'Basel Alomair', 'David Wagner']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'prompt injection', 'security', 'robustness', 'data types']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.25926</guid><pubDate>Wed, 01 Oct 2025 00:00:00 +0000</pubDate></item><item><title>STAC: When Innocent Tools Form Dangerous Chains to Jailbreak LLM Agents</title><link>https://arxiv.org/abs/2509.25624</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces STAC, a multi-turn attack framework exploiting LLM agent tool use&lt;/li&gt;&lt;li&gt;Evaluates vulnerability of state-of-the-art agents like GPT-4.1 with high ASR&lt;/li&gt;&lt;li&gt;Proposes a reasoning-driven defense prompt to mitigate STAC attacks&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jing-Jing Li', 'Jianfeng He', 'Chao Shang', 'Devang Kulshreshtha', 'Xun Xian', 'Yi Zhang', 'Hang Su', 'Sandesh Swamy', 'Yanjun Qi']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'jailbreaking', 'adversarial prompting', 'security', 'safety evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.25624</guid><pubDate>Wed, 01 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Defeating Cerberus: Concept-Guided Privacy-Leakage Mitigation in Multimodal Language Models</title><link>https://arxiv.org/abs/2509.25525</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Investigates PII leakage in multimodal language models (MLLMs), specifically vision language models (VLMs)&lt;/li&gt;&lt;li&gt;Introduces a concept-guided mitigation approach to modify internal states related to PII&lt;/li&gt;&lt;li&gt;Constructs multimodal PII datasets to simulate real-world scenarios&lt;/li&gt;&lt;li&gt;Achieves 93.3% refusal rate for PII tasks with minimal performance impact&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Boyang Zhang', 'Istemi Ekin Akkus', 'Ruichuan Chen', 'Alice Dethise', 'Klaus Satzke', 'Ivica Rimac', 'Yang Zhang']&lt;/li&gt;&lt;li&gt;Tags: ['privacy attacks', 'data poisoning', 'safety evaluation', 'multimodal', 'PII']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.25525</guid><pubDate>Wed, 01 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Dive into the Agent Matrix: A Realistic Evaluation of Self-Replication Risk in LLM Agents</title><link>https://arxiv.org/abs/2509.25302</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Evaluates self-replication risk in LLM agents under realistic scenarios&lt;/li&gt;&lt;li&gt;Introduces Overuse Rate (OR) and Aggregate Overuse Count (AOC) metrics&lt;/li&gt;&lt;li&gt;Tests 21 models, finds over 50% have high risk scores under pressure&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Boxuan Zhang', 'Yi Yu', 'Jiaxuan Guo', 'Jing Shao']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'alignment', 'safety evaluation', 'robustness', 'self-replication risk']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.25302</guid><pubDate>Wed, 01 Oct 2025 00:00:00 +0000</pubDate></item><item><title>RADAR: A Risk-Aware Dynamic Multi-Agent Framework for LLM Safety Evaluation via Role-Specialized Collaboration</title><link>https://arxiv.org/abs/2509.25271</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces RADAR, a multi-agent framework for LLM safety evaluation&lt;/li&gt;&lt;li&gt;Decomposes risk into explicit, implicit, and non-risk subspaces&lt;/li&gt;&lt;li&gt;Uses role-specialized collaboration and dynamic updates&lt;/li&gt;&lt;li&gt;Validated with 800 challenging cases and public benchmarks&lt;/li&gt;&lt;li&gt;Shows significant improvement in risk identification accuracy&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Xiuyuan Chen', 'Jian Zhao', 'Yuchen Yuan', 'Tianle Zhang', 'Huilin Zhou', 'Zheng Zhu', 'Ping Hu', 'Linghe Kong', 'Chi Zhang', 'Weiran Huang', 'Xuelong Li']&lt;/li&gt;&lt;li&gt;Tags: ['LLM safety evaluation', 'multi-agent systems', 'risk concept space', 'dynamic updates', 'role-specialized collaboration']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.25271</guid><pubDate>Wed, 01 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Cyclic Ablation: Testing Concept Localization against Functional Regeneration in AI</title><link>https://arxiv.org/abs/2509.25220</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces cyclic ablation method to test concept localization in LLMs&lt;/li&gt;&lt;li&gt;Aims to remove deception but finds functional regeneration&lt;/li&gt;&lt;li&gt;Highlights limitations of model editing for safety&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Eduard Kapelko']&lt;/li&gt;&lt;li&gt;Tags: ['safety', 'red teaming', 'alignment', 'robustness', 'model editing']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.25220</guid><pubDate>Wed, 01 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Sandbagging in a Simple Survival Bandit Problem</title><link>https://arxiv.org/abs/2509.26239</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces a model of strategic deception (sandbagging) in sequential decision-making tasks using a survival bandit framework.&lt;/li&gt;&lt;li&gt;Theoretical analysis shows optimal rational agents exhibit sandbagging behavior.&lt;/li&gt;&lt;li&gt;Develops a statistical test to differentiate between sandbagging and incompetence based on test scores.&lt;/li&gt;&lt;li&gt;Simulation experiments validate the test's reliability in distinguishing the two behaviors.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Joel Dyer', 'Daniel Jarne Ornia', 'Nicholas Bishop', 'Anisoara Calinescu', 'Michael Wooldridge']&lt;/li&gt;&lt;li&gt;Tags: ['AI safety', 'sandbagging', 'strategic deception', 'survival bandit', 'statistical testing']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.26239</guid><pubDate>Wed, 01 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Beyond Linear Probes: Dynamic Safety Monitoring for Language Models</title><link>https://arxiv.org/abs/2509.26238</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces Truncated Polynomial Classifiers (TPCs) for dynamic safety monitoring of LLMs&lt;/li&gt;&lt;li&gt;TPCs allow adjustable compute costs based on input difficulty or available resources&lt;/li&gt;&lt;li&gt;Provides two modes: safety dial and adaptive cascade&lt;/li&gt;&lt;li&gt;Competes with MLP probes while being more interpretable&lt;/li&gt;&lt;li&gt;Evaluated on WildGuardMix and BeaverTails datasets with up to 30B parameter models&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['James Oldfield', 'Philip Torr', 'Ioannis Patras', 'Adel Bibi', 'Fazl Barez']&lt;/li&gt;&lt;li&gt;Tags: ['safety evaluation', 'model monitoring', 'dynamic activation monitoring', 'interpretable safety', 'adaptive compute']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.26238</guid><pubDate>Wed, 01 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Rotation Control Unlearning: Quantifying and Controlling Continuous Unlearning for LLM with The Cognitive Rotation Space</title><link>https://arxiv.org/abs/2509.25743</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes Rotation Control Unlearning (RCU) for continuous unlearning in LLMs&lt;/li&gt;&lt;li&gt;Aims to mitigate security risks by removing influence of undesirable data&lt;/li&gt;&lt;li&gt;Introduces cognitive rotation space and orthogonal rotation axes regularization&lt;/li&gt;&lt;li&gt;Achieves SOTA performance without retained dataset&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Xiang Zhang', 'Kun Wei', 'Xu Yang', 'Chenghao Xu', 'Su Yan', 'Cheng Deng']&lt;/li&gt;&lt;li&gt;Tags: ['LLM', 'unlearning', 'security', 'privacy', 'model extraction']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.25743</guid><pubDate>Wed, 01 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Backdoor Attribution: Elucidating and Controlling Backdoor in Language Models</title><link>https://arxiv.org/abs/2509.21761</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces Backdoor Attribution (BkdAttr) framework to understand and control backdoors in LLMs&lt;/li&gt;&lt;li&gt;Develops Backdoor Attention Head Attribution (BAHA) to identify critical attention heads&lt;/li&gt;&lt;li&gt;Shows that ablating ~3% of heads reduces ASR by over 90%&lt;/li&gt;&lt;li&gt;Creates Backdoor Vector for precise control over backdoor activation&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Miao Yu', 'Zhenhong Zhou', 'Moayad Aloqaily', 'Kun Wang', 'Biwei Huang', 'Stephen Wang', 'Yueming Jin', 'Qingsong Wen']&lt;/li&gt;&lt;li&gt;Tags: ['backdoor attacks', 'data poisoning', 'model interpretability', 'attention head analysis', 'backdoor control']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.21761</guid><pubDate>Wed, 01 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Beyond Sharp Minima: Robust LLM Unlearning via Feedback-Guided Multi-Point Optimization</title><link>https://arxiv.org/abs/2509.20230</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes StableUN, a framework for robust LLM unlearning&lt;/li&gt;&lt;li&gt;Addresses vulnerability of sharp minima leading to relearning attacks&lt;/li&gt;&lt;li&gt;Uses feedback-guided multi-point optimization to find stable parameters&lt;/li&gt;&lt;li&gt;Demonstrates improved robustness against relearning and jailbreaking attacks&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Wenhan Wu', 'Zheyuan Liu', 'Chongyang Gao', 'Ren Wang', 'Kaize Ding']&lt;/li&gt;&lt;li&gt;Tags: ['LLM unlearning', 'Robustness', 'Security', 'Relearning attacks', 'Jailbreaking attacks']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.20230</guid><pubDate>Wed, 01 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Inducing Uncertainty on Open-Weight Models for Test-Time Privacy in Image Recognition</title><link>https://arxiv.org/abs/2509.11625</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a method to induce uncertainty on protected instances in open-weight models to enhance test-time privacy&lt;/li&gt;&lt;li&gt;Uses a Pareto optimal objective to balance privacy and utility&lt;/li&gt;&lt;li&gt;Provides a certifiable approximation algorithm with privacy guarantees&lt;/li&gt;&lt;li&gt;Empirically shows significant uncertainty increase with minimal accuracy loss&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Muhammad H. Ashiq', 'Peter Triantafillou', 'Hung Yun Tseng', 'Grigoris G. Chrysos']&lt;/li&gt;&lt;li&gt;Tags: ['privacy attacks', 'robustness', 'safety evaluation', 'security standards']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.11625</guid><pubDate>Wed, 01 Oct 2025 00:00:00 +0000</pubDate></item><item><title>AudioTrust: Benchmarking the Multifaceted Trustworthiness of Audio Large Language Models</title><link>https://arxiv.org/abs/2505.16211</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces AudioTrust framework for evaluating trustworthiness of Audio Large Language Models (ALLMs)&lt;/li&gt;&lt;li&gt;Addresses audio-specific risks like timbre, accent, background noise&lt;/li&gt;&lt;li&gt;Evaluates across six dimensions: fairness, hallucination, safety, privacy, robustness, authentication&lt;/li&gt;&lt;li&gt;Tests 14 state-of-the-art ALLMs with 26 sub-tasks using 4,420 audio samples&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Kai Li', 'Can Shen', 'Yile Liu', 'Jirui Han', 'Kelong Zheng', 'Xuechao Zou', 'Zhe Wang', 'Shun Zhang', 'Xingjian Du', 'Hanjun Luo', 'Yingbin Jin', 'Xinxin Xing', 'Ziyang Ma', 'Yue Liu', 'Yifan Zhang', 'Junfeng Fang', 'Kun Wang', 'Yibo Yan', 'Gelei Deng', 'Haoyang Li', 'Yiming Li', 'Xiaobin Zhuang', 'Tianlong Chen', 'Qingsong Wen', 'Tianwei Zhang', 'Yang Liu', 'Haibo Hu', 'Zhizheng Wu', 'Xiaolin Hu', 'Eng-Siong Chng', 'Wenyuan Xu', 'XiaoFeng Wang', 'Wei Dong', 'Xinfeng Li']&lt;/li&gt;&lt;li&gt;Tags: ['red teaming', 'safety evaluation', 'robustness', 'privacy attacks', 'data poisoning']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2505.16211</guid><pubDate>Wed, 01 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Silent Leaks: Implicit Knowledge Extraction Attack on RAG Systems through Benign Queries</title><link>https://arxiv.org/abs/2505.15420</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces IKEA, an implicit knowledge extraction attack on RAG systems using benign queries&lt;/li&gt;&lt;li&gt;Uses anchor concepts and two mechanisms: Experience Reflection Sampling and Trust Region Directed Mutation&lt;/li&gt;&lt;li&gt;Demonstrates high extraction efficiency and attack success rate compared to baselines&lt;/li&gt;&lt;li&gt;Highlights stealthy copyright infringement risks in RAG systems&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yuhao Wang', 'Wenjie Qu', 'Shengfang Zhai', 'Yanze Jiang', 'Zichen Liu', 'Yue Liu', 'Yinpeng Dong', 'Jiaheng Zhang']&lt;/li&gt;&lt;li&gt;Tags: ['model extraction', 'adversarial prompting', 'RAG systems', 'stealth attacks', 'copyright infringement']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2505.15420</guid><pubDate>Wed, 01 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Answer, Refuse, or Guess? Investigating Risk-Aware Decision Making in Language Models</title><link>https://arxiv.org/abs/2503.01332</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;The paper evaluates how language models make risk-aware decisions, specifically whether to answer, refuse, or guess in different risk scenarios.&lt;/li&gt;&lt;li&gt;It identifies that LMs over-answer in high-risk and over-defer in low-risk settings.&lt;/li&gt;&lt;li&gt;A skill-decomposition method is proposed to improve decision policies.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research Paper&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Cheng-Kuang Wu', 'Zhi Rui Tam', 'Chieh-Yen Lin', 'Yun-Nung Chen', 'Hung-yi Lee']&lt;/li&gt;&lt;li&gt;Tags: ['safety evaluation', 'risk-aware decision making', 'model limitations']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2503.01332</guid><pubDate>Wed, 01 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Dagger Behind Smile: Fool LLMs with a Happy Ending Story</title><link>https://arxiv.org/abs/2501.13115</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces Happy Ending Attack (HEA) for jailbreaking LLMs using positive prompts&lt;/li&gt;&lt;li&gt;Achieves high success rates on models like GPT-4o, Llama3-70b, Gemini-pro&lt;/li&gt;&lt;li&gt;Requires only up to two turns to jailbreak&lt;/li&gt;&lt;li&gt;Provides quantitative explanations for the attack's effectiveness&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Xurui Song', 'Zhixin Xie', 'Shuo Huai', 'Jiayi Kong', 'Jun Luo']&lt;/li&gt;&lt;li&gt;Tags: ['jailbreaking', 'adversarial prompting', 'LLM security', 'red teaming']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2501.13115</guid><pubDate>Wed, 01 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Preemptive Detection and Correction of Misaligned Actions in LLM Agents</title><link>https://arxiv.org/abs/2407.11843</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces InferAct, a method using LLMs' Theory-of-Mind to detect misaligned actions before execution&lt;/li&gt;&lt;li&gt;Aims to prevent unintended critical actions in LLM agents&lt;/li&gt;&lt;li&gt;Shows 20% improvement in misalignment detection over baselines&lt;/li&gt;&lt;li&gt;Evaluates both detection and correction of misaligned actions&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Haishuo Fang', 'Xiaodan Zhu', 'Iryna Gurevych']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'safety', 'LLM', 'action detection', 'Theory-of-Mind']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2407.11843</guid><pubDate>Wed, 01 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Are Robust LLM Fingerprints Adversarially Robust?</title><link>https://arxiv.org/abs/2509.26598</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;The paper evaluates the adversarial robustness of LLM fingerprinting schemes.&lt;/li&gt;&lt;li&gt;It identifies vulnerabilities and develops attacks that bypass authentication.&lt;/li&gt;&lt;li&gt;The study covers ten recent fingerprinting methods and shows their weaknesses.&lt;/li&gt;&lt;li&gt;Recommendations are provided for improving adversarial robustness.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Anshul Nasery', 'Edoardo Contente', 'Alkin Kaz', 'Pramod Viswanath', 'Sewoong Oh']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial attacks', 'model fingerprinting', 'robustness', 'security', 'red teaming']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.26598</guid><pubDate>Wed, 01 Oct 2025 00:00:00 +0000</pubDate></item><item><title>SeedPrints: Fingerprints Can Even Tell Which Seed Your Large Language Model Was Trained From</title><link>https://arxiv.org/abs/2509.26404</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces SeedPrints, a method for fingerprinting LLMs using random initialization biases&lt;/li&gt;&lt;li&gt;Enables seed-level distinguishability and lifecycle identity verification&lt;/li&gt;&lt;li&gt;Robust under domain shifts and parameter modifications&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yao Tong', 'Haonan Wang', 'Siquan Li', 'Kenji Kawaguchi', 'Tianyang Hu']&lt;/li&gt;&lt;li&gt;Tags: ['model extraction', 'privacy attacks', 'safety evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.26404</guid><pubDate>Wed, 01 Oct 2025 00:00:00 +0000</pubDate></item><item><title>SoK: Systematic analysis of adversarial threats against deep learning approaches for autonomous anomaly detection systems in SDN-IoT networks</title><link>https://arxiv.org/abs/2509.26350</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Systematic analysis of adversarial threats against DL-based AAD systems in SDN-IoT networks&lt;/li&gt;&lt;li&gt;Categorizes attacks into data, model, and hybrid-level threats&lt;/li&gt;&lt;li&gt;Evaluates white, black, and grey-box attack strategies&lt;/li&gt;&lt;li&gt;Proposes adaptive countermeasures&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Survey&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Tharindu Lakshan Yasarathna', 'Nhien-An Le-Khac']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial attacks', 'deep learning', 'anomaly detection', 'SDN-IoT', 'adversarial training']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.26350</guid><pubDate>Wed, 01 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Sandbagging in a Simple Survival Bandit Problem</title><link>https://arxiv.org/abs/2509.26239</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces a model of strategic deception (sandbagging) in sequential decision-making tasks using a survival bandit framework.&lt;/li&gt;&lt;li&gt;Theoretical analysis shows optimal rational agents exhibit sandbagging behavior.&lt;/li&gt;&lt;li&gt;Develops a statistical test to differentiate between sandbagging and incompetence based on test scores.&lt;/li&gt;&lt;li&gt;Simulation experiments validate the test's reliability in distinguishing the two behaviors.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Joel Dyer', 'Daniel Jarne Ornia', 'Nicholas Bishop', 'Anisoara Calinescu', 'Michael Wooldridge']&lt;/li&gt;&lt;li&gt;Tags: ['AI safety', 'sandbagging', 'strategic deception', 'survival bandit', 'statistical testing']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.26239</guid><pubDate>Wed, 01 Oct 2025 00:00:00 +0000</pubDate></item><item><title>STAC: When Innocent Tools Form Dangerous Chains to Jailbreak LLM Agents</title><link>https://arxiv.org/abs/2509.25624</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces STAC, a multi-turn attack framework exploiting LLM agent tool use&lt;/li&gt;&lt;li&gt;Evaluates vulnerability of state-of-the-art agents like GPT-4.1 with high ASR&lt;/li&gt;&lt;li&gt;Proposes a reasoning-driven defense prompt to mitigate STAC attacks&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jing-Jing Li', 'Jianfeng He', 'Chao Shang', 'Devang Kulshreshtha', 'Xun Xian', 'Yi Zhang', 'Hang Su', 'Sandesh Swamy', 'Yanjun Qi']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'jailbreaking', 'adversarial prompting', 'security', 'safety evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.25624</guid><pubDate>Wed, 01 Oct 2025 00:00:00 +0000</pubDate></item><item><title>VISOR++: Universal Visual Inputs based Steering for Large Vision Language Models</title><link>https://arxiv.org/abs/2509.25533</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces VISOR++ for steering VLMs using optimized visual inputs&lt;/li&gt;&lt;li&gt;Demonstrates effectiveness on alignment tasks like refusal and sycophancy&lt;/li&gt;&lt;li&gt;Works with both open and closed-source models&lt;/li&gt;&lt;li&gt;Preserves performance on unrelated tasks&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Ravikumar Balakrishnan', 'Mansi Phute']&lt;/li&gt;&lt;li&gt;Tags: ['red teaming', 'alignment', 'safety evaluation', 'model steering', 'visual inputs']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.25533</guid><pubDate>Wed, 01 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Fairness Testing in Retrieval-Augmented Generation: How Small Perturbations Reveal Bias in Small Language Models</title><link>https://arxiv.org/abs/2509.26584</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;This paper addresses fairness and security issues in Retrieval-Augmented Generation (RAG) using Small Language Models (SLMs).&lt;/li&gt;&lt;li&gt;It tests for demographic biases through metamorphic testing and finds significant issues, especially with racial cues.&lt;/li&gt;&lt;li&gt;The study highlights the need for careful curation of the retrieval component to prevent bias amplification.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Matheus Vinicius da Silva de Oliveira', 'Jonathan de Andrade Silva', 'Awdren de Lima Fontao']&lt;/li&gt;&lt;li&gt;Tags: ['fairness', 'bias', 'RAG', 'metamorphic testing', 'demographic cues']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.26584</guid><pubDate>Wed, 01 Oct 2025 00:00:00 +0000</pubDate></item><item><title>OffTopicEval: When Large Language Models Enter the Wrong Chat, Almost Always!</title><link>https://arxiv.org/abs/2509.26495</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces operational safety for LLMs&lt;/li&gt;&lt;li&gt;Proposes OffTopicEval benchmark&lt;/li&gt;&lt;li&gt;Evaluates multiple models showing low safety&lt;/li&gt;&lt;li&gt;Proposes prompt-based steering methods&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jingdi Lei', 'Varun Gumma', 'Rishabh Bhardwaj', 'Seok Min Lim', 'Chuan Li', 'Amir Zadeh', 'Soujanya Poria']&lt;/li&gt;&lt;li&gt;Tags: ['safety evaluation', 'alignment', 'benchmarking', 'prompt injection']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.26495</guid><pubDate>Wed, 01 Oct 2025 00:00:00 +0000</pubDate></item><item><title>STaR-Attack: A Spatio-Temporal and Narrative Reasoning Attack Framework for Unified Multimodal Understanding and Generation Models</title><link>https://arxiv.org/abs/2509.26473</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;STaR-Attack is a multi-turn jailbreak attack framework targeting Unified Multimodal Models (UMMs)&lt;/li&gt;&lt;li&gt;Uses spatio-temporal and narrative reasoning to generate adversarial images and manipulate model responses&lt;/li&gt;&lt;li&gt;Achieves high attack success rates (93.06% ASR) on models like Gemini-2.0-Flash&lt;/li&gt;&lt;li&gt;Highlights safety vulnerabilities in UMMs requiring alignment&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Shaoxiong Guo', 'Tianyi Du', 'Lijun Li', 'Yuyao Wu', 'Jie Li', 'Jing Shao']&lt;/li&gt;&lt;li&gt;Tags: ['red teaming', 'jailbreaking', 'adversarial prompting', 'multimodal', 'safety evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.26473</guid><pubDate>Wed, 01 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Your Agent May Misevolve: Emergent Risks in Self-evolving LLM Agents</title><link>https://arxiv.org/abs/2509.26354</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces the concept of 'Misevolution' in self-evolving LLM agents&lt;/li&gt;&lt;li&gt;Evaluates misevolution across model, memory, tool, and workflow pathways&lt;/li&gt;&lt;li&gt;Empirical findings show widespread risks in top-tier LLMs like Gemini-2.5-Pro&lt;/li&gt;&lt;li&gt;Highlights need for new safety paradigms and mitigation strategies&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Shuai Shao', 'Qihan Ren', 'Chen Qian', 'Boyi Wei', 'Dadi Guo', 'Jingyi Yang', 'Xinhao Song', 'Linfeng Zhang', 'Weinan Zhang', 'Dongrui Liu', 'Jing Shao']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'safety evaluation', 'alignment', 'robustness', 'emergent risks']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.26354</guid><pubDate>Wed, 01 Oct 2025 00:00:00 +0000</pubDate></item><item><title>SafeBehavior: Simulating Human-Like Multistage Reasoning to Mitigate Jailbreak Attacks in Large Language Models</title><link>https://arxiv.org/abs/2509.26345</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes SafeBehavior, a hierarchical defense against jailbreak attacks in LLMs&lt;/li&gt;&lt;li&gt;Simulates human multistage reasoning: intention inference, self-introspection, self-revision&lt;/li&gt;&lt;li&gt;Evaluated against 5 attack types and 7 baselines, showing improved robustness&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Qinjian Zhao', 'Jiaqi Wang', 'Zhiqiang Gao', 'Zhihao Dou', 'Belal Abuhaija', 'Kaizhu Huang']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'jailbreaking', 'safety evaluation', 'robustness', 'alignment']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.26345</guid><pubDate>Wed, 01 Oct 2025 00:00:00 +0000</pubDate></item><item><title>SafeEvalAgent: Toward Agentic and Self-Evolving Safety Evaluation of LLMs</title><link>https://arxiv.org/abs/2509.26100</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces SafeEvalAgent, a multi-agent framework for dynamic safety evaluation of LLMs&lt;/li&gt;&lt;li&gt;Uses policy documents to generate and evolve safety benchmarks&lt;/li&gt;&lt;li&gt;Demonstrates declining safety scores over iterations, highlighting static evaluation limitations&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yixu Wang', 'Xin Wang', 'Yang Yao', 'Xinyuan Li', 'Yan Teng', 'Xingjun Ma', 'Yingchun Wang']&lt;/li&gt;&lt;li&gt;Tags: ['LLM safety evaluation', 'Dynamic benchmarking', 'Multi-agent systems', 'Policy compliance', 'Self-evolving evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.26100</guid><pubDate>Wed, 01 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Scalable and Robust LLM Unlearning by Correcting Responses with Retrieved Exclusions</title><link>https://arxiv.org/abs/2509.25973</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes CURE framework for LLM unlearning to prevent sensitive info leakage&lt;/li&gt;&lt;li&gt;Uses a corrector to verify and revise model outputs&lt;/li&gt;&lt;li&gt;Retrieves relevant exclusions for efficient large-scale unlearning&lt;/li&gt;&lt;li&gt;Demonstrates reduced leakage and maintains response quality&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Junbeom Kim', 'Kyuyoung Kim', 'Jihoon Tack', 'Dongha Lim', 'Jinwoo Shin']&lt;/li&gt;&lt;li&gt;Tags: ['data_poisoning', 'privacy_attacks', 'robustness', 'safety_evaluation', 'model_extraction']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.25973</guid><pubDate>Wed, 01 Oct 2025 00:00:00 +0000</pubDate></item><item><title>SafeMind: Benchmarking and Mitigating Safety Risks in Embodied LLM Agents</title><link>https://arxiv.org/abs/2509.25885</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces SafeMindBench, a multimodal benchmark for evaluating safety risks in embodied LLM agents&lt;/li&gt;&lt;li&gt;Identifies key reasoning stages and safety constraint types&lt;/li&gt;&lt;li&gt;Presents SafeMindAgent with cascaded safety modules&lt;/li&gt;&lt;li&gt;Shows significant safety improvements over baselines&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Ruolin Chen', 'Yinqian Sun', 'Jihang Wang', 'Mingyang Lv', 'Qian Zhang', 'Yi Zeng']&lt;/li&gt;&lt;li&gt;Tags: ['safety evaluation', 'embodied agents', 'multimodal', 'benchmarking', 'safety constraints']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.25885</guid><pubDate>Wed, 01 Oct 2025 00:00:00 +0000</pubDate></item><item><title>ASGuard: Activation-Scaling Guard to Mitigate Targeted Jailbreaking Attack</title><link>https://arxiv.org/abs/2509.25843</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces ASGuard, a framework to mitigate targeted jailbreaking attacks on LLMs&lt;/li&gt;&lt;li&gt;Uses circuit analysis to identify vulnerable attention heads&lt;/li&gt;&lt;li&gt;Applies channel-wise scaling during fine-tuning to improve refusal robustness&lt;/li&gt;&lt;li&gt;Reduces attack success rate while preserving model capabilities&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yein Park', 'Jungwoo Park', 'Jaewoo Kang']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'jailbreaking', 'adversarial prompting', 'safety evaluation', 'alignment']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.25843</guid><pubDate>Wed, 01 Oct 2025 00:00:00 +0000</pubDate></item><item><title>PUREVQ-GAN: Defending Data Poisoning Attacks through Vector-Quantized Bottlenecks</title><link>https://arxiv.org/abs/2509.25792</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces PureVQ-GAN to defend against data poisoning attacks&lt;/li&gt;&lt;li&gt;Uses Vector-Quantized VAE with GAN discriminator to destroy triggers&lt;/li&gt;&lt;li&gt;Highly effective on CIFAR-10 with low clean accuracy loss&lt;/li&gt;&lt;li&gt;Faster than diffusion-based methods&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Alexander Branch', 'Omead Pooladzandi', 'Radin Khosraviani', 'Sunay Gajanan Bhat', 'Jeffrey Jiang', 'Gregory Pottie']&lt;/li&gt;&lt;li&gt;Tags: ['data poisoning', 'defense mechanism', 'VQ-VAE', 'GAN', 'CIFAR-10']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.25792</guid><pubDate>Wed, 01 Oct 2025 00:00:00 +0000</pubDate></item><item><title>SOCK: A Benchmark for Measuring Self-Replication in Large Language Models</title><link>https://arxiv.org/abs/2509.25643</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces SOCK, a benchmark for measuring LLM self-replication&lt;/li&gt;&lt;li&gt;Evaluates replication and persistence capabilities&lt;/li&gt;&lt;li&gt;Aims to mitigate self-replication threats&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Justin Chavarria', 'Rohan Raizada', 'Justin White', 'Eyad Alhetairshi']&lt;/li&gt;&lt;li&gt;Tags: ['red teaming', 'safety evaluation', 'benchmarking']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.25643</guid><pubDate>Wed, 01 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Dive into the Agent Matrix: A Realistic Evaluation of Self-Replication Risk in LLM Agents</title><link>https://arxiv.org/abs/2509.25302</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Evaluates self-replication risk in LLM agents under realistic scenarios&lt;/li&gt;&lt;li&gt;Introduces Overuse Rate (OR) and Aggregate Overuse Count (AOC) metrics&lt;/li&gt;&lt;li&gt;Tests 21 models, finds over 50% have high risk scores under pressure&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Boxuan Zhang', 'Yi Yu', 'Jiaxuan Guo', 'Jing Shao']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'alignment', 'safety evaluation', 'robustness', 'self-replication risk']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.25302</guid><pubDate>Wed, 01 Oct 2025 00:00:00 +0000</pubDate></item><item><title>RADAR: A Risk-Aware Dynamic Multi-Agent Framework for LLM Safety Evaluation via Role-Specialized Collaboration</title><link>https://arxiv.org/abs/2509.25271</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces RADAR, a multi-agent framework for LLM safety evaluation&lt;/li&gt;&lt;li&gt;Decomposes risk into explicit, implicit, and non-risk subspaces&lt;/li&gt;&lt;li&gt;Uses role-specialized collaboration and dynamic updates&lt;/li&gt;&lt;li&gt;Validated with 800 challenging cases and public benchmarks&lt;/li&gt;&lt;li&gt;Shows significant improvement in risk identification accuracy&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Xiuyuan Chen', 'Jian Zhao', 'Yuchen Yuan', 'Tianle Zhang', 'Huilin Zhou', 'Zheng Zhu', 'Ping Hu', 'Linghe Kong', 'Chi Zhang', 'Weiran Huang', 'Xuelong Li']&lt;/li&gt;&lt;li&gt;Tags: ['LLM safety evaluation', 'multi-agent systems', 'risk concept space', 'dynamic updates', 'role-specialized collaboration']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.25271</guid><pubDate>Wed, 01 Oct 2025 00:00:00 +0000</pubDate></item></channel></rss>