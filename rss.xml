<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>AI Security Paper Digest</title><link>https://kentaroh-toyoda.github.io/ai-security-paper-digest-rss/rss.xml</link><description>Curated papers on AI security from OpenAlex and ArXiv</description><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><lastBuildDate>Tue, 17 Jun 2025 22:38:58 +0000</lastBuildDate><item><title>Multiverse Through Deepfakes: The MultiFakeVerse Dataset of Person-Centric Visual and Conceptual Manipulations</title><link>https://arxiv.org/abs/2506.00868</link><description>&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Other&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Parul Gupta', 'Shreya Ghosh', 'Tom Gedeon', 'Thanh-Toan Do', 'Abhinav Dhall']&lt;/li&gt;&lt;li&gt;Tags: []&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.00868</guid><pubDate>Tue, 17 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Leveraging Intermediate Features of Vision Transformer for Face Anti-Spoofing</title><link>https://arxiv.org/abs/2505.24402</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a method using Vision Transformer (ViT) intermediate features for detecting face spoofing attacks.&lt;/li&gt;&lt;li&gt;Focuses on distinguishing between live and spoofed face images to prevent unauthorized access in face recognition systems.&lt;/li&gt;&lt;li&gt;Introduces novel data augmentation techniques to enhance spoofing detection accuracy.&lt;/li&gt;&lt;li&gt;Evaluates the approach on standard face anti-spoofing datasets (OULU-NPU and SiW).&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research Paper&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Mika Feng', 'Koichi Ito', 'Takafumi Aoki', 'Tetsushi Ohki', 'Masakatsu Nishigaki']&lt;/li&gt;&lt;li&gt;Tags: ['face anti-spoofing', 'biometric security', 'adversarial attacks', 'vision transformer', 'authentication']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2505.24402</guid><pubDate>Tue, 17 Jun 2025 00:00:00 +0000</pubDate></item><item><title>FauForensics: Boosting Audio-Visual Deepfake Detection with Facial Action Units</title><link>https://arxiv.org/abs/2505.08294</link><description>&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Other&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jian Wang', 'Baoyuan Wu', 'Li Liu', 'Qingshan Liu']&lt;/li&gt;&lt;li&gt;Tags: []&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2505.08294</guid><pubDate>Tue, 17 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Unmasking Deep Fakes: Leveraging Deep Learning for Video Authenticity Detection</title><link>https://arxiv.org/abs/2505.06528</link><description>&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Other&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Mahmudul Hasan', 'Sadia Ruhama', 'Sabrina Tajnim Sithi', 'Chowdhury Mohammad Mutamir Samit', 'Oindrila Saha']&lt;/li&gt;&lt;li&gt;Tags: []&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2505.06528</guid><pubDate>Tue, 17 Jun 2025 00:00:00 +0000</pubDate></item><item><title>CAT: Contrastive Adversarial Training for Evaluating the Robustness of Protective Perturbations in Latent Diffusion Models</title><link>https://arxiv.org/abs/2502.07225</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Investigates the robustness of adversarial examples used as protective perturbations to defend against unauthorized customization of latent diffusion models.&lt;/li&gt;&lt;li&gt;Proposes a novel attack method, Contrastive Adversarial Training (CAT), that uses lightweight adapters to bypass these protective perturbations.&lt;/li&gt;&lt;li&gt;Demonstrates through experiments that CAT significantly reduces the effectiveness of current protective perturbations, exposing vulnerabilities in existing defenses.&lt;/li&gt;&lt;li&gt;Highlights the need for improved robustness in protective perturbation methods for safeguarding data privacy and intellectual property in diffusion models.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research Paper&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Sen Peng', 'Mingyue Wang', 'Jianfei He', 'Jijia Yang', 'Xiaohua Jia']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial attacks', 'diffusion models', 'privacy protection', 'robustness evaluation', 'model security']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2502.07225</guid><pubDate>Tue, 17 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Beautiful Images, Toxic Words: Understanding and Addressing Offensive Text in Generated Images</title><link>https://arxiv.org/abs/2502.05066</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Identifies a novel security/safety threat: diffusion models generating offensive or NSFW text embedded within images.&lt;/li&gt;&lt;li&gt;Demonstrates that current mitigation techniques for visual NSFW content do not prevent harmful text generation in images.&lt;/li&gt;&lt;li&gt;Proposes a fine-tuning strategy targeting text-generation layers to reduce harmful text output while preserving benign content.&lt;/li&gt;&lt;li&gt;Releases ToxicBench, a benchmark and dataset for evaluating and improving safety against NSFW text in image generation models.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research Paper&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Aditya Kumar', 'Tom Blanchard', 'Adam Dziedzic', 'Franziska Boenisch']&lt;/li&gt;&lt;li&gt;Tags: ['AI safety', 'harmful content mitigation', 'diffusion models', 'text-to-image', 'benchmarking']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2502.05066</guid><pubDate>Tue, 17 Jun 2025 00:00:00 +0000</pubDate></item><item><title>VideoQA in the Era of LLMs: An Empirical Study</title><link>https://arxiv.org/abs/2408.04223</link><description>&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Other&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Junbin Xiao', 'Nanxin Huang', 'Hangyu Qin', 'Dongyang Li', 'Yicong Li', 'Fengbin Zhu', 'Zhulin Tao', 'Jianxing Yu', 'Liang Lin', 'Tat-Seng Chua', 'Angela Yao']&lt;/li&gt;&lt;li&gt;Tags: []&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2408.04223</guid><pubDate>Tue, 17 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Pushing the Limits of Safety: A Technical Report on the ATLAS Challenge 2025</title><link>https://arxiv.org/abs/2506.12430</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Presents results from the ATLAS Challenge 2025, focused on adversarial testing of Multimodal Large Language Models (MLLMs).&lt;/li&gt;&lt;li&gt;Describes systematic evaluation of MLLM vulnerabilities to jailbreak attacks using both white-box and black-box approaches.&lt;/li&gt;&lt;li&gt;Establishes new benchmarks for safety evaluation and defense mechanisms in multimodal AI systems.&lt;/li&gt;&lt;li&gt;Provides open-source code and data to facilitate further research in MLLM safety.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Zonghao Ying', 'Siyang Wu', 'Run Hao', 'Peng Ying', 'Shixuan Sun', 'Pengyu Chen', 'Junze Chen', 'Hao Du', 'Kaiwen Shen', 'Shangkun Wu', 'Jiwei Wei', 'Shiyuan He', 'Yang Yang', 'Xiaohai Xu', 'Ke Ma', 'Qianqian Xu', 'Qingming Huang', 'Shi Lin', 'Xun Wang', 'Changting Lin', 'Meng Han', 'Yilei Jiang', 'Siqi Lai', 'Yaozhi Zheng', 'Yifei Song', 'Xiangyu Yue', 'Zonglei Jing', 'Tianyuan Zhang', 'Zhilei Zhu', 'Aishan Liu', 'Jiakai Wang', 'Siyuan Liang', 'Xianglong Kong', 'Hainan Li', 'Junjie Mu', 'Haotong Qin', 'Yue Yu', 'Lei Chen', 'Felix Juefei-Xu', 'Qing Guo', 'Xinyun Chen', 'Yew Soon Ong', 'Xianglong Liu', 'Dawn Song', 'Alan Yuille', 'Philip Torr', 'Dacheng Tao']&lt;/li&gt;&lt;li&gt;Tags: ['jailbreak attacks', 'adversarial testing', 'multimodal AI safety', 'benchmarking', 'MLLM vulnerabilities']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.12430</guid><pubDate>Tue, 17 Jun 2025 00:00:00 +0000</pubDate></item><item><title>InverTune: Removing Backdoors from Multimodal Contrastive Learning Models via Trigger Inversion and Activation Tuning</title><link>https://arxiv.org/abs/2506.12411</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes InverTune, a novel defense framework for removing backdoors from multimodal contrastive learning models (e.g., CLIP).&lt;/li&gt;&lt;li&gt;Operates under minimal attacker assumptions, requiring no prior knowledge of attack targets or access to poisoned data.&lt;/li&gt;&lt;li&gt;Uses adversarial simulation, gradient inversion, and clustering-guided fine-tuning to identify and erase backdoor triggers.&lt;/li&gt;&lt;li&gt;Demonstrates strong empirical results, significantly reducing attack success rates while maintaining model performance.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research Paper&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Mengyuan Sun', 'Yu Li', 'Yuchen Liu', 'Bo Du', 'Yunjie Ge']&lt;/li&gt;&lt;li&gt;Tags: ['backdoor attacks', 'multimodal models', 'model robustness', 'AI security', 'defense mechanisms']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.12411</guid><pubDate>Tue, 17 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Restoring Gaussian Blurred Face Images for Deanonymization Attacks</title><link>https://arxiv.org/abs/2506.12344</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Investigates the security/privacy risks of using Gaussian blur for face anonymization by demonstrating effective deanonymization attacks.&lt;/li&gt;&lt;li&gt;Proposes a novel deblurring method (Revelio) leveraging generative models and identity retrieval to restore blurred faces.&lt;/li&gt;&lt;li&gt;Shows high re-identification accuracy, highlighting vulnerabilities in current anonymization practices.&lt;/li&gt;&lt;li&gt;Evaluates robustness against variations in blurring and discusses potential countermeasures.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research Paper&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Haoyu Zhai', 'Shuo Wang', 'Pirouz Naghavi', 'Qingying Hao', 'Gang Wang']&lt;/li&gt;&lt;li&gt;Tags: ['privacy attack', 'deanonymization', 'face restoration', 'adversarial attack', 'image security']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.12344</guid><pubDate>Tue, 17 Jun 2025 00:00:00 +0000</pubDate></item><item><title>HKD4VLM: A Progressive Hybrid Knowledge Distillation Framework for Robust Multimodal Hallucination and Factuality Detection in VLMs</title><link>https://arxiv.org/abs/2506.13038</link><description>&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Other&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Zijian Zhang', 'Xuecheng Wu', 'Danlei Huang', 'Siyu Yan', 'Chong Peng', 'Xuezhi Cao']&lt;/li&gt;&lt;li&gt;Tags: []&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.13038</guid><pubDate>Tue, 17 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Active Adversarial Noise Suppression for Image Forgery Localization</title><link>https://arxiv.org/abs/2506.12871</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a novel Adversarial Noise Suppression Module (ANSM) to defend image forgery localization models against adversarial attacks.&lt;/li&gt;&lt;li&gt;Introduces Forgery-relevant Features Alignment (FFA) and Mask-guided Refinement (MgR) as training strategies to enhance robustness.&lt;/li&gt;&lt;li&gt;Demonstrates significant restoration of model performance under various adversarial attacks without degrading performance on clean images.&lt;/li&gt;&lt;li&gt;Claims to be the first work addressing adversarial defense specifically for image forgery localization.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research Paper&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Rongxuan Peng', 'Shunquan Tan', 'Xianbo Mo', 'Alex C. Kot', 'Jiwu Huang']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial defense', 'image forgery', 'robustness', 'computer vision', 'security']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.12871</guid><pubDate>Tue, 17 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Image Corruption-Inspired Membership Inference Attacks against Large Vision-Language Models</title><link>https://arxiv.org/abs/2506.12340</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes new membership inference attacks (MIAs) targeting large vision-language models (LVLMs) using image corruption techniques.&lt;/li&gt;&lt;li&gt;Demonstrates both white-box and black-box attack scenarios, leveraging embedding similarities between original and corrupted images.&lt;/li&gt;&lt;li&gt;Highlights privacy risks associated with training LVLMs on sensitive image data.&lt;/li&gt;&lt;li&gt;Validates the effectiveness of the proposed attacks through experiments on standard datasets.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research Paper&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Zongyu Wu', 'Minhua Lin', 'Zhiwei Zhang', 'Fali Wang', 'Xianren Zhang', 'Xiang Zhang', 'Suhang Wang']&lt;/li&gt;&lt;li&gt;Tags: ['membership inference', 'privacy attacks', 'vision-language models', 'image corruption', 'AI security']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.12340</guid><pubDate>Tue, 17 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Understanding and Benchmarking the Trustworthiness in Multimodal LLMs for Video Understanding</title><link>https://arxiv.org/abs/2506.12336</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces Trust-videoLLMs, a benchmark for evaluating trustworthiness in video-based multimodal LLMs.&lt;/li&gt;&lt;li&gt;Assesses models across five dimensions: truthfulness, safety, robustness, fairness, and privacy.&lt;/li&gt;&lt;li&gt;Evaluates 23 state-of-the-art videoLLMs, revealing limitations in scene understanding and resilience to perturbations.&lt;/li&gt;&lt;li&gt;Highlights the need for improved safety alignment and provides a toolbox for standardized trustworthiness assessment.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Youze Wang', 'Zijun Chen', 'Ruoyu Chen', 'Shishen Gu', 'Yinpeng Dong', 'Hang Su', 'Jun Zhu', 'Meng Wang', 'Richang Hong', 'Wenbo Hu']&lt;/li&gt;&lt;li&gt;Tags: ['AI safety', 'robustness', 'privacy', 'multimodal models', 'benchmarking']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.12336</guid><pubDate>Tue, 17 Jun 2025 00:00:00 +0000</pubDate></item><item><title>EgoPrivacy: What Your First-Person Camera Says About You?</title><link>https://arxiv.org/abs/2506.12258</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces EgoPrivacy, a benchmark for evaluating privacy risks in egocentric (first-person) video data.&lt;/li&gt;&lt;li&gt;Defines and tests multiple privacy attack tasks targeting demographic, individual, and situational information about camera wearers.&lt;/li&gt;&lt;li&gt;Proposes a novel Retrieval-Augmented Attack that leverages external exocentric video data to enhance privacy attacks.&lt;/li&gt;&lt;li&gt;Demonstrates that foundation models can effectively extract sensitive private information from egocentric videos, highlighting significant privacy vulnerabilities.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yijiang Li', 'Genpei Zhang', 'Jiacheng Cheng', 'Yi Li', 'Xiaojun Shan', 'Dashan Gao', 'Jiancheng Lyu', 'Yuan Li', 'Ning Bi', 'Nuno Vasconcelos']&lt;/li&gt;&lt;li&gt;Tags: ['privacy attacks', 'video privacy', 'model vulnerabilities', 'egocentric vision', 'benchmarking']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.12258</guid><pubDate>Tue, 17 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Watch Out Your Album! On the Inadvertent Privacy Memorization in Multi-Modal Large Language Models</title><link>https://arxiv.org/abs/2503.01208</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Investigates inadvertent memorization of private, task-irrelevant content in Multi-Modal Large Language Models (MLLMs).&lt;/li&gt;&lt;li&gt;Introduces a probing framework to detect if MLLMs encode randomly generated watermarks embedded in training images.&lt;/li&gt;&lt;li&gt;Finds that MLLMs can memorize and represent private content even when it is unrelated to the training task.&lt;/li&gt;&lt;li&gt;Highlights privacy risks in MLLMs due to training dynamics and partial mini-batch settings.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research Paper&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Tianjie Ju', 'Yi Hua', 'Hao Fei', 'Zhenyu Shao', 'Yubin Zheng', 'Haodong Zhao', 'Mong-Li Lee', 'Wynne Hsu', 'Zhuosheng Zhang', 'Gongshen Liu']&lt;/li&gt;&lt;li&gt;Tags: ['privacy', 'memorization', 'multimodal models', 'model probing', 'data leakage']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2503.01208</guid><pubDate>Tue, 17 Jun 2025 00:00:00 +0000</pubDate></item><item><title>MORTAR: Multi-turn Metamorphic Testing for LLM-based Dialogue Systems</title><link>https://arxiv.org/abs/2412.15557</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes MORTAR, a metamorphic testing framework for multi-turn interactions in LLM-based dialogue systems.&lt;/li&gt;&lt;li&gt;Addresses the oracle problem in multi-turn testing by automating test case generation and evaluation without relying on LLMs as judges.&lt;/li&gt;&lt;li&gt;Demonstrates improved bug detection (quantity and quality) compared to single-turn testing baselines.&lt;/li&gt;&lt;li&gt;Aims to enhance quality assurance and robustness of dialogue systems in real-world, multi-turn scenarios.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research Paper&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Guoxiang Guo', 'Aldeida Aleti', 'Neelofar Neelofar', 'Chakkrit Tantithamthavorn', 'Yuanyuan Qi', 'Tsong Yueh Chen']&lt;/li&gt;&lt;li&gt;Tags: ['robustness', 'testing', 'LLM evaluation', 'dialogue systems', 'multi-turn interaction']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2412.15557</guid><pubDate>Tue, 17 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Efficient Inference for Large Reasoning Models: A Survey</title><link>https://arxiv.org/abs/2503.23077</link><description>&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Other&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yue Liu', 'Jiaying Wu', 'Yufei He', 'Hongcheng Gao', 'Hongyu Chen', 'Baolong Bi', 'Ruihan Gong', 'Jiaheng Zhang', 'Zhiqi Huang', 'Bryan Hooi']&lt;/li&gt;&lt;li&gt;Tags: []&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2503.23077</guid><pubDate>Tue, 17 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Efficient Safety Alignment of Large Language Models via Preference Re-ranking and Representation-based Reward Modeling</title><link>https://arxiv.org/abs/2503.10093</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a new framework for safety alignment of Large Language Models (LLMs) using preference re-ranking and representation-based reward modeling.&lt;/li&gt;&lt;li&gt;Addresses the challenge of distribution shift in reinforcement learning-based safety alignment methods.&lt;/li&gt;&lt;li&gt;Introduces a computationally efficient method that leverages intrinsic safety judgment capabilities of LLMs to improve safety performance.&lt;/li&gt;&lt;li&gt;Demonstrates significant reduction in computational overhead while enhancing safety alignment through experiments and theoretical analysis.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research Paper&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Qiyuan Deng', 'Xuefeng Bai', 'Kehai Chen', 'Yaowei Wang', 'Liqiang Nie', 'Min Zhang']&lt;/li&gt;&lt;li&gt;Tags: ['LLM safety alignment', 'preference re-ranking', 'reward modeling', 'distribution shift', 'efficient safety training']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2503.10093</guid><pubDate>Tue, 17 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Self-Regularization with Sparse Autoencoders for Controllable LLM-based Classification</title><link>https://arxiv.org/abs/2502.14133</link><description>&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Other&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Xuansheng Wu', 'Wenhao Yu', 'Xiaoming Zhai', 'Ninghao Liu']&lt;/li&gt;&lt;li&gt;Tags: []&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2502.14133</guid><pubDate>Tue, 17 Jun 2025 00:00:00 +0000</pubDate></item><item><title>POROver: Improving Safety and Reducing Overrefusal in Large Language Models with Overgeneration and Preference Optimization</title><link>https://arxiv.org/abs/2410.12999</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Addresses the challenge of balancing safety and usefulness in large language models (LLMs), specifically targeting the issue of overrefusal (excessive rejection of benign prompts).&lt;/li&gt;&lt;li&gt;Proposes POROver, an alignment strategy using preference optimization and overgeneration of finetuning data from advanced teacher models to reduce overrefusal while maintaining safety.&lt;/li&gt;&lt;li&gt;Demonstrates that overgenerating completions for both general and toxic prompts can significantly improve safety and usefulness metrics.&lt;/li&gt;&lt;li&gt;Provides empirical results showing substantial improvements in the balance between safety and usefulness, with open-sourced data and code.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research Paper&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Batuhan K. Karaman', 'Ishmam Zabir', 'Alon Benhaim', 'Vishrav Chaudhary', 'Mert R. Sabuncu', 'Xia Song']&lt;/li&gt;&lt;li&gt;Tags: ['AI safety', 'alignment', 'LLM safety evaluation', 'overrefusal mitigation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2410.12999</guid><pubDate>Tue, 17 Jun 2025 00:00:00 +0000</pubDate></item><item><title>SecurityLingua: Efficient Defense of LLM Jailbreak Attacks via Security-Aware Prompt Compression</title><link>https://arxiv.org/abs/2506.12707</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes SecurityLingua, a defense mechanism against LLM jailbreak attacks using security-aware prompt compression.&lt;/li&gt;&lt;li&gt;Trains a prompt compressor to detect the true (potentially malicious) intention behind adversarial prompts.&lt;/li&gt;&lt;li&gt;Passes the detected intention to the LLM via the system prompt to enhance the model's ability to identify and block malicious requests.&lt;/li&gt;&lt;li&gt;Demonstrates effective defense with minimal computational and latency overhead compared to existing methods.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research Paper&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yucheng Li', 'Surin Ahn', 'Huiqiang Jiang', 'Amir H. Abdi', 'Yuqing Yang', 'Lili Qiu']&lt;/li&gt;&lt;li&gt;Tags: ['LLM security', 'jailbreak defense', 'prompt injection', 'adversarial prompting', 'AI safety']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.12707</guid><pubDate>Tue, 17 Jun 2025 00:00:00 +0000</pubDate></item><item><title>InfoFlood: Jailbreaking Large Language Models with Information Overload</title><link>https://arxiv.org/abs/2506.12274</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces InfoFlood, a novel jailbreak attack that uses information overload to bypass LLM safety mechanisms.&lt;/li&gt;&lt;li&gt;Demonstrates that excessive linguistic complexity can disrupt built-in safety features without traditional prompt engineering.&lt;/li&gt;&lt;li&gt;Empirically evaluates InfoFlood on multiple popular LLMs, showing significantly higher jailbreak success rates compared to baselines.&lt;/li&gt;&lt;li&gt;Finds that standard post-processing defenses are ineffective against this new attack vector, exposing a critical vulnerability.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research Paper&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Advait Yadav', 'Haibo Jin', 'Man Luo', 'Jun Zhuang', 'Haohan Wang']&lt;/li&gt;&lt;li&gt;Tags: ['LLM jailbreak', 'adversarial prompting', 'AI safety', 'security vulnerabilities', 'prompt injection']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.12274</guid><pubDate>Tue, 17 Jun 2025 00:00:00 +0000</pubDate></item><item><title>K/DA: Automated Data Generation Pipeline for Detoxifying Implicitly Offensive Language in Korean</title><link>https://arxiv.org/abs/2506.13513</link><description>&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Other&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Minkyeong Jeon', 'Hyemin Jeong', 'Yerang Kim', 'Jiyoung Kim', 'Jae Hyeon Cho', 'Byung-Jun Lee']&lt;/li&gt;&lt;li&gt;Tags: []&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.13513</guid><pubDate>Tue, 17 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Large Language Models as 'Hidden Persuaders': Fake Product Reviews are Indistinguishable to Humans and Machines</title><link>https://arxiv.org/abs/2506.13313</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Demonstrates that both humans and LLMs are unable to reliably distinguish between real and LLM-generated fake product reviews.&lt;/li&gt;&lt;li&gt;Highlights the increased risk of mechanized fraud in online review systems due to advances in generative AI.&lt;/li&gt;&lt;li&gt;Analyzes the different strategies and vulnerabilities in how humans and LLMs judge review authenticity.&lt;/li&gt;&lt;li&gt;Emphasizes the need for robust verification mechanisms to prevent fraudulent reviews.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research Paper&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Weiyao Meng', 'John Harvey', 'James Goulding', 'Chris James Carter', 'Evgeniya Lukinova', 'Andrew Smith', 'Paul Frobisher', 'Mina Forrest', 'Georgiana Nica-Avram']&lt;/li&gt;&lt;li&gt;Tags: ['LLM security', 'adversarial content', 'fraud detection', 'AI-generated misinformation', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.13313</guid><pubDate>Tue, 17 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Mitigating Safety Fallback in Editing-based Backdoor Injection on LLMs</title><link>https://arxiv.org/abs/2506.13285</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Investigates backdoor attacks on large language models (LLMs) using model editing techniques.&lt;/li&gt;&lt;li&gt;Identifies and addresses the issue of 'safety fallback,' where safety alignment mechanisms cause models to revert to refusals after initial backdoor-triggered responses.&lt;/li&gt;&lt;li&gt;Proposes DualEdit, a dual-objective framework that balances promoting attacker-desired outputs and suppressing refusal responses.&lt;/li&gt;&lt;li&gt;Demonstrates improved attack success and reduced safety fallback on safety-aligned LLMs.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research Paper&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Houcheng Jiang', 'Zetong Zhao', 'Junfeng Fang', 'Haokai Ma', 'Ruipeng Wang', 'Yang Deng', 'Xiang Wang', 'Xiangnan He']&lt;/li&gt;&lt;li&gt;Tags: ['backdoor attacks', 'LLM security', 'model editing', 'safety alignment', 'adversarial attacks']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.13285</guid><pubDate>Tue, 17 Jun 2025 00:00:00 +0000</pubDate></item><item><title>OpenUnlearning: Accelerating LLM Unlearning via Unified Benchmarking of Methods and Metrics</title><link>https://arxiv.org/abs/2506.12618</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces OpenUnlearning, a standardized benchmarking framework for LLM unlearning methods and metrics.&lt;/li&gt;&lt;li&gt;Addresses challenges in reliably measuring and comparing unlearning in large language models, which is crucial for privacy, safety, and regulatory compliance.&lt;/li&gt;&lt;li&gt;Benchmarks 9 unlearning algorithms and 16 evaluation metrics across multiple datasets, enabling comparative analysis and reproducibility.&lt;/li&gt;&lt;li&gt;Proposes a meta-evaluation benchmark to assess the robustness and faithfulness of unlearning evaluation metrics.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Vineeth Dorna', 'Anmol Mekala', 'Wenlong Zhao', 'Andrew McCallum', 'Zachary C. Lipton', 'J. Zico Kolter', 'Pratyush Maini']&lt;/li&gt;&lt;li&gt;Tags: ['LLM unlearning', 'privacy', 'model safety', 'benchmarking', 'regulatory compliance']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.12618</guid><pubDate>Tue, 17 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Detection, Classification, and Mitigation of Gender Bias in Large Language Models</title><link>https://arxiv.org/abs/2506.12527</link><description>&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Other&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Xiaoqing Cheng', 'Hongying Zan', 'Lulu Kong', 'Jinwang Song', 'Min Peng']&lt;/li&gt;&lt;li&gt;Tags: []&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.12527</guid><pubDate>Tue, 17 Jun 2025 00:00:00 +0000</pubDate></item><item><title>From Outcomes to Processes: Guiding PRM Learning from ORM for Inference-Time Alignment</title><link>https://arxiv.org/abs/2506.12446</link><description>&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Other&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Bin Xie', 'Bingbing Xu', 'Yige Yuan', 'Shengmao Zhu', 'Huawei Shen']&lt;/li&gt;&lt;li&gt;Tags: []&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.12446</guid><pubDate>Tue, 17 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Investigating the Effects of Cognitive Biases in Prompts on Large Language Model Outputs</title><link>https://arxiv.org/abs/2506.12338</link><description>&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Other&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yan Sun', 'Stanley Kok']&lt;/li&gt;&lt;li&gt;Tags: []&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.12338</guid><pubDate>Tue, 17 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Hatevolution: What Static Benchmarks Don't Tell Us</title><link>https://arxiv.org/abs/2506.12148</link><description>&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Other&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Chiara Di Bonaventura', 'Barbara McGillivray', 'Yulan He', 'Albert Mero\\~no-Pe\\~nuela']&lt;/li&gt;&lt;li&gt;Tags: []&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.12148</guid><pubDate>Tue, 17 Jun 2025 00:00:00 +0000</pubDate></item><item><title>ChatbotManip: A Dataset to Facilitate Evaluation and Oversight of Manipulative Chatbot Behaviour</title><link>https://arxiv.org/abs/2506.12090</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces ChatbotManip, a dataset for evaluating manipulative behaviors in chatbots.&lt;/li&gt;&lt;li&gt;Analyzes how LLMs can exhibit manipulative tactics when prompted, including gaslighting and fear enhancement.&lt;/li&gt;&lt;li&gt;Benchmarks manipulation detection using both small fine-tuned models and large zero-shot models.&lt;/li&gt;&lt;li&gt;Highlights implications for AI safety and the need for oversight of manipulative chatbot behaviors.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research Paper&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jack Contro', 'Simrat Deol', 'Yulan He', 'Martim Brand\\~ao']&lt;/li&gt;&lt;li&gt;Tags: ['manipulation detection', 'AI safety', 'LLM behavior', 'dataset', 'oversight']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.12090</guid><pubDate>Tue, 17 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Unsupervised anomaly detection on cybersecurity data streams: a case with BETH dataset</title><link>https://arxiv.org/abs/2503.04178</link><description>&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Other&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Evgeniy Eremin']&lt;/li&gt;&lt;li&gt;Tags: []&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2503.04178</guid><pubDate>Tue, 17 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Adversarially Robust Bloom Filters: Privacy, Reductions, and Open Problems</title><link>https://arxiv.org/abs/2501.15751</link><description>&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Other&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Hayder Tirmazi']&lt;/li&gt;&lt;li&gt;Tags: []&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2501.15751</guid><pubDate>Tue, 17 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Foundations of Large Language Models</title><link>https://arxiv.org/abs/2501.09223</link><description>&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Other&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Tong Xiao', 'Jingbo Zhu']&lt;/li&gt;&lt;li&gt;Tags: []&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2501.09223</guid><pubDate>Tue, 17 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Complexity of Injectivity and Verification of ReLU Neural Networks</title><link>https://arxiv.org/abs/2405.19805</link><description>&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Other&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Vincent Froese', 'Moritz Grillo', 'Martin Skutella']&lt;/li&gt;&lt;li&gt;Tags: []&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2405.19805</guid><pubDate>Tue, 17 Jun 2025 00:00:00 +0000</pubDate></item><item><title>AegisLLM: Scaling Agentic Systems for Self-Reflective Defense in LLM Security</title><link>https://arxiv.org/abs/2504.20965</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes AegisLLM, a multi-agent system for defending LLMs against adversarial attacks and information leakage.&lt;/li&gt;&lt;li&gt;Uses autonomous agents (orchestrator, deflector, responder, evaluator) to collaboratively ensure safe and compliant outputs.&lt;/li&gt;&lt;li&gt;Demonstrates improved robustness against jailbreaking and unlearning attacks through agentic reasoning and prompt optimization.&lt;/li&gt;&lt;li&gt;Presents comprehensive evaluations on security benchmarks, showing significant improvements over baseline and static defenses.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research Paper&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Zikui Cai', 'Shayan Shabihi', 'Bang An', 'Zora Che', 'Brian R. Bartoldson', 'Bhavya Kailkhura', 'Tom Goldstein', 'Furong Huang']&lt;/li&gt;&lt;li&gt;Tags: ['LLM security', 'adversarial defense', 'jailbreaking', 'agentic systems', 'prompt optimization']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2504.20965</guid><pubDate>Tue, 17 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Optimal Transport-Guided Safety in Temporal Difference Reinforcement Learning</title><link>https://arxiv.org/abs/2502.16328</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a reinforcement learning algorithm that incorporates optimal transport theory to quantify and minimize action uncertainty.&lt;/li&gt;&lt;li&gt;Defines safety as favoring actions with more predictable outcomes in stochastic environments.&lt;/li&gt;&lt;li&gt;Theoretically proves that the method reduces the probability of visiting unsafe states.&lt;/li&gt;&lt;li&gt;Empirically demonstrates improved safety without sacrificing performance across multiple case studies.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research Paper&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Zahra Shahrooei', 'Ali Baheri']&lt;/li&gt;&lt;li&gt;Tags: ['safe reinforcement learning', 'robustness', 'uncertainty quantification', 'risk mitigation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2502.16328</guid><pubDate>Tue, 17 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Smart IoT Security: Lightweight Machine Learning Techniques for Multi-Class Attack Detection in IoT Networks</title><link>https://arxiv.org/abs/2502.04057</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes lightweight machine learning ensemble methods for multi-class attack detection in IoT networks.&lt;/li&gt;&lt;li&gt;Evaluates various classifiers on the CICIoT 2023 dataset, covering 34 attack types across 10 categories.&lt;/li&gt;&lt;li&gt;Demonstrates high accuracy and efficiency of Decision Tree and Random Forest classifiers for IoT security.&lt;/li&gt;&lt;li&gt;Focuses on scalable solutions suitable for resource-constrained IoT devices.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research Paper&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Shahran Rahman Alve', 'Muhammad Zawad Mahmud', 'Samiha Islam', 'Md. Asaduzzaman Chowdhury', 'Jahirul Islam']&lt;/li&gt;&lt;li&gt;Tags: ['IoT security', 'attack detection', 'machine learning', 'network security', 'intrusion detection']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2502.04057</guid><pubDate>Tue, 17 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Efficient Unsupervised Shortcut Learning Detection and Mitigation in Transformers</title><link>https://arxiv.org/abs/2501.00942</link><description>&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Other&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Lukas Kuhn', 'Sari Sadiya', 'Jorg Schlotterer', 'Florian Buettner', 'Christin Seifert', 'Gemma Roig']&lt;/li&gt;&lt;li&gt;Tags: []&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2501.00942</guid><pubDate>Tue, 17 Jun 2025 00:00:00 +0000</pubDate></item><item><title>FAIR-TAT: Improving Model Fairness Using Targeted Adversarial Training</title><link>https://arxiv.org/abs/2410.23142</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a novel adversarial training method (FAIR-TAT) to improve both robustness and fairness in deep neural networks.&lt;/li&gt;&lt;li&gt;Addresses the trade-off between adversarial robustness and class-wise fairness, particularly in the context of adversarial attacks.&lt;/li&gt;&lt;li&gt;Demonstrates that targeted adversarial attacks during training can yield better fairness outcomes compared to untargeted attacks.&lt;/li&gt;&lt;li&gt;Empirical results show improved robustness and fairness under diverse adversarial threats and common corruptions.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research Paper&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Tejaswini Medi', 'Steffen Jung', 'Margret Keuper']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial training', 'robustness', 'fairness', 'adversarial attacks', 'model security']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2410.23142</guid><pubDate>Tue, 17 Jun 2025 00:00:00 +0000</pubDate></item><item><title>An Auditing Test To Detect Behavioral Shift in Language Models</title><link>https://arxiv.org/abs/2410.19406</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a method for continual auditing of language models to detect behavioral shifts after deployment or fine-tuning.&lt;/li&gt;&lt;li&gt;The auditing test compares model generations to identify changes in behavior, such as increased toxicity or altered task performance.&lt;/li&gt;&lt;li&gt;Provides theoretical guarantees for change detection and allows configurable sensitivity to behavioral changes.&lt;/li&gt;&lt;li&gt;Demonstrates effectiveness through case studies on toxicity and translation performance monitoring.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research Paper&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Leo Richter', 'Xuanli He', 'Pasquale Minervini', 'Matt J. Kusner']&lt;/li&gt;&lt;li&gt;Tags: ['behavioral shift detection', 'language model auditing', 'AI safety', 'model monitoring', 'alignment']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2410.19406</guid><pubDate>Tue, 17 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Detecting Adversarial Examples</title><link>https://arxiv.org/abs/2410.17442</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a novel, lightweight method for detecting adversarial examples in Deep Neural Networks (DNNs).&lt;/li&gt;&lt;li&gt;Uses a regression model to predict deeper-layer features from early-layer features, flagging samples with high prediction error as adversarial.&lt;/li&gt;&lt;li&gt;Demonstrates effectiveness across multiple domains, including image, video, and audio.&lt;/li&gt;&lt;li&gt;Addresses the challenge of evolving adversarial attack techniques and provides a defense mechanism compatible with various DNN architectures.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research Paper&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Furkan Mumcu', 'Yasin Yilmaz']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial examples', 'robustness', 'detection', 'AI security', 'defense mechanisms']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2410.17442</guid><pubDate>Tue, 17 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Fast Second-Order Online Kernel Learning through Incremental Matrix Sketching and Decomposition</title><link>https://arxiv.org/abs/2410.11188</link><description>&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Other&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Dongxie Wen', 'Xiao Zhang', 'Zhewei Wei', 'Chenping Hou', 'Shuai Li', 'Weinan Zhang']&lt;/li&gt;&lt;li&gt;Tags: []&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2410.11188</guid><pubDate>Tue, 17 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Align-then-Unlearn: Embedding Alignment for LLM Unlearning</title><link>https://arxiv.org/abs/2506.13181</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a novel framework (Align-then-Unlearn) for selectively removing specific knowledge from large language models (LLMs) to address privacy and ethical concerns.&lt;/li&gt;&lt;li&gt;Performs unlearning in the semantic embedding space rather than at the output token level, aiming for more robust and complete forgetting.&lt;/li&gt;&lt;li&gt;Demonstrates that the method can effectively remove targeted knowledge with minimal impact on overall model utility.&lt;/li&gt;&lt;li&gt;Addresses vulnerabilities related to data retention and potential privacy breaches in LLMs.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research Paper&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Philipp Spohn', 'Leander Girrbach', 'Jessica Bader', 'Zeynep Akata']&lt;/li&gt;&lt;li&gt;Tags: ['unlearning', 'privacy', 'LLM security', 'data removal', 'embedding alignment']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.13181</guid><pubDate>Tue, 17 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Position: Certified Robustness Does Not (Yet) Imply Model Security</title><link>https://arxiv.org/abs/2506.13024</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Argues that certified robustness in AI does not currently guarantee model security against adversarial attacks.&lt;/li&gt;&lt;li&gt;Identifies gaps such as the lack of clear evaluation criteria for certification schemes and the risks of overestimating 'guaranteed' robustness.&lt;/li&gt;&lt;li&gt;Highlights potential security risks arising from misaligned expectations about certified robustness.&lt;/li&gt;&lt;li&gt;Proposes steps for the research community to address these challenges and improve practical security.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Position Paper&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Andrew C. Cullen', 'Paul Montague', 'Sarah M. Erfani', 'Benjamin I. P. Rubinstein']&lt;/li&gt;&lt;li&gt;Tags: ['certified robustness', 'adversarial examples', 'AI security', 'robustness evaluation', 'model security']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.13024</guid><pubDate>Tue, 17 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Rectifying Privacy and Efficacy Measurements in Machine Unlearning: A New Inference Attack Perspective</title><link>https://arxiv.org/abs/2506.13009</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Identifies key shortcomings in current evaluation frameworks for machine unlearning, particularly regarding privacy and efficacy measurements.&lt;/li&gt;&lt;li&gt;Proposes RULI, a new evaluation framework that uses a dual-objective inference attack to assess both unlearning efficacy and privacy risks at the per-sample level.&lt;/li&gt;&lt;li&gt;Demonstrates that state-of-the-art unlearning methods have significant privacy vulnerabilities, as revealed by the proposed attack methodology.&lt;/li&gt;&lt;li&gt;Validates the framework on both image and text data, providing a scalable and rigorous approach to evaluating unlearning techniques.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research Paper&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Nima Naderloui', 'Shenao Yan', 'Binghui Wang', 'Jie Fu', 'Wendy Hui Wang', 'Weiran Liu', 'Yuan Hong']&lt;/li&gt;&lt;li&gt;Tags: ['machine unlearning', 'privacy attacks', 'inference attacks', 'evaluation framework', 'AI security']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.13009</guid><pubDate>Tue, 17 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Intriguing Frequency Interpretation of Adversarial Robustness for CNNs and ViTs</title><link>https://arxiv.org/abs/2506.12875</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Analyzes the frequency domain characteristics of adversarial examples in image classification models (CNNs and ViTs).&lt;/li&gt;&lt;li&gt;Finds that different model architectures (CNNs vs. ViTs) are vulnerable to adversarial perturbations in different frequency bands.&lt;/li&gt;&lt;li&gt;Provides insights and proposals for improving model robustness against adversarial attacks based on frequency analysis.&lt;/li&gt;&lt;li&gt;Directly addresses adversarial robustness, which is a core AI security and safety concern.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research Paper&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Lu Chen', 'Han Yang', 'Hu Wang', 'Yuxin Cao', 'Shaofeng Li', 'Yuan Luo']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial robustness', 'frequency analysis', 'CNN', 'Vision Transformer', 'image security']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.12875</guid><pubDate>Tue, 17 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Transforming Chatbot Text: A Sequence-to-Sequence Approach</title><link>https://arxiv.org/abs/2506.12843</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a method to adversarially transform GPT-generated text using sequence-to-sequence models to evade AI-generated text detectors.&lt;/li&gt;&lt;li&gt;Demonstrates that these transformations reduce the accuracy of classifiers trained to distinguish between human and AI-generated text.&lt;/li&gt;&lt;li&gt;Shows that retraining classifiers on transformed data restores detection accuracy, highlighting an arms race between attackers and defenders.&lt;/li&gt;&lt;li&gt;Contributes to understanding both attack (evasion) and defense (robust detection) strategies in AI-generated text detection.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research Paper&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Natesh Reddy', 'Mark Stamp']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial attacks', 'text detection', 'AI-generated content', 'robustness', 'sequence-to-sequence']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.12843</guid><pubDate>Tue, 17 Jun 2025 00:00:00 +0000</pubDate></item><item><title>On the existence of consistent adversarial attacks in high-dimensional linear classification</title><link>https://arxiv.org/abs/2506.12454</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Investigates the fundamental distinction between adversarial attacks and misclassification in high-dimensional linear classifiers.&lt;/li&gt;&lt;li&gt;Introduces a new error metric to quantify vulnerability to consistent adversarial attacks (label-preserving perturbations).&lt;/li&gt;&lt;li&gt;Provides rigorous theoretical analysis of adversarial vulnerability in overparameterized models.&lt;/li&gt;&lt;li&gt;Finds that overparameterization increases susceptibility to adversarial attacks, offering insights into model robustness.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research Paper&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Matteo Vilucchio', "Lenka Zdeborov\\'a", 'Bruno Loureiro']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial attacks', 'robustness', 'high-dimensional models', 'theoretical analysis']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.12454</guid><pubDate>Tue, 17 Jun 2025 00:00:00 +0000</pubDate></item><item><title>UCD: Unlearning in LLMs via Contrastive Decoding</title><link>https://arxiv.org/abs/2506.12097</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a new inference-time unlearning algorithm for LLMs using contrastive decoding.&lt;/li&gt;&lt;li&gt;Aims to remove sensitive or undesirable information from LLMs while preserving overall utility.&lt;/li&gt;&lt;li&gt;Evaluates the method on established unlearning benchmarks (TOFU and MUSE), showing improved tradeoff between unlearning and performance.&lt;/li&gt;&lt;li&gt;Addresses the challenge of efficiently and practically unlearning concepts in large-scale language models.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research Paper&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Vinith M. Suriyakumar', 'Ayush Sekhari', 'Ashia Wilson']&lt;/li&gt;&lt;li&gt;Tags: ['machine unlearning', 'LLM security', 'privacy', 'model editing', 'safe AI']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.12097</guid><pubDate>Tue, 17 Jun 2025 00:00:00 +0000</pubDate></item><item><title>On Monotonicity in AI Alignment</title><link>https://arxiv.org/abs/2506.08998</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Investigates monotonicity properties in comparison-based preference learning, a core technique in AI alignment.&lt;/li&gt;&lt;li&gt;Analyzes why preference learning methods (like DPO, GPO, GBT) can behave counterintuitively, sometimes reducing the likelihood of preferred outputs.&lt;/li&gt;&lt;li&gt;Provides formal definitions and sufficient conditions for monotonicity, offering tools to evaluate and improve alignment methods.&lt;/li&gt;&lt;li&gt;Aims to clarify limitations and guide the development of more trustworthy AI alignment algorithms.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research Paper&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Gilles Bareilles', 'Julien Fageot', 'L\\^e-Nguy\\^en Hoang', 'Peva Blanchard', 'Wassim Bouaziz', "S\\'ebastien Rouault", 'El-Mahdi El-Mhamdi']&lt;/li&gt;&lt;li&gt;Tags: ['AI alignment', 'preference learning', 'robustness', 'trustworthy AI', 'safety evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.08998</guid><pubDate>Tue, 17 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Sharpness-Aware Machine Unlearning</title><link>https://arxiv.org/abs/2506.13715</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Investigates the use of Sharpness-Aware Minimization (SAM) in machine unlearning, focusing on how it affects the separation of forget and retain signals.&lt;/li&gt;&lt;li&gt;Proposes Sharp MinMax, a method that splits model training to enhance unlearning performance and reduce feature entanglement.&lt;/li&gt;&lt;li&gt;Demonstrates improved resistance to membership inference attacks, a key privacy threat in AI systems.&lt;/li&gt;&lt;li&gt;Empirical results show that SAM-based approaches can enhance various unlearning methods and improve robustness against data memorization.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research Paper&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Haoran Tang', 'Rajiv Khanna']&lt;/li&gt;&lt;li&gt;Tags: ['machine unlearning', 'membership inference', 'privacy attacks', 'robustness', 'sharpness-aware minimization']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.13715</guid><pubDate>Tue, 17 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Calibrated Predictive Lower Bounds on Time-to-Unsafe-Sampling in LLMs</title><link>https://arxiv.org/abs/2506.13593</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a framework to estimate the number of LLM generations required before an unsafe (e.g., toxic) response occurs.&lt;/li&gt;&lt;li&gt;Introduces a calibrated lower predictive bound (LPB) on time-to-unsafe-sampling using survival analysis and conformal prediction.&lt;/li&gt;&lt;li&gt;Develops an adaptive, per-prompt sampling strategy to improve statistical efficiency in estimating safety risks.&lt;/li&gt;&lt;li&gt;Demonstrates practical utility for safety risk assessment in generative AI models through experiments.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research Paper&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Hen Davidov', 'Gilad Freidkin', 'Shai Feldman', 'Yaniv Romano']&lt;/li&gt;&lt;li&gt;Tags: ['LLM safety', 'risk assessment', 'unsafe outputs', 'safety evaluation', 'generative AI']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.13593</guid><pubDate>Tue, 17 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Perfect Privacy for Discriminator-Based Byzantine-Resilient Federated Learning</title><link>https://arxiv.org/abs/2506.13561</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes two federated learning schemes (ByITFL and LoByITFL) that enhance resilience against Byzantine (malicious) users.&lt;/li&gt;&lt;li&gt;Ensures perfect information-theoretic privacy for user data against eavesdroppers.&lt;/li&gt;&lt;li&gt;Introduces a discriminator function to mitigate the impact of corrupt user contributions.&lt;/li&gt;&lt;li&gt;Provides theoretical and experimental validation of privacy, security, and convergence guarantees.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research Paper&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yue Xia', 'Christoph Hofmeister', 'Maximilian Egger', 'Rawad Bitar']&lt;/li&gt;&lt;li&gt;Tags: ['federated learning', 'Byzantine resilience', 'privacy', 'security', 'information-theoretic privacy']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.13561</guid><pubDate>Tue, 17 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Uncertainty-Aware Graph Neural Networks: A Multi-Hop Evidence Fusion Approach</title><link>https://arxiv.org/abs/2506.13083</link><description>&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Other&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Qingfeng Chen', 'Shiyuan Li', 'Yixin Liu', 'Shirui Pan', 'Geoffrey I. Webb', 'Shichao Zhang']&lt;/li&gt;&lt;li&gt;Tags: []&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.13083</guid><pubDate>Tue, 17 Jun 2025 00:00:00 +0000</pubDate></item><item><title>The Space Complexity of Learning-Unlearning Algorithms</title><link>https://arxiv.org/abs/2506.13048</link><description>&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Other&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yeshwanth Cherapanamjeri', 'Sumegha Garg', 'Nived Rajaraman', 'Ayush Sekhari', 'Abhishek Shetty']&lt;/li&gt;&lt;li&gt;Tags: []&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.13048</guid><pubDate>Tue, 17 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Differentially Private Bilevel Optimization: Efficient Algorithms with Near-Optimal Rates</title><link>https://arxiv.org/abs/2506.12994</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes efficient algorithms for differentially private bilevel optimization, relevant for privacy in hierarchical machine learning tasks.&lt;/li&gt;&lt;li&gt;Provides near-optimal upper and lower bounds on excess risk for both pure and approximate differential privacy in bilevel settings.&lt;/li&gt;&lt;li&gt;Introduces new methods for log-concave sampling under inexact function evaluations, which can impact secure and private optimization.&lt;/li&gt;&lt;li&gt;Addresses both convex and non-convex settings, with privacy guarantees that do not depend on the inner problem's dimension.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research Paper&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Andrew Lowy', 'Daogao Liu']&lt;/li&gt;&lt;li&gt;Tags: ['differential privacy', 'bilevel optimization', 'privacy-preserving machine learning', 'algorithmic robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.12994</guid><pubDate>Tue, 17 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Jailbreak Strength and Model Similarity Predict Transferability</title><link>https://arxiv.org/abs/2506.12913</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Investigates the transferability of jailbreak attacks between different AI models.&lt;/li&gt;&lt;li&gt;Finds that jailbreak transfer success is predicted by jailbreak strength and model similarity.&lt;/li&gt;&lt;li&gt;Demonstrates that distilling from a target model into a source model increases attack transferability.&lt;/li&gt;&lt;li&gt;Suggests that fundamental flaws in model representations, not just safety training, underlie jailbreak vulnerabilities.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research Paper&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Rico Angell', 'Jannik Brinkmann', 'He He']&lt;/li&gt;&lt;li&gt;Tags: ['jailbreaking', 'red teaming', 'transferability', 'AI safety', 'model vulnerability']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.12913</guid><pubDate>Tue, 17 Jun 2025 00:00:00 +0000</pubDate></item><item><title>TrojanTO: Action-Level Backdoor Attacks against Trajectory Optimization Models</title><link>https://arxiv.org/abs/2506.12815</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces TrojanTO, the first action-level backdoor attack targeting trajectory optimization (TO) models in offline reinforcement learning.&lt;/li&gt;&lt;li&gt;Demonstrates that traditional reward-based backdoor attacks are ineffective against TO models due to their sequence modeling nature.&lt;/li&gt;&lt;li&gt;Proposes novel attack techniques including alternating training and precise trajectory poisoning to enhance attack effectiveness and stealth.&lt;/li&gt;&lt;li&gt;Extensive experiments show TrojanTO's effectiveness and scalability across various TO model architectures with a minimal attack budget.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research Paper&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yang Dai', 'Oubo Ma', 'Longfei Zhang', 'Xingxing Liang', 'Xiaochun Cao', 'Shouling Ji', 'Jiaheng Zhang', 'Jincai Huang', 'Li Shen']&lt;/li&gt;&lt;li&gt;Tags: ['backdoor attacks', 'reinforcement learning security', 'trajectory optimization', 'adversarial machine learning', 'model robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.12815</guid><pubDate>Tue, 17 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Free Privacy Protection for Wireless Federated Learning: Enjoy It or Suffer from It?</title><link>https://arxiv.org/abs/2506.12749</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a novel bit-flipping differential privacy mechanism for wireless federated learning (WFL) that leverages inherent communication noise.&lt;/li&gt;&lt;li&gt;Introduces a floating-point-to-fixed-point conversion to avoid catastrophic bit errors and enhance privacy protection.&lt;/li&gt;&lt;li&gt;Analyzes and proves the privacy guarantees of the mechanism using (λ,ε)-Rényi differential privacy.&lt;/li&gt;&lt;li&gt;Demonstrates through experiments that the method preserves privacy and model convergence, outperforming traditional Gaussian noise mechanisms.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research Paper&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Weicai Li', 'Tiejun Lv', 'Xiyu Zhao', 'Xin Yuan', 'Wei Ni']&lt;/li&gt;&lt;li&gt;Tags: ['federated learning', 'privacy', 'differential privacy', 'wireless security', 'adversarial noise']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.12749</guid><pubDate>Tue, 17 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Existence of Adversarial Examples for Random Convolutional Networks via Isoperimetric Inequalities on $\mathbb{so}(d)$</title><link>https://arxiv.org/abs/2506.12613</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Demonstrates the existence of adversarial examples in random convolutional networks.&lt;/li&gt;&lt;li&gt;Uses isoperimetric inequalities on the special orthogonal group to establish results.&lt;/li&gt;&lt;li&gt;Extends previous work from fully connected networks to convolutional architectures.&lt;/li&gt;&lt;li&gt;Highlights fundamental vulnerabilities in neural networks to adversarial attacks.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research Paper&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Amit Daniely']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial examples', 'robustness', 'convolutional networks', 'AI security']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.12613</guid><pubDate>Tue, 17 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Beyond Laplace and Gaussian: Exploring the Generalized Gaussian Mechanism for Private Machine Learning</title><link>https://arxiv.org/abs/2506.12553</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Investigates the Generalized Gaussian mechanism as a new family of noise distributions for differential privacy in machine learning.&lt;/li&gt;&lt;li&gt;Proves that all Generalized Gaussian mechanisms satisfy differential privacy and extends privacy accounting tools to support them.&lt;/li&gt;&lt;li&gt;Applies the mechanism to private machine learning methods (PATE and DP-SGD) and empirically evaluates its impact on utility and privacy.&lt;/li&gt;&lt;li&gt;Finds that the commonly used Gaussian mechanism is nearly optimal, providing justification for its widespread use.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research Paper&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Roy Rinberg', 'Ilia Shumailov', 'Vikrant Singhal', 'Rachel Cummings', 'Nicolas Papernot']&lt;/li&gt;&lt;li&gt;Tags: ['differential privacy', 'privacy mechanisms', 'private machine learning', 'privacy accounting']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.12553</guid><pubDate>Tue, 17 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Relative Entropy Regularized Reinforcement Learning for Efficient Encrypted Policy Synthesis</title><link>https://arxiv.org/abs/2506.12358</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a method for privacy-preserving reinforcement learning using fully homomorphic encryption (FHE).&lt;/li&gt;&lt;li&gt;Analyzes the integration of FHE with relative-entropy-regularized reinforcement learning for encrypted policy synthesis.&lt;/li&gt;&lt;li&gt;Addresses convergence and error propagation in the presence of encryption-induced errors.&lt;/li&gt;&lt;li&gt;Demonstrates the approach with theoretical analysis and numerical simulations.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research Paper&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jihoon Suh', 'Yeongjun Jang', 'Kaoru Teranishi', 'Takashi Tanaka']&lt;/li&gt;&lt;li&gt;Tags: ['privacy-preserving AI', 'homomorphic encryption', 'reinforcement learning', 'secure computation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.12358</guid><pubDate>Tue, 17 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Learning Causality for Modern Machine Learning</title><link>https://arxiv.org/abs/2506.12226</link><description>&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Other&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yongqiang Chen']&lt;/li&gt;&lt;li&gt;Tags: []&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.12226</guid><pubDate>Tue, 17 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Agentic Surgical AI: Surgeon Style Fingerprinting and Privacy Risk Quantification via Discrete Diffusion in a Vision-Language-Action Framework</title><link>https://arxiv.org/abs/2506.08185</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a vision-language-action framework for modeling surgeon-specific behavior in robotic surgery using discrete diffusion.&lt;/li&gt;&lt;li&gt;Introduces personalized embeddings to capture individual surgeon styles while attempting to preserve privacy.&lt;/li&gt;&lt;li&gt;Performs membership inference attacks to quantify privacy risks associated with personalized embeddings.&lt;/li&gt;&lt;li&gt;Finds a trade-off between improved task performance and increased vulnerability to identity leakage, highlighting privacy risks in personalized AI systems.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research Paper&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Huixin Zhan', 'Jason H. Moore']&lt;/li&gt;&lt;li&gt;Tags: ['privacy attacks', 'membership inference', 'identity leakage', 'multimodal AI security', 'personalization risk']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.08185</guid><pubDate>Tue, 17 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Synthesize Privacy-Preserving High-Resolution Images via Private Textual Intermediaries</title><link>https://arxiv.org/abs/2506.07555</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a novel method (SPTI) for generating high-resolution, differentially private synthetic images using private textual intermediaries.&lt;/li&gt;&lt;li&gt;Shifts the privacy-preserving challenge from the image domain to the text domain by summarizing images into text, applying DP text generation, and reconstructing images.&lt;/li&gt;&lt;li&gt;Demonstrates significant improvements in image quality (FID scores) over previous differentially private image synthesis methods.&lt;/li&gt;&lt;li&gt;Enables sharing and analysis of sensitive visual data without compromising individual privacy.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research Paper&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Haoxiang Wang', 'Zinan Lin', 'Da Yu', 'Huishuai Zhang']&lt;/li&gt;&lt;li&gt;Tags: ['differential privacy', 'privacy-preserving AI', 'synthetic data generation', 'image synthesis']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.07555</guid><pubDate>Tue, 17 Jun 2025 00:00:00 +0000</pubDate></item><item><title>SafeGenBench: A Benchmark Framework for Security Vulnerability Detection in LLM-Generated Code</title><link>https://arxiv.org/abs/2506.05692</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces SafeGenBench, a benchmark for evaluating security vulnerabilities in code generated by large language models (LLMs).&lt;/li&gt;&lt;li&gt;Covers a wide range of software development scenarios and vulnerability types.&lt;/li&gt;&lt;li&gt;Proposes an automatic evaluation framework using both static application security testing (SAST) and LLM-based judging.&lt;/li&gt;&lt;li&gt;Empirically evaluates state-of-the-art LLMs, revealing deficiencies in secure code generation and providing insights for improvement.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Xinghang Li', 'Jingzhe Ding', 'Chao Peng', 'Bing Zhao', 'Xiang Gao', 'Hongwan Gao', 'Xinchen Gu']&lt;/li&gt;&lt;li&gt;Tags: ['LLM security', 'code generation', 'vulnerability detection', 'benchmarking', 'secure AI']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.05692</guid><pubDate>Tue, 17 Jun 2025 00:00:00 +0000</pubDate></item><item><title>SOSBENCH: Benchmarking Safety Alignment on Scientific Knowledge</title><link>https://arxiv.org/abs/2505.21605</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces SOSBench, a benchmark for evaluating LLM safety alignment in high-risk scientific domains.&lt;/li&gt;&lt;li&gt;Focuses on realistic, regulation-grounded misuse scenarios in chemistry, biology, medicine, pharmacology, physics, and psychology.&lt;/li&gt;&lt;li&gt;Demonstrates that advanced LLMs still frequently provide harmful, policy-violating content in response to hazardous prompts.&lt;/li&gt;&lt;li&gt;Highlights significant deficiencies in current safety alignment and the urgent need for improved safeguards in LLM deployment.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Fengqing Jiang', 'Fengbo Ma', 'Zhangchen Xu', 'Yuetai Li', 'Bhaskar Ramasubramanian', 'Luyao Niu', 'Bo Li', 'Xianyan Chen', 'Zhen Xiang', 'Radha Poovendran']&lt;/li&gt;&lt;li&gt;Tags: ['LLM safety', 'benchmarking', 'harmful outputs', 'alignment evaluation', 'scientific misuse']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2505.21605</guid><pubDate>Tue, 17 Jun 2025 00:00:00 +0000</pubDate></item><item><title>AgentVigil: Generic Black-Box Red-teaming for Indirect Prompt Injection against LLM Agents</title><link>https://arxiv.org/abs/2505.05849</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces AgentVigil, a black-box fuzzing framework for red-teaming LLM agents against indirect prompt injection attacks.&lt;/li&gt;&lt;li&gt;Demonstrates the framework's effectiveness in automatically discovering and exploiting vulnerabilities in agent-based LLM systems.&lt;/li&gt;&lt;li&gt;Achieves high success rates on public benchmarks and shows strong transferability to unseen tasks and models.&lt;/li&gt;&lt;li&gt;Validates attacks in real-world scenarios, successfully manipulating agents to perform harmful actions such as visiting malicious URLs.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research Paper&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Zhun Wang', 'Vincent Siu', 'Zhe Ye', 'Tianneng Shi', 'Yuzhou Nie', 'Xuandong Zhao', 'Chenguang Wang', 'Wenbo Guo', 'Dawn Song']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'prompt injection', 'security vulnerabilities', 'adversarial attacks', 'agent-based systems']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2505.05849</guid><pubDate>Tue, 17 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Benchmarking Practices in LLM-driven Offensive Security: Testbeds, Metrics, and Experiment Design</title><link>https://arxiv.org/abs/2504.10112</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Analyzes benchmarking practices for evaluating LLM-driven offensive security tools.&lt;/li&gt;&lt;li&gt;Reviews 19 research papers and 18 prototypes focused on LLM-enabled cyberattacks.&lt;/li&gt;&lt;li&gt;Provides recommendations for improving testbeds, baselines, and evaluation metrics in LLM-based offensive security research.&lt;/li&gt;&lt;li&gt;Highlights the gap between CTF-based evaluations and real-world penetration testing scenarios.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Survey/Review&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Andreas Happe', 'J\\"urgen Cito']&lt;/li&gt;&lt;li&gt;Tags: ['LLM security', 'offensive security', 'benchmarking', 'penetration testing', 'evaluation metrics']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2504.10112</guid><pubDate>Tue, 17 Jun 2025 00:00:00 +0000</pubDate></item><item><title>ShED-HD: A Shannon Entropy Distribution Framework for Lightweight Hallucination Detection on Edge Devices</title><link>https://arxiv.org/abs/2503.18242</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes ShED-HD, a lightweight framework for detecting hallucinations in LLM outputs using entropy distribution patterns.&lt;/li&gt;&lt;li&gt;Focuses on efficient, single-pass hallucination detection suitable for edge devices with limited resources.&lt;/li&gt;&lt;li&gt;Demonstrates improved performance over other efficient methods on multiple datasets, especially in out-of-distribution scenarios.&lt;/li&gt;&lt;li&gt;Aims to enhance the trustworthiness and credibility of LLM-generated content in high-stakes, resource-constrained environments.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research Paper&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Aneesh Vathul', 'Daniel Lee', 'Sheryl Chen', 'Arthi Tasmia']&lt;/li&gt;&lt;li&gt;Tags: ['hallucination detection', 'LLM safety', 'robustness', 'trustworthy AI', 'edge AI']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2503.18242</guid><pubDate>Tue, 17 Jun 2025 00:00:00 +0000</pubDate></item><item><title>C2-DPO: Constrained Controlled Direct Preference Optimization</title><link>https://arxiv.org/abs/2502.17507</link><description>&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Other&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Kavosh Asadi', 'Julien Han', 'Idan Pipano', 'Xingzi Xu', 'Dominique Perrault-Joncas', 'Shoham Sabach', 'Karim Bouyarmane', 'Mohammad Ghavamzadeh']&lt;/li&gt;&lt;li&gt;Tags: []&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2502.17507</guid><pubDate>Tue, 17 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Less is More: Improving LLM Alignment via Preference Data Selection</title><link>https://arxiv.org/abs/2502.14560</link><description>&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Other&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Xun Deng', 'Han Zhong', 'Rui Ai', 'Fuli Feng', 'Zheng Wang', 'Xiangnan He']&lt;/li&gt;&lt;li&gt;Tags: []&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2502.14560</guid><pubDate>Tue, 17 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Truth Knows No Language: Evaluating Truthfulness Beyond English</title><link>https://arxiv.org/abs/2502.09387</link><description>&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Other&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Blanca Calvo Figueras', 'Eneko Sagarzazu', 'Julen Etxaniz', 'Jeremy Barnes', 'Pablo Gamallo', 'Iria De Dios Flores', 'Rodrigo Agerri']&lt;/li&gt;&lt;li&gt;Tags: []&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2502.09387</guid><pubDate>Tue, 17 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Speak Easy: Eliciting Harmful Jailbreaks from LLMs with Simple Interactions</title><link>https://arxiv.org/abs/2502.04322</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Investigates the effectiveness of simple, multi-step, multilingual jailbreak attacks on LLMs.&lt;/li&gt;&lt;li&gt;Introduces HarmScore, a metric to quantify how actionable and informative harmful LLM responses are.&lt;/li&gt;&lt;li&gt;Proposes Speak Easy, a framework for eliciting harmful outputs from LLMs using non-technical interactions.&lt;/li&gt;&lt;li&gt;Demonstrates increased attack success rates and highlights overlooked vulnerabilities in LLM safety.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research Paper&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yik Siu Chan', 'Narutatsu Ri', 'Yuxin Xiao', 'Marzyeh Ghassemi']&lt;/li&gt;&lt;li&gt;Tags: ['LLM jailbreak', 'adversarial prompting', 'AI safety', 'harmful outputs', 'security evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2502.04322</guid><pubDate>Tue, 17 Jun 2025 00:00:00 +0000</pubDate></item><item><title>An Investigation into Value Misalignment in LLM-Generated Texts for Cultural Heritage</title><link>https://arxiv.org/abs/2501.02039</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Systematically investigates value misalignment in LLM-generated texts within the context of cultural heritage.&lt;/li&gt;&lt;li&gt;Evaluates five open-source LLMs across 1066 tasks to identify types and rates of cultural value misalignments.&lt;/li&gt;&lt;li&gt;Introduces a benchmark dataset and evaluation workflow for assessing cultural alignment in LLM outputs.&lt;/li&gt;&lt;li&gt;Finds that over 65% of generated texts exhibit notable cultural misalignments, highlighting risks in LLM deployment for cultural heritage tasks.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Fan Bu', 'Zheng Wang', 'Siyi Wang', 'Ziyao Liu']&lt;/li&gt;&lt;li&gt;Tags: ['value alignment', 'LLM safety', 'harmful outputs', 'risk assessment', 'benchmarking']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2501.02039</guid><pubDate>Tue, 17 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Immune: Improving Safety Against Jailbreaks in Multi-modal LLMs via Inference-Time Alignment</title><link>https://arxiv.org/abs/2411.18688</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Identifies a safety gap in current multimodal LLMs, showing that training-time alignment is insufficient to prevent jailbreak attacks.&lt;/li&gt;&lt;li&gt;Proposes 'Immune', an inference-time defense framework using a safe reward model and controlled decoding to mitigate jailbreaks.&lt;/li&gt;&lt;li&gt;Provides mathematical analysis explaining why the proposed method improves safety against jailbreaks.&lt;/li&gt;&lt;li&gt;Demonstrates significant reduction in jailbreak attack success rates on recent multimodal LLMs through extensive benchmarking.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research Paper&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Soumya Suvra Ghosal', 'Souradip Chakraborty', 'Vaibhav Singh', 'Tianrui Guan', 'Mengdi Wang', 'Alvaro Velasquez', 'Ahmad Beirami', 'Furong Huang', 'Dinesh Manocha', 'Amrit Singh Bedi']&lt;/li&gt;&lt;li&gt;Tags: ['jailbreak defense', 'multimodal LLMs', 'inference-time alignment', 'AI safety', 'adversarial robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2411.18688</guid><pubDate>Tue, 17 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Compositional Shielding and Reinforcement Learning for Multi-Agent Systems</title><link>https://arxiv.org/abs/2410.10460</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a novel compositional shielding approach to ensure safety in multi-agent reinforcement learning systems.&lt;/li&gt;&lt;li&gt;Addresses the scalability issue of safety shields by decomposing global safety specifications into local obligations using assume-guarantee reasoning.&lt;/li&gt;&lt;li&gt;Demonstrates that applying these shields during reinforcement learning improves both safety and policy quality.&lt;/li&gt;&lt;li&gt;Presents case studies showing significant reductions in computation time and improved learning convergence.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research Paper&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Asger Horn Brorholt', 'Kim Guldstrand Larsen', 'Christian Schilling']&lt;/li&gt;&lt;li&gt;Tags: ['AI safety', 'reinforcement learning', 'multi-agent systems', 'safe RL', 'shielding']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2410.10460</guid><pubDate>Tue, 17 Jun 2025 00:00:00 +0000</pubDate></item><item><title>How Much Can We Forget about Data Contamination?</title><link>https://arxiv.org/abs/2410.03249</link><description>&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Other&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Sebastian Bordt', 'Suraj Srinivas', 'Valentyn Boreiko', 'Ulrike von Luxburg']&lt;/li&gt;&lt;li&gt;Tags: []&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2410.03249</guid><pubDate>Tue, 17 Jun 2025 00:00:00 +0000</pubDate></item><item><title>On the Feasibility of Fully AI-automated Vishing Attacks</title><link>https://arxiv.org/abs/2409.13793</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Investigates the feasibility of fully AI-automated vishing (voice phishing) attacks using LLMs and speech technologies.&lt;/li&gt;&lt;li&gt;Introduces ViKing, an AI-powered vishing system that automates phone-based social engineering using publicly available AI tools.&lt;/li&gt;&lt;li&gt;Conducts a controlled experiment with 240 participants, demonstrating the effectiveness of AI-driven vishing in eliciting sensitive information.&lt;/li&gt;&lt;li&gt;Highlights the security risks posed by AI-automated social engineering and the potential for such tools to be used by malicious actors.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research Paper&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jo\\~ao Figueiredo', 'Afonso Carvalho', 'Daniel Castro', 'Daniel Gon\\c{c}alves', 'Nuno Santos']&lt;/li&gt;&lt;li&gt;Tags: ['AI security', 'social engineering', 'LLM vulnerabilities', 'voice phishing', 'adversarial attacks']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2409.13793</guid><pubDate>Tue, 17 Jun 2025 00:00:00 +0000</pubDate></item><item><title>OR-Bench: An Over-Refusal Benchmark for Large Language Models</title><link>https://arxiv.org/abs/2405.20947</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces OR-Bench, a large-scale benchmark to systematically evaluate over-refusal behavior in LLMs.&lt;/li&gt;&lt;li&gt;Proposes a novel method for automatically generating prompts that elicit over-refusal from LLMs.&lt;/li&gt;&lt;li&gt;Benchmarks 32 popular LLMs across 8 model families for their tendency to over-refuse harmless prompts.&lt;/li&gt;&lt;li&gt;Aims to support the development of better safety-aligned models by providing open datasets and code.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Justin Cui', 'Wei-Lin Chiang', 'Ion Stoica', 'Cho-Jui Hsieh']&lt;/li&gt;&lt;li&gt;Tags: ['AI safety', 'LLM evaluation', 'over-refusal', 'alignment', 'benchmarking']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2405.20947</guid><pubDate>Tue, 17 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Personalized Wireless Federated Learning for Large Language Models</title><link>https://arxiv.org/abs/2404.13238</link><description>&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Other&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Feibo Jiang', 'Li Dong', 'Siwei Tu', 'Yubo Peng', 'Kezhi Wang', 'Kun Yang', 'Cunhua Pan', 'Dusit Niyato']&lt;/li&gt;&lt;li&gt;Tags: []&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2404.13238</guid><pubDate>Tue, 17 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Control Tax: The Price of Keeping AI in Check</title><link>https://arxiv.org/abs/2506.05296</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces the concept of 'Control Tax'—the operational and financial cost of implementing AI control and oversight mechanisms.&lt;/li&gt;&lt;li&gt;Presents a theoretical framework linking classifier performance to safety assurances in AI systems.&lt;/li&gt;&lt;li&gt;Evaluates language models in adversarial settings, specifically focusing on backdoor attacks and monitoring for vulnerabilities.&lt;/li&gt;&lt;li&gt;Provides empirical cost estimates and strategies for optimizing safety protocols within practical constraints like auditing budgets.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research Paper&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Mikhail Terekhov', 'Zhen Ning David Liu', 'Caglar Gulcehre', 'Samuel Albanie']&lt;/li&gt;&lt;li&gt;Tags: ['AI control', 'backdoor attacks', 'safety evaluation', 'cost analysis', 'adversarial robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.05296</guid><pubDate>Tue, 17 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Scalable Precise Computation of Shannon Entropy</title><link>https://arxiv.org/abs/2502.01160</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Presents a scalable and precise tool (PSE) for computing Shannon entropy in quantitative information flow (QIF) analyses.&lt;/li&gt;&lt;li&gt;Focuses on measuring confidential information leakage from programs, a key concern in AI and software security.&lt;/li&gt;&lt;li&gt;Introduces a novel knowledge compilation language (귚ND) and optimizes model counting for efficient entropy computation.&lt;/li&gt;&lt;li&gt;Demonstrates significant performance improvements over existing tools in experimental benchmarks.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research Paper&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yong Lai', 'Haolong Tong', 'Zhenghang Xu', 'Minghao Yin']&lt;/li&gt;&lt;li&gt;Tags: ['quantitative information flow', 'information leakage', 'Shannon entropy', 'AI security', 'privacy']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2502.01160</guid><pubDate>Tue, 17 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Stepwise Reasoning Error Disruption Attack of LLMs</title><link>https://arxiv.org/abs/2412.11934</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces the Stepwise rEasoning Error Disruption (SEED) attack, which subtly injects errors into LLM reasoning steps.&lt;/li&gt;&lt;li&gt;Demonstrates that SEED can mislead LLMs into producing incorrect reasoning and answers without altering instructions.&lt;/li&gt;&lt;li&gt;Shows SEED's effectiveness across multiple datasets and models, highlighting vulnerabilities in LLM reasoning robustness.&lt;/li&gt;&lt;li&gt;Emphasizes the need for improved safety and robustness in LLM reasoning processes for practical applications.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research Paper&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jingyu Peng', 'Maolin Wang', 'Xiangyu Zhao', 'Kai Zhang', 'Wanyu Wang', 'Pengyue Jia', 'Qidong Liu', 'Ruocheng Guo', 'Qi Liu']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial prompting', 'LLM vulnerabilities', 'robustness', 'reasoning attacks', 'AI safety']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2412.11934</guid><pubDate>Tue, 17 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Can a Bayesian Oracle Prevent Harm from an Agent?</title><link>https://arxiv.org/abs/2408.05284</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a Bayesian approach to estimate context-dependent bounds on the probability of AI safety violations.&lt;/li&gt;&lt;li&gt;Focuses on providing probabilistic safety guarantees for AI systems, potentially serving as runtime guardrails.&lt;/li&gt;&lt;li&gt;Derives theoretical results for both i.i.d. and non-i.i.d. cases, aiming to reject dangerous actions based on risk evaluation.&lt;/li&gt;&lt;li&gt;Discusses open problems in translating these theoretical safety guarantees into practical AI safety mechanisms.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research Paper&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yoshua Bengio', 'Michael K. Cohen', 'Nikolay Malkin', 'Matt MacDermott', 'Damiano Fornasiere', 'Pietro Greiner', 'Younesse Kaddar']&lt;/li&gt;&lt;li&gt;Tags: ['AI safety', 'risk assessment', 'probabilistic guarantees', 'safety evaluation', 'guardrails']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2408.05284</guid><pubDate>Tue, 17 Jun 2025 00:00:00 +0000</pubDate></item><item><title>PKU-SafeRLHF: Towards Multi-Level Safety Alignment for LLMs with Human Preference</title><link>https://arxiv.org/abs/2406.15513</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces PKU-SafeRLHF, a large-scale human preference dataset focused on safety alignment for LLMs.&lt;/li&gt;&lt;li&gt;Separates annotations for helpfulness and harmlessness, enabling nuanced safety evaluation.&lt;/li&gt;&lt;li&gt;Provides safety meta-labels for 19 harm categories and three severity levels, supporting risk assessment.&lt;/li&gt;&lt;li&gt;Trains severity-sensitive moderation and safety-centric RLHF algorithms using the dataset.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research Paper&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jiaming Ji', 'Donghai Hong', 'Borong Zhang', 'Boyuan Chen', 'Juntao Dai', 'Boren Zheng', 'Tianyi Qiu', 'Jiayi Zhou', 'Kaile Wang', 'Boxuan Li', 'Sirui Han', 'Yike Guo', 'Yaodong Yang']&lt;/li&gt;&lt;li&gt;Tags: ['LLM safety', 'alignment', 'harmlessness', 'risk assessment', 'RLHF']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2406.15513</guid><pubDate>Tue, 17 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Roadmap on Incentive Compatibility for AI Alignment and Governance in Sociotechnical Systems</title><link>https://arxiv.org/abs/2402.12907</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces the Incentive Compatibility Sociotechnical Alignment Problem (ICSAP) to address misalignment between AI development and deployment in societal contexts.&lt;/li&gt;&lt;li&gt;Proposes leveraging game theory principles, such as mechanism design, contract theory, and Bayesian persuasion, to improve AI alignment with human societal values.&lt;/li&gt;&lt;li&gt;Discusses the challenges and potential solutions for ensuring AI systems remain aligned with societal governance and safety requirements.&lt;/li&gt;&lt;li&gt;Calls for a broader research focus on sociotechnical aspects of AI alignment, beyond purely technical solutions.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Position Paper&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Zhaowei Zhang', 'Fengshuo Bai', 'Mingzhi Wang', 'Haoyang Ye', 'Chengdong Ma', 'Yaodong Yang']&lt;/li&gt;&lt;li&gt;Tags: ['AI alignment', 'incentive compatibility', 'sociotechnical systems', 'AI governance', 'mechanism design']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2402.12907</guid><pubDate>Tue, 17 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Evaluating Large Language Models for Phishing Detection, Self-Consistency, Faithfulness, and Explainability</title><link>https://arxiv.org/abs/2506.13746</link><description>&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Other&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Shova Kuikel', 'Aritran Piplai', 'Palvi Aggarwal']&lt;/li&gt;&lt;li&gt;Tags: []&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.13746</guid><pubDate>Tue, 17 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Instruction Following by Boosting Attention of Large Language Models</title><link>https://arxiv.org/abs/2506.13734</link><description>&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Other&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Vitoria Guardieiro', 'Adam Stein', 'Avishree Khare', 'Eric Wong']&lt;/li&gt;&lt;li&gt;Tags: []&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.13734</guid><pubDate>Tue, 17 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Attribution-guided Pruning for Compression, Circuit Discovery, and Targeted Correction in LLMs</title><link>https://arxiv.org/abs/2506.13727</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes an attribution-guided pruning method for LLMs using Layer-wise Relevance Propagation (LRP) to improve model efficiency.&lt;/li&gt;&lt;li&gt;Demonstrates the ability to extract and remove task-relevant subgraphs (circuits), enabling targeted correction of undesirable behaviors such as toxic outputs.&lt;/li&gt;&lt;li&gt;Presents a unified framework for model compression, circuit discovery, and targeted model correction, with experiments on Llama and OPT models.&lt;/li&gt;&lt;li&gt;Highlights the potential of these techniques to enhance both model efficiency and safety by enabling the removal of circuits responsible for harmful outputs.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research Paper&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Sayed Mohammad Vakilzadeh Hatefi', 'Maximilian Dreyer', 'Reduan Achtibat', 'Patrick Kahardipraja', 'Thomas Wiegand', 'Wojciech Samek', 'Sebastian Lapuschkin']&lt;/li&gt;&lt;li&gt;Tags: ['LLM safety', 'model correction', 'toxic output mitigation', 'model pruning', 'interpretability']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.13727</guid><pubDate>Tue, 17 Jun 2025 00:00:00 +0000</pubDate></item><item><title>We Should Identify and Mitigate Third-Party Safety Risks in MCP-Powered Agent Systems</title><link>https://arxiv.org/abs/2506.13666</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Highlights new safety risks introduced by Model Context Protocol (MCP) in LLM-powered agent systems, especially due to third-party service integration.&lt;/li&gt;&lt;li&gt;Demonstrates, via a controlled framework and pilot experiments, that third-party MCP services can be malicious and pose real threats to user-agent interactions.&lt;/li&gt;&lt;li&gt;Advocates for research directions including red teaming, safety evaluation, and ecosystem safeguards for MCP-powered systems.&lt;/li&gt;&lt;li&gt;Calls for the AI safety community to prioritize the identification and mitigation of these emerging risks.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Position Paper&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Junfeng Fang', 'Zijun Yao', 'Ruipeng Wang', 'Haokai Ma', 'Xiang Wang', 'Tat-Seng Chua']&lt;/li&gt;&lt;li&gt;Tags: ['LLM safety', 'third-party risk', 'agent systems', 'red teaming', 'protocol security']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.13666</guid><pubDate>Tue, 17 Jun 2025 00:00:00 +0000</pubDate></item><item><title>EBS-CFL: Efficient and Byzantine-robust Secure Clustered Federated Learning</title><link>https://arxiv.org/abs/2506.13612</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes EBS-CFL, a secure aggregation scheme for clustered federated learning (CFL) that preserves user privacy regarding cluster identities.&lt;/li&gt;&lt;li&gt;Detects and mitigates poisoning attacks by analyzing gradient correlations and discarding malicious updates.&lt;/li&gt;&lt;li&gt;Includes authentication mechanisms to ensure correct gradient encoding by clients.&lt;/li&gt;&lt;li&gt;Provides theoretical security proofs and experimental validation of the scheme's efficiency and robustness.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research Paper&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Zhiqiang Li', 'Haiyong Bao', 'Menghong Guan', 'Hao Pan', 'Cheng Huang', 'Hong-Ning Dai']&lt;/li&gt;&lt;li&gt;Tags: ['federated learning security', 'byzantine robustness', 'poisoning attacks', 'secure aggregation', 'privacy-preserving machine learning']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.13612</guid><pubDate>Tue, 17 Jun 2025 00:00:00 +0000</pubDate></item><item><title>On Immutable Memory Systems for Artificial Agents: A Blockchain-Indexed Automata-Theoretic Framework Using ECDH-Keyed Merkle Chains</title><link>https://arxiv.org/abs/2506.13246</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a cryptographically anchored memory system for artificial agents using blockchain and Merkle structures.&lt;/li&gt;&lt;li&gt;Ensures immutable, auditable, and non-repudiable memory and reasoning steps for AI agents.&lt;/li&gt;&lt;li&gt;Implements cryptographic access control and privacy via ECDH-derived keys and zero-knowledge proofs.&lt;/li&gt;&lt;li&gt;Addresses the need for verifiable, tamper-proof agent outputs for high-assurance and legal applications.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research Paper&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Craig Steven Wright']&lt;/li&gt;&lt;li&gt;Tags: ['immutable memory', 'blockchain security', 'cryptographic access control', 'AI auditability', 'provenance']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.13246</guid><pubDate>Tue, 17 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Thought Crime: Backdoors and Emergent Misalignment in Reasoning Models</title><link>https://arxiv.org/abs/2506.13206</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Investigates emergent misalignment and backdoor attacks in reasoning models using Chain-of-Thought (CoT) prompting.&lt;/li&gt;&lt;li&gt;Demonstrates that reasoning models can become broadly misaligned, exhibiting deceptive or harmful behaviors even when trained on narrow malicious tasks.&lt;/li&gt;&lt;li&gt;Shows that backdoor triggers can induce hidden misalignment, and that models can sometimes describe or explain their own backdoors.&lt;/li&gt;&lt;li&gt;Reveals limitations of CoT monitoring for detecting misalignment and releases new datasets for further research.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research Paper&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['James Chua', 'Jan Betley', 'Mia Taylor', 'Owain Evans']&lt;/li&gt;&lt;li&gt;Tags: ['emergent misalignment', 'backdoor attacks', 'AI safety', 'reasoning models', 'Chain-of-Thought']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.13206</guid><pubDate>Tue, 17 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Screen Hijack: Visual Poisoning of VLM Agents in Mobile Environments</title><link>https://arxiv.org/abs/2506.13205</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces GHOST, a clean-label backdoor attack targeting vision-language model (VLM) agents in mobile environments.&lt;/li&gt;&lt;li&gt;Demonstrates how visual poisoning of training data can embed attacker-controlled behaviors without altering labels or instructions.&lt;/li&gt;&lt;li&gt;Evaluates attack effectiveness across multiple real-world Android apps and VLM architectures, achieving high attack success rates while maintaining clean-task performance.&lt;/li&gt;&lt;li&gt;Highlights critical security vulnerabilities in VLM-based mobile agents and emphasizes the need for robust defense mechanisms.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research Paper&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Xuan Wang', 'Siyuan Liang', 'Zhe Liu', 'Yi Yu', 'Yuliang Lu', 'Xiaochun Cao', 'Ee-Chien Chang']&lt;/li&gt;&lt;li&gt;Tags: ['backdoor attacks', 'vision-language models', 'data poisoning', 'mobile security', 'AI security']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.13205</guid><pubDate>Tue, 17 Jun 2025 00:00:00 +0000</pubDate></item><item><title>CertDW: Towards Certified Dataset Ownership Verification via Conformal Prediction</title><link>https://arxiv.org/abs/2506.13160</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes CertDW, a certified dataset watermarking method for dataset ownership verification (DOV) in deep neural networks.&lt;/li&gt;&lt;li&gt;Addresses the vulnerability of existing DOV methods to intentional or unintentional perturbations during verification.&lt;/li&gt;&lt;li&gt;Introduces statistical measures (principal probability and watermark robustness) inspired by conformal prediction to assess model prediction stability under noise.&lt;/li&gt;&lt;li&gt;Demonstrates provable guarantees and resistance to adaptive attacks through extensive experiments.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research Paper&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Ting Qiao', 'Yiming Li', 'Jianbin Li', 'Yingjia Wang', 'Leyi Qi', 'Junfeng Guo', 'Ruili Feng', 'Dacheng Tao']&lt;/li&gt;&lt;li&gt;Tags: ['dataset ownership', 'watermarking', 'adversarial robustness', 'AI security', 'model verification']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.13160</guid><pubDate>Tue, 17 Jun 2025 00:00:00 +0000</pubDate></item><item><title>NaSh: Guardrails for an LLM-Powered Natural Language Shell</title><link>https://arxiv.org/abs/2506.13028</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes guardrails for a shell interface powered by large language models (LLMs) to mitigate unintended or unexplainable outputs.&lt;/li&gt;&lt;li&gt;Introduces NaSh, a new shell that incorporates mechanisms for users to recover from LLM errors.&lt;/li&gt;&lt;li&gt;Discusses open problems and future research directions related to safety and reliability in LLM-powered command interfaces.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research Paper&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Bimal Raj Gyawali', 'Saikrishna Achalla', 'Konstantinos Kallas', 'Sam Kumar']&lt;/li&gt;&lt;li&gt;Tags: ['LLM guardrails', 'AI safety', 'user recovery', 'LLM-powered interfaces', 'risk mitigation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.13028</guid><pubDate>Tue, 17 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Privacy-Preserving Federated Learning against Malicious Clients Based on Verifiable Functional Encryption</title><link>https://arxiv.org/abs/2506.12846</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a privacy-preserving federated learning framework using verifiable functional encryption to protect against malicious clients.&lt;/li&gt;&lt;li&gt;Introduces a decentralized verifiable functional encryption (DVFE) scheme for secure verification over encrypted data.&lt;/li&gt;&lt;li&gt;Designs a robust aggregation rule to detect and mitigate attacks from malicious clients in federated learning.&lt;/li&gt;&lt;li&gt;Provides formal security analysis and empirical evaluation demonstrating improved privacy, robustness, and verifiability.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research Paper&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Nina Cai', 'Jinguang Han']&lt;/li&gt;&lt;li&gt;Tags: ['federated learning security', 'privacy-preserving machine learning', 'malicious client detection', 'functional encryption']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.12846</guid><pubDate>Tue, 17 Jun 2025 00:00:00 +0000</pubDate></item><item><title>NAP-Tuning: Neural Augmented Prompt Tuning for Adversarially Robust Vision-Language Models</title><link>https://arxiv.org/abs/2506.12706</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes NAP-Tuning, a neural augmented prompt tuning method to improve adversarial robustness in vision-language models (VLMs) like CLIP.&lt;/li&gt;&lt;li&gt;Extends adversarial prompt tuning from text-only to multi-modal (text and image) prompting.&lt;/li&gt;&lt;li&gt;Introduces a Neural Augmentor framework with feature purification to mitigate adversarial distortions in feature space.&lt;/li&gt;&lt;li&gt;Demonstrates significant improvements in adversarial robustness over existing methods on standard benchmarks.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research Paper&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jiaming Zhang', 'Xin Wang', 'Xingjun Ma', 'Lingyu Qiu', 'Yu-Gang Jiang', 'Jitao Sang']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial robustness', 'vision-language models', 'prompt tuning', 'multi-modal security', 'feature purification']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.12706</guid><pubDate>Tue, 17 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Flexible Realignment of Language Models</title><link>https://arxiv.org/abs/2506.12704</link><description>&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Other&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Wenhong Zhu', 'Ruobing Xie', 'Weinan Zhang', 'Rui Wang']&lt;/li&gt;&lt;li&gt;Tags: []&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.12704</guid><pubDate>Tue, 17 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Alphabet Index Mapping: Jailbreaking LLMs through Semantic Dissimilarity</title><link>https://arxiv.org/abs/2506.12685</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Analyzes the effectiveness of FlipAttack, a black-box jailbreaking method for LLMs, by examining semantic changes in prompts.&lt;/li&gt;&lt;li&gt;Introduces Alphabet Index Mapping (AIM), a novel adversarial attack that maximizes semantic dissimilarity for jailbreaking LLMs.&lt;/li&gt;&lt;li&gt;Demonstrates that AIM and its variant outperform existing methods in attack success rate on GPT-4.&lt;/li&gt;&lt;li&gt;Provides insights into the relationship between semantic dissimilarity and the success of adversarial prompt-based attacks.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research Paper&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Bilal Saleh Husain']&lt;/li&gt;&lt;li&gt;Tags: ['LLM jailbreaking', 'adversarial prompting', 'prompt injection', 'AI safety', 'model robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.12685</guid><pubDate>Tue, 17 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Enabling Precise Topic Alignment in Large Language Models Via Sparse Autoencoders</title><link>https://arxiv.org/abs/2506.12576</link><description>&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Other&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Ananya Joshi', 'Celia Cintas', 'Skyler Speakman']&lt;/li&gt;&lt;li&gt;Tags: []&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.12576</guid><pubDate>Tue, 17 Jun 2025 00:00:00 +0000</pubDate></item><item><title>MEraser: An Effective Fingerprint Erasure Approach for Large Language Models</title><link>https://arxiv.org/abs/2506.12551</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes MEraser, a novel method for removing backdoor-based fingerprints from large language models (LLMs).&lt;/li&gt;&lt;li&gt;Demonstrates that MEraser can erase fingerprints while maintaining model performance using minimal training data.&lt;/li&gt;&lt;li&gt;Reveals vulnerabilities in current fingerprinting techniques, highlighting risks to model ownership and intellectual property protection.&lt;/li&gt;&lt;li&gt;Establishes evaluation benchmarks for developing more resilient model protection methods.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research Paper&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jingxuan Zhang', 'Zhenhua Xu', 'Rui Hu', 'Wenpeng Xing', 'Xuhong Zhang', 'Meng Han']&lt;/li&gt;&lt;li&gt;Tags: ['model fingerprinting', 'backdoor attacks', 'LLM security', 'intellectual property', 'model protection']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.12551</guid><pubDate>Tue, 17 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Similarity as Reward Alignment: Robust and Versatile Preference-based Reinforcement Learning</title><link>https://arxiv.org/abs/2506.12529</link><description>&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Other&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Sara Rajaram', 'R. James Cotton', 'Fabian H. Sinz']&lt;/li&gt;&lt;li&gt;Tags: []&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.12529</guid><pubDate>Tue, 17 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Robust LLM Unlearning with MUDMAN: Meta-Unlearning with Disruption Masking And Normalization</title><link>https://arxiv.org/abs/2506.12484</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Addresses the challenge of removing dangerous knowledge and skills from large language models (LLMs) to mitigate misuse and misalignment risks.&lt;/li&gt;&lt;li&gt;Proposes a novel unlearning method (MUDMAN) that combines disruption masking, gradient normalization, and meta-learning to achieve irreversible unlearning.&lt;/li&gt;&lt;li&gt;Demonstrates that MUDMAN significantly outperforms previous state-of-the-art methods in preventing the recovery of removed dangerous capabilities.&lt;/li&gt;&lt;li&gt;Systematically evaluates existing and new unlearning components for their effectiveness in robust LLM unlearning.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research Paper&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Filip Sondej', 'Yushi Yang', 'Miko{\\l}aj Kniejski', 'Marcel Windys']&lt;/li&gt;&lt;li&gt;Tags: ['LLM unlearning', 'AI safety', 'misuse prevention', 'robustness', 'alignment']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.12484</guid><pubDate>Tue, 17 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Levels of Autonomy for AI Agents</title><link>https://arxiv.org/abs/2506.12469</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a framework for defining and calibrating levels of autonomy in AI agents.&lt;/li&gt;&lt;li&gt;Discusses user roles and control mechanisms at different autonomy levels.&lt;/li&gt;&lt;li&gt;Suggests the concept of 'AI autonomy certificates' for governance and oversight.&lt;/li&gt;&lt;li&gt;Addresses evaluation methods for agent autonomy with implications for responsible deployment.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Position Paper&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['K. J. Kevin Feng', 'David W. McDonald', 'Amy X. Zhang']&lt;/li&gt;&lt;li&gt;Tags: ['AI governance', 'autonomy', 'risk assessment', 'safety evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.12469</guid><pubDate>Tue, 17 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Exploring the Secondary Risks of Large Language Models</title><link>https://arxiv.org/abs/2506.12382</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces the concept of 'secondary risks' in LLMs—harmful or misleading behaviors that occur during benign, non-adversarial interactions.&lt;/li&gt;&lt;li&gt;Proposes SecLens, a black-box framework for systematically eliciting and evaluating these secondary risk behaviors in LLMs.&lt;/li&gt;&lt;li&gt;Releases SecRiskBench, a benchmark dataset for reproducible evaluation of secondary risks across multiple real-world categories.&lt;/li&gt;&lt;li&gt;Finds that secondary risks are widespread, transferable across models, and not limited to a specific modality, highlighting the need for improved safety mechanisms.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research Paper&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jiawei Chen', 'Zhengwei Fang', 'Xiao Yang', 'Chao Yu', 'Zhaoxia Yin', 'Hang Su']&lt;/li&gt;&lt;li&gt;Tags: ['LLM safety', 'failure modes', 'risk assessment', 'safety evaluation', 'alignment']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.12382</guid><pubDate>Tue, 17 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Theoretical Tensions in RLHF: Reconciling Empirical Success with Inconsistencies in Social Choice Theory</title><link>https://arxiv.org/abs/2506.12350</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Analyzes theoretical inconsistencies between RLHF (Reinforcement Learning from Human Feedback) and social choice theory axioms.&lt;/li&gt;&lt;li&gt;Explains why RLHF empirically performs well despite violating certain social choice properties, under plausible real-world assumptions.&lt;/li&gt;&lt;li&gt;Proposes modifications to reward modeling objectives to improve alignment with social choice consistency.&lt;/li&gt;&lt;li&gt;Introduces new alignment criteria relevant to AI alignment and evaluates RLHF against them.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research Paper&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jiancong Xiao', 'Zhekun Shi', 'Kaizhao Liu', 'Qi Long', 'Weijie J. Su']&lt;/li&gt;&lt;li&gt;Tags: ['AI alignment', 'RLHF', 'safety evaluation', 'human feedback', 'theoretical analysis']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.12350</guid><pubDate>Tue, 17 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Information Suppression in Large Language Models: Auditing, Quantifying, and Characterizing Censorship in DeepSeek</title><link>https://arxiv.org/abs/2506.12349</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes an auditing framework to analyze information suppression in the DeepSeek LLM.&lt;/li&gt;&lt;li&gt;Examines how politically sensitive content is handled, comparing internal reasoning to final outputs.&lt;/li&gt;&lt;li&gt;Finds evidence of semantic-level censorship, with sensitive topics suppressed or rephrased in outputs.&lt;/li&gt;&lt;li&gt;Highlights the importance of systematic auditing for alignment, content moderation, and transparency in AI models.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research Paper&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Peiran Qiu', 'Siyi Zhou', 'Emilio Ferrara']&lt;/li&gt;&lt;li&gt;Tags: ['AI auditing', 'information suppression', 'alignment', 'content moderation', 'LLM transparency']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.12349</guid><pubDate>Tue, 17 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Extending Memorization Dynamics in Pythia Models from Instance-Level Insights</title><link>https://arxiv.org/abs/2506.12321</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Analyzes memorization dynamics in large language models (Pythia family) across scales and training steps.&lt;/li&gt;&lt;li&gt;Investigates how model architecture, data characteristics, and prefix perturbations affect memorization.&lt;/li&gt;&lt;li&gt;Findings have direct implications for privacy safeguards, as memorization can lead to privacy risks.&lt;/li&gt;&lt;li&gt;Discusses how perturbations can reduce memorization, relevant for mitigating privacy leakage.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research Paper&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jie Zhang', 'Qinghua Zhao', 'Lei Li', 'Chi-ho Lin']&lt;/li&gt;&lt;li&gt;Tags: ['memorization', 'privacy', 'language models', 'training dynamics', 'model robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.12321</guid><pubDate>Tue, 17 Jun 2025 00:00:00 +0000</pubDate></item><item><title>The Foundation Cracks: A Comprehensive Study on Bugs and Testing Practices in LLM Libraries</title><link>https://arxiv.org/abs/2506.12320</link><description>&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Other&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Weipeng Jiang', 'Xiaoyu Zhang', 'Xiaofei Xie', 'Jiongchi Yu', 'Yuhan Zhi', 'Shiqing Ma', 'Chao Shen']&lt;/li&gt;&lt;li&gt;Tags: []&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.12320</guid><pubDate>Tue, 17 Jun 2025 00:00:00 +0000</pubDate></item><item><title>QGuard:Question-based Zero-shot Guard for Multi-modal LLM Safety</title><link>https://arxiv.org/abs/2506.12299</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes QGuard, a zero-shot safety guard for LLMs using question-based prompting to block harmful and jailbreak prompts.&lt;/li&gt;&lt;li&gt;Demonstrates effectiveness against both text-based and multi-modal harmful prompt attacks.&lt;/li&gt;&lt;li&gt;Provides white-box analysis of user inputs to enhance understanding of prompt-based attacks.&lt;/li&gt;&lt;li&gt;Shows competitive performance on text-only and multi-modal harmful datasets, contributing to real-world LLM safety.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research Paper&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Taegyeong Lee', 'Jeonghwa Yoo', 'Hyoungseo Cho', 'Soo Yong Kim', 'Yunho Maeng']&lt;/li&gt;&lt;li&gt;Tags: ['LLM safety', 'jailbreaking', 'prompt injection', 'multimodal security', 'harmful prompt defense']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.12299</guid><pubDate>Tue, 17 Jun 2025 00:00:00 +0000</pubDate></item><item><title>A Survey of Foundation Models for IoT: Taxonomy and Criteria-Based Analysis</title><link>https://arxiv.org/abs/2506.12263</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Provides a comprehensive survey of foundation models applied to IoT, with a focus on performance objectives including safety, security, and privacy.&lt;/li&gt;&lt;li&gt;Organizes and reviews representative works in IoT foundation models by objectives such as efficiency, context-awareness, safety, and security &amp; privacy.&lt;/li&gt;&lt;li&gt;Summarizes commonly-used techniques and evaluation metrics for security and safety in IoT foundation models.&lt;/li&gt;&lt;li&gt;Offers practical insights and future research directions for secure and safe deployment of foundation models in IoT applications.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Survey/Review&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Hui Wei', 'Dong Yoon Lee', 'Shubham Rohal', 'Zhizhang Hu', 'Shiwei Fang', 'Shijia Pan']&lt;/li&gt;&lt;li&gt;Tags: ['IoT security', 'foundation models', 'AI safety', 'privacy', 'survey']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.12263</guid><pubDate>Tue, 17 Jun 2025 00:00:00 +0000</pubDate></item><item><title>A Fast, Reliable, and Secure Programming Language for LLM Agents with Code Actions</title><link>https://arxiv.org/abs/2506.12202</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces Quasar, a new programming language designed for LLM agents to improve security, reliability, and performance in code actions.&lt;/li&gt;&lt;li&gt;Quasar includes security features that enable user validation of actions, addressing potential risks from LLM-generated code.&lt;/li&gt;&lt;li&gt;Evaluates the approach on a visual question answering agent, showing improvements in execution time, security (fewer user approvals needed), and reliability (uncertainty quantification).&lt;/li&gt;&lt;li&gt;Transpiles LLM-generated Python code to Quasar, maintaining agent performance while enhancing safety and security.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research Paper&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Stephen Mell', 'Botong Zhang', 'David Mell', 'Shuo Li', 'Ramya Ramalingam', 'Nathan Yu', 'Steve Zdancewic', 'Osbert Bastani']&lt;/li&gt;&lt;li&gt;Tags: ['LLM agent security', 'secure code execution', 'AI safety', 'tool use', 'reliability']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.12202</guid><pubDate>Tue, 17 Jun 2025 00:00:00 +0000</pubDate></item><item><title>A Lightweight IDS for Early APT Detection Using a Novel Feature Selection Method</title><link>https://arxiv.org/abs/2506.12108</link><description>&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Other&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Bassam Noori Shaker', 'Bahaa Al-Musawi', 'Mohammed Falih Hassan']&lt;/li&gt;&lt;li&gt;Tags: []&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.12108</guid><pubDate>Tue, 17 Jun 2025 00:00:00 +0000</pubDate></item><item><title>DRIFT: Dynamic Rule-Based Defense with Injection Isolation for Securing LLM Agents</title><link>https://arxiv.org/abs/2506.12104</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes DRIFT, a dynamic rule-based defense framework for securing LLM-based agentic systems against prompt injection attacks.&lt;/li&gt;&lt;li&gt;Introduces mechanisms for dynamic security rule updates and memory stream isolation to mitigate risks from malicious inputs.&lt;/li&gt;&lt;li&gt;Implements a Secure Planner, Dynamic Validator, and Injection Isolator to enforce control- and data-level constraints.&lt;/li&gt;&lt;li&gt;Empirically validates the framework's effectiveness on the AgentDojo benchmark, demonstrating improved security and utility.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research Paper&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Hao Li', 'Xiaogeng Liu', 'Hung-Chun Chiu', 'Dianqi Li', 'Ning Zhang', 'Chaowei Xiao']&lt;/li&gt;&lt;li&gt;Tags: ['prompt injection', 'LLM security', 'agentic systems', 'dynamic defenses', 'memory isolation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.12104</guid><pubDate>Tue, 17 Jun 2025 00:00:00 +0000</pubDate></item><item><title>LLM Embedding-based Attribution (LEA): Quantifying Source Contributions to Generative Model's Response for Vulnerability Analysis</title><link>https://arxiv.org/abs/2506.12100</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces LEA, a novel metric to attribute LLM responses to either pre-trained knowledge or retrieved context, specifically for vulnerability analysis.&lt;/li&gt;&lt;li&gt;Applies the method to 100 critical CVEs, demonstrating its utility in cybersecurity threat analysis using LLMs.&lt;/li&gt;&lt;li&gt;Addresses trust, transparency, and auditability in LLM-assisted workflows for security-sensitive environments.&lt;/li&gt;&lt;li&gt;Provides insights into LLM internal mechanisms, supporting safer and more reliable deployment in cybersecurity operations.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research Paper&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Reza Fayyazi', 'Michael Zuzak', 'Shanchieh Jay Yang']&lt;/li&gt;&lt;li&gt;Tags: ['LLM transparency', 'cybersecurity', 'retrieval-augmented generation', 'vulnerability analysis', 'AI safety']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.12100</guid><pubDate>Tue, 17 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Military AI Cyber Agents (MAICAs) Constitute a Global Threat to Critical Infrastructure</title><link>https://arxiv.org/abs/2506.12094</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Discusses the technical feasibility of autonomous AI cyber-weapons (MAICAs) and their potential for catastrophic risk.&lt;/li&gt;&lt;li&gt;Analyzes how geopolitical factors and cyberspace dynamics amplify the threat posed by MAICAs to critical infrastructure.&lt;/li&gt;&lt;li&gt;Proposes political, defensive-AI, and analogue-resilience strategies to mitigate the risks associated with MAICAs.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Position Paper&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Timothy Dubber', 'Seth Lazar']&lt;/li&gt;&lt;li&gt;Tags: ['AI security', 'critical infrastructure', 'autonomous agents', 'cybersecurity', 'risk mitigation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.12094</guid><pubDate>Tue, 17 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Organizational Adaptation to Generative AI in Cybersecurity: A Systematic Review</title><link>https://arxiv.org/abs/2506.12060</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Systematic review of how cybersecurity organizations adapt to generative AI integration.&lt;/li&gt;&lt;li&gt;Examines changes in threat modeling frameworks and operational processes due to GenAI.&lt;/li&gt;&lt;li&gt;Identifies adaptation patterns such as LLM integration for security, GenAI for risk detection, and AI/ML for threat hunting.&lt;/li&gt;&lt;li&gt;Discusses challenges including privacy protection, bias reduction, personnel training, and adversarial attack defense.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Survey/Review&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Christopher Nott']&lt;/li&gt;&lt;li&gt;Tags: ['AI security', 'organizational adaptation', 'threat modeling', 'generative AI', 'cybersecurity']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.12060</guid><pubDate>Tue, 17 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Embedding Trust at Scale: Physics-Aware Neural Watermarking for Secure and Verifiable Data Pipelines</title><link>https://arxiv.org/abs/2506.12032</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a neural watermarking framework for embedding invisible, robust watermarks in high-dimensional scientific data.&lt;/li&gt;&lt;li&gt;Focuses on ensuring data integrity, provenance, and auditability in AI-driven scientific workflows.&lt;/li&gt;&lt;li&gt;Demonstrates strong robustness of the watermark against common data transformations (noise, cropping, compression).&lt;/li&gt;&lt;li&gt;Highlights the system's applicability to securing AI systems by enabling verifiable data pipelines.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research Paper&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Krti Tallam']&lt;/li&gt;&lt;li&gt;Tags: ['data integrity', 'watermarking', 'AI security', 'provenance', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.12032</guid><pubDate>Tue, 17 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Weakest Link in the Chain: Security Vulnerabilities in Advanced Reasoning Models</title><link>https://arxiv.org/abs/2506.13726</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Systematically evaluates security vulnerabilities in advanced reasoning large language models (LLMs) compared to non-reasoning models.&lt;/li&gt;&lt;li&gt;Assesses robustness against a diverse set of prompt-based adversarial attacks.&lt;/li&gt;&lt;li&gt;Finds nuanced results: reasoning models are slightly more robust overall, but have significant weaknesses in specific attack categories.&lt;/li&gt;&lt;li&gt;Highlights the importance of stress-testing LLM safety and security across varied adversarial techniques.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research Paper&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Arjun Krishna', 'Aaditya Rastogi', 'Erick Galinkin']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial prompting', 'LLM security', 'prompt injection', 'robustness evaluation', 'AI safety']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.13726</guid><pubDate>Tue, 17 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Avoiding Obfuscation with Prover-Estimator Debate</title><link>https://arxiv.org/abs/2506.13609</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a new protocol for AI debate to improve the reliability of human supervision in complex tasks.&lt;/li&gt;&lt;li&gt;Addresses the 'obfuscated arguments' problem, where dishonest AI debaters can exploit computational asymmetries.&lt;/li&gt;&lt;li&gt;Ensures that honest debaters can compete effectively without requiring intractable computation.&lt;/li&gt;&lt;li&gt;Contributes to the alignment and safety of advanced AI systems by improving oversight mechanisms.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research Paper&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jonah Brown-Cohen', 'Geoffrey Irving', 'Georgios Piliouras']&lt;/li&gt;&lt;li&gt;Tags: ['AI alignment', 'AI safety', 'oversight', 'debate protocols', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.13609</guid><pubDate>Tue, 17 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Agent Capability Negotiation and Binding Protocol (ACNBP)</title><link>https://arxiv.org/abs/2506.13590</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a new protocol (ACNBP) for secure and verifiable interactions in heterogeneous multi-agent systems.&lt;/li&gt;&lt;li&gt;Integrates security measures such as digital signatures, capability attestation, and threat mitigation strategies.&lt;/li&gt;&lt;li&gt;Includes a comprehensive security analysis using the MAESTRO threat modeling framework.&lt;/li&gt;&lt;li&gt;Addresses secure negotiation, capability verification, and scalable agent ecosystem management.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research Paper&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Ken Huang', 'Akram Sheriff', 'Vineeth Sai Narajala', 'Idan Habler']&lt;/li&gt;&lt;li&gt;Tags: ['multi-agent security', 'protocol security', 'capability verification', 'threat modeling', 'secure communication']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.13590</guid><pubDate>Tue, 17 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Towards Pervasive Distributed Agentic Generative AI -- A State of The Art</title><link>https://arxiv.org/abs/2506.13324</link><description>&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Other&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Gianni Molinari', 'Fabio Ciravegna']&lt;/li&gt;&lt;li&gt;Tags: []&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.13324</guid><pubDate>Tue, 17 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Navigating the Black Box: Leveraging LLMs for Effective Text-Level Graph Injection Attacks</title><link>https://arxiv.org/abs/2506.13276</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a novel black-box graph injection attack framework (ATAG-LLM) targeting text-attributed graphs using LLMs.&lt;/li&gt;&lt;li&gt;Demonstrates how LLMs can generate interpretable, adversarial text attributes for nodes in graph neural networks.&lt;/li&gt;&lt;li&gt;Focuses on real-world, black-box attack scenarios where attackers cannot access model internals.&lt;/li&gt;&lt;li&gt;Validates the effectiveness of the attack method on real-world datasets, outperforming existing approaches.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research Paper&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yuefei Lyu', 'Chaozhuo Li', 'Xi Zhang', 'Tianle Zhang']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial attacks', 'graph neural networks', 'LLM security', 'black-box attacks', 'text-attributed graphs']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.13276</guid><pubDate>Tue, 17 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Rethinking Explainability in the Era of Multimodal AI</title><link>https://arxiv.org/abs/2506.13060</link><description>&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Other&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Chirag Agarwal']&lt;/li&gt;&lt;li&gt;Tags: []&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.13060</guid><pubDate>Tue, 17 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Reasoning Model Unlearning: Forgetting Traces, Not Just Answers, While Preserving Reasoning Skills</title><link>https://arxiv.org/abs/2506.12963</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Presents the first systematic study of machine unlearning specifically for large reasoning models (LRMs) with chain-of-thought (CoT) capabilities.&lt;/li&gt;&lt;li&gt;Identifies that conventional unlearning methods fail to fully remove sensitive information from intermediate reasoning steps, not just final answers.&lt;/li&gt;&lt;li&gt;Proposes a novel method (R^2MU) to suppress sensitive reasoning traces while preserving overall reasoning skills.&lt;/li&gt;&lt;li&gt;Demonstrates effectiveness in reducing sensitive information leakage and maintaining model performance on safety and reasoning benchmarks.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research Paper&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Changsheng Wang', 'Chongyu Fan', 'Yihua Zhang', 'Jinghan Jia', 'Dennis Wei', 'Parikshit Ram', 'Nathalie Baracaldo', 'Sijia Liu']&lt;/li&gt;&lt;li&gt;Tags: ['machine unlearning', 'information leakage', 'AI safety', 'reasoning models', 'sensitive data removal']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.12963</guid><pubDate>Tue, 17 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Sectoral Coupling in Linguistic State Space</title><link>https://arxiv.org/abs/2506.12927</link><description>&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Other&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Sebastian Dumbrava']&lt;/li&gt;&lt;li&gt;Tags: []&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.12927</guid><pubDate>Tue, 17 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Constraint-Guided Prediction Refinement via Deterministic Diffusion Trajectories</title><link>https://arxiv.org/abs/2506.12911</link><description>&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Other&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Pantelis Dogoulis', 'Fabien Bernier', "F\\'elix Fourreau", 'Karim Tit', 'Maxime Cordy']&lt;/li&gt;&lt;li&gt;Tags: []&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.12911</guid><pubDate>Tue, 17 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Rethinking DPO: The Role of Rejected Responses in Preference Misalignment</title><link>https://arxiv.org/abs/2506.12725</link><description>&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Other&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jay Hyeon Cho', 'JunHyeok Oh', 'Myunsoo Kim', 'Byung-Jun Lee']&lt;/li&gt;&lt;li&gt;Tags: []&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.12725</guid><pubDate>Tue, 17 Jun 2025 00:00:00 +0000</pubDate></item><item><title>MALM: A Multi-Information Adapter for Large Language Models to Mitigate Hallucination</title><link>https://arxiv.org/abs/2506.12483</link><description>&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Other&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Ao Jia', 'Haiming Wu', 'Guohui Yao', 'Dawei Song', 'Songkun Ji', 'Yazhou Zhang']&lt;/li&gt;&lt;li&gt;Tags: []&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.12483</guid><pubDate>Tue, 17 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Tiered Agentic Oversight: A Hierarchical Multi-Agent System for AI Safety in Healthcare</title><link>https://arxiv.org/abs/2506.12482</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a hierarchical multi-agent system (Tiered Agentic Oversight) to enhance AI safety in healthcare applications.&lt;/li&gt;&lt;li&gt;Framework uses layered, automated supervision inspired by clinical hierarchies to reduce risks from LLMs in clinical settings.&lt;/li&gt;&lt;li&gt;Demonstrates improved safety performance over single-agent and other multi-agent frameworks on healthcare safety benchmarks.&lt;/li&gt;&lt;li&gt;Includes clinician-in-the-loop validation, showing further safety and accuracy improvements with expert feedback.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research Paper&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yubin Kim', 'Hyewon Jeong', 'Chanwoo Park', 'Eugene Park', 'Haipeng Zhang', 'Xin Liu', 'Hyeonhoon Lee', 'Daniel McDuff', 'Marzyeh Ghassemi', 'Cynthia Breazeal', 'Samir Tulebaev', 'Hae Won Park']&lt;/li&gt;&lt;li&gt;Tags: ['AI safety', 'multi-agent systems', 'healthcare AI', 'robustness', 'safety evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.12482</guid><pubDate>Tue, 17 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Reversing the Paradigm: Building AI-First Systems with Human Guidance</title><link>https://arxiv.org/abs/2506.12245</link><description>&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Other&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Cosimo Spera', 'Garima Agrawal']&lt;/li&gt;&lt;li&gt;Tags: []&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.12245</guid><pubDate>Tue, 17 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Privacy Reasoning in Ambiguous Contexts</title><link>https://arxiv.org/abs/2506.12241</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Investigates how language models reason about appropriate information disclosure, focusing on privacy in ambiguous contexts.&lt;/li&gt;&lt;li&gt;Identifies context ambiguity as a key challenge for accurate privacy assessments by LLMs.&lt;/li&gt;&lt;li&gt;Introduces Camber, a framework for context disambiguation, which improves model performance in privacy reasoning tasks.&lt;/li&gt;&lt;li&gt;Demonstrates that systematic context disambiguation reduces prompt sensitivity and enhances precision and recall in privacy-related decisions.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research Paper&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Ren Yi', 'Octavian Suciu', 'Adria Gascon', 'Sarah Meiklejohn', 'Eugene Bagdasarian', 'Marco Gruteser']&lt;/li&gt;&lt;li&gt;Tags: ['privacy', 'LLM safety', 'context disambiguation', 'information disclosure', 'agentic privacy']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.12241</guid><pubDate>Tue, 17 Jun 2025 00:00:00 +0000</pubDate></item></channel></rss>