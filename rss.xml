<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>AI Security Paper Digest</title><link>https://kentaroh-toyoda.github.io/ai-security-paper-digest-rss/rss.xml</link><description>Curated papers on AI security from OpenAlex and ArXiv</description><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><lastBuildDate>Wed, 30 Jul 2025 22:27:59 +0000</lastBuildDate><item><title>PRISM: Programmatic Reasoning with Image Sequence Manipulation for LVLM Jailbreaking</title><link>https://arxiv.org/abs/2507.21540</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces PRISM, a novel jailbreak framework for LVLMs using visual gadgets&lt;/li&gt;&lt;li&gt;Inspired by ROP, decomposes harmful instructions into benign visual components&lt;/li&gt;&lt;li&gt;Validated on SafeBench and MM-SafetyBench with high attack success rates&lt;/li&gt;&lt;li&gt;Highlights vulnerability in compositional reasoning of LVLMs&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Quanchen Zou', 'Zonghao Ying', 'Moyang Chen', 'Wenzhuo Xu', 'Yisong Xiao', 'Yakai Li', 'Deyue Zhang', 'Dongdong Yang', 'Zhao Liu', 'Xiangzheng Zhang']&lt;/li&gt;&lt;li&gt;Tags: ['jailbreaking', 'adversarial prompting', 'LVLM', 'red teaming', 'safety evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2507.21540</guid><pubDate>Wed, 30 Jul 2025 00:00:00 +0000</pubDate></item><item><title>Unmasking Synthetic Realities in Generative AI: A Comprehensive Review of Adversarially Robust Deepfake Detection Systems</title><link>https://arxiv.org/abs/2507.21157</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Reviews state-of-the-art deepfake detection methods&lt;/li&gt;&lt;li&gt;Highlights vulnerability to adversarial perturbations&lt;/li&gt;&lt;li&gt;Emphasizes need for adversarial resilience&lt;/li&gt;&lt;li&gt;Provides open-source implementations for testing&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Survey&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Naseem Khan', 'Tuan Nguyen', 'Amine Bermak', 'Issa Khalil']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial robustness', 'deepfake detection', 'security', 'red teaming', 'survey']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2507.21157</guid><pubDate>Wed, 30 Jul 2025 00:00:00 +0000</pubDate></item><item><title>ZIUM: Zero-Shot Intent-Aware Adversarial Attack on Unlearned Models</title><link>https://arxiv.org/abs/2507.21985</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes ZIUM, a zero-shot intent-aware adversarial attack on unlearned models&lt;/li&gt;&lt;li&gt;Addresses challenges in generating intent-aligned content and reducing computational costs&lt;/li&gt;&lt;li&gt;Demonstrates effectiveness in customizing attacks and reducing time for previously attacked concepts&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Hyun Jun Yook', 'Ga San Jhun', 'Jae Hyun Cho', 'Min Jeon', 'Donghyun Kim', 'Tae Hyung Kim', 'Youn Kyu Lee']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial prompting', 'machine unlearning', 'security', 'red teaming', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2507.21985</guid><pubDate>Wed, 30 Jul 2025 00:00:00 +0000</pubDate></item><item><title>Anyone Can Jailbreak: Prompt-Based Attacks on LLMs and T2Is</title><link>https://arxiv.org/abs/2507.21820</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Presents a taxonomy of prompt-based jailbreak strategies for LLMs and T2Is&lt;/li&gt;&lt;li&gt;Highlights non-expert accessible techniques like narrative escalation and lexical camouflage&lt;/li&gt;&lt;li&gt;Shows current safety mechanisms can be bypassed at every moderation stage&lt;/li&gt;&lt;li&gt;Calls for context-aware defenses against low-effort attacks&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Ahmed B Mustafa', 'Zihan Ye', 'Yang Lu', 'Michael P Pound', 'Shreyank N Gowda']&lt;/li&gt;&lt;li&gt;Tags: ['jailbreaking', 'prompt injection', 'adversarial prompting', 'safety evaluation', 'text-to-image']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2507.21820</guid><pubDate>Wed, 30 Jul 2025 00:00:00 +0000</pubDate></item><item><title>The Problem with Safety Classification is not just the Models</title><link>https://arxiv.org/abs/2507.21782</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Position paper highlighting multilingual disparities in safety classification models&lt;/li&gt;&lt;li&gt;Evaluates 5 safety models across 18 languages&lt;/li&gt;&lt;li&gt;Identifies issues with current evaluation datasets&lt;/li&gt;&lt;li&gt;Argues that safety classifier shortcomings are due to both models and datasets&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Position&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Sowmya Vajjala']&lt;/li&gt;&lt;li&gt;Tags: ['safety evaluation', 'multilingual', 'datasets', 'LLMs', 'position paper']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2507.21782</guid><pubDate>Wed, 30 Jul 2025 00:00:00 +0000</pubDate></item><item><title>Adversarial Defence without Adversarial Defence: Enhancing Language Model Robustness via Instance-level Principal Component Removal</title><link>https://arxiv.org/abs/2507.21750</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Enhances adversarial robustness of language models without adversarial training&lt;/li&gt;&lt;li&gt;Removes instance-level principal components from embedding space&lt;/li&gt;&lt;li&gt;Reduces susceptibility to adversarial perturbations while preserving semantics&lt;/li&gt;&lt;li&gt;Maintains accuracy and reduces computational costs compared to traditional defenses&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yang Wang', 'Chenghao Xiao', 'Yizhi Li', 'Stuart E. Middleton', 'Noura Al Moubayed', 'Chenghua Lin']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial robustness', 'language models', 'principal component analysis', 'embedding space', 'adversarial defense']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2507.21750</guid><pubDate>Wed, 30 Jul 2025 00:00:00 +0000</pubDate></item><item><title>UnsafeChain: Enhancing Reasoning Model Safety via Hard Cases</title><link>https://arxiv.org/abs/2507.21652</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces UnsafeChain, a safety alignment dataset focusing on hard prompts that elicit harmful outputs&lt;/li&gt;&lt;li&gt;Fine-tunes LRMs on UnsafeChain and shows improved safety performance over baselines&lt;/li&gt;&lt;li&gt;Demonstrates effectiveness with even a 1K subset matching/surpassing baseline performance&lt;/li&gt;&lt;li&gt;Releases dataset and code for further research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Raj Vardhan Tomar', 'Preslav Nakov', 'Yuxia Wang']&lt;/li&gt;&lt;li&gt;Tags: ['safety evaluation', 'alignment', 'dataset', 'adversarial prompting', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2507.21652</guid><pubDate>Wed, 30 Jul 2025 00:00:00 +0000</pubDate></item><item><title>MAGIC: A Multi-Hop and Graph-Based Benchmark for Inter-Context Conflicts in Retrieval-Augmented Generation</title><link>https://arxiv.org/abs/2507.21544</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces MAGIC benchmark for evaluating knowledge conflict detection in RAG systems&lt;/li&gt;&lt;li&gt;Uses knowledge graphs to generate varied and subtle conflicts&lt;/li&gt;&lt;li&gt;Shows LLMs struggle with multi-hop conflict detection&lt;/li&gt;&lt;li&gt;Provides foundation for improving conflict resolution in LLMs&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jungyeon Lee', 'Kangmin Lee', 'Taeuk Kim']&lt;/li&gt;&lt;li&gt;Tags: ['safety evaluation', 'robustness', 'adversarial prompting', 'benchmarking', 'retrieval-augmented generation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2507.21544</guid><pubDate>Wed, 30 Jul 2025 00:00:00 +0000</pubDate></item><item><title>PAR-AdvGAN: Improving Adversarial Attack Capability with Progressive Auto-Regression AdvGAN</title><link>https://arxiv.org/abs/2502.12207</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces PAR-AdvGAN, a novel GAN-based method for generating adversarial examples&lt;/li&gt;&lt;li&gt;Incorporates auto-regressive iteration mechanism for enhanced attack capability&lt;/li&gt;&lt;li&gt;Demonstrates superior performance over state-of-the-art black-box attacks&lt;/li&gt;&lt;li&gt;Achieves high generation speeds (335.5 FPS on Inception-v3)&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jiayu Zhang', 'Zhiyu Zhu', 'Xinyi Wang', 'Silin Liao', 'Zhibo Jin', 'Flora D. Salim', 'Huaming Chen']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial attacks', 'GAN', 'transferability', 'red teaming', 'image']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2502.12207</guid><pubDate>Wed, 30 Jul 2025 00:00:00 +0000</pubDate></item><item><title>Latent Adversarial Training Improves Robustness to Persistent Harmful Behaviors in LLMs</title><link>https://arxiv.org/abs/2407.15549</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces targeted latent adversarial training (LAT) to improve LLM robustness&lt;/li&gt;&lt;li&gt;Outperforms R2D2 baseline with significantly less compute in jailbreak defense&lt;/li&gt;&lt;li&gt;Enhances backdoor removal and unlearning of undesirable tasks&lt;/li&gt;&lt;li&gt;Leverages targeted adversarial attacks in latent space to augment existing methods&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Abhay Sheshadri', 'Aidan Ewart', 'Phillip Guo', 'Aengus Lynch', 'Cindy Wu', 'Vivek Hebbar', 'Henry Sleight', 'Asa Cooper Stickland', 'Ethan Perez', 'Dylan Hadfield-Menell', 'Stephen Casper']&lt;/li&gt;&lt;li&gt;Tags: ['red teaming', 'jailbreaking', 'adversarial training', 'robustness', 'latent space']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2407.15549</guid><pubDate>Wed, 30 Jul 2025 00:00:00 +0000</pubDate></item><item><title>Adversarial attacks and defenses in explainable artificial intelligence: A survey</title><link>https://arxiv.org/abs/2306.06123</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Comprehensive survey of adversarial attacks on XAI methods&lt;/li&gt;&lt;li&gt;Introduces taxonomy and notation for AdvXAI&lt;/li&gt;&lt;li&gt;Discusses defenses and robustness&lt;/li&gt;&lt;li&gt;Outlines future research directions&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Survey&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Hubert Baniecki', 'Przemyslaw Biecek']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial attacks', 'explainable AI', 'security', 'survey']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2306.06123</guid><pubDate>Wed, 30 Jul 2025 00:00:00 +0000</pubDate></item><item><title>Secure Tug-of-War (SecTOW): Iterative Defense-Attack Training with Reinforcement Learning for Multimodal Model Security</title><link>https://arxiv.org/abs/2507.22037</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces Secure Tug-of-War (SecTOW) for enhancing MLLM security&lt;/li&gt;&lt;li&gt;Uses iterative defense-attack training with reinforcement learning&lt;/li&gt;&lt;li&gt;Generates diverse jailbreak data to improve robustness&lt;/li&gt;&lt;li&gt;Maintains performance while reducing over-refusal of harmless inputs&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Muzhi Dai', 'Shixuan Liu', 'Zhiyuan Zhao', 'Junyu Gao', 'Hao Sun', 'Xuelong Li']&lt;/li&gt;&lt;li&gt;Tags: ['red teaming', 'jailbreak', 'multimodal', 'reinforcement learning', 'security', 'adversarial training']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2507.22037</guid><pubDate>Wed, 30 Jul 2025 00:00:00 +0000</pubDate></item><item><title>Teach Me to Trick: Exploring Adversarial Transferability via Knowledge Distillation</title><link>https://arxiv.org/abs/2507.21992</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Investigates using knowledge distillation from multiple teachers to enhance adversarial example transferability&lt;/li&gt;&lt;li&gt;Compares curriculum-based switching and joint optimization strategies&lt;/li&gt;&lt;li&gt;Evaluates attack success rates against a black-box model using FG, FGS, and PGD&lt;/li&gt;&lt;li&gt;Achieves comparable performance to ensemble methods with faster generation times&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Siddhartha Pradhan', 'Shikshya Shiwakoti', 'Neha Bathuri']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial attacks', 'knowledge distillation', 'transferability', 'model compression', 'black-box attacks']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2507.21992</guid><pubDate>Wed, 30 Jul 2025 00:00:00 +0000</pubDate></item><item><title>Evaluation and Benchmarking of LLM Agents: A Survey</title><link>https://arxiv.org/abs/2507.21504</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Survey of LLM agent evaluation&lt;/li&gt;&lt;li&gt;Taxonomy of evaluation objectives and processes&lt;/li&gt;&lt;li&gt;Includes safety as an evaluation objective&lt;/li&gt;&lt;li&gt;Highlights enterprise challenges&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Survey&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Mahmoud Mohammadi', 'Yipeng Li', 'Jane Lo', 'Wendy Yip']&lt;/li&gt;&lt;li&gt;Tags: ['safety evaluation', 'benchmarking', 'LLM agents', 'survey']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2507.21504</guid><pubDate>Wed, 30 Jul 2025 00:00:00 +0000</pubDate></item><item><title>NCCR: to Evaluate the Robustness of Neural Networks and Adversarial Examples</title><link>https://arxiv.org/abs/2507.21483</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces Neuron Cover Change Rate (NCCR) metric to evaluate model robustness against adversarial attacks&lt;/li&gt;&lt;li&gt;Applies to image recognition and speaker recognition models&lt;/li&gt;&lt;li&gt;Can detect adversarial examples by measuring input stability&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Pu Shi']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial examples', 'robustness evaluation', 'security metrics', 'neural network security', 'adversarial detection']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2507.21483</guid><pubDate>Wed, 30 Jul 2025 00:00:00 +0000</pubDate></item><item><title>SDD: Self-Degraded Defense against Malicious Fine-tuning</title><link>https://arxiv.org/abs/2507.21182</link><description>&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Other&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Zixuan Chen', 'Weikai Lu', 'Xin Lin', 'Ziqian Zeng']&lt;/li&gt;&lt;li&gt;Tags: []&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2507.21182</guid><pubDate>Wed, 30 Jul 2025 00:00:00 +0000</pubDate></item><item><title>OneShield -- the Next Generation of LLM Guardrails</title><link>https://arxiv.org/abs/2507.21170</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes OneShield, a model-agnostic framework for safeguarding LLMs&lt;/li&gt;&lt;li&gt;Focuses on defining risk factors, safety policies, and risk mitigation&lt;/li&gt;&lt;li&gt;Emphasizes customization and scalability for specific customer needs&lt;/li&gt;&lt;li&gt;Provides implementation details and usage statistics&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Chad DeLuca', 'Anna Lisa Gentile', 'Shubhi Asthana', 'Bing Zhang', 'Pawan Chowdhary', 'Kellen Cheng', 'Basel Shbita', 'Pengyuan Li', 'Guang-Jie Ren', 'Sandeep Gopisetty']&lt;/li&gt;&lt;li&gt;Tags: ['safety', 'compliance', 'risk_mitigation', 'model_agnostic', 'policy_enforcement']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2507.21170</guid><pubDate>Wed, 30 Jul 2025 00:00:00 +0000</pubDate></item><item><title>Generating Adversarial Point Clouds Using Diffusion Model</title><link>https://arxiv.org/abs/2507.21163</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a black-box adversarial attack method for 3D point clouds using a diffusion model&lt;/li&gt;&lt;li&gt;Aims to improve attack success and imperceptibility without white-box access&lt;/li&gt;&lt;li&gt;Uses a 3D diffusion model to generate adversarial points based on compressed features&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Ruiyang Zhao', 'Bingbing Zhu', 'Chuxuan Tong', 'Xiaoyi Zhou', 'Xi Zheng']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial attacks', 'black-box attacks', 'diffusion models', '3D point clouds', 'security evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2507.21163</guid><pubDate>Wed, 30 Jul 2025 00:00:00 +0000</pubDate></item><item><title>Towards Unifying Quantitative Security Benchmarking for Multi Agent Systems</title><link>https://arxiv.org/abs/2507.21146</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces Agent Cascading Injection (ACI) attack vector for multi-agent systems&lt;/li&gt;&lt;li&gt;Proposes quantitative benchmarking framework for agent-to-agent security evaluation&lt;/li&gt;&lt;li&gt;Aligns with OWASP's Agentic AI Vulnerability Scoring System&lt;/li&gt;&lt;li&gt;Focuses on cascading trust failures and system resilience&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Gauri Sharma', 'Vidhi Kulkarni', 'Miles King', 'Ken Huang']&lt;/li&gt;&lt;li&gt;Tags: ['multi-agent security', 'adversarial attacks', 'security benchmarks', 'OWASP', 'red teaming']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2507.21146</guid><pubDate>Wed, 30 Jul 2025 00:00:00 +0000</pubDate></item><item><title>Analysis of Threat-Based Manipulation in Large Language Models: A Dual Perspective on Vulnerabilities and Performance Enhancement Opportunities</title><link>https://arxiv.org/abs/2507.21133</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Comprehensive analysis of LLM responses under threat-based manipulations&lt;/li&gt;&lt;li&gt;Introduced novel threat taxonomy and multi-metric evaluation framework&lt;/li&gt;&lt;li&gt;Found significant vulnerabilities and performance enhancements across multiple domains&lt;/li&gt;&lt;li&gt;Statistical evidence of certainty manipulation and improved response quality&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Atil Samancioglu']&lt;/li&gt;&lt;li&gt;Tags: ['red teaming', 'adversarial prompting', 'safety evaluation', 'alignment', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2507.21133</guid><pubDate>Wed, 30 Jul 2025 00:00:00 +0000</pubDate></item><item><title>Self-Aware Safety Augmentation: Leveraging Internal Semantic Understanding to Enhance Safety in Vision-Language Models</title><link>https://arxiv.org/abs/2507.21637</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Investigates safety vulnerabilities in vision-language models (LVLMs)&lt;/li&gt;&lt;li&gt;Introduces Self-Aware Safety Augmentation (SASA) technique&lt;/li&gt;&lt;li&gt;Enhances safety perception using internal semantic understanding&lt;/li&gt;&lt;li&gt;Demonstrates significant safety improvements with minimal utility impact&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Wanying Wang', 'Zeyu Ma', 'Han Zheng', 'Xin Tan', 'Mingang Chen']&lt;/li&gt;&lt;li&gt;Tags: ['safety', 'vision-language models', 'adversarial defense', 'alignment', 'internal model dynamics']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2507.21637</guid><pubDate>Wed, 30 Jul 2025 00:00:00 +0000</pubDate></item><item><title>Tell Me You're Biased Without Telling Me You're Biased -- Toward Revealing Implicit Biases in Medical LLMs</title><link>https://arxiv.org/abs/2507.21176</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Presents a novel framework combining knowledge graphs and auxiliary LLMs to detect implicit biases in medical LLMs&lt;/li&gt;&lt;li&gt;Uses adversarial perturbation techniques to identify subtle bias patterns&lt;/li&gt;&lt;li&gt;Conducts comprehensive experiments across multiple datasets, LLMs, and bias types&lt;/li&gt;&lt;li&gt;Demonstrates improved ability and scalability in bias detection compared to baselines&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Farzana Islam Adiba', 'Rahmatollah Beheshti']&lt;/li&gt;&lt;li&gt;Tags: ['bias detection', 'adversarial testing', 'medical LLMs', 'safety evaluation', 'knowledge graphs']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2507.21176</guid><pubDate>Wed, 30 Jul 2025 00:00:00 +0000</pubDate></item><item><title>The Geometry of Harmfulness in LLMs through Subconcept Probing</title><link>https://arxiv.org/abs/2507.21141</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces a multidimensional framework for probing harmful content in LLMs using 55 subconcepts&lt;/li&gt;&lt;li&gt;Finds a low-rank harmfulness subspace in activation space&lt;/li&gt;&lt;li&gt;Tests ablation and steering in the subspace to reduce harmfulness with minimal utility loss&lt;/li&gt;&lt;li&gt;Advances safety evaluation and alignment methods for LLMs&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['McNair Shah', 'Saleena Angeline', 'Adhitya Rajendra Kumar', 'Naitik Chheda', 'Kevin Zhu', 'Vasu Sharma', "Sean O'Brien", 'Will Cai']&lt;/li&gt;&lt;li&gt;Tags: ['safety evaluation', 'alignment', 'probing', 'subspace methods']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2507.21141</guid><pubDate>Wed, 30 Jul 2025 00:00:00 +0000</pubDate></item><item><title>Can You Trust an LLM with Your Life-Changing Decision? An Investigation into AI High-Stakes Responses</title><link>https://arxiv.org/abs/2507.21132</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Investigates LLM safety in high-stakes advice scenarios&lt;/li&gt;&lt;li&gt;Uses multiple-choice tests, free-response analysis, and activation steering&lt;/li&gt;&lt;li&gt;Finds that some models are robust, others not&lt;/li&gt;&lt;li&gt;Shows that asking clarifying questions is a key safety feature&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Joshua Adrian Cahyono', 'Saran Subramanian']&lt;/li&gt;&lt;li&gt;Tags: ['safety evaluation', 'alignment', 'robustness', 'activation steering']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2507.21132</guid><pubDate>Wed, 30 Jul 2025 00:00:00 +0000</pubDate></item><item><title>NPO: Learning Alignment and Meta-Alignment through Structured Human Feedback</title><link>https://arxiv.org/abs/2507.21131</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces NPO framework for alignment-aware learning using structured human feedback&lt;/li&gt;&lt;li&gt;Formalizes alignment loss and meta-alignment for measurable improvement&lt;/li&gt;&lt;li&gt;Demonstrates convergence under stochastic feedback with empirical validation&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Madhava Gaikwad (Microsoft)', 'Ashwini Ramchandra Doke (Amrita University)']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'safety evaluation', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2507.21131</guid><pubDate>Wed, 30 Jul 2025 00:00:00 +0000</pubDate></item></channel></rss>