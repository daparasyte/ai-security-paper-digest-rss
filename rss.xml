<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>AI Security Paper Digest</title><link>https://kentaroh-toyoda.github.io/ai-security-paper-digest-rss/rss.xml</link><description>Curated papers on AI security from ArXiv</description><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><lastBuildDate>Fri, 05 Sep 2025 22:20:36 +0000</lastBuildDate><item><title>Defending LVLMs Against Vision Attacks through Partial-Perception Supervision</title><link>https://arxiv.org/abs/2412.12722</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes DPS (Defense through Partial-Perception Supervision) to defend LVLMs against vision attacks&lt;/li&gt;&lt;li&gt;Uses responses from partially perceived images to supervise the model's original response&lt;/li&gt;&lt;li&gt;Reduces attack success rate by 76.3% across multiple datasets and models&lt;/li&gt;&lt;li&gt;Maintains response quality on clean inputs by adjusting confidence levels&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Qi Zhou', 'Tianlin Li', 'Qing Guo', 'Dongxia Wang', 'Yun Lin', 'Yang Liu', 'Jin Song Dong']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial prompting', 'model robustness', 'vision attacks', 'multimodal security', 'LVLM defense']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2412.12722</guid><pubDate>Fri, 05 Sep 2025 00:00:00 +0000</pubDate></item><item><title>EigenBench: A Comparative Behavioral Measure of Value Alignment</title><link>https://arxiv.org/abs/2509.01938</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;EigenBench is a black-box method for benchmarking language models' value alignment&lt;/li&gt;&lt;li&gt;Uses models to judge each other's outputs and aggregates with EigenTrust&lt;/li&gt;&lt;li&gt;Measures alignment without ground truth labels by leveraging ensemble judgments&lt;/li&gt;&lt;li&gt;Tests show variance is mostly from prompts but some from model disposition&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jonathn Chang', 'Leonhard Piff', 'Suvadip Sana', 'Jasmine X. Li', 'Lionel Levine']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'safety evaluation', 'benchmarking', 'EigenTrust', 'model comparison']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.01938</guid><pubDate>Fri, 05 Sep 2025 00:00:00 +0000</pubDate></item><item><title>Oyster-I: Beyond Refusal -- Constructive Safety Alignment for Responsible Language Models</title><link>https://arxiv.org/abs/2509.01909</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces Constructive Safety Alignment (CSA) paradigm for LLMs&lt;/li&gt;&lt;li&gt;Implements CSA in Oyster-I (Oy1) model&lt;/li&gt;&lt;li&gt;Achieves state-of-the-art safety while retaining capabilities&lt;/li&gt;&lt;li&gt;Shows robustness against jailbreak attacks and constructive engagement&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Ranjie Duan', 'Jiexi Liu', 'Xiaojun Jia', 'Shiji Zhao', 'Ruoxi Cheng', 'Fengxiang Wang', 'Cheng Wei', 'Yong Xie', 'Chang Liu', 'Defeng Li', 'Yinpeng Dong', 'Yichi Zhang', 'Yuefeng Chen', 'Chongwen Wang', 'Xingjun Ma', 'Xingxing Wei', 'Yang Liu', 'Hang Su', 'Jun Zhu', 'Xinfeng Li', 'Yitong Sun', 'Jie Zhang', 'Jinzhao Hu', 'Sha Xu', 'Yitong Yang', 'Jialing Tao', 'Hui Xue']&lt;/li&gt;&lt;li&gt;Tags: ['safety alignment', 'red teaming', 'jailbreaking', 'constructive engagement', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.01909</guid><pubDate>Fri, 05 Sep 2025 00:00:00 +0000</pubDate></item><item><title>Forewarned is Forearmed: Pre-Synthesizing Jailbreak-like Instructions to Enhance LLM Safety Guardrail to Potential Attacks</title><link>https://arxiv.org/abs/2508.20038</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes IMAGINE framework to generate jailbreak-like instructions&lt;/li&gt;&lt;li&gt;Aims to fill distributional gap between training data and real attacks&lt;/li&gt;&lt;li&gt;Demonstrates reduced attack success rates on multiple LLMs&lt;/li&gt;&lt;li&gt;Maintains model utility while enhancing safety&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Sheng Liu', 'Qiang Sheng', 'Danding Wang', 'Yang Li', 'Guang Yang', 'Juan Cao']&lt;/li&gt;&lt;li&gt;Tags: ['jailbreak', 'red teaming', 'LLM safety', 'adversarial prompting', 'data augmentation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2508.20038</guid><pubDate>Fri, 05 Sep 2025 00:00:00 +0000</pubDate></item><item><title>Context Reasoner: Incentivizing Reasoning Capability for Contextualized Privacy and Safety Compliance via Reinforcement Learning</title><link>https://arxiv.org/abs/2505.14585</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Uses reinforcement learning to enhance LLM compliance with GDPR, EU AI Act, and HIPAA&lt;/li&gt;&lt;li&gt;Aligns safety/privacy with Contextual Integrity theory&lt;/li&gt;&lt;li&gt;Improves reasoning capabilities while maintaining compliance&lt;/li&gt;&lt;li&gt;Demonstrates +8.58% accuracy in safety/privacy benchmarks&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Wenbin Hu', 'Haoran Li', 'Huihao Jing', 'Qi Hu', 'Ziqian Zeng', 'Sirui Han', 'Heli Xu', 'Tianshu Chu', 'Peizhao Hu', 'Yangqiu Song']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'safety evaluation', 'robustness', 'compliance', 'reinforcement learning']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2505.14585</guid><pubDate>Fri, 05 Sep 2025 00:00:00 +0000</pubDate></item><item><title>DynaSaur: Large Language Agents Beyond Predefined Actions</title><link>https://arxiv.org/abs/2411.01747</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces DynaSaur framework for dynamic action generation in LLM agents&lt;/li&gt;&lt;li&gt;Improves adaptability and recovery in edge case scenarios&lt;/li&gt;&lt;li&gt;Outperforms fixed-action-set methods in benchmarks&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Dang Nguyen', 'Viet Dac Lai', 'Seunghyun Yoon', 'Ryan A. Rossi', 'Handong Zhao', 'Ruiyi Zhang', 'Puneet Mathur', 'Nedim Lipka', 'Yu Wang', 'Trung Bui', 'Franck Dernoncourt', 'Tianyi Zhou']&lt;/li&gt;&lt;li&gt;Tags: ['robustness', 'safety evaluation', 'LLM agents']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2411.01747</guid><pubDate>Fri, 05 Sep 2025 00:00:00 +0000</pubDate></item><item><title>Evaluating the Robustness of Retrieval-Augmented Generation to Adversarial Evidence in the Health Domain</title><link>https://arxiv.org/abs/2509.03787</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Evaluates RAG system robustness against adversarial evidence in health domain&lt;/li&gt;&lt;li&gt;Tests combinations of helpful, harmful, and adversarial documents&lt;/li&gt;&lt;li&gt;Finds that helpful evidence can mitigate adversarial impact&lt;/li&gt;&lt;li&gt;Provides insights for safer RAG system design&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Shakiba Amirshahi', 'Amin Bigdeli', 'Charles L. A. Clarke', 'Amira Ghenai']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial prompting', 'robustness', 'safety evaluation', 'alignment', 'red teaming']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.03787</guid><pubDate>Fri, 05 Sep 2025 00:00:00 +0000</pubDate></item><item><title>Inverse IFEval: Can LLMs Unlearn Stubborn Training Conventions to Follow Real Instructions?</title><link>https://arxiv.org/abs/2509.04292</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces Inverse IFEval benchmark to measure LLMs' ability to override training biases&lt;/li&gt;&lt;li&gt;Features 8 challenge types including adversarial instructions&lt;/li&gt;&lt;li&gt;Evaluated on leading LLMs showing need for adaptability&lt;/li&gt;&lt;li&gt;Emphasizes importance of unconventional context handling for safety&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Qinyan Zhang', 'Xinping Lei', 'Ruijie Miao', 'Yu Fu', 'Haojie Fan', 'Le Chang', 'Jiafan Hou', 'Dingling Zhang', 'Zhongfei Hou', 'Ziqiang Yang', 'Changxin Pu', 'Fei Hu', 'Jingkai Liu', 'Mengyun Liu', 'Yang Liu', 'Xiang Gao', 'Jiaheng Liu', 'Tong Yang', 'Zaiyuan Wang', 'Ge Zhang', 'Wenhao Huang']&lt;/li&gt;&lt;li&gt;Tags: ['red teaming', 'adversarial prompting', 'safety evaluation', 'alignment', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.04292</guid><pubDate>Fri, 05 Sep 2025 00:00:00 +0000</pubDate></item><item><title>False Sense of Security: Why Probing-based Malicious Input Detection Fails to Generalize</title><link>https://arxiv.org/abs/2509.03888</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Systematically re-examines probing-based malicious input detection for LLMs&lt;/li&gt;&lt;li&gt;Finds that probes learn superficial patterns (instructional formats, trigger words) instead of semantic harmfulness&lt;/li&gt;&lt;li&gt;Highlights need for better model design and evaluation protocols&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Cheng Wang', 'Zeming Wei', 'Qin Liu', 'Muhao Chen']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'safety evaluation', 'adversarial prompting', 'model robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.03888</guid><pubDate>Fri, 05 Sep 2025 00:00:00 +0000</pubDate></item><item><title>A Comprehensive Survey on Trustworthiness in Reasoning with Large Language Models</title><link>https://arxiv.org/abs/2509.03871</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Surveys recent work on reasoning models and CoT techniques&lt;/li&gt;&lt;li&gt;Focuses on five core dimensions: truthfulness, safety, robustness, fairness, privacy&lt;/li&gt;&lt;li&gt;Provides structured overviews of studies, methodologies, findings, and limitations&lt;/li&gt;&lt;li&gt;Highlights vulnerabilities in safety, robustness, and privacy of reasoning models&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Survey&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yanbo Wang', 'Yongcan Yu', 'Jian Liang', 'Ran He']&lt;/li&gt;&lt;li&gt;Tags: ['safety', 'robustness', 'privacy', 'trustworthiness', 'LLM', 'survey']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.03871</guid><pubDate>Fri, 05 Sep 2025 00:00:00 +0000</pubDate></item><item><title>Real-Time Detection of Hallucinated Entities in Long-Form Generation</title><link>https://arxiv.org/abs/2509.03531</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces real-time hallucination detection for long-form LLM outputs&lt;/li&gt;&lt;li&gt;Focuses on entity-level hallucinations (names, dates, citations)&lt;/li&gt;&lt;li&gt;Uses web search-based annotations and efficient linear probes&lt;/li&gt;&lt;li&gt;Achieves high accuracy across multiple model families&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Oscar Obeso', 'Andy Arditi', 'Javier Ferrando', 'Joshua Freeman', 'Cameron Holmes', 'Neel Nanda']&lt;/li&gt;&lt;li&gt;Tags: ['hallucination detection', 'safety evaluation', 'long-form generation', 'entity-level safety', 'real-time detection']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.03531</guid><pubDate>Fri, 05 Sep 2025 00:00:00 +0000</pubDate></item><item><title>EigenBench: A Comparative Behavioral Measure of Value Alignment</title><link>https://arxiv.org/abs/2509.01938</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;EigenBench is a black-box method for benchmarking language models' value alignment&lt;/li&gt;&lt;li&gt;Uses models to judge each other's outputs and aggregates with EigenTrust&lt;/li&gt;&lt;li&gt;Measures alignment without ground truth labels by leveraging ensemble judgments&lt;/li&gt;&lt;li&gt;Tests show variance is mostly from prompts but some from model disposition&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jonathn Chang', 'Leonhard Piff', 'Suvadip Sana', 'Jasmine X. Li', 'Lionel Levine']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'safety evaluation', 'benchmarking', 'EigenTrust', 'model comparison']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.01938</guid><pubDate>Fri, 05 Sep 2025 00:00:00 +0000</pubDate></item><item><title>WASP: A Weight-Space Approach to Detecting Learned Spuriousness</title><link>https://arxiv.org/abs/2410.18970</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces WASP: a weight-space approach to detect spurious correlations in ML models&lt;/li&gt;&lt;li&gt;Analyzes model weights instead of data/errors to find hidden spurious correlations&lt;/li&gt;&lt;li&gt;Works across multiple modalities (image, text)&lt;/li&gt;&lt;li&gt;Can detect issues not present in training/validation data&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Cristian Daniel P\\u{a}duraru', 'Antonio B\\u{a}rb\\u{a}lau', 'Radu Filipescu', 'Andrei Liviu Nicolicioiu', 'Elena Burceanu']&lt;/li&gt;&lt;li&gt;Tags: ['safety evaluation', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2410.18970</guid><pubDate>Fri, 05 Sep 2025 00:00:00 +0000</pubDate></item><item><title>Real-Time Detection of Hallucinated Entities in Long-Form Generation</title><link>https://arxiv.org/abs/2509.03531</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces real-time hallucination detection for long-form LLM outputs&lt;/li&gt;&lt;li&gt;Focuses on entity-level hallucinations (names, dates, citations)&lt;/li&gt;&lt;li&gt;Uses web search-based annotations and efficient linear probes&lt;/li&gt;&lt;li&gt;Achieves high accuracy across multiple model families&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Oscar Obeso', 'Andy Arditi', 'Javier Ferrando', 'Joshua Freeman', 'Cameron Holmes', 'Neel Nanda']&lt;/li&gt;&lt;li&gt;Tags: ['hallucination detection', 'safety evaluation', 'long-form generation', 'entity-level safety', 'real-time detection']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.03531</guid><pubDate>Fri, 05 Sep 2025 00:00:00 +0000</pubDate></item><item><title>Defending LVLMs Against Vision Attacks through Partial-Perception Supervision</title><link>https://arxiv.org/abs/2412.12722</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes DPS (Defense through Partial-Perception Supervision) to defend LVLMs against vision attacks&lt;/li&gt;&lt;li&gt;Uses responses from partially perceived images to supervise the model's original response&lt;/li&gt;&lt;li&gt;Reduces attack success rate by 76.3% across multiple datasets and models&lt;/li&gt;&lt;li&gt;Maintains response quality on clean inputs by adjusting confidence levels&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Qi Zhou', 'Tianlin Li', 'Qing Guo', 'Dongxia Wang', 'Yun Lin', 'Yang Liu', 'Jin Song Dong']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial prompting', 'model robustness', 'vision attacks', 'multimodal security', 'LVLM defense']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2412.12722</guid><pubDate>Fri, 05 Sep 2025 00:00:00 +0000</pubDate></item><item><title>EigenBench: A Comparative Behavioral Measure of Value Alignment</title><link>https://arxiv.org/abs/2509.01938</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;EigenBench is a black-box method for benchmarking language models' value alignment&lt;/li&gt;&lt;li&gt;Uses models to judge each other's outputs and aggregates with EigenTrust&lt;/li&gt;&lt;li&gt;Measures alignment without ground truth labels by leveraging ensemble judgments&lt;/li&gt;&lt;li&gt;Tests show variance is mostly from prompts but some from model disposition&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jonathn Chang', 'Leonhard Piff', 'Suvadip Sana', 'Jasmine X. Li', 'Lionel Levine']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'safety evaluation', 'benchmarking', 'EigenTrust', 'model comparison']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.01938</guid><pubDate>Fri, 05 Sep 2025 00:00:00 +0000</pubDate></item><item><title>Oyster-I: Beyond Refusal -- Constructive Safety Alignment for Responsible Language Models</title><link>https://arxiv.org/abs/2509.01909</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces Constructive Safety Alignment (CSA) paradigm for LLMs&lt;/li&gt;&lt;li&gt;Implements CSA in Oyster-I (Oy1) model&lt;/li&gt;&lt;li&gt;Achieves state-of-the-art safety while retaining capabilities&lt;/li&gt;&lt;li&gt;Shows robustness against jailbreak attacks and constructive engagement&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Ranjie Duan', 'Jiexi Liu', 'Xiaojun Jia', 'Shiji Zhao', 'Ruoxi Cheng', 'Fengxiang Wang', 'Cheng Wei', 'Yong Xie', 'Chang Liu', 'Defeng Li', 'Yinpeng Dong', 'Yichi Zhang', 'Yuefeng Chen', 'Chongwen Wang', 'Xingjun Ma', 'Xingxing Wei', 'Yang Liu', 'Hang Su', 'Jun Zhu', 'Xinfeng Li', 'Yitong Sun', 'Jie Zhang', 'Jinzhao Hu', 'Sha Xu', 'Yitong Yang', 'Jialing Tao', 'Hui Xue']&lt;/li&gt;&lt;li&gt;Tags: ['safety alignment', 'red teaming', 'jailbreaking', 'constructive engagement', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.01909</guid><pubDate>Fri, 05 Sep 2025 00:00:00 +0000</pubDate></item><item><title>WASP: A Weight-Space Approach to Detecting Learned Spuriousness</title><link>https://arxiv.org/abs/2410.18970</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces WASP: a weight-space approach to detect spurious correlations in ML models&lt;/li&gt;&lt;li&gt;Analyzes model weights instead of data/errors to find hidden spurious correlations&lt;/li&gt;&lt;li&gt;Works across multiple modalities (image, text)&lt;/li&gt;&lt;li&gt;Can detect issues not present in training/validation data&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Cristian Daniel P\\u{a}duraru', 'Antonio B\\u{a}rb\\u{a}lau', 'Radu Filipescu', 'Andrei Liviu Nicolicioiu', 'Elena Burceanu']&lt;/li&gt;&lt;li&gt;Tags: ['safety evaluation', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2410.18970</guid><pubDate>Fri, 05 Sep 2025 00:00:00 +0000</pubDate></item><item><title>NeuroBreak: Unveil Internal Jailbreak Mechanisms in Large Language Models</title><link>https://arxiv.org/abs/2509.03985</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces NeuroBreak, a system for analyzing neuron-level safety mechanisms in LLMs&lt;/li&gt;&lt;li&gt;Focuses on identifying vulnerabilities to jailbreak attacks through layer-wise representation probing&lt;/li&gt;&lt;li&gt;Provides mechanistic insights for developing next-generation defense strategies&lt;/li&gt;&lt;li&gt;Includes quantitative evaluations and case studies&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Chuhan Zhang', 'Ye Zhang', 'Bowen Shi', 'Yuyou Gan', 'Tianyu Du', 'Shouling Ji', 'Dazhan Deng', 'Yingcai Wu']&lt;/li&gt;&lt;li&gt;Tags: ['jailbreak', 'red teaming', 'LLM security', 'neuron analysis', 'adversarial prompting']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.03985</guid><pubDate>Fri, 05 Sep 2025 00:00:00 +0000</pubDate></item><item><title>A Comprehensive Survey on Trustworthiness in Reasoning with Large Language Models</title><link>https://arxiv.org/abs/2509.03871</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Surveys recent work on reasoning models and CoT techniques&lt;/li&gt;&lt;li&gt;Focuses on five core dimensions: truthfulness, safety, robustness, fairness, privacy&lt;/li&gt;&lt;li&gt;Provides structured overviews of studies, methodologies, findings, and limitations&lt;/li&gt;&lt;li&gt;Highlights vulnerabilities in safety, robustness, and privacy of reasoning models&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Survey&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yanbo Wang', 'Yongcan Yu', 'Jian Liang', 'Ran He']&lt;/li&gt;&lt;li&gt;Tags: ['safety', 'robustness', 'privacy', 'trustworthiness', 'LLM', 'survey']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.03871</guid><pubDate>Fri, 05 Sep 2025 00:00:00 +0000</pubDate></item><item><title>Real-Time Detection of Hallucinated Entities in Long-Form Generation</title><link>https://arxiv.org/abs/2509.03531</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces real-time hallucination detection for long-form LLM outputs&lt;/li&gt;&lt;li&gt;Focuses on entity-level hallucinations (names, dates, citations)&lt;/li&gt;&lt;li&gt;Uses web search-based annotations and efficient linear probes&lt;/li&gt;&lt;li&gt;Achieves high accuracy across multiple model families&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Oscar Obeso', 'Andy Arditi', 'Javier Ferrando', 'Joshua Freeman', 'Cameron Holmes', 'Neel Nanda']&lt;/li&gt;&lt;li&gt;Tags: ['hallucination detection', 'safety evaluation', 'long-form generation', 'entity-level safety', 'real-time detection']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.03531</guid><pubDate>Fri, 05 Sep 2025 00:00:00 +0000</pubDate></item><item><title>Meta-Policy Reflexion: Reusable Reflective Memory and Rule Admissibility for Resource-Efficient LLM Agent</title><link>https://arxiv.org/abs/2509.03990</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces Meta-Policy Reflexion (MPR) framework for LLM agents&lt;/li&gt;&lt;li&gt;Includes rule admissibility checks to reduce unsafe/invalid actions&lt;/li&gt;&lt;li&gt;Validates safety improvements through empirical results&lt;/li&gt;&lt;li&gt;Focuses on cross-task adaptability and resource efficiency&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Chunlong Wu', 'Zhibo Qu']&lt;/li&gt;&lt;li&gt;Tags: ['safety', 'robustness', 'rule_admissibility', 'LLM_agents', 'meta_policy']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.03990</guid><pubDate>Fri, 05 Sep 2025 00:00:00 +0000</pubDate></item><item><title>RAGuard: A Novel Approach for in-context Safe Retrieval Augmented Generation for LLMs</title><link>https://arxiv.org/abs/2509.03768</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces RAGuard and SafetyClamp to integrate safety-critical documents in LLM retrieval&lt;/li&gt;&lt;li&gt;Evaluates using Technical and Safety Recall metrics across different retrieval paradigms&lt;/li&gt;&lt;li&gt;Achieves &gt;50% Safety Recall while maintaining &gt;60% Technical Recall&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Connor Walker', 'Koorosh Aslansefat', 'Mohammad Naveed Akram', 'Yiannis Papadopoulos']&lt;/li&gt;&lt;li&gt;Tags: ['safety', 'LLM', 'RAG', 'evaluation', 'recall']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.03768</guid><pubDate>Fri, 05 Sep 2025 00:00:00 +0000</pubDate></item><item><title>PersonaTeaming: Exploring How Introducing Personas Can Improve Automated AI Red-Teaming</title><link>https://arxiv.org/abs/2509.03728</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces PersonaTeaming method for persona-based automated red-teaming&lt;/li&gt;&lt;li&gt;Compares red-teaming expert vs regular user personas in prompt mutation&lt;/li&gt;&lt;li&gt;Develops dynamic persona generation algorithm&lt;/li&gt;&lt;li&gt;Introduces new mutation distance metrics for prompt diversity&lt;/li&gt;&lt;li&gt;Shows up to 144.1% improvement in attack success over state-of-the-art&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Wesley Hanwen Deng', 'Sunnie S. Y. Kim', 'Akshita Jha', 'Ken Holstein', 'Motahhare Eslami', 'Lauren Wilcox', 'Leon A Gatys']&lt;/li&gt;&lt;li&gt;Tags: ['red teaming', 'adversarial prompting', 'personas', 'automated red-teaming', 'prompt mutation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.03728</guid><pubDate>Fri, 05 Sep 2025 00:00:00 +0000</pubDate></item></channel></rss>