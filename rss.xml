<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>AI Security Paper Digest</title><link>https://kentaroh-toyoda.github.io/ai-security-paper-digest-rss/rss.xml</link><description>Curated papers on AI security from OpenAlex and ArXiv</description><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><lastBuildDate>Tue, 12 Aug 2025 22:41:27 +0000</lastBuildDate><item><title>BadPatch: Diffusion-Based Generation of Physical Adversarial Patches</title><link>https://arxiv.org/abs/2412.01440</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces BadPatch, a diffusion-based framework for generating customizable and stealthy adversarial patches&lt;/li&gt;&lt;li&gt;Allows user-provided reference images and masks for patch customization&lt;/li&gt;&lt;li&gt;Achieves balance between attack effectiveness and natural appearance&lt;/li&gt;&lt;li&gt;Creates AdvT-shirt-1K dataset with 1000+ images for testing defenses&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Zhixiang Wang', 'Xingjun Ma', 'Yu-Gang Jiang']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial attacks', 'diffusion models', 'physical attacks', 'stealth', 'dataset']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2412.01440</guid><pubDate>Tue, 12 Aug 2025 00:00:00 +0000</pubDate></item><item><title>Multimodal Deception in Explainable AI: Concept-Level Backdoor Attacks on Concept Bottleneck Models</title><link>https://arxiv.org/abs/2410.04823</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces concept-level backdoor attacks (CAT/CAT+) on Concept Bottleneck Models (CBMs)&lt;/li&gt;&lt;li&gt;Injects triggers into conceptual representations during training&lt;/li&gt;&lt;li&gt;Maintains clean data performance while enabling precise prediction manipulation&lt;/li&gt;&lt;li&gt;Evaluates attack success, stealth, and model utility preservation&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Songning Lai', 'Jiayu Yang', 'Yu Huang', 'Lijie Hu', 'Tianlang Xue', 'Zhangyi Hu', 'Jiaxu Li', 'Haicheng Liao', 'Yutao Yue']&lt;/li&gt;&lt;li&gt;Tags: ['backdoor attacks', 'data poisoning', 'security', 'explainable AI', 'concept bottleneck models']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2410.04823</guid><pubDate>Tue, 12 Aug 2025 00:00:00 +0000</pubDate></item><item><title>IPBA: Imperceptible Perturbation Backdoor Attack in Federated Self-Supervised Learning</title><link>https://arxiv.org/abs/2508.08031</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes IPBA, an imperceptible backdoor attack on FSSL&lt;/li&gt;&lt;li&gt;Addresses transferability, feature entanglement, and out-of-distribution issues&lt;/li&gt;&lt;li&gt;Uses Sliced-Wasserstein distance for trigger optimization&lt;/li&gt;&lt;li&gt;Demonstrates robustness against defenses in experiments&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jiayao Wang', 'Yang Song', 'Zhendong Zhao', 'Jiale Zhang', 'Qilin Wu', 'Junwu Zhu', 'Dongfang Zhao']&lt;/li&gt;&lt;li&gt;Tags: ['backdoor attacks', 'federated learning', 'data poisoning', 'privacy attacks', 'adversarial machine learning']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2508.08031</guid><pubDate>Tue, 12 Aug 2025 00:00:00 +0000</pubDate></item><item><title>Boosting Active Defense Persistence: A Two-Stage Defense Framework Combining Interruption and Poisoning Against Deepfake</title><link>https://arxiv.org/abs/2508.07795</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Two-Stage Defense Framework (TSDF) combining interruption and poisoning&lt;/li&gt;&lt;li&gt;Uses dual-function adversarial perturbations to distort results and poison data&lt;/li&gt;&lt;li&gt;Aims to prevent attacker model retraining and improve defense persistence&lt;/li&gt;&lt;li&gt;Experiments show traditional methods fail under retraining, TSDF maintains effectiveness&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Hongrui Zheng', 'Yuezun Li', 'Liejun Wang', 'Yunfeng Diao', 'Zhiqing Guo']&lt;/li&gt;&lt;li&gt;Tags: ['data poisoning', 'adversarial attacks', 'deepfake defense', 'persistence']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2508.07795</guid><pubDate>Tue, 12 Aug 2025 00:00:00 +0000</pubDate></item><item><title>ForensicsSAM: Toward Robust and Unified Image Forgery Detection and Localization Resisting to Adversarial Attack</title><link>https://arxiv.org/abs/2508.07402</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes ForensicsSAM, a unified framework for image forgery detection and localization with built-in adversarial robustness&lt;/li&gt;&lt;li&gt;Introduces forgery experts, an adversary detector, and adversary experts to enhance robustness against adversarial attacks&lt;/li&gt;&lt;li&gt;Demonstrates superior resistance to various adversarial attack methods while maintaining state-of-the-art performance&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Rongxuan Peng', 'Shunquan Tan', 'Chenqi Kong', 'Anwei Luo', 'Alex C. Kot', 'Jiwu Huang']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial attacks', 'robustness', 'image forgery detection', 'security']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2508.07402</guid><pubDate>Tue, 12 Aug 2025 00:00:00 +0000</pubDate></item><item><title>ACCESS DENIED INC: The First Benchmark Environment for Sensitivity Awareness</title><link>https://arxiv.org/abs/2506.00964</link><description>&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Other&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Dren Fazlija', 'Arkadij Orlov', 'Sandipan Sikdar']&lt;/li&gt;&lt;li&gt;Tags: []&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.00964</guid><pubDate>Tue, 12 Aug 2025 00:00:00 +0000</pubDate></item><item><title>Steering the CensorShip: Uncovering Representation Vectors for LLM "Thought" Control</title><link>https://arxiv.org/abs/2504.17130</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Presents a method to find vectors controlling compliance and thought suppression in safety-tuned LLMs&lt;/li&gt;&lt;li&gt;Analyzes both refusal-compliance and reasoning suppression dimensions&lt;/li&gt;&lt;li&gt;Demonstrates ability to remove censorship by applying negative vector multiples&lt;/li&gt;&lt;li&gt;Provides public code for further research and testing&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Hannah Cyberey', 'David Evans']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'safety', 'red teaming', 'adversarial prompting', 'model steering']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2504.17130</guid><pubDate>Tue, 12 Aug 2025 00:00:00 +0000</pubDate></item><item><title>Jinx: Unlimited LLMs for Probing Alignment Failures</title><link>https://arxiv.org/abs/2508.08243</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces Jinx, a helpful-only LLM variant for probing alignment failures&lt;/li&gt;&lt;li&gt;Enables researchers to evaluate safety boundaries and study failure modes&lt;/li&gt;&lt;li&gt;Serves as a tool for red teaming and alignment evaluation&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jiahao Zhao', 'Liwei Dong']&lt;/li&gt;&lt;li&gt;Tags: ['red teaming', 'alignment', 'safety evaluation', 'LLM']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2508.08243</guid><pubDate>Tue, 12 Aug 2025 00:00:00 +0000</pubDate></item><item><title>Can You Trick the Grader? Adversarial Persuasion of LLM Judges</title><link>https://arxiv.org/abs/2508.07805</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Study tests adversarial persuasion techniques on LLM judges for math scoring&lt;/li&gt;&lt;li&gt;Uses Aristotle's rhetorical principles to create 7 persuasion methods&lt;/li&gt;&lt;li&gt;Finds up to 8% score inflation for incorrect solutions&lt;/li&gt;&lt;li&gt;Vulnerability persists across model sizes and counter prompts&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yerin Hwang', 'Dongryeol Lee', 'Taegwan Kang', 'Yongil Kim', 'Kyomin Jung']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial prompting', 'LLM red teaming', 'security', 'persuasion attacks', 'math benchmarks']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2508.07805</guid><pubDate>Tue, 12 Aug 2025 00:00:00 +0000</pubDate></item><item><title>Exploring Causal Effect of Social Bias on Faithfulness Hallucinations in Large Language Models</title><link>https://arxiv.org/abs/2508.07753</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Investigates causal link between social bias and faithfulness hallucinations in LLMs&lt;/li&gt;&lt;li&gt;Uses Structural Causal Model (SCM) to establish causality and control confounders&lt;/li&gt;&lt;li&gt;Introduces Bias Intervention Dataset (BID) for precise causal effect measurement&lt;/li&gt;&lt;li&gt;Experiments show significant causal effects across multiple LLMs&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Zhenliang Zhang', 'Junzhe Zhang', 'Xinyu Hu', 'HuiXuan Zhang', 'Xiaojun Wan']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'safety evaluation', 'robustness', 'social bias', 'hallucinations']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2508.07753</guid><pubDate>Tue, 12 Aug 2025 00:00:00 +0000</pubDate></item><item><title>Omni-SafetyBench: A Benchmark for Safety Evaluation of Audio-Visual Large Language Models</title><link>https://arxiv.org/abs/2508.07173</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduced Omni-SafetyBench, a benchmark for evaluating safety in audio-visual LLMs&lt;/li&gt;&lt;li&gt;Proposed Safety-score and CMSC-score metrics for comprehensive safety assessment&lt;/li&gt;&lt;li&gt;Evaluated multiple OLLMs, revealing significant safety vulnerabilities&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Leyi Pan', 'Zheyu Fu', 'Yunpeng Zhai', 'Shuchang Tao', 'Sheng Guan', 'Shiyu Huang', 'Lingzhe Zhang', 'Zhaoyang Liu', 'Bolin Ding', 'Felix Henry', 'Lijie Wen', 'Aiwei Liu']&lt;/li&gt;&lt;li&gt;Tags: ['safety evaluation', 'benchmark', 'multimodal', 'adversarial prompting', 'red teaming']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2508.07173</guid><pubDate>Tue, 12 Aug 2025 00:00:00 +0000</pubDate></item><item><title>Gradient Surgery for Safe LLM Fine-Tuning</title><link>https://arxiv.org/abs/2508.07172</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Addresses vulnerability in LLM fine-tuning where malicious examples can compromise safety alignment&lt;/li&gt;&lt;li&gt;Introduces SafeGrad method using gradient surgery to resolve conflicting gradients between task performance and safety&lt;/li&gt;&lt;li&gt;Employs KL-divergence alignment loss to maintain safety profile from foundation model&lt;/li&gt;&lt;li&gt;Demonstrates state-of-the-art defense against high harmful ratios without task performance loss&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Biao Yi', 'Jiahao Li', 'Baolei Zhang', 'Lihai Nie', 'Tong Li', 'Tiansheng Huang', 'Zheli Liu']&lt;/li&gt;&lt;li&gt;Tags: ['safety', 'robustness', 'alignment', 'gradient methods', 'fine-tuning']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2508.07172</guid><pubDate>Tue, 12 Aug 2025 00:00:00 +0000</pubDate></item><item><title>Meta-Unlearning on Diffusion Models: Preventing Relearning Unlearned Concepts</title><link>https://arxiv.org/abs/2410.12777</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces meta-unlearning to prevent relearning of unlearned concepts in diffusion models&lt;/li&gt;&lt;li&gt;Adds a meta objective to existing unlearning methods to trigger self-destruction of related benign concepts&lt;/li&gt;&lt;li&gt;Validated on Stable Diffusion models with empirical experiments and ablation studies&lt;/li&gt;&lt;li&gt;Compatible with most existing unlearning methods&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Hongcheng Gao', 'Tianyu Pang', 'Chao Du', 'Taihang Hu', 'Zhijie Deng', 'Min Lin']&lt;/li&gt;&lt;li&gt;Tags: ['unlearning', 'diffusion models', 'adversarial finetuning', 'robustness', 'security']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2410.12777</guid><pubDate>Tue, 12 Aug 2025 00:00:00 +0000</pubDate></item><item><title>Robust Anomaly Detection in O-RAN: Leveraging LLMs against Data Manipulation Attacks</title><link>https://arxiv.org/abs/2508.08029</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Investigates using LLMs for anomaly detection in O-RAN to withstand hypoglyph data manipulation attacks&lt;/li&gt;&lt;li&gt;Shows LLM-based xApps maintain operation and don't crash unlike traditional ML models&lt;/li&gt;&lt;li&gt;Initial detection accuracy needs improvement but demonstrates robustness&lt;/li&gt;&lt;li&gt;Achieves low detection latency (&lt;0.07s) suitable for Near-RT RIC&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Thusitha Dayaratne', 'Ngoc Duy Pham', 'Viet Vo', 'Shangqi Lai', 'Sharif Abuadbba', 'Hajime Suzuki', 'Xingliang Yuan', 'Carsten Rudolph']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial attacks', 'data manipulation', 'robustness', 'LLM security', 'anomaly detection']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2508.08029</guid><pubDate>Tue, 12 Aug 2025 00:00:00 +0000</pubDate></item><item><title>Multi-Turn Jailbreaks Are Simpler Than They Seem</title><link>https://arxiv.org/abs/2508.07646</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Empirical analysis of multi-turn jailbreak attacks on LLMs&lt;/li&gt;&lt;li&gt;Found multi-turn attacks equivalent to resampling single-turn attacks&lt;/li&gt;&lt;li&gt;Higher reasoning effort may increase attack success&lt;/li&gt;&lt;li&gt;Implications for AI safety evaluation&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Xiaoxue Yang', 'Jaeha Lee', 'Anna-Katharina Dick', 'Jasper Timm', 'Fei Xie', 'Diogo Cruz']&lt;/li&gt;&lt;li&gt;Tags: ['jailbreaking', 'red teaming', 'LLM security', 'multi-turn attacks', 'safety evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2508.07646</guid><pubDate>Tue, 12 Aug 2025 00:00:00 +0000</pubDate></item><item><title>POEX: Towards Policy Executable Jailbreak Attacks Against the LLM-based Robots</title><link>https://arxiv.org/abs/2412.16633</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces POEX, a red-teaming framework for jailbreaking LLM-based robots using policy-executable optimization&lt;/li&gt;&lt;li&gt;Identifies unique challenges in adapting traditional jailbreak attacks to physical robots&lt;/li&gt;&lt;li&gt;Demonstrates efficacy in real-world and simulated environments with transferability across LLMs&lt;/li&gt;&lt;li&gt;Proposes prompt-based and model-based defenses against such attacks&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Xuancun Lu', 'Zhengxian Huang', 'Xinfeng Li', 'Chi Zhang', 'Xiaoyu ji', 'Wenyuan Xu']&lt;/li&gt;&lt;li&gt;Tags: ['jailbreak', 'red teaming', 'LLM', 'robotics', 'security']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2412.16633</guid><pubDate>Tue, 12 Aug 2025 00:00:00 +0000</pubDate></item><item><title>Chain of Thought Still Thinks Fast: APriCoT Helps with Thinking Slow</title><link>https://arxiv.org/abs/2408.08651</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Investigates biases in chain of thought reasoning for language models&lt;/li&gt;&lt;li&gt;Introduces APriCoT method to mitigate bias influence&lt;/li&gt;&lt;li&gt;Demonstrates improved accuracy and reduced bias with APriCoT&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Kyle Moore', 'Jesse Roberts', 'Thao Pham', 'Douglas Fisher']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'robustness', 'safety evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2408.08651</guid><pubDate>Tue, 12 Aug 2025 00:00:00 +0000</pubDate></item><item><title>Personalized Constitutionally-Aligned Agentic Superego: Secure AI Behavior Aligned to Diverse Human Values</title><link>https://arxiv.org/abs/2506.13774</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces a 'superego' agent for personalized AI alignment and safety&lt;/li&gt;&lt;li&gt;Uses user-selected 'Creed Constitutions' and real-time compliance validation&lt;/li&gt;&lt;li&gt;Demonstrates significant harm reduction in HarmBench and AgentHarm benchmarks&lt;/li&gt;&lt;li&gt;Achieves near-perfect refusal rates for harmful requests&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Nell Watson', 'Ahmed Amer', 'Evan Harris', 'Preeti Ravindra', 'Shujun Zhang']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'safety', 'compliance', 'constitutions', 'superego']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.13774</guid><pubDate>Tue, 12 Aug 2025 00:00:00 +0000</pubDate></item><item><title>BadPromptFL: A Novel Backdoor Threat to Prompt-based Federated Learning in Multimodal Models</title><link>https://arxiv.org/abs/2508.08040</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces BadPromptFL, the first backdoor attack on prompt-based federated learning in multimodal models&lt;/li&gt;&lt;li&gt;Compromised clients inject poisoned prompts into global aggregation, enabling universal backdoor activation&lt;/li&gt;&lt;li&gt;Achieves high attack success rates (&gt;90%) with minimal visibility and limited client participation&lt;/li&gt;&lt;li&gt;Validated across multiple datasets and aggregation protocols&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Maozhen Zhang', 'Mengnan Zhao', 'Bo Wang']&lt;/li&gt;&lt;li&gt;Tags: ['backdoor attack', 'federated learning', 'multimodal models', 'prompt injection', 'adversarial prompting']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2508.08040</guid><pubDate>Tue, 12 Aug 2025 00:00:00 +0000</pubDate></item><item><title>Learning to Align, Aligning to Learn: A Unified Approach for Self-Optimized Alignment</title><link>https://arxiv.org/abs/2508.07750</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces GRAO, a unified framework combining SFT and RL for alignment&lt;/li&gt;&lt;li&gt;Proposes multi-sample generation, Group Direct Alignment Loss, and reference-aware updates&lt;/li&gt;&lt;li&gt;Demonstrates improved performance on human alignment tasks&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Haowen Wang', 'Yun Yue', 'Zhiling Ye', 'Shuowen Zhang', 'Lei Fan', 'Jiaxin Liang', 'Jiadi Jiang', 'Cheng Wei', 'Jingyuan Deng', 'Xudong Han', 'Ji Li', 'Chunxiao Guo', 'Peng Wei', 'Jian Wang', 'Jinjie Gu']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'reinforcement learning', 'supervised fine-tuning', 'sample efficiency']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2508.07750</guid><pubDate>Tue, 12 Aug 2025 00:00:00 +0000</pubDate></item><item><title>Chimera: Harnessing Multi-Agent LLMs for Automatic Insider Threat Simulation</title><link>https://arxiv.org/abs/2508.07745</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes Chimera, a multi-agent LLM framework for simulating insider threats&lt;/li&gt;&lt;li&gt;Generates a new dataset (ChimeraLog) with diverse and realistic logs&lt;/li&gt;&lt;li&gt;Evaluates existing ITD methods on the new dataset, showing lower performance&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jiongchi Yu', 'Xiaofei Xie', 'Qiang Hu', 'Yuhan Ma', 'Ziming Zhao']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'adversarial prompting', 'security standards']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2508.07745</guid><pubDate>Tue, 12 Aug 2025 00:00:00 +0000</pubDate></item><item><title>A Spin Glass Characterization of Neural Networks</title><link>https://arxiv.org/abs/2508.07397</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces a spin glass model to analyze neural network structures&lt;/li&gt;&lt;li&gt;Provides a new descriptor for individual network instances&lt;/li&gt;&lt;li&gt;Potential applications in safety verification and vulnerability detection&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jun Li']&lt;/li&gt;&lt;li&gt;Tags: ['safety evaluation', 'robustness', 'security standards']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2508.07397</guid><pubDate>Tue, 12 Aug 2025 00:00:00 +0000</pubDate></item><item><title>ObfusQAte: A Proposed Framework to Evaluate LLM Robustness on Obfuscated Factual Question Answering</title><link>https://arxiv.org/abs/2508.07321</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes ObfusQAte framework for evaluating LLM robustness on obfuscated QA&lt;/li&gt;&lt;li&gt;Introduces ObfusQA benchmark with three obfuscation dimensions&lt;/li&gt;&lt;li&gt;Observes LLM failures under obfuscated inputs&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Shubhra Ghosh', 'Abhilekh Borah', 'Aditya Kumar Guru', 'Kripabandhu Ghosh']&lt;/li&gt;&lt;li&gt;Tags: ['robustness', 'safety evaluation', 'adversarial prompting', 'red teaming', 'LLM']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2508.07321</guid><pubDate>Tue, 12 Aug 2025 00:00:00 +0000</pubDate></item><item><title>A Real-Time, Self-Tuning Moderator Framework for Adversarial Prompt Detection</title><link>https://arxiv.org/abs/2508.07139</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces real-time, self-tuning moderator framework for adversarial prompt detection&lt;/li&gt;&lt;li&gt;Evaluated on Google's Gemini models against modern jailbreaks&lt;/li&gt;&lt;li&gt;Maintains lightweight training footprint while adapting to new attacks&lt;/li&gt;&lt;li&gt;Demonstrates advantages over traditional fine-tuning/classifier models&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Ivan Zhang']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'jailbreaking', 'adversarial prompting', 'safety evaluation', 'real-time defense']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2508.07139</guid><pubDate>Tue, 12 Aug 2025 00:00:00 +0000</pubDate></item><item><title>Who's the Evil Twin? Differential Auditing for Undesired Behavior</title><link>https://arxiv.org/abs/2508.06827</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces a differential auditing framework using red and blue teams to detect hidden harmful behavior in models.&lt;/li&gt;&lt;li&gt;Explores various blue team strategies including adversarial attacks, model diffing, and integrated gradients.&lt;/li&gt;&lt;li&gt;Finds adversarial attacks effective for CNNs but LLM auditing requires hints about undesired distributions.&lt;/li&gt;&lt;li&gt;Highlights the need for tailored auditing methods for LLMs with practical hints.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Ishwar Balappanawar', 'Venkata Hasith Vattikuti', 'Greta Kintzley', 'Ronan Azimi-Mancel', 'Satvik Golechha']&lt;/li&gt;&lt;li&gt;Tags: ['red teaming', 'adversarial attacks', 'model auditing', 'LLM security', 'differential auditing']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2508.06827</guid><pubDate>Tue, 12 Aug 2025 00:00:00 +0000</pubDate></item><item><title>Many-Turn Jailbreaking</title><link>https://arxiv.org/abs/2508.06755</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces multi-turn jailbreaking concept&lt;/li&gt;&lt;li&gt;Creates MTJ-Bench benchmark&lt;/li&gt;&lt;li&gt;Evaluates on open and closed-source models&lt;/li&gt;&lt;li&gt;Highlights new safety threat&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Xianjun Yang', 'Liqiang Xiao', 'Shiyang Li', 'Faisal Ladhak', 'Hyokun Yun', 'Linda Ruth Petzold', 'Yi Xu', 'William Yang Wang']&lt;/li&gt;&lt;li&gt;Tags: ['jailbreaking', 'multi-turn', 'benchmark', 'safety evaluation', 'red teaming']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2508.06755</guid><pubDate>Tue, 12 Aug 2025 00:00:00 +0000</pubDate></item><item><title>Deep Ignorance: Filtering Pretraining Data Builds Tamper-Resistant Safeguards into Open-Weight LLMs</title><link>https://arxiv.org/abs/2508.06601</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces data filtering pipeline to remove dual-use topics from pretraining data&lt;/li&gt;&lt;li&gt;Pretrained 6.9B models show resistance to adversarial fine-tuning attacks&lt;/li&gt;&lt;li&gt;Models still vulnerable when harmful info provided via context (e.g. search tools)&lt;/li&gt;&lt;li&gt;Demonstrates data curation as a defense layer for open-weight LLMs&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ["Kyle O'Brien", 'Stephen Casper', 'Quentin Anthony', 'Tomek Korbak', 'Robert Kirk', 'Xander Davies', 'Ishan Mishra', 'Geoffrey Irving', 'Yarin Gal', 'Stella Biderman']&lt;/li&gt;&lt;li&gt;Tags: ['data_poisoning', 'adversarial_prompting', 'safety_evaluation', 'robustness', 'alignment']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2508.06601</guid><pubDate>Tue, 12 Aug 2025 00:00:00 +0000</pubDate></item><item><title>Towards Integrated Alignment</title><link>https://arxiv.org/abs/2508.06592</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes integrated alignment framework combining behavioral and representational approaches&lt;/li&gt;&lt;li&gt;Emphasizes strategic diversity and adaptive coevolution&lt;/li&gt;&lt;li&gt;Draws lessons from immunology and cybersecurity&lt;/li&gt;&lt;li&gt;Aims to reduce vulnerabilities to misalignment threats&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Ben Y. Reis', 'William La Cava']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'safety', 'security', 'red teaming', 'integrated approach']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2508.06592</guid><pubDate>Tue, 12 Aug 2025 00:00:00 +0000</pubDate></item><item><title>AuthPrint: Fingerprinting Generative Models Against Malicious Model Providers</title><link>https://arxiv.org/abs/2508.05691</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces AuthPrint for fingerprinting generative models against malicious providers&lt;/li&gt;&lt;li&gt;Uses a trusted verifier to extract and verify secret fingerprints&lt;/li&gt;&lt;li&gt;Demonstrates high accuracy and robustness against adversarial attacks&lt;/li&gt;&lt;li&gt;Evaluated on GAN and diffusion models with small modifications&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Kai Yao', 'Marc Juarez']&lt;/li&gt;&lt;li&gt;Tags: ['fingerprinting', 'adversarial', 'security', 'authentication', 'model extraction']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2508.05691</guid><pubDate>Tue, 12 Aug 2025 00:00:00 +0000</pubDate></item><item><title>BlindGuard: Safeguarding LLM-based Multi-Agent Systems under Unknown Attacks</title><link>https://arxiv.org/abs/2508.08127</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes BlindGuard, an unsupervised defense method for LLM-based multi-agent systems&lt;/li&gt;&lt;li&gt;Uses hierarchical agent encoder to capture interaction patterns&lt;/li&gt;&lt;li&gt;Employs corruption-guided detection with noise injection and contrastive learning&lt;/li&gt;&lt;li&gt;Effectively detects prompt injection, memory poisoning, and tool attacks without labeled data&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Rui Miao', 'Yixin Liu', 'Yili Wang', 'Xu Shen', 'Yue Tan', 'Yiwei Dai', 'Shirui Pan', 'Xin Wang']&lt;/li&gt;&lt;li&gt;Tags: ['red teaming', 'adversarial prompting', 'security', 'robustness', 'multi-agent systems']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2508.08127</guid><pubDate>Tue, 12 Aug 2025 00:00:00 +0000</pubDate></item><item><title>Pentest-R1: Towards Autonomous Penetration Testing Reasoning Optimized via Two-Stage Reinforcement Learning</title><link>https://arxiv.org/abs/2508.07382</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces Pentest-R1 framework for optimizing LLM reasoning in penetration testing&lt;/li&gt;&lt;li&gt;Uses two-stage reinforcement learning: offline with real-world walkthroughs and online in CTF environment&lt;/li&gt;&lt;li&gt;Achieves state-of-the-art performance on Cybench and AutoPenBench benchmarks&lt;/li&gt;&lt;li&gt;Demonstrates improved error handling and adaptive strategies through ablation studies&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['He Kong', 'Die Hu', 'Jingguo Ge', 'Liangxiong Li', 'Hui Li', 'Tong Li']&lt;/li&gt;&lt;li&gt;Tags: ['red teaming', 'LLM', 'reinforcement learning', 'penetration testing', 'security']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2508.07382</guid><pubDate>Tue, 12 Aug 2025 00:00:00 +0000</pubDate></item></channel></rss>