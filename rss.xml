<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>AI Security Paper Digest</title><link>https://kentaroh-toyoda.github.io/ai-security-paper-digest-rss/rss.xml</link><description>Curated papers on AI security from OpenAlex and ArXiv</description><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><lastBuildDate>Fri, 01 Aug 2025 22:13:16 +0000</lastBuildDate><item><title>Probabilistic Modeling of Jailbreak on Multimodal LLMs: From Quantification to Application</title><link>https://arxiv.org/abs/2503.06989</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces jailbreak probability to quantify jailbreak potential of inputs&lt;/li&gt;&lt;li&gt;Proposes Jailbreak Probability Prediction Network (JPPN) to model jailbreak probabilities&lt;/li&gt;&lt;li&gt;Develops Jailbreak-Probability-based Attack (JPA) and Multimodal JPA (MJPA) for generating adversarial inputs&lt;/li&gt;&lt;li&gt;Presents Jailbreak-Probability-based Finetuning (JPF) to improve model safety&lt;/li&gt;&lt;li&gt;Demonstrates significant improvements in both attack and defense through extensive experiments&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Wenzhuo Xu', 'Zhipeng Wei', 'Xiongtao Sun', 'Zonghao Ying', 'Deyue Zhang', 'Dongdong Yang', 'Xiangzheng Zhang', 'Quanchen Zou']&lt;/li&gt;&lt;li&gt;Tags: ['jailbreaking', 'adversarial prompting', 'multimodal LLMs', 'security', 'red teaming', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2503.06989</guid><pubDate>Fri, 01 Aug 2025 00:00:00 +0000</pubDate></item><item><title>Adversarial-Guided Diffusion for Multimodal LLM Attacks</title><link>https://arxiv.org/abs/2507.23202</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes Adversarial-Guided Diffusion (AGD) for attacking multimodal LLMs&lt;/li&gt;&lt;li&gt;Injects target semantics into noise component of reverse diffusion&lt;/li&gt;&lt;li&gt;Demonstrates robustness against defenses like low-pass filtering&lt;/li&gt;&lt;li&gt;Outperforms state-of-the-art in attack efficacy and defense resistance&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Chengwei Xia', 'Fan Ma', 'Ruijie Quan', 'Kun Zhan', 'Yi Yang']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial-attacks', 'multimodal', 'diffusion-models', 'red-teaming', 'model-robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2507.23202</guid><pubDate>Fri, 01 Aug 2025 00:00:00 +0000</pubDate></item><item><title>Counterfactual Evaluation for Blind Attack Detection in LLM-based Evaluation Systems</title><link>https://arxiv.org/abs/2507.23453</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces counterfactual evaluation framework to detect blind attacks in LLM-based systems&lt;/li&gt;&lt;li&gt;Augments standard evaluation with deliberate false ground-truth answers&lt;/li&gt;&lt;li&gt;Significantly improves attack detection with minimal performance trade-offs&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Lijia Liu', 'Takumi Kondo', 'Kyohei Atarashi', 'Koh Takeuchi', 'Jiyi Li', 'Shigeru Saito', 'Hisashi Kashima']&lt;/li&gt;&lt;li&gt;Tags: ['prompt injection', 'red teaming', 'security', 'LLM', 'evaluation systems']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2507.23453</guid><pubDate>Fri, 01 Aug 2025 00:00:00 +0000</pubDate></item><item><title>T-Detect: Tail-Aware Statistical Normalization for Robust Detection of Adversarial Machine-Generated Text</title><link>https://arxiv.org/abs/2507.23577</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces T-Detect, a novel method for detecting adversarial machine-generated text&lt;/li&gt;&lt;li&gt;Uses Student's t-distribution normalization to handle heavy-tailed statistical artifacts&lt;/li&gt;&lt;li&gt;Validated on RAID and HART datasets with significant AUROC improvements&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Alva West', 'Luodan Zhang', 'Liuliu Zhang', 'Minjun Zhu', 'Yixuan Weng', 'Yue Zhang']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial prompting', 'detection', 'robustness', 'text']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2507.23577</guid><pubDate>Fri, 01 Aug 2025 00:00:00 +0000</pubDate></item><item><title>CEE: An Inference-Time Jailbreak Defense for Embodied Intelligence via Subspace Concept Rotation</title><link>https://arxiv.org/abs/2504.13201</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces Concept Enhancement Engineering (CEE) for inference-time jailbreak defense in Embodied Intelligence systems&lt;/li&gt;&lt;li&gt;Manipulates internal model representations without additional training or external modules&lt;/li&gt;&lt;li&gt;Demonstrates significant defense improvements across multimodal LLMs and safety benchmarks&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jirui Yang', 'Zheyu Lin', 'Zhihui Lu', 'Yinggui Wang', 'Lei Wang', 'Tao Wei', 'Xin Du', 'Shuhan Yang']&lt;/li&gt;&lt;li&gt;Tags: ['jailbreaking', 'defense mechanism', 'embodied intelligence', 'inference-time', 'representation engineering']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2504.13201</guid><pubDate>Fri, 01 Aug 2025 00:00:00 +0000</pubDate></item><item><title>InfAlign: Inference-aware language model alignment</title><link>https://arxiv.org/abs/2412.19792</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes InfAlign framework for inference-aware alignment to optimize win rates during inference&lt;/li&gt;&lt;li&gt;Introduces specific reward transformations for Best-of-N sampling and jailbreaking&lt;/li&gt;&lt;li&gt;Demonstrates 3-8% improvement in inference-time win rates against base model&lt;/li&gt;&lt;li&gt;Includes reward calibration method as a strong baseline for standard win rate&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Ananth Balashankar', 'Ziteng Sun', 'Jonathan Berant', 'Jacob Eisenstein', 'Michael Collins', 'Adrian Hutter', 'Jong Lee', 'Chirag Nagpal', 'Flavien Prost', 'Aradhana Sinha', 'Ananda Theertha Suresh', 'Ahmad Beirami']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'jailbreaking', 'red teaming', 'robustness', 'LLM security']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2412.19792</guid><pubDate>Fri, 01 Aug 2025 00:00:00 +0000</pubDate></item><item><title>A Single Direction of Truth: An Observer Model's Linear Residual Probe Exposes and Steers Contextual Hallucinations</title><link>https://arxiv.org/abs/2507.23221</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces an observer model with a linear probe to detect contextual hallucinations&lt;/li&gt;&lt;li&gt;Demonstrates causal steering of generator hallucination rates&lt;/li&gt;&lt;li&gt;Releases the ContraTales benchmark for evaluation&lt;/li&gt;&lt;li&gt;Highlights MLP sub-circuits involved in hallucination tracking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ["Charles O'Neill", 'Slava Chalnev', 'Chi Chi Zhao', 'Max Kirkby', 'Mudith Jayasekara']&lt;/li&gt;&lt;li&gt;Tags: ['safety evaluation', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2507.23221</guid><pubDate>Fri, 01 Aug 2025 00:00:00 +0000</pubDate></item><item><title>AgentSpec: Customizable Runtime Enforcement for Safe and Reliable LLM Agents</title><link>https://arxiv.org/abs/2503.18666</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes AgentSpec, a DSL for runtime constraints on LLM agents&lt;/li&gt;&lt;li&gt;Demonstrates effectiveness across code, embodied, and AV domains&lt;/li&gt;&lt;li&gt;Evaluates automated rule generation with LLMs&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Haoyu Wang', 'Christopher M. Poskitt', 'Jun Sun']&lt;/li&gt;&lt;li&gt;Tags: ['runtime enforcement', 'safety constraints', 'LLM agents', 'security', 'safety evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2503.18666</guid><pubDate>Fri, 01 Aug 2025 00:00:00 +0000</pubDate></item><item><title>Role-Aware Language Models for Secure and Contextualized Access Control in Organizations</title><link>https://arxiv.org/abs/2507.23465</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Investigates role-aware LLMs for secure access control in enterprises&lt;/li&gt;&lt;li&gt;Explores three modeling strategies: BERT-based, LLM-based classifier, role-conditioned generation&lt;/li&gt;&lt;li&gt;Constructs two datasets: adapted from instruction-tuning and synthetic role-sensitive scenarios&lt;/li&gt;&lt;li&gt;Evaluates robustness against prompt injection, role mismatch, and jailbreak attempts&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Saeed Almheiri', 'Yerulan Kongrat', 'Adrian Santosh', 'Ruslan Tasmukhanov', 'Josemaria Vera', 'Muhammad Dehan Al Kautsar', 'Fajri Koto']&lt;/li&gt;&lt;li&gt;Tags: ['LLM security', 'role-based access control', 'prompt injection', 'jailbreaking', 'adversarial prompting', 'enterprise AI']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2507.23465</guid><pubDate>Fri, 01 Aug 2025 00:00:00 +0000</pubDate></item><item><title>Trustworthy Reasoning: Evaluating and Enhancing Factual Accuracy in LLM Intermediate Thought Processes</title><link>https://arxiv.org/abs/2507.22940</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces RELIANCE framework to enhance factual accuracy in LLM reasoning steps&lt;/li&gt;&lt;li&gt;Components include fact-checking classifier, GRPO reinforcement learning, and activation analysis&lt;/li&gt;&lt;li&gt;Evaluates across 10 models, showing significant factual accuracy improvements&lt;/li&gt;&lt;li&gt;Provides insights into model activation changes for future optimization&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Rui Jiao', 'Yue Zhang', 'Jinku Li']&lt;/li&gt;&lt;li&gt;Tags: ['safety', 'robustness', 'evaluation', 'alignment', 'reinforcement learning']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2507.22940</guid><pubDate>Fri, 01 Aug 2025 00:00:00 +0000</pubDate></item></channel></rss>