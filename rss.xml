<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>AI Security Paper Digest</title><link>https://kentaroh-toyoda.github.io/ai-security-paper-digest-rss/rss.xml</link><description>Curated papers on AI security from OpenAlex and ArXiv</description><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><lastBuildDate>Mon, 04 Aug 2025 22:15:43 +0000</lastBuildDate><item><title>Boosting Adversarial Transferability with Low-Cost Optimization via Maximin Expected Flatness</title><link>https://arxiv.org/abs/2405.16181</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces MEF attack for improved adversarial transferability&lt;/li&gt;&lt;li&gt;Theoretical foundation for flatness-based transferability&lt;/li&gt;&lt;li&gt;Outperforms state-of-the-art attacks with lower cost&lt;/li&gt;&lt;li&gt;Evaluations across multiple models and defenses&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Chunlin Qiu', 'Ang Li', 'Yiheng Duan', 'Shenyi Zhang', 'Yuanjie Zhang', 'Lingchen Zhao', 'Qian Wang']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial attacks', 'transferability', 'red teaming', 'optimization', 'flatness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2405.16181</guid><pubDate>Mon, 04 Aug 2025 00:00:00 +0000</pubDate></item><item><title>Revisiting Adversarial Patch Defenses on Object Detectors: Unified Evaluation, Large-Scale Dataset, and New Insights</title><link>https://arxiv.org/abs/2508.00649</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Presents a benchmark for adversarial patch defenses on object detectors&lt;/li&gt;&lt;li&gt;Introduces a large-scale dataset with 94,000 images and 94 patch types&lt;/li&gt;&lt;li&gt;Provides new insights into defense effectiveness and adaptive attacks&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Junhao Zheng', 'Jiahao Sun', 'Chenhao Lin', 'Zhengyu Zhao', 'Chen Ma', 'Chong Zhang', 'Cong Wang', 'Qian Wang', 'Chao Shen']&lt;/li&gt;&lt;li&gt;Tags: ['robustness', 'safety evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2508.00649</guid><pubDate>Mon, 04 Aug 2025 00:00:00 +0000</pubDate></item><item><title>LLMs Encode Harmfulness and Refusal Separately</title><link>https://arxiv.org/abs/2507.11878</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Identifies separate internal representations for harmfulness and refusal in LLMs&lt;/li&gt;&lt;li&gt;Shows jailbreak methods reduce refusal signals without altering harmfulness perception&lt;/li&gt;&lt;li&gt;Introduces Latent Guard using harmfulness detection for robust safety&lt;/li&gt;&lt;li&gt;Demonstrates resilience against adversarial finetuning attacks&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jiachen Zhao', 'Jing Huang', 'Zhengxuan Wu', 'David Bau', 'Weiyan Shi']&lt;/li&gt;&lt;li&gt;Tags: ['LLM safety', 'red teaming', 'jailbreak', 'adversarial prompting', 'internal representations']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2507.11878</guid><pubDate>Mon, 04 Aug 2025 00:00:00 +0000</pubDate></item><item><title>Watch the Weights: Unsupervised monitoring and control of fine-tuned LLMs</title><link>https://arxiv.org/abs/2508.00161</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces a weight-based method to monitor and control fine-tuned LLMs&lt;/li&gt;&lt;li&gt;Detects backdoor attacks with high precision and low false positive rate&lt;/li&gt;&lt;li&gt;Identifies unlearning of topics and can help recover erased information&lt;/li&gt;&lt;li&gt;Enables pre-deployment auditing of commercial models&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Ziqian Zhong', 'Aditi Raghunathan']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'Backdoor detection', 'Unlearning', 'Safety evaluation', 'Model auditing']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2508.00161</guid><pubDate>Mon, 04 Aug 2025 00:00:00 +0000</pubDate></item><item><title>Policy Maps: Tools for Guiding the Unbounded Space of LLM Behaviors</title><link>https://arxiv.org/abs/2409.18203</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces policy maps approach for AI policy design inspired by physical mapmaking&lt;/li&gt;&lt;li&gt;Presents Policy Projector interactive tool for defining and navigating policy regions&lt;/li&gt;&lt;li&gt;Evaluated with AI safety experts on issues like gender assumptions and physical safety threats&lt;/li&gt;&lt;li&gt;Supports LLM classification, steering, and visualization of policy regions&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Michelle S. Lam', 'Fred Hohman', 'Dominik Moritz', 'Jeffrey P. Bigham', 'Kenneth Holstein', 'Mary Beth Kery']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'safety evaluation', 'policy design', 'LLM', 'interactive tools']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2409.18203</guid><pubDate>Mon, 04 Aug 2025 00:00:00 +0000</pubDate></item><item><title>Backdoor Attacks on Deep Learning Face Detection</title><link>https://arxiv.org/abs/2508.00620</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces face generation attacks targeting face detection models&lt;/li&gt;&lt;li&gt;Presents a novel landmark shift attack on coordinate regression&lt;/li&gt;&lt;li&gt;Offers mitigations against these backdoor vulnerabilities&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Quentin Le Roux', 'Yannick Teglia', 'Teddy Furon', 'Philippe Loubet-Moundi']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial attacks', 'backdoor', 'face detection', 'mitigations', 'security']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2508.00620</guid><pubDate>Mon, 04 Aug 2025 00:00:00 +0000</pubDate></item><item><title>LeakSealer: A Semisupervised Defense for LLMs Against Prompt Injection and Leakage Attacks</title><link>https://arxiv.org/abs/2508.00602</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces a methodology to analyze historical LLM interaction data for adversarial patterns&lt;/li&gt;&lt;li&gt;Proposes LeakSealer, a semisupervised defense framework combining static analysis and dynamic detection&lt;/li&gt;&lt;li&gt;Empirically evaluated on prompt injection and PII leakage with strong performance results&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Francesco Panebianco', 'Stefano Bonfanti', 'Francesco Trov\\`o', 'Michele Carminati']&lt;/li&gt;&lt;li&gt;Tags: ['prompt injection', 'LLM security', 'jailbreaking', 'data leakage', 'semisupervised defense']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2508.00602</guid><pubDate>Mon, 04 Aug 2025 00:00:00 +0000</pubDate></item><item><title>Activation-Guided Local Editing for Jailbreaking Attacks</title><link>https://arxiv.org/abs/2508.00555</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes AGILE, a two-stage jailbreaking framework combining scenario-based generation and activation-guided editing&lt;/li&gt;&lt;li&gt;Achieves state-of-the-art attack success rates with up to 37.74% improvement over baselines&lt;/li&gt;&lt;li&gt;Demonstrates strong transferability to black-box models and effectiveness against defenses&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jiecong Wang', 'Haoran Li', 'Hao Peng', 'Ziqian Zeng', 'Zihao Wang', 'Haohua Du', 'Zhengtao Yu']&lt;/li&gt;&lt;li&gt;Tags: ['jailbreaking', 'red teaming', 'adversarial attacks', 'prompt injection', 'model robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2508.00555</guid><pubDate>Mon, 04 Aug 2025 00:00:00 +0000</pubDate></item><item><title>R1-ACT: Efficient Reasoning Model Safety Alignment by Activating Safety Knowledge</title><link>https://arxiv.org/abs/2508.00324</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes R1-ACT, a post-training method for safety alignment of large reasoning models&lt;/li&gt;&lt;li&gt;Activates existing safety knowledge through structured reasoning process&lt;/li&gt;&lt;li&gt;Achieves strong safety improvements with minimal training resources&lt;/li&gt;&lt;li&gt;Demonstrates robustness across multiple model backbones and sizes&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yeonjun In', 'Wonjoong Kim', 'Sangwu Park', 'Chanyoung Park']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'safety', 'post-training', 'reasoning', 'efficiency']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2508.00324</guid><pubDate>Mon, 04 Aug 2025 00:00:00 +0000</pubDate></item></channel></rss>