<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>AI Security Paper Digest</title><link>https://kentaroh-toyoda.github.io/ai-security-paper-digest-rss/rss.xml</link><description>Curated papers on AI security from OpenAlex and ArXiv</description><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><lastBuildDate>Tue, 19 Aug 2025 22:31:55 +0000</lastBuildDate><item><title>Drifting Away from Truth: GenAI-Driven News Diversity Challenges LVLM-Based Misinformation Detection</title><link>https://arxiv.org/abs/2508.12711</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces DriftBench, a large-scale benchmark for studying GenAI-driven news diversity challenges&lt;/li&gt;&lt;li&gt;Evaluates robustness of truth verification under multi-level drift&lt;/li&gt;&lt;li&gt;Tests susceptibility to adversarial evidence contamination generated by GenAI&lt;/li&gt;&lt;li&gt;Finds significant performance drops (avg F1 -14.8%) and unstable reasoning in state-of-the-art LVLM-based detectors&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Fanxiao Li', 'Jiaying Wu', 'Tingchao Fu', 'Yunyun Dong', 'Bingbing Song', 'Wei Zhou']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial attacks', 'robustness', 'safety evaluation', 'red teaming', 'multimodal']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2508.12711</guid><pubDate>Tue, 19 Aug 2025 00:00:00 +0000</pubDate></item><item><title>Concealment of Intent: A Game-Theoretic Analysis</title><link>https://arxiv.org/abs/2505.20841</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces intent-hiding adversarial prompting for LLMs&lt;/li&gt;&lt;li&gt;Develops game-theoretic framework for attacker-defender analysis&lt;/li&gt;&lt;li&gt;Proposes defense mechanism against intent-hiding attacks&lt;/li&gt;&lt;li&gt;Empirically validates attack effectiveness on real-world LLMs&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Xinbo Wu', 'Abhishek Umrawal', 'Lav R. Varshney']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial prompting', 'game theory', 'LLM security', 'red teaming', 'defense mechanisms']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2505.20841</guid><pubDate>Tue, 19 Aug 2025 00:00:00 +0000</pubDate></item><item><title>Mitigating Jailbreaks with Intent-Aware LLMs</title><link>https://arxiv.org/abs/2508.12072</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes Intent-FT, a fine-tuning approach to make LLMs infer instruction intent before responding&lt;/li&gt;&lt;li&gt;Evaluates against parametric and non-parametric jailbreak attacks across multiple models&lt;/li&gt;&lt;li&gt;Reduces attack success rates while preserving model capabilities and reducing over-refusal&lt;/li&gt;&lt;li&gt;Enables transfer of learned intent detection to enhance other models' defenses&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Wei Jie Yeo', 'Ranjan Satapathy', 'Erik Cambria']&lt;/li&gt;&lt;li&gt;Tags: ['jailbreak', 'adversarial prompting', 'safety', 'intent inference', 'LLM', 'fine-tuning', 'red teaming']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2508.12072</guid><pubDate>Tue, 19 Aug 2025 00:00:00 +0000</pubDate></item><item><title>Improving Detection of Watermarked Language Models</title><link>https://arxiv.org/abs/2508.13131</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Investigates combining watermark detectors with non-watermark methods&lt;/li&gt;&lt;li&gt;Aims to improve detection of LLM-generated text after post-training&lt;/li&gt;&lt;li&gt;Observes performance gains across various experimental conditions&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Dara Bahri', 'John Wieting']&lt;/li&gt;&lt;li&gt;Tags: ['watermarking', 'detection', 'hybrid models', 'security', 'adversarial prompting']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2508.13131</guid><pubDate>Tue, 19 Aug 2025 00:00:00 +0000</pubDate></item><item><title>Limitation Learning: Catching Adverse Dialog with GAIL</title><link>https://arxiv.org/abs/2508.11767</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Applies Generative Adversarial Imitation Learning (GAIL) to dialog generation&lt;/li&gt;&lt;li&gt;Trains a policy to generate responses and a discriminator to detect expert vs synthetic dialog&lt;/li&gt;&lt;li&gt;Discriminator learns to identify limitations and adverse behavior in dialog models&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Noah Kasmanoff', 'Rahul Zalkikar']&lt;/li&gt;&lt;li&gt;Tags: ['safety evaluation', 'red teaming', 'dialog models', 'imitation learning', 'adversarial detection']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2508.11767</guid><pubDate>Tue, 19 Aug 2025 00:00:00 +0000</pubDate></item><item><title>From Teacher to Student: Tracking Memorization Through Model Distillation</title><link>https://arxiv.org/abs/2506.16170</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Study explores how knowledge distillation affects memorization in fine-tuned LLMs&lt;/li&gt;&lt;li&gt;Distilling from a teacher model reduces memorization risks compared to standard fine-tuning&lt;/li&gt;&lt;li&gt;Findings highlight potential privacy and security benefits of using distilled models&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Simardeep Singh']&lt;/li&gt;&lt;li&gt;Tags: ['data poisoning', 'privacy attacks', 'memorization', 'knowledge distillation', 'security']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.16170</guid><pubDate>Tue, 19 Aug 2025 00:00:00 +0000</pubDate></item><item><title>Heuristic-Induced Multimodal Risk Distribution Jailbreak Attack for Multimodal Large Language Models</title><link>https://arxiv.org/abs/2412.05934</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes HIMRD, a black-box jailbreak attack for multimodal LLMs&lt;/li&gt;&lt;li&gt;Uses multimodal risk distribution and heuristic-induced search strategies&lt;/li&gt;&lt;li&gt;Achieves high attack success rates on both open and closed-source models&lt;/li&gt;&lt;li&gt;Reveals cross-modal security vulnerabilities in MLLMs&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Ma Teng', 'Jia Xiaojun', 'Duan Ranjie', 'Li Xinfeng', 'Huang Yihao', 'Jia Xiaoshuang', 'Chu Zhixuan', 'Ren Wenqi']&lt;/li&gt;&lt;li&gt;Tags: ['jailbreaking', 'multimodal', 'red teaming', 'adversarial attacks', 'security evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2412.05934</guid><pubDate>Tue, 19 Aug 2025 00:00:00 +0000</pubDate></item><item><title>LD-Scene: LLM-Guided Diffusion for Controllable Generation of Adversarial Safety-Critical Driving Scenarios</title><link>https://arxiv.org/abs/2505.11247</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes LD-Scene framework integrating LLMs and LDMs for generating adversarial driving scenarios&lt;/li&gt;&lt;li&gt;Uses natural language queries for user-controllable scenario generation&lt;/li&gt;&lt;li&gt;Achieves state-of-the-art performance on nuScenes dataset&lt;/li&gt;&lt;li&gt;Provides fine-grained control over adversarial behaviors&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Mingxing Peng', 'Yuting Xie', 'Xusen Guo', 'Ruoyu Yao', 'Hai Yang', 'Jun Ma']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial scenarios', 'autonomous driving', 'safety evaluation', 'robustness', 'LLM-guided generation', 'user-controllable', 'nuScenes']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2505.11247</guid><pubDate>Tue, 19 Aug 2025 00:00:00 +0000</pubDate></item><item><title>RepreGuard: Detecting LLM-Generated Text by Revealing Hidden Representation Patterns</title><link>https://arxiv.org/abs/2508.13152</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes RepreGuard, a method to detect LLM-generated text by analyzing internal representation patterns&lt;/li&gt;&lt;li&gt;Outperforms existing methods with 94.92% AUROC on both in-distribution and OOD scenarios&lt;/li&gt;&lt;li&gt;Demonstrates robustness against various text sizes and mainstream attacks&lt;/li&gt;&lt;li&gt;Uses surrogate model to extract activation features that distinguish generated vs human text&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Xin Chen', 'Junchao Wu', 'Shu Yang', 'Runzhe Zhan', 'Zeyu Wu', 'Ziyang Luo', 'Di Wang', 'Min Yang', 'Lidia S. Chao', 'Derek F. Wong']&lt;/li&gt;&lt;li&gt;Tags: ['LLM detection', 'security', 'robustness', 'representation learning', 'adversarial defense']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2508.13152</guid><pubDate>Tue, 19 Aug 2025 00:00:00 +0000</pubDate></item><item><title>SecFSM: Knowledge Graph-Guided Verilog Code Generation for Secure Finite State Machines in Systems-on-Chip</title><link>https://arxiv.org/abs/2508.12910</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes SecFSM, a method using a security knowledge graph to guide LLMs in generating secure Verilog code for FSMs&lt;/li&gt;&lt;li&gt;Constructs a FSM Security Knowledge Graph (FSKG) to identify and address vulnerabilities&lt;/li&gt;&lt;li&gt;Evaluates against 25 security test cases with a pass rate of 21/25 using DeepSeek-R1&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Ziteng Hu', 'Yingjie Xia', 'Xiyuan Chen', 'Li Kuang']&lt;/li&gt;&lt;li&gt;Tags: ['security', 'code generation', 'knowledge graph', 'LLM', 'FSM']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2508.12910</guid><pubDate>Tue, 19 Aug 2025 00:00:00 +0000</pubDate></item><item><title>LinguaSafe: A Comprehensive Multilingual Safety Benchmark for Large Language Models</title><link>https://arxiv.org/abs/2508.12733</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduction of LinguaSafe, a comprehensive multilingual safety benchmark for LLMs&lt;/li&gt;&lt;li&gt;Dataset includes 45k entries across 12 languages&lt;/li&gt;&lt;li&gt;Focuses on multidimensional safety evaluation including oversensitivity&lt;/li&gt;&lt;li&gt;Provides metrics for in-depth safety assessment&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Zhiyuan Ning', 'Tianle Gu', 'Jiaxin Song', 'Shixin Hong', 'Lingyu Li', 'Huacan Liu', 'Jie Li', 'Yixu Wang', 'Meng Lingyu', 'Yan Teng', 'Yingchun Wang']&lt;/li&gt;&lt;li&gt;Tags: ['safety evaluation', 'multilingual', 'benchmark', 'LLM', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2508.12733</guid><pubDate>Tue, 19 Aug 2025 00:00:00 +0000</pubDate></item><item><title>Systematic Analysis of MCP Security</title><link>https://arxiv.org/abs/2508.12538</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces MCP Attack Library (MCPLIB) categorizing 31 attack methods&lt;/li&gt;&lt;li&gt;Conducts quantitative analysis of attack efficacy&lt;/li&gt;&lt;li&gt;Reveals key vulnerabilities in MCP ecosystems&lt;/li&gt;&lt;li&gt;Provides framework for secure MCP evolution&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yongjian Guo', 'Puzhuo Liu', 'Wanlun Ma', 'Zehang Deng', 'Xiaogang Zhu', 'Peng Di', 'Xi Xiao', 'Sheng Wen']&lt;/li&gt;&lt;li&gt;Tags: ['red teaming', 'security', 'MCP', 'LLM', 'tool poisoning', 'attack taxonomy']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2508.12538</guid><pubDate>Tue, 19 Aug 2025 00:00:00 +0000</pubDate></item><item><title>CorrSteer: Steering Improves Task Performance and Safety in LLMs through Correlation-based Sparse Autoencoder Feature Selection</title><link>https://arxiv.org/abs/2508.12535</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces CorrSteer, a method to improve LLM safety and performance through correlation-based feature selection&lt;/li&gt;&lt;li&gt;Demonstrates significant improvements in jailbreaking prevention (+22.9% on HarmBench)&lt;/li&gt;&lt;li&gt;Uses sparse autoencoder activations correlated with sample correctness for automated steering&lt;/li&gt;&lt;li&gt;Applies to QA, bias mitigation, reasoning tasks with minimal data requirements&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Seonglae Cho', 'Zekun Wu', 'Adriano Koshiyama']&lt;/li&gt;&lt;li&gt;Tags: ['jailbreaking', 'safety', 'adversarial prompting', 'feature selection', 'sparse autoencoders']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2508.12535</guid><pubDate>Tue, 19 Aug 2025 00:00:00 +0000</pubDate></item><item><title>Rethinking Safety in LLM Fine-tuning: An Optimization Perspective</title><link>https://arxiv.org/abs/2508.12531</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Challenges the belief that fine-tuning LLMs inherently harms safety&lt;/li&gt;&lt;li&gt;Demonstrates that optimization hyperparameters significantly affect safety&lt;/li&gt;&lt;li&gt;Introduces EMA momentum technique to preserve safety during fine-tuning&lt;/li&gt;&lt;li&gt;Reduces unsafe responses from 16% to 5% without additional safety data&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Minseon Kim', 'Jin Myung Kwak', 'Lama Alssum', 'Bernard Ghanem', 'Philip Torr', 'David Krueger', 'Fazl Barez', 'Adel Bibi']&lt;/li&gt;&lt;li&gt;Tags: ['LLM safety', 'red teaming', 'optimization', 'fine-tuning', 'adversarial prompting']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2508.12531</guid><pubDate>Tue, 19 Aug 2025 00:00:00 +0000</pubDate></item><item><title>Adversarial Attacks on VQA-NLE: Exposing and Alleviating Inconsistencies in Visual Question Answering Explanations</title><link>https://arxiv.org/abs/2508.12430</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces adversarial attacks on VQA-NLE systems to expose inconsistent explanations&lt;/li&gt;&lt;li&gt;Proposes a novel image perturbation strategy to induce spurious outputs&lt;/li&gt;&lt;li&gt;Presents a knowledge-based mitigation method to improve robustness&lt;/li&gt;&lt;li&gt;Evaluates on standard benchmarks and models to validate effectiveness&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yahsin Yeh', 'Yilun Wu', 'Bokai Ruan', 'Honghan Shuai']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial attacks', 'visual question answering', 'explanations', 'robustness', 'knowledge-based defenses']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2508.12430</guid><pubDate>Tue, 19 Aug 2025 00:00:00 +0000</pubDate></item><item><title>Where to Start Alignment? Diffusion Large Language Model May Demand a Distinct Position</title><link>https://arxiv.org/abs/2508.12398</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;First safety analysis for Diffusion Large Language Models (dLLMs)&lt;/li&gt;&lt;li&gt;Identifies asymmetry between defender and attacker focus on middle vs initial tokens&lt;/li&gt;&lt;li&gt;Introduces Middle-tOken Safety Alignment (MOSA) method using reinforcement learning&lt;/li&gt;&lt;li&gt;Evaluates against 8 attack methods and 3 utility benchmarks&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Zhixin Xie', 'Xurui Song', 'Jun Luo']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'safety evaluation', 'adversarial prompting', 'model-specific security', 'reinforcement learning']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2508.12398</guid><pubDate>Tue, 19 Aug 2025 00:00:00 +0000</pubDate></item><item><title>Uncovering Systematic Failures of LLMs in Verifying Code Against Natural Language Specifications</title><link>https://arxiv.org/abs/2508.12358</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Uncovered systematic failures in LLMs verifying code against natural language specs&lt;/li&gt;&lt;li&gt;LLMs frequently misclassify correct code as incorrect or defective&lt;/li&gt;&lt;li&gt;Prompt engineering techniques increase misjudgment rates&lt;/li&gt;&lt;li&gt;Proposed improved prompting strategies for mitigation&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Haolin Jin', 'Huaming Chen']&lt;/li&gt;&lt;li&gt;Tags: ['LLM robustness', 'code verification', 'prompt engineering', 'safety evaluation', 'adversarial prompting']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2508.12358</guid><pubDate>Tue, 19 Aug 2025 00:00:00 +0000</pubDate></item><item><title>Enhancing GraphQL Security by Detecting Malicious Queries Using Large Language Models, Sentence Transformers, and Convolutional Neural Networks</title><link>https://arxiv.org/abs/2508.11711</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Presents an AI-driven system for real-time detection of malicious GraphQL queries&lt;/li&gt;&lt;li&gt;Uses LLMs for schema-based configuration, Sentence Transformers for embeddings, and CNNs/RF/MLPs for classification&lt;/li&gt;&lt;li&gt;Optimized for production with ONNX Runtime and parallel processing&lt;/li&gt;&lt;li&gt;High accuracy in detecting SQLi, OS command injection, XSS, DoS, SSRF&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Irash Perera (Department of Computer Science and Engineering', 'University of Moratuwa', 'Colombo', 'Sri Lanka)', 'Hiranya Abeyrathne (WSO2', 'Colombo', 'Sri Lanka)', 'Sanjeewa Malalgoda (WSO2', 'Colombo', 'Sri Lanka)', 'Arshardh Ifthikar (WSO2', 'Colombo', 'Sri Lanka)']&lt;/li&gt;&lt;li&gt;Tags: ['red teaming', 'adversarial prompting', 'security evaluation', 'AI security', 'GraphQL']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2508.11711</guid><pubDate>Tue, 19 Aug 2025 00:00:00 +0000</pubDate></item><item><title>Assessing Representation Stability for Transformer Models</title><link>https://arxiv.org/abs/2508.11667</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces Representation Stability (RS) framework for detecting adversarial text examples&lt;/li&gt;&lt;li&gt;Measures embedding sensitivity when masking important words&lt;/li&gt;&lt;li&gt;Achieves over 88% detection accuracy across multiple datasets and attack types&lt;/li&gt;&lt;li&gt;Generalizes well to unseen datasets, attacks, and models without retraining&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Bryan E. Tuck', 'Rakesh M. Verma']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial attacks', 'detection', 'transformers', 'representation stability']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2508.11667</guid><pubDate>Tue, 19 Aug 2025 00:00:00 +0000</pubDate></item></channel></rss>