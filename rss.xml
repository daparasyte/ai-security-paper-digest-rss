<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>AI Security Paper Digest</title><link>https://kentaroh-toyoda.github.io/ai-security-paper-digest-rss/rss.xml</link><description>Curated papers on AI security from ArXiv</description><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><lastBuildDate>Mon, 15 Sep 2025 22:29:41 +0000</lastBuildDate><item><title>Survivability of Backdoor Attacks on Unconstrained Face Recognition Systems</title><link>https://arxiv.org/abs/2507.01607</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;First comprehensive system-level analysis of backdoor attacks on face recognition systems&lt;/li&gt;&lt;li&gt;Demonstrates susceptibility of large margin metric learning models to backdoor attacks&lt;/li&gt;&lt;li&gt;Analyzes 20 pipeline configurations and 15 attack scenarios showing single backdoor can compromise entire system&lt;/li&gt;&lt;li&gt;Proposes effective best practices and countermeasures for stakeholders&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Quentin Le Roux', 'Yannick Teglia', 'Teddy Furon', 'Philippe Loubet-Moundi', 'Eric Bourbao']&lt;/li&gt;&lt;li&gt;Tags: ['backdoor attacks', 'face recognition', 'red teaming', 'security', 'countermeasures']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2507.01607</guid><pubDate>Mon, 15 Sep 2025 00:00:00 +0000</pubDate></item><item><title>AdvI2I: Adversarial Image Attack on Image-to-Image Diffusion models</title><link>https://arxiv.org/abs/2410.21471</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces AdvI2I framework for adversarial image attacks on I2I diffusion models&lt;/li&gt;&lt;li&gt;Bypasses safety mechanisms like Safe Latent Diffusion (SLD) without altering text prompts&lt;/li&gt;&lt;li&gt;Includes adaptive version (AdvI2I-Adaptive) to counter potential defenses&lt;/li&gt;&lt;li&gt;Demonstrates effectiveness against current safeguards through experiments&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yaopei Zeng', 'Yuanpu Cao', 'Bochuan Cao', 'Yurui Chang', 'Jinghui Chen', 'Lu Lin']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial prompting', 'image-to-image', 'diffusion models', 'safety', 'red teaming']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2410.21471</guid><pubDate>Mon, 15 Sep 2025 00:00:00 +0000</pubDate></item><item><title>Immunizing Images from Text to Image Editing via Adversarial Cross-Attention</title><link>https://arxiv.org/abs/2509.10359</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes Attention Attack to disrupt cross-attention in text-to-image editing&lt;/li&gt;&lt;li&gt;Uses auto-generated captions as proxies for edit prompts&lt;/li&gt;&lt;li&gt;Introduces Caption Similarity and semantic IoU evaluation metrics&lt;/li&gt;&lt;li&gt;Demonstrates significant performance degradation on TEDBench++&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Matteo Trippodo', 'Federico Becattini', 'Lorenzo Seidenari']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial attacks', 'text-to-image editing', 'cross-attention', 'safety evaluation', 'image immunization']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.10359</guid><pubDate>Mon, 15 Sep 2025 00:00:00 +0000</pubDate></item><item><title>Oyster-I: Beyond Refusal -- Constructive Safety Alignment for Responsible Language Models</title><link>https://arxiv.org/abs/2509.01909</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces Constructive Safety Alignment (CSA) paradigm for LLMs&lt;/li&gt;&lt;li&gt;Oyster-I (Oy1) achieves state-of-the-art safety while retaining capabilities&lt;/li&gt;&lt;li&gt;Strong performance on Strata-Sword jailbreak dataset and Constructive Benchmark&lt;/li&gt;&lt;li&gt;Shifts from refusal-based to guidance-based safety approach&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Ranjie Duan', 'Jiexi Liu', 'Xiaojun Jia', 'Shiji Zhao', 'Ruoxi Cheng', 'Fengxiang Wang', 'Cheng Wei', 'Yong Xie', 'Chang Liu', 'Defeng Li', 'Yinpeng Dong', 'Yichi Zhang', 'Yuefeng Chen', 'Chongwen Wang', 'Xingjun Ma', 'Xingxing Wei', 'Yang Liu', 'Hang Su', 'Jun Zhu', 'Xinfeng Li', 'Yitong Sun', 'Jie Zhang', 'Jinzhao Hu', 'Sha Xu', 'Yitong Yang', 'Jialing Tao', 'Hui Xue']&lt;/li&gt;&lt;li&gt;Tags: ['safety alignment', 'red teaming', 'jailbreaking', 'robustness', 'constructive engagement']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.01909</guid><pubDate>Mon, 15 Sep 2025 00:00:00 +0000</pubDate></item><item><title>Can LLM Prompting Serve as a Proxy for Static Analysis in Vulnerability Detection</title><link>https://arxiv.org/abs/2412.12039</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Investigates LLM prompting for vulnerability detection as an alternative to static analysis&lt;/li&gt;&lt;li&gt;Uses contrastive chain-of-thought prompting with synthetic samples&lt;/li&gt;&lt;li&gt;Shows improved accuracy, F1-scores, and reduced false negatives compared to static analyzers&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Ira Ceka', 'Feitong Qiao', 'Anik Dey', 'Aastha Valecha', 'Gail Kaiser', 'Baishakhi Ray']&lt;/li&gt;&lt;li&gt;Tags: ['safety evaluation', 'adversarial prompting', 'LLM red teaming', 'vulnerability detection', 'static analysis']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2412.12039</guid><pubDate>Mon, 15 Sep 2025 00:00:00 +0000</pubDate></item><item><title>Differential Robustness in Transformer Language Models: Empirical Evaluation Under Adversarial Text Attacks</title><link>https://arxiv.org/abs/2509.09706</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Empirical evaluation of Flan-T5, BERT, and RoBERTa-Base against adversarial text attacks using TextFooler and BERTAttack&lt;/li&gt;&lt;li&gt;Significant variations in robustness found: RoBERTa-Base and FlanT5 showed 0% attack success, while BERT-Base had 93.75% success rate&lt;/li&gt;&lt;li&gt;Highlights computational resource requirements for effective defenses&lt;/li&gt;&lt;li&gt;Proposes recommendations for more efficient defensive strategies&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Taniya Gidatkar', 'Oluwaseun Ajao', 'Matthew Shardlow']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial attacks', 'robustness', 'LLM security', 'TextFooler', 'BERTAttack', 'empirical evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.09706</guid><pubDate>Mon, 15 Sep 2025 00:00:00 +0000</pubDate></item><item><title>Beyond I'm Sorry, I Can't: Dissecting Large Language Model Refusal</title><link>https://arxiv.org/abs/2509.09708</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Uses sparse autoencoders to identify latent features mediating model refusal&lt;/li&gt;&lt;li&gt;Develops a pipeline to find minimal feature sets that can flip refusal to compliance&lt;/li&gt;&lt;li&gt;Discovers redundant features that become active when primary features are suppressed&lt;/li&gt;&lt;li&gt;Provides insights into mechanistic basis of safety behaviors&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Nirmalendu Prakash', 'Yeo Wei Jie', 'Amir Abdullah', 'Ranjan Satapathy', 'Erik Cambria', 'Roy Ka Wei Lee']&lt;/li&gt;&lt;li&gt;Tags: ['jailbreaking', 'red teaming', 'safety evaluation', 'adversarial prompting', 'model internals']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.09708</guid><pubDate>Mon, 15 Sep 2025 00:00:00 +0000</pubDate></item><item><title>CTCC: A Robust and Stealthy Fingerprinting Framework for Large Language Models via Cross-Turn Contextual Correlation Backdoor</title><link>https://arxiv.org/abs/2509.09703</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces CTCC, a novel fingerprinting framework for LLMs using cross-turn contextual correlations&lt;/li&gt;&lt;li&gt;Aims to improve stealth, robustness, and generalizability over existing methods&lt;/li&gt;&lt;li&gt;Employs rule-driven encoding of semantic relationships like counterfactuals across dialogue turns&lt;/li&gt;&lt;li&gt;Demonstrates superior performance across multiple LLM architectures&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Zhenhua Xu', 'Xixiang Zhao', 'Xubin Yue', 'Shengwei Tian', 'Changting Lin', 'Meng Han']&lt;/li&gt;&lt;li&gt;Tags: ['model_fingerprinting', 'LLM_security', 'adversarial_robustness', 'stealth', 'intellectual_property_protection']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.09703</guid><pubDate>Mon, 15 Sep 2025 00:00:00 +0000</pubDate></item><item><title>Survivability of Backdoor Attacks on Unconstrained Face Recognition Systems</title><link>https://arxiv.org/abs/2507.01607</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;First comprehensive system-level analysis of backdoor attacks on face recognition systems&lt;/li&gt;&lt;li&gt;Demonstrates susceptibility of large margin metric learning models to backdoor attacks&lt;/li&gt;&lt;li&gt;Analyzes 20 pipeline configurations and 15 attack scenarios showing single backdoor can compromise entire system&lt;/li&gt;&lt;li&gt;Proposes effective best practices and countermeasures for stakeholders&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Quentin Le Roux', 'Yannick Teglia', 'Teddy Furon', 'Philippe Loubet-Moundi', 'Eric Bourbao']&lt;/li&gt;&lt;li&gt;Tags: ['backdoor attacks', 'face recognition', 'red teaming', 'security', 'countermeasures']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2507.01607</guid><pubDate>Mon, 15 Sep 2025 00:00:00 +0000</pubDate></item><item><title>Is Adversarial Training with Compressed Datasets Effective?</title><link>https://arxiv.org/abs/2402.05675</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Investigates adversarial robustness of models trained on compressed datasets&lt;/li&gt;&lt;li&gt;Finds current dataset condensation methods ineffective for transferring robustness&lt;/li&gt;&lt;li&gt;Proposes a new method based on Minimal Finite Covering (MFC) for provable robustness&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Tong Chen', 'Raghavendra Selvan']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial robustness', 'dataset compression', 'dataset condensation', 'minimal finite covering', 'adversarial training']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2402.05675</guid><pubDate>Mon, 15 Sep 2025 00:00:00 +0000</pubDate></item><item><title>Analyzing the Impact of Adversarial Examples on Explainable Machine Learning</title><link>https://arxiv.org/abs/2307.08327</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Analyzes impact of adversarial attacks on model explainability for text classification&lt;/li&gt;&lt;li&gt;Develops text classification model and applies adversarial perturbations&lt;/li&gt;&lt;li&gt;Compares model performance and interpretability before/after attack&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Prathyusha Devabhakthini', 'Sasmita Parida', 'Raj Mani Shukla', 'Suvendu Chandan Nayak', 'Tapadhir Das']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial prompting', 'model robustness', 'safety evaluation', 'explainable AI']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2307.08327</guid><pubDate>Mon, 15 Sep 2025 00:00:00 +0000</pubDate></item><item><title>Prompt Injection Attacks on LLM Generated Reviews of Scientific Publications</title><link>https://arxiv.org/abs/2509.10248</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Investigates prompt injection attacks on LLM-generated scientific paper reviews&lt;/li&gt;&lt;li&gt;Finds simple prompt injections can achieve 100% acceptance rates&lt;/li&gt;&lt;li&gt;Reveals LLM review bias towards high acceptance scores (&gt;95% in many models)&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Janis Keuper']&lt;/li&gt;&lt;li&gt;Tags: ['prompt injection', 'red teaming', 'adversarial prompting', 'model robustness', 'security evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.10248</guid><pubDate>Mon, 15 Sep 2025 00:00:00 +0000</pubDate></item><item><title>Between a Rock and a Hard Place: Exploiting Ethical Reasoning to Jailbreak LLMs</title><link>https://arxiv.org/abs/2509.05367</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces TRIAL framework for jailbreaking LLMs using ethical dilemmas&lt;/li&gt;&lt;li&gt;Leverages trolley problem-style scenarios to exploit model reasoning&lt;/li&gt;&lt;li&gt;Demonstrates high success rates on both open and closed-source models&lt;/li&gt;&lt;li&gt;Highlights limitations in current safety alignment strategies&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Shei Pern Chua', 'Zhen Leng Thai', 'Teh Kai Jun', 'Xiao Li', 'Xiaolin Hu']&lt;/li&gt;&lt;li&gt;Tags: ['jailbreak', 'red teaming', 'ethical reasoning', 'trolley problem', 'safety alignment']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.05367</guid><pubDate>Mon, 15 Sep 2025 00:00:00 +0000</pubDate></item><item><title>Survivability of Backdoor Attacks on Unconstrained Face Recognition Systems</title><link>https://arxiv.org/abs/2507.01607</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;First comprehensive system-level analysis of backdoor attacks on face recognition systems&lt;/li&gt;&lt;li&gt;Demonstrates susceptibility of large margin metric learning models to backdoor attacks&lt;/li&gt;&lt;li&gt;Analyzes 20 pipeline configurations and 15 attack scenarios showing single backdoor can compromise entire system&lt;/li&gt;&lt;li&gt;Proposes effective best practices and countermeasures for stakeholders&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Quentin Le Roux', 'Yannick Teglia', 'Teddy Furon', 'Philippe Loubet-Moundi', 'Eric Bourbao']&lt;/li&gt;&lt;li&gt;Tags: ['backdoor attacks', 'face recognition', 'red teaming', 'security', 'countermeasures']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2507.01607</guid><pubDate>Mon, 15 Sep 2025 00:00:00 +0000</pubDate></item><item><title>Can LLM Prompting Serve as a Proxy for Static Analysis in Vulnerability Detection</title><link>https://arxiv.org/abs/2412.12039</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Investigates LLM prompting for vulnerability detection as an alternative to static analysis&lt;/li&gt;&lt;li&gt;Uses contrastive chain-of-thought prompting with synthetic samples&lt;/li&gt;&lt;li&gt;Shows improved accuracy, F1-scores, and reduced false negatives compared to static analyzers&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Ira Ceka', 'Feitong Qiao', 'Anik Dey', 'Aastha Valecha', 'Gail Kaiser', 'Baishakhi Ray']&lt;/li&gt;&lt;li&gt;Tags: ['safety evaluation', 'adversarial prompting', 'LLM red teaming', 'vulnerability detection', 'static analysis']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2412.12039</guid><pubDate>Mon, 15 Sep 2025 00:00:00 +0000</pubDate></item><item><title>AdvI2I: Adversarial Image Attack on Image-to-Image Diffusion models</title><link>https://arxiv.org/abs/2410.21471</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces AdvI2I framework for adversarial image attacks on I2I diffusion models&lt;/li&gt;&lt;li&gt;Bypasses safety mechanisms like Safe Latent Diffusion (SLD) without altering text prompts&lt;/li&gt;&lt;li&gt;Includes adaptive version (AdvI2I-Adaptive) to counter potential defenses&lt;/li&gt;&lt;li&gt;Demonstrates effectiveness against current safeguards through experiments&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yaopei Zeng', 'Yuanpu Cao', 'Bochuan Cao', 'Yurui Chang', 'Jinghui Chen', 'Lu Lin']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial prompting', 'image-to-image', 'diffusion models', 'safety', 'red teaming']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2410.21471</guid><pubDate>Mon, 15 Sep 2025 00:00:00 +0000</pubDate></item><item><title>Analyzing the Impact of Adversarial Examples on Explainable Machine Learning</title><link>https://arxiv.org/abs/2307.08327</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Analyzes impact of adversarial attacks on model explainability for text classification&lt;/li&gt;&lt;li&gt;Develops text classification model and applies adversarial perturbations&lt;/li&gt;&lt;li&gt;Compares model performance and interpretability before/after attack&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Prathyusha Devabhakthini', 'Sasmita Parida', 'Raj Mani Shukla', 'Suvendu Chandan Nayak', 'Tapadhir Das']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial prompting', 'model robustness', 'safety evaluation', 'explainable AI']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2307.08327</guid><pubDate>Mon, 15 Sep 2025 00:00:00 +0000</pubDate></item><item><title>Oyster-I: Beyond Refusal -- Constructive Safety Alignment for Responsible Language Models</title><link>https://arxiv.org/abs/2509.01909</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces Constructive Safety Alignment (CSA) paradigm for LLMs&lt;/li&gt;&lt;li&gt;Oyster-I (Oy1) achieves state-of-the-art safety while retaining capabilities&lt;/li&gt;&lt;li&gt;Strong performance on Strata-Sword jailbreak dataset and Constructive Benchmark&lt;/li&gt;&lt;li&gt;Shifts from refusal-based to guidance-based safety approach&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Ranjie Duan', 'Jiexi Liu', 'Xiaojun Jia', 'Shiji Zhao', 'Ruoxi Cheng', 'Fengxiang Wang', 'Cheng Wei', 'Yong Xie', 'Chang Liu', 'Defeng Li', 'Yinpeng Dong', 'Yichi Zhang', 'Yuefeng Chen', 'Chongwen Wang', 'Xingjun Ma', 'Xingxing Wei', 'Yang Liu', 'Hang Su', 'Jun Zhu', 'Xinfeng Li', 'Yitong Sun', 'Jie Zhang', 'Jinzhao Hu', 'Sha Xu', 'Yitong Yang', 'Jialing Tao', 'Hui Xue']&lt;/li&gt;&lt;li&gt;Tags: ['safety alignment', 'red teaming', 'jailbreaking', 'robustness', 'constructive engagement']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.01909</guid><pubDate>Mon, 15 Sep 2025 00:00:00 +0000</pubDate></item><item><title>Securing LLM-Generated Embedded Firmware through AI Agent-Driven Validation and Patching</title><link>https://arxiv.org/abs/2509.09970</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Three-phase methodology combining LLM-based firmware generation with automated security validation and iterative refinement&lt;/li&gt;&lt;li&gt;AI agents for threat detection, performance optimization, and compliance verification&lt;/li&gt;&lt;li&gt;Security testing using fuzzing, static analysis, and runtime monitoring&lt;/li&gt;&lt;li&gt;Iterative LLM-generated patches based on detected vulnerabilities&lt;/li&gt;&lt;li&gt;Significant improvements in vulnerability remediation and security metrics&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Seyed Moein Abtahi', 'Akramul Azim']&lt;/li&gt;&lt;li&gt;Tags: ['LLM security', 'vulnerability detection', 'AI agents', 'firmware', 'CWE']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.09970</guid><pubDate>Mon, 15 Sep 2025 00:00:00 +0000</pubDate></item><item><title>SmartCoder-R1: Towards Secure and Explainable Smart Contract Generation with Security-Aware Group Relative Policy Optimization</title><link>https://arxiv.org/abs/2509.09942</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes SmartCoder-R1 framework for secure and explainable smart contract generation&lt;/li&gt;&lt;li&gt;Uses Security-Aware Group Relative Policy Optimization (S-GRPO) to optimize for security compliance&lt;/li&gt;&lt;li&gt;Achieves state-of-the-art results in vulnerability rate (VulRate 8.60%) and safety evaluation (SafeAval 80.16%)&lt;/li&gt;&lt;li&gt;Generated reasoning scores high in human evaluations for security (85.3%) and clarity (90.7%)&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Lei Yu', 'Jingyuan Zhang', 'Xin Wang', 'Jiajia Ma', 'Li Yang', 'Fengjun Zhang']&lt;/li&gt;&lt;li&gt;Tags: ['security', 'safety evaluation', 'robustness', 'code generation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.09942</guid><pubDate>Mon, 15 Sep 2025 00:00:00 +0000</pubDate></item><item><title>Beyond I'm Sorry, I Can't: Dissecting Large Language Model Refusal</title><link>https://arxiv.org/abs/2509.09708</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Uses sparse autoencoders to identify latent features mediating model refusal&lt;/li&gt;&lt;li&gt;Develops a pipeline to find minimal feature sets that can flip refusal to compliance&lt;/li&gt;&lt;li&gt;Discovers redundant features that become active when primary features are suppressed&lt;/li&gt;&lt;li&gt;Provides insights into mechanistic basis of safety behaviors&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Nirmalendu Prakash', 'Yeo Wei Jie', 'Amir Abdullah', 'Ranjan Satapathy', 'Erik Cambria', 'Roy Ka Wei Lee']&lt;/li&gt;&lt;li&gt;Tags: ['jailbreaking', 'red teaming', 'safety evaluation', 'adversarial prompting', 'model internals']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.09708</guid><pubDate>Mon, 15 Sep 2025 00:00:00 +0000</pubDate></item><item><title>Differential Robustness in Transformer Language Models: Empirical Evaluation Under Adversarial Text Attacks</title><link>https://arxiv.org/abs/2509.09706</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Empirical evaluation of Flan-T5, BERT, and RoBERTa-Base against adversarial text attacks using TextFooler and BERTAttack&lt;/li&gt;&lt;li&gt;Significant variations in robustness found: RoBERTa-Base and FlanT5 showed 0% attack success, while BERT-Base had 93.75% success rate&lt;/li&gt;&lt;li&gt;Highlights computational resource requirements for effective defenses&lt;/li&gt;&lt;li&gt;Proposes recommendations for more efficient defensive strategies&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Taniya Gidatkar', 'Oluwaseun Ajao', 'Matthew Shardlow']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial attacks', 'robustness', 'LLM security', 'TextFooler', 'BERTAttack', 'empirical evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.09706</guid><pubDate>Mon, 15 Sep 2025 00:00:00 +0000</pubDate></item><item><title>CTCC: A Robust and Stealthy Fingerprinting Framework for Large Language Models via Cross-Turn Contextual Correlation Backdoor</title><link>https://arxiv.org/abs/2509.09703</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces CTCC, a novel fingerprinting framework for LLMs using cross-turn contextual correlations&lt;/li&gt;&lt;li&gt;Aims to improve stealth, robustness, and generalizability over existing methods&lt;/li&gt;&lt;li&gt;Employs rule-driven encoding of semantic relationships like counterfactuals across dialogue turns&lt;/li&gt;&lt;li&gt;Demonstrates superior performance across multiple LLM architectures&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Zhenhua Xu', 'Xixiang Zhao', 'Xubin Yue', 'Shengwei Tian', 'Changting Lin', 'Meng Han']&lt;/li&gt;&lt;li&gt;Tags: ['model_fingerprinting', 'LLM_security', 'adversarial_robustness', 'stealth', 'intellectual_property_protection']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.09703</guid><pubDate>Mon, 15 Sep 2025 00:00:00 +0000</pubDate></item></channel></rss>