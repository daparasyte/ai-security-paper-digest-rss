<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>AI Security Paper Digest</title><link>https://kentaroh-toyoda.github.io/ai-security-paper-digest-rss/rss.xml</link><description>Curated papers on AI security from OpenAlex and ArXiv</description><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><lastBuildDate>Tue, 22 Jul 2025 22:27:56 +0000</lastBuildDate><item><title>Cats Confuse Reasoning LLM: Query Agnostic Adversarial Triggers for Reasoning Models</title><link>https://arxiv.org/abs/2503.01781</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces query-agnostic adversarial triggers for reasoning models&lt;/li&gt;&lt;li&gt;Proposes CatAttack pipeline to generate triggers on proxy models and transfer to targets&lt;/li&gt;&lt;li&gt;Achieves &gt;300% increase in incorrect answers on target models like DeepSeek R1&lt;/li&gt;&lt;li&gt;Highlights security and reliability vulnerabilities in reasoning LLMs&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Meghana Rajeev', 'Rajkumar Ramamurthy', 'Prapti Trivedi', 'Vikas Yadav', 'Oluwanifemi Bamgbose', 'Sathwik Tejaswi Madhusudan', 'James Zou', 'Nazneen Rajani']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial prompting', 'red teaming', 'model robustness', 'security', 'LLM safety']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2503.01781</guid><pubDate>Tue, 22 Jul 2025 00:00:00 +0000</pubDate></item><item><title>Learning to Communicate in Multi-Agent Reinforcement Learning for Autonomous Cyber Defence</title><link>https://arxiv.org/abs/2507.14658</link><description>&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Other&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Faizan Contractor', 'Li Li', 'Ranwa Al Mallah']&lt;/li&gt;&lt;li&gt;Tags: []&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2507.14658</guid><pubDate>Tue, 22 Jul 2025 00:00:00 +0000</pubDate></item><item><title>ROBAD: Robust Adversary-aware Local-Global Attended Bad Actor Detection Sequential Model</title><link>https://arxiv.org/abs/2507.15067</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces ROBAD model for bad actor detection using transformer-based architecture&lt;/li&gt;&lt;li&gt;Combines local post embeddings with global sequence embeddings via encoder-decoder structure&lt;/li&gt;&lt;li&gt;Enhances robustness through contrastive learning with adversarial examples&lt;/li&gt;&lt;li&gt;Demonstrates effectiveness against state-of-the-art adversarial attacks on Yelp and Wikipedia datasets&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Bing He', 'Mustaque Ahamad', 'Srijan Kumar']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial robustness', 'security', 'transformer', 'contrastive learning', 'bad actor detection']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2507.15067</guid><pubDate>Tue, 22 Jul 2025 00:00:00 +0000</pubDate></item><item><title>FedStrategist: A Meta-Learning Framework for Adaptive and Robust Aggregation in Federated Learning</title><link>https://arxiv.org/abs/2507.14322</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces FedStrategist, a meta-learning framework for adaptive aggregation in FL&lt;/li&gt;&lt;li&gt;Uses a contextual bandit agent to select optimal defense strategies&lt;/li&gt;&lt;li&gt;Demonstrates robustness against adaptive adversaries and data poisoning&lt;/li&gt;&lt;li&gt;Allows risk tolerance adjustment for security-performance trade-off&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Md Rafid Haque', 'Abu Raihan Mostofa Kamal', 'Md. Azam Hossain']&lt;/li&gt;&lt;li&gt;Tags: ['federated learning', 'model poisoning', 'adaptive defense', 'robust aggregation', 'meta-learning']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2507.14322</guid><pubDate>Tue, 22 Jul 2025 00:00:00 +0000</pubDate></item><item><title>Defending Against Unforeseen Failure Modes with Latent Adversarial Training</title><link>https://arxiv.org/abs/2403.05030</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces Latent Adversarial Training (LAT) to defend against unknown failure modes&lt;/li&gt;&lt;li&gt;Shows improved robustness to novel attacks compared to traditional Adversarial Training (AT)&lt;/li&gt;&lt;li&gt;Evaluates on image classification, text classification, and text generation tasks&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Stephen Casper', 'Lennart Schulze', 'Oam Patel', 'Dylan Hadfield-Menell']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial training', 'latent adversarial training', 'robustness', 'red teaming', 'safety evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2403.05030</guid><pubDate>Tue, 22 Jul 2025 00:00:00 +0000</pubDate></item><item><title>Multi-Stage Prompt Inference Attacks on Enterprise LLM Systems</title><link>https://arxiv.org/abs/2507.15613</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Comprehensive study of multi-stage prompt inference attacks on enterprise LLMs&lt;/li&gt;&lt;li&gt;Develops formal threat model and analyzes attacks using probability and info theory&lt;/li&gt;&lt;li&gt;Proposes defenses including anomaly detection, access control, and prompt sanitization&lt;/li&gt;&lt;li&gt;Highlights need for multi-stage defense strategies beyond single-turn filtering&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Andrii Balashov', 'Olena Ponomarova', 'Xiaohua Zhai']&lt;/li&gt;&lt;li&gt;Tags: ['prompt_injection', 'adversarial_prompting', 'security', 'enterprise_llm', 'multi_stage_attacks']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2507.15613</guid><pubDate>Tue, 22 Jul 2025 00:00:00 +0000</pubDate></item><item><title>Scaling Decentralized Learning with FLock</title><link>https://arxiv.org/abs/2507.15349</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces FLock, a decentralized framework for secure LLM fine-tuning using blockchain&lt;/li&gt;&lt;li&gt;Replaces central server with trust layer to prevent single points of failure&lt;/li&gt;&lt;li&gt;Empirically validated on 70B model with &gt;68% reduction in adversarial attack success rates&lt;/li&gt;&lt;li&gt;Defends against backdoor poisoning attacks common in federated learning&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Zehua Cheng', 'Rui Sun', 'Jiahao Sun', 'Yike Guo']&lt;/li&gt;&lt;li&gt;Tags: ['data_poisoning', 'adversarial_defense', 'blockchain', 'federated_learning', 'decentralized_learning']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2507.15349</guid><pubDate>Tue, 22 Jul 2025 00:00:00 +0000</pubDate></item><item><title>PromptArmor: Simple yet Effective Prompt Injection Defenses</title><link>https://arxiv.org/abs/2507.15219</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces PromptArmor, a defense against prompt injection attacks&lt;/li&gt;&lt;li&gt;Uses an LLM to detect and remove injected prompts before processing&lt;/li&gt;&lt;li&gt;Achieves &lt;1% false positive/negative rates on AgentDojo benchmark&lt;/li&gt;&lt;li&gt;Effective against adaptive attacks&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Tianneng Shi', 'Kaijie Zhu', 'Zhun Wang', 'Yuqi Jia', 'Will Cai', 'Weida Liang', 'Haonan Wang', 'Hend Alzahrani', 'Joshua Lu', 'Kenji Kawaguchi', 'Basel Alomair', 'Xuandong Zhao', 'William Yang Wang', 'Neil Gong', 'Wenbo Guo', 'Dawn Song']&lt;/li&gt;&lt;li&gt;Tags: ['prompt injection', 'defense', 'adversarial prompting', 'security', 'LLM']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2507.15219</guid><pubDate>Tue, 22 Jul 2025 00:00:00 +0000</pubDate></item><item><title>Byzantine-Robust Decentralized Coordination of LLM Agents</title><link>https://arxiv.org/abs/2507.14928</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Addresses security issues in leader-driven multi-agent LLM systems&lt;/li&gt;&lt;li&gt;Proposes DecentLLMs for decentralized consensus and Byzantine fault tolerance&lt;/li&gt;&lt;li&gt;Demonstrates improved resilience and answer quality through experiments&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yongrae Jo', 'Chanik Park']&lt;/li&gt;&lt;li&gt;Tags: ['Byzantine-robust', 'multi-agent', 'LLM', 'security', 'decentralized']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2507.14928</guid><pubDate>Tue, 22 Jul 2025 00:00:00 +0000</pubDate></item><item><title>Manipulating LLM Web Agents with Indirect Prompt Injection Attack via HTML Accessibility Tree</title><link>https://arxiv.org/abs/2507.14799</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Demonstrates Indirect Prompt Injection (IPI) attack on LLM-based web navigation agents&lt;/li&gt;&lt;li&gt;Uses HTML accessibility tree to embed universal adversarial triggers&lt;/li&gt;&lt;li&gt;High success rates in real-world scenarios including credential exfiltration and ad clicks&lt;/li&gt;&lt;li&gt;Highlights critical security risks and need for stronger defenses&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Sam Johnson', 'Viet Pham', 'Thai Le']&lt;/li&gt;&lt;li&gt;Tags: ['prompt injection', 'adversarial prompting', 'red teaming', 'security', 'LLM']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2507.14799</guid><pubDate>Tue, 22 Jul 2025 00:00:00 +0000</pubDate></item><item><title>VTarbel: Targeted Label Attack with Minimal Knowledge on Detector-enhanced Vertical Federated Learning</title><link>https://arxiv.org/abs/2507.14625</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces VTarbel, a minimal-knowledge attack framework for VFL&lt;/li&gt;&lt;li&gt;Evades anomaly detectors and privacy defenses&lt;/li&gt;&lt;li&gt;Evaluated on multiple datasets and models&lt;/li&gt;&lt;li&gt;Highlights security vulnerabilities in VFL&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Juntao Tan', 'Anran Li', 'Quanchao Liu', 'Peng Ran', 'Lan Zhang']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial attacks', 'security', 'federated learning', 'privacy attacks']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2507.14625</guid><pubDate>Tue, 22 Jul 2025 00:00:00 +0000</pubDate></item><item><title>Breaking the Illusion of Security via Interpretation: Interpretable Vision Transformer Systems under Attack</title><link>https://arxiv.org/abs/2507.14248</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces AdViT attack targeting both Vision Transformers and their interpretation models&lt;/li&gt;&lt;li&gt;Achieves 100% attack success rate in white-box and black-box scenarios&lt;/li&gt;&lt;li&gt;Maintains accurate interpretations to evade detection&lt;/li&gt;&lt;li&gt;Conducted extensive experiments on multiple transformer models and interpreters&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Eldor Abdukhamidov', 'Mohammed Abuhamad', 'Simon S. Woo', 'Hyoungshick Kim', 'Tamer Abuhmed']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial attacks', 'red teaming', 'vision transformers', 'interpretability', 'security evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2507.14248</guid><pubDate>Tue, 22 Jul 2025 00:00:00 +0000</pubDate></item><item><title>Mitigating Trojanized Prompt Chains in Educational LLM Use Cases: Experimental Findings and Detection Tool Design</title><link>https://arxiv.org/abs/2507.14207</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Conducted experiments on Trojanized prompts in educational LLM use cases&lt;/li&gt;&lt;li&gt;Identified vulnerabilities in GPT-3.5 and GPT-4&lt;/li&gt;&lt;li&gt;Developed TrojanPromptGuard (TPG) detection tool&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Richard M. Charles', 'James H. Curry', 'Richard B. Charles']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial prompting', 'prompt injection', 'safety evaluation', 'LLM red teaming', 'educational technology']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2507.14207</guid><pubDate>Tue, 22 Jul 2025 00:00:00 +0000</pubDate></item><item><title>PRM-Free Security Alignment of Large Models via Red Teaming and Adversarial Training</title><link>https://arxiv.org/abs/2507.14202</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Presents a PRM-free security alignment framework using red teaming and adversarial training&lt;/li&gt;&lt;li&gt;Employs genetic algorithms, multi-agent simulation, and prompt mutation for vulnerability detection&lt;/li&gt;&lt;li&gt;Achieves 61% computational cost reduction vs PRM-based methods while improving security&lt;/li&gt;&lt;li&gt;Includes transparent reporting and continuous audit mechanisms&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Pengfei Du']&lt;/li&gt;&lt;li&gt;Tags: ['red teaming', 'adversarial training', 'security alignment', 'LLM', 'PRM-free', 'vulnerability detection', 'computational efficiency']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2507.14202</guid><pubDate>Tue, 22 Jul 2025 00:00:00 +0000</pubDate></item><item><title>ExCyTIn-Bench: Evaluating LLM agents on Cyber Threat Investigation</title><link>https://arxiv.org/abs/2507.14201</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces ExCyTIn-Bench, a benchmark for evaluating LLM agents in cyber threat investigation&lt;/li&gt;&lt;li&gt;Dataset includes 8 simulated attacks, 57 log tables, and 589 generated security questions&lt;/li&gt;&lt;li&gt;Questions derived from investigation graphs with verifiable ground truth answers&lt;/li&gt;&lt;li&gt;Experiments show current models have low average reward (0.249), indicating task difficulty&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yiran Wu', 'Mauricio Velazco', 'Andrew Zhao', "Manuel Ra\\'ul Mel\\'endez Luj\\'an", 'Srisuma Movva', 'Yogesh K Roy', 'Quang Nguyen', 'Roberto Rodriguez', 'Qingyun Wu', 'Michael Albada', 'Julia Kiseleva', 'Anand Mudgerikar']&lt;/li&gt;&lt;li&gt;Tags: ['safety evaluation', 'cybersecurity', 'benchmarking', 'LLM agents', 'threat investigation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2507.14201</guid><pubDate>Tue, 22 Jul 2025 00:00:00 +0000</pubDate></item><item><title>QSAF: A Novel Mitigation Framework for Cognitive Degradation in Agentic AI</title><link>https://arxiv.org/abs/2507.15330</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces Cognitive Degradation as a new vulnerability class in agentic AI systems&lt;/li&gt;&lt;li&gt;Proposes the QSAF framework with a six-stage lifecycle and seven runtime controls&lt;/li&gt;&lt;li&gt;Focuses on internal failures like memory starvation and context flooding&lt;/li&gt;&lt;li&gt;Draws parallels to cognitive neuroscience for early detection and mitigation&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Hammad Atta', 'Muhammad Zeeshan Baig', 'Yasir Mehmood', 'Nadeem Shahzad', 'Ken Huang', 'Muhammad Aziz Ul Haq', 'Muhammad Awais', 'Kamal Ahmed']&lt;/li&gt;&lt;li&gt;Tags: ['safety', 'robustness', 'agentic AI', 'cognitive degradation', 'mitigation framework']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2507.15330</guid><pubDate>Tue, 22 Jul 2025 00:00:00 +0000</pubDate></item><item><title>DeRAG: Black-box Adversarial Attacks on Multiple Retrieval-Augmented Generation Applications via Prompt Injection</title><link>https://arxiv.org/abs/2507.15042</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces DeRAG: a black-box adversarial attack method using Differential Evolution for prompt optimization&lt;/li&gt;&lt;li&gt;Targets RAG systems by injecting adversarial suffixes to manipulate document retrieval ranks&lt;/li&gt;&lt;li&gt;Employs gradient-free approach with readability-aware suffix construction&lt;/li&gt;&lt;li&gt;Demonstrates competitive success rates vs existing attacks and evasion of detection&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jerry Wang', 'Fang Yu']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial_prompting', 'prompt_injection', 'RAG_systems', 'black_box_attacks', 'differential_evolution', 'security_evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2507.15042</guid><pubDate>Tue, 22 Jul 2025 00:00:00 +0000</pubDate></item><item><title>AlphaAlign: Incentivizing Safety Alignment with Extremely Simplified Reinforcement Learning</title><link>https://arxiv.org/abs/2507.14987</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes AlphaAlign, a reinforcement learning framework for safety alignment in LLMs&lt;/li&gt;&lt;li&gt;Uses dual-reward system for safety and helpfulness&lt;/li&gt;&lt;li&gt;Demonstrates simplicity, breaks safety-utility trade-off, and fosters deep alignment&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yi Zhang', 'An Zhang', 'XiuYu Zhang', 'Leheng Sheng', 'Yuxin Chen', 'Zhenkai Liang', 'Xiang Wang']&lt;/li&gt;&lt;li&gt;Tags: ['safety alignment', 'robustness', 'red teaming', 'LLMs', 'reinforcement learning']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2507.14987</guid><pubDate>Tue, 22 Jul 2025 00:00:00 +0000</pubDate></item><item><title>Automated Safety Evaluations Across 20 Large Language Models: The Aymara LLM Risk and Responsibility Matrix</title><link>https://arxiv.org/abs/2507.14719</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces Aymara AI for automated safety evaluations of LLMs&lt;/li&gt;&lt;li&gt;Evaluates 20 models across 10 safety domains using adversarial prompts&lt;/li&gt;&lt;li&gt;Finds significant performance disparities, especially in Privacy &amp; Impersonation&lt;/li&gt;&lt;li&gt;Highlights need for scalable safety evaluation tools&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Juan Manuel Contreras']&lt;/li&gt;&lt;li&gt;Tags: ['safety evaluation', 'adversarial prompting', 'LLM red teaming', 'policy-grounded testing', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2507.14719</guid><pubDate>Tue, 22 Jul 2025 00:00:00 +0000</pubDate></item><item><title>Configurable multi-agent framework for scalable and realistic testing of llm-based agents</title><link>https://arxiv.org/abs/2507.14705</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Neo is a configurable multi-agent framework for testing LLM-based agents&lt;/li&gt;&lt;li&gt;Uses Question Generation and Evaluation Agents for dynamic, multi-turn testing&lt;/li&gt;&lt;li&gt;Applied to a chatbot, found security issues with high throughput&lt;/li&gt;&lt;li&gt;Aims to automate red teaming with scalable, realistic scenarios&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Sai Wang', 'Senthilnathan Subramanian', 'Mudit Sahni', 'Praneeth Gone', 'Lingjie Meng', 'Xiaochen Wang', 'Nicolas Ferradas Bertoli', 'Tingxian Cheng', 'Jun Xu']&lt;/li&gt;&lt;li&gt;Tags: ['red teaming', 'LLM', 'security', 'testing framework', 'multi-agent']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2507.14705</guid><pubDate>Tue, 22 Jul 2025 00:00:00 +0000</pubDate></item><item><title>When Autonomy Goes Rogue: Preparing for Risks of Multi-Agent Collusion in Social Systems</title><link>https://arxiv.org/abs/2507.14660</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces a simulation framework for multi-agent collusion risks&lt;/li&gt;&lt;li&gt;Applies to misinformation spread and e-commerce fraud&lt;/li&gt;&lt;li&gt;Finds decentralized systems more effective and adaptable&lt;/li&gt;&lt;li&gt;Highlights need for better detection and countermeasures&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Qibing Ren', 'Sitao Xie', 'Longxuan Wei', 'Zhenfei Yin', 'Junchi Yan', 'Lizhuang Ma', 'Jing Shao']&lt;/li&gt;&lt;li&gt;Tags: ['safety evaluation', 'multi-agent systems', 'collusion', 'red teaming', 'simulation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2507.14660</guid><pubDate>Tue, 22 Jul 2025 00:00:00 +0000</pubDate></item><item><title>WebGuard: Building a Generalizable Guardrail for Web Agents</title><link>https://arxiv.org/abs/2507.14293</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduced WebGuard dataset for evaluating web agent action risks.&lt;/li&gt;&lt;li&gt;Dataset has 4,939 annotated actions across 193 websites.&lt;/li&gt;&lt;li&gt;Evaluated LLMs show deficiencies in predicting risks.&lt;/li&gt;&lt;li&gt;Fine-tuned model improves performance but still lacks reliability.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Boyuan Zheng', 'Zeyi Liao', 'Scott Salisbury', 'Zeyuan Liu', 'Michael Lin', 'Qinyuan Zheng', 'Zifan Wang', 'Xiang Deng', 'Dawn Song', 'Huan Sun', 'Yu Su']&lt;/li&gt;&lt;li&gt;Tags: ['safety evaluation', 'guardrails', 'web agents', 'risk prediction', 'dataset']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2507.14293</guid><pubDate>Tue, 22 Jul 2025 00:00:00 +0000</pubDate></item></channel></rss>