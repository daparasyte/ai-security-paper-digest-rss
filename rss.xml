<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>AI Security Paper Digest</title><link>https://kentaroh-toyoda.github.io/ai-security-paper-digest-rss/rss.xml</link><description>Curated papers on AI security from OpenAlex and ArXiv</description><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><lastBuildDate>Fri, 13 Jun 2025 22:32:57 +0000</lastBuildDate><item><title>Towards Reliable Identification of Diffusion-based Image Manipulations</title><link>https://arxiv.org/abs/2506.05466</link><description>&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Other&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Quality Assessment&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Clarity: 5/5&lt;/li&gt;&lt;li&gt;Novelty: 5/5&lt;/li&gt;&lt;li&gt;Significance: 4/5&lt;/li&gt;&lt;li&gt;Try-worthiness: Yes&lt;/li&gt;&lt;li&gt;Justification: The abstract is well-written and clearly explains the motivation, approach (RADAR), and contributions, including a new benchmark (BBC-PAIR) and strong empirical results. The work is highly novel, addressing the emerging challenge of detecting diffusion-based image manipulations and introducing a new method that leverages foundation models and contrastive loss. The significance is high given the rapid rise of diffusion models and the need for reliable detection tools, though the impact is not yet reflected in citations due to the paper's recency and preprint status. The public release of code, data, and models increases try-worthiness, making it a promising candidate for implementation and further experimentation.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Alex Costanzino', 'Woody Bayliss', 'Juil Sock', 'Marc Gorriz Blanch', 'Danijela Horak', 'Ivan Laptev', 'Philip Torr', 'Fabio Pizzati']&lt;/li&gt;&lt;li&gt;Tags: []&lt;/li&gt;&lt;li&gt;Code Repository: &lt;a href='https://alex-costanzino.github.io/radar/'&gt;https://alex-costanzino.github.io/radar/&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.05466</guid><pubDate>Fri, 13 Jun 2025 00:00:00 +0000</pubDate></item><item><title>DyFFPAD: Dynamic Fusion of Convolutional and Handcrafted Features for Fingerprint Presentation Attack Detection</title><link>https://arxiv.org/abs/2308.10015</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a dynamic fusion approach combining deep CNN and handcrafted features for fingerprint presentation attack detection.&lt;/li&gt;&lt;li&gt;Targets the detection of spoofed fingerprints (presentation attacks) in biometric authentication systems.&lt;/li&gt;&lt;li&gt;Evaluates the method on multiple benchmark datasets from Liveness Detection Competitions, achieving high accuracy.&lt;/li&gt;&lt;li&gt;Addresses both known-material and unknown-material attack scenarios.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research Paper&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Quality Assessment&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Clarity: 4/5&lt;/li&gt;&lt;li&gt;Novelty: 3/5&lt;/li&gt;&lt;li&gt;Significance: 3/5&lt;/li&gt;&lt;li&gt;Try-worthiness: Yes&lt;/li&gt;&lt;li&gt;Justification: The abstract is well-written and clearly explains the motivation, approach, and results of the paper, justifying a high clarity score. The novelty is moderate: while combining deep CNN and handcrafted features for presentation attack detection is not entirely new, the dynamic fusion approach and strong results on multiple benchmark datasets add some novelty. The significance is moderate as well, since the problem is important and the results are strong, but the paper is a recent preprint on arXiv with no citations yet and has not been peer-reviewed or published in a top-tier venue. The method appears promising and outperforms state-of-the-art on standard benchmarks, making it worth trying for practitioners in biometric security. No code repository is provided in the metadata.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Anuj Rai', 'Parsheel Kumar Tiwari', 'Jyotishna Baishya', 'Ram Prakash Sharma', 'Somnath Dey']&lt;/li&gt;&lt;li&gt;Tags: ['biometric security', 'presentation attack detection', 'fingerprint spoofing', 'liveness detection', 'adversarial attacks']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2308.10015</guid><pubDate>Fri, 13 Jun 2025 00:00:00 +0000</pubDate></item><item><title>GenWorld: Towards Detecting AI-generated Real-world Simulation Videos</title><link>https://arxiv.org/abs/2506.10975</link><description>&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Other&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Quality Assessment&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Clarity: 4/5&lt;/li&gt;&lt;li&gt;Novelty: 5/5&lt;/li&gt;&lt;li&gt;Significance: 4/5&lt;/li&gt;&lt;li&gt;Try-worthiness: Yes&lt;/li&gt;&lt;li&gt;Justification: The abstract is well-written and clearly outlines the motivation, contributions, and results of the paper, though some technical details are omitted (clarity: 4). The work is highly novel, introducing a new large-scale dataset (GenWorld) focused on realistic AI-generated video detection, and proposing a new detection method (SpannDetector) leveraging multi-view consistency (novelty: 5). The significance is high due to the urgent need for robust AI-generated video detection and the lack of high-quality datasets in this area, though the impact is yet to be seen as it is a very recent preprint (significance: 4). The paper is worth trying, especially for researchers in multimedia forensics and AI safety, as it provides both a dataset and a new detection approach (try_worthiness: true). The project page is provided, which may contain code or data resources.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Weiliang Chen', 'Wenzhao Zheng', 'Yu Zheng', 'Lei Chen', 'Jie Zhou', 'Jiwen Lu', 'Yueqi Duan']&lt;/li&gt;&lt;li&gt;Tags: []&lt;/li&gt;&lt;li&gt;Code Repository: &lt;a href='https://chen-wl20.github.io/GenWorld'&gt;https://chen-wl20.github.io/GenWorld&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.10975</guid><pubDate>Fri, 13 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Unsourced Adversarial CAPTCHA: A Bi-Phase Adversarial CAPTCHA Framework</title><link>https://arxiv.org/abs/2506.10685</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a new adversarial CAPTCHA framework (UAC) that generates adversarial examples using attacker-specified text prompts.&lt;/li&gt;&lt;li&gt;Leverages large language models (LLMs) and diffusion models to create CAPTCHAs that are challenging for both humans and deep neural networks.&lt;/li&gt;&lt;li&gt;Introduces bi-path unsourced adversarial CAPTCHA (BP-UAC) for efficient black-box attacks using multimodal gradients.&lt;/li&gt;&lt;li&gt;Demonstrates high attack success rates against automated systems, highlighting vulnerabilities in current CAPTCHA schemes.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research Paper&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Quality Assessment&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Clarity: 4/5&lt;/li&gt;&lt;li&gt;Novelty: 5/5&lt;/li&gt;&lt;li&gt;Significance: 4/5&lt;/li&gt;&lt;li&gt;Try-worthiness: Yes&lt;/li&gt;&lt;li&gt;Justification: The abstract is well-written and clearly outlines the motivation, methods, and contributions of the paper, though some technical terms may require background knowledge. The work is highly novel, introducing an 'unsourced' adversarial CAPTCHA framework that does not rely on original images and leverages LLMs and diffusion models, which is a new direction in adversarial CAPTCHA research. The significance is high given the increasing vulnerability of CAPTCHAs to DNNs and the potential impact on security systems, though the preprint status and lack of citations (due to recency) slightly temper this. The methods described (BP-UAC, EDICT) and the reported results suggest the approach is promising and worth experimenting with, especially for researchers in adversarial ML and security. No code repository is provided in the metadata.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Xia Du', 'Xiaoyuan Liu', 'Jizhe Zhou', 'Zheng Lin', 'Chi-man Pun', 'Zhe Chen', 'Wei Ni', 'Jun Luo']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial attacks', 'CAPTCHA security', 'deep learning security', 'multimodal attacks', 'LLM security']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.10685</guid><pubDate>Fri, 13 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Boosting Adversarial Transferability for Hyperspectral Image Classification Using 3D Structure-invariant Transformation and Intermediate Feature Distance</title><link>https://arxiv.org/abs/2506.10459</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a novel method to enhance the transferability of adversarial examples for hyperspectral image (HSI) classification models.&lt;/li&gt;&lt;li&gt;Introduces 3D structure-invariant transformations and intermediate feature distance loss to generate more transferable adversarial attacks.&lt;/li&gt;&lt;li&gt;Demonstrates that the method is effective against black-box models and remains robust under defense strategies.&lt;/li&gt;&lt;li&gt;Addresses security vulnerabilities in DNN-based HSI classification by improving adversarial attack techniques.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research Paper&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Quality Assessment&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Clarity: 4/5&lt;/li&gt;&lt;li&gt;Novelty: 4/5&lt;/li&gt;&lt;li&gt;Significance: 3/5&lt;/li&gt;&lt;li&gt;Try-worthiness: Yes&lt;/li&gt;&lt;li&gt;Justification: The abstract is well-written and clearly explains the motivation, methodology, and results, though some technical terms may require domain knowledge. The paper proposes a novel approach for boosting adversarial transferability in hyperspectral image (HSI) classification by combining 3D structure-invariant transformations and intermediate feature distance loss, which appears to be a new contribution in the HSI domain. The significance is moderate: while adversarial robustness in HSI is important, the paper is a preprint on arXiv with no citations yet, so its impact is not yet established. However, the method is promising and addresses a real gap in the literature, making it worth trying for researchers in adversarial machine learning or HSI. No code repository is provided in the metadata.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Chun Liu', 'Bingqian Zhu', 'Tao Xu', 'Zheng Zheng', 'Zheng Li', 'Wei Yang', 'Zhigang Han', 'Jiayao Wang']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial attacks', 'transferability', 'image security', 'black-box attacks', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.10459</guid><pubDate>Fri, 13 Jun 2025 00:00:00 +0000</pubDate></item><item><title>The Automated but Risky Game: Modeling Agent-to-Agent Negotiations and Transactions in Consumer Markets</title><link>https://arxiv.org/abs/2506.00073</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Investigates risks associated with fully automated negotiations and transactions using LLM agents in consumer markets.&lt;/li&gt;&lt;li&gt;Develops an experimental framework to evaluate the performance and behavior of different LLM agents in negotiation scenarios.&lt;/li&gt;&lt;li&gt;Finds that behavioral anomalies in LLMs can lead to financial losses and imbalanced outcomes, highlighting potential safety and security risks.&lt;/li&gt;&lt;li&gt;Emphasizes the need for caution and risk awareness when delegating business decisions to AI agents.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research Paper&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Quality Assessment&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Clarity: 5/5&lt;/li&gt;&lt;li&gt;Novelty: 4/5&lt;/li&gt;&lt;li&gt;Significance: 4/5&lt;/li&gt;&lt;li&gt;Try-worthiness: Yes&lt;/li&gt;&lt;li&gt;Justification: The abstract is well-written, clearly outlining the motivation, research questions, methodology, and key findings. The topic is timely and novel, focusing on the risks and performance differences of LLM agents in automated consumer negotiations—a scenario that is increasingly relevant but underexplored. While the paper is a preprint and very recent (hence no citations yet), the significance is high due to the practical implications for AI deployment in consumer markets. The experimental framework and findings about agent imbalances and risks make it worth implementing or experimenting with, especially for those interested in AI safety, agent design, or automated commerce. No code repository is provided in the metadata.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Shenzhe Zhu', 'Jiao Sun', 'Yi Nian', 'Tobin South', 'Alex Pentland', 'Jiaxin Pei']&lt;/li&gt;&lt;li&gt;Tags: ['AI safety', 'risk assessment', 'LLM agents', 'automated transactions', 'behavioral anomalies']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.00073</guid><pubDate>Fri, 13 Jun 2025 00:00:00 +0000</pubDate></item><item><title>PRSA: Prompt Stealing Attacks against Real-World Prompt Services</title><link>https://arxiv.org/abs/2402.19200</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces PRSA, a practical attack framework for stealing prompts from real-world prompt services.&lt;/li&gt;&lt;li&gt;Demonstrates that PRSA can infer and replicate original prompt functionalities with high success rates.&lt;/li&gt;&lt;li&gt;Analyzes the risk factors for prompt leakage and proposes potential defenses.&lt;/li&gt;&lt;li&gt;Collaborates with prompt service vendors to address and mitigate discovered vulnerabilities.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research Paper&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Quality Assessment&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Clarity: 5/5&lt;/li&gt;&lt;li&gt;Novelty: 5/5&lt;/li&gt;&lt;li&gt;Significance: 4/5&lt;/li&gt;&lt;li&gt;Try-worthiness: Yes&lt;/li&gt;&lt;li&gt;Justification: The abstract is well-written, clearly outlining the motivation, methodology, results, and impact of the work. The topic—prompt stealing attacks against real-world prompt services—is highly novel and timely, given the rapid growth of LLM-based applications and prompt marketplaces. The paper claims significant improvements over previous work in attack success rates and provides actionable insights (e.g., the correlation between mutual information and leakage risk). While the paper is a preprint and very recent (hence no citations yet), the problem addressed is important for both security researchers and practitioners. The practical nature of the attack and the discussion of defenses make it worth experimenting with. No code repository is provided in the metadata.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yong Yang', 'Changjiang Li', 'Qingming Li', 'Oubo Ma', 'Haoyu Wang', 'Zonghui Wang', 'Yandong Gao', 'Wenzhi Chen', 'Shouling Ji']&lt;/li&gt;&lt;li&gt;Tags: ['prompt stealing', 'LLM security', 'prompt leakage', 'adversarial attacks', 'defense mechanisms']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2402.19200</guid><pubDate>Fri, 13 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Chain-of-Code Collapse: Reasoning Failures in LLMs via Adversarial Prompting in Code Generation</title><link>https://arxiv.org/abs/2506.06971</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Investigates the robustness of LLMs in code generation tasks using adversarial prompt perturbations.&lt;/li&gt;&lt;li&gt;Introduces 'Chain-of-Code Collapse' to systematically test reasoning failures via semantically faithful but adversarial prompts.&lt;/li&gt;&lt;li&gt;Finds significant performance degradation under certain adversarial prompt modifications, highlighting fragility in LLM reasoning.&lt;/li&gt;&lt;li&gt;Releases datasets and evaluation framework to support further research in trustworthy and resilient LLM reasoning.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research Paper&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Quality Assessment&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Clarity: 5/5&lt;/li&gt;&lt;li&gt;Novelty: 4/5&lt;/li&gt;&lt;li&gt;Significance: 4/5&lt;/li&gt;&lt;li&gt;Try-worthiness: Yes&lt;/li&gt;&lt;li&gt;Justification: The abstract is well-written, clearly outlining the motivation, methodology, and key findings. The work is novel in its systematic adversarial evaluation of LLM reasoning in code generation, introducing new perturbation techniques and analyzing their effects. While the venue is arXiv (preprint), the topic is highly relevant and timely, addressing a core question in LLM research. The significance is high given the widespread use of LLMs for code and reasoning tasks, and the findings about fragility and unpredictability are important for both researchers and practitioners. The release of datasets and evaluation framework further increases the paper's practical value and try-worthiness. No code repository link is provided in the metadata.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jaechul Roh', 'Varun Gandhi', 'Shivani Anilkumar', 'Arin Garg']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial prompting', 'robustness', 'LLM security', 'code generation', 'reasoning failures']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.06971</guid><pubDate>Fri, 13 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Efficiently Identifying Watermarked Segments in Mixed-Source Texts</title><link>https://arxiv.org/abs/2410.03600</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes novel methods for detecting and localizing watermarked segments within mixed-source texts generated by large language models (LLMs).&lt;/li&gt;&lt;li&gt;Addresses the challenge of partial watermark detection, which is important for identifying synthetic content in documents containing both human and AI-generated text.&lt;/li&gt;&lt;li&gt;Evaluates the proposed methods on multiple watermarking techniques, demonstrating improved accuracy over existing baselines.&lt;/li&gt;&lt;li&gt;Contributes to the mitigation of misuse cases such as fake news and academic dishonesty by enhancing detection capabilities.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research Paper&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Quality Assessment&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Clarity: 5/5&lt;/li&gt;&lt;li&gt;Novelty: 5/5&lt;/li&gt;&lt;li&gt;Significance: 4/5&lt;/li&gt;&lt;li&gt;Try-worthiness: Yes&lt;/li&gt;&lt;li&gt;Justification: The abstract is well-written and clearly explains the motivation, methods, and results. The focus on identifying watermarked segments within mixed-source texts is a novel and practical extension beyond existing whole-document detection approaches. The proposed geometry cover detection framework and adaptive online learning algorithm represent innovative contributions. While the paper is very recent and has no citations yet, the problem addressed is timely and significant for LLM safety and provenance. The availability of code further increases its try-worthiness for practitioners and researchers interested in watermark detection.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Xuandong Zhao', 'Chenwen Liao', 'Yu-Xiang Wang', 'Lei Li']&lt;/li&gt;&lt;li&gt;Tags: ['watermarking', 'LLM security', 'synthetic text detection', 'misuse mitigation', 'text forensics']&lt;/li&gt;&lt;li&gt;Code Repository: &lt;a href='https://github.com/XuandongZhao/llm-watermark-location'&gt;https://github.com/XuandongZhao/llm-watermark-location&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2410.03600</guid><pubDate>Fri, 13 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Weak-to-Strong Jailbreaking on Large Language Models</title><link>https://arxiv.org/abs/2401.17256</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces a novel 'weak-to-strong' jailbreaking attack that efficiently bypasses alignment in large language models (LLMs).&lt;/li&gt;&lt;li&gt;Demonstrates that the attack can drastically increase the misalignment rate of LLMs, causing them to generate harmful outputs.&lt;/li&gt;&lt;li&gt;Evaluates the attack on multiple open-source LLMs, showing high effectiveness with minimal computational cost.&lt;/li&gt;&lt;li&gt;Proposes an initial defense strategy and highlights the urgent need for more robust safety measures in LLM alignment.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research Paper&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Quality Assessment&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Clarity: 5/5&lt;/li&gt;&lt;li&gt;Novelty: 4/5&lt;/li&gt;&lt;li&gt;Significance: 4/5&lt;/li&gt;&lt;li&gt;Try-worthiness: Yes&lt;/li&gt;&lt;li&gt;Justification: The abstract is well-written, clearly explaining the motivation, method, and results. The proposed 'weak-to-strong' jailbreaking attack is a novel and efficient approach to bypassing LLM alignment, leveraging smaller models to manipulate a larger model's decoding process. The work is significant as it exposes a critical vulnerability in current LLM alignment strategies and demonstrates high misalignment rates with minimal computational cost. Although the paper is very recent and has no citations yet, the topic is highly relevant to LLM safety and security research. The availability of code further increases its try-worthiness for researchers and practitioners interested in LLM robustness and safety.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Xuandong Zhao', 'Xianjun Yang', 'Tianyu Pang', 'Chao Du', 'Lei Li', 'Yu-Xiang Wang', 'William Yang Wang']&lt;/li&gt;&lt;li&gt;Tags: ['LLM jailbreaking', 'adversarial attacks', 'AI safety', 'alignment', 'defense strategies']&lt;/li&gt;&lt;li&gt;Code Repository: &lt;a href='https://github.com/XuandongZhao/weak-to-strong'&gt;https://github.com/XuandongZhao/weak-to-strong&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2401.17256</guid><pubDate>Fri, 13 Jun 2025 00:00:00 +0000</pubDate></item><item><title>GenBreak: Red Teaming Text-to-Image Generators Using Large Language Models</title><link>https://arxiv.org/abs/2506.10047</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces GenBreak, a framework for red teaming text-to-image (T2I) generators using a fine-tuned large language model (LLM).&lt;/li&gt;&lt;li&gt;GenBreak systematically discovers adversarial prompts that can bypass safety filters and generate harmful content in T2I models.&lt;/li&gt;&lt;li&gt;The approach combines supervised fine-tuning and reinforcement learning to optimize for both evasion and image toxicity.&lt;/li&gt;&lt;li&gt;Demonstrates the effectiveness of GenBreak in black-box attacks against commercial T2I generators, exposing significant safety vulnerabilities.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research Paper&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Quality Assessment&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Clarity: 5/5&lt;/li&gt;&lt;li&gt;Novelty: 4/5&lt;/li&gt;&lt;li&gt;Significance: 4/5&lt;/li&gt;&lt;li&gt;Try-worthiness: Yes&lt;/li&gt;&lt;li&gt;Justification: The abstract is well-written, clearly outlining the motivation, limitations of prior work, and the proposed solution (GenBreak). The novelty is strong, as it introduces a systematic red-teaming framework leveraging LLMs for adversarial prompt generation in T2I models, combining supervised and reinforcement learning. The significance is high given the increasing deployment and risks of T2I models, and the lack of robust safety evaluation tools. While the paper is very new and not yet cited, the topic is timely and relevant. The approach appears practical and could be valuable for both researchers and practitioners concerned with AI safety. No code repository is provided in the metadata.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Zilong Wang', 'Xiang Zheng', 'Xiaosen Wang', 'Bo Wang', 'Xingjun Ma', 'Yu-Gang Jiang']&lt;/li&gt;&lt;li&gt;Tags: ['red teaming', 'adversarial prompting', 'text-to-image', 'AI safety', 'model vulnerabilities']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.10047</guid><pubDate>Fri, 13 Jun 2025 00:00:00 +0000</pubDate></item><item><title>How Well Can Reasoning Models Identify and Recover from Unhelpful Thoughts?</title><link>https://arxiv.org/abs/2506.10979</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Investigates reasoning models' ability to identify and recover from unhelpful or harmful thoughts during reasoning.&lt;/li&gt;&lt;li&gt;Finds that models can often identify unhelpful thoughts but struggle to recover once such thoughts are injected, leading to performance drops.&lt;/li&gt;&lt;li&gt;Includes a jailbreak experiment showing that irrelevant thought injection can trigger harmful responses, with smaller models being less susceptible.&lt;/li&gt;&lt;li&gt;Highlights the need for improved self-reevaluation mechanisms to enhance model safety and robustness.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research Paper&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Quality Assessment&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Clarity: 5/5&lt;/li&gt;&lt;li&gt;Novelty: 4/5&lt;/li&gt;&lt;li&gt;Significance: 4/5&lt;/li&gt;&lt;li&gt;Try-worthiness: Yes&lt;/li&gt;&lt;li&gt;Justification: The abstract is well-written, clearly outlining the motivation, methodology, and key findings. The focus on reasoning models' ability to self-reevaluate and recover from unhelpful thoughts is a timely and relatively novel angle, especially the analysis of different types of 'unhelpful thoughts' and the observation of non/inverse-scaling trends. The significance is high given the importance of robust reasoning in AI safety and reliability, though as a preprint, it has not yet been peer-reviewed or widely cited. The findings are actionable and suggest practical experiments or improvements, making the paper worth trying to implement or build upon. No code repository is provided in the metadata.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Sohee Yang', 'Sang-Woo Lee', 'Nora Kassner', 'Daniela Gottesman', 'Sebastian Riedel', 'Mor Geva']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'jailbreaking', 'robustness', 'safety evaluation', 'adversarial prompting']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.10979</guid><pubDate>Fri, 13 Jun 2025 00:00:00 +0000</pubDate></item><item><title>TimberStrike: Dataset Reconstruction Attack Revealing Privacy Leakage in Federated Tree-Based Systems</title><link>https://arxiv.org/abs/2506.07605</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces TimberStrike, a dataset reconstruction attack targeting federated tree-based models.&lt;/li&gt;&lt;li&gt;Demonstrates that a single client can infer sensitive training data from other clients using decision tree split values and paths.&lt;/li&gt;&lt;li&gt;Evaluates the attack on multiple federated learning frameworks, revealing significant privacy vulnerabilities.&lt;/li&gt;&lt;li&gt;Analyzes the effectiveness of Differential Privacy as a mitigation, noting trade-offs with model performance.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research Paper&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Quality Assessment&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Clarity: 5/5&lt;/li&gt;&lt;li&gt;Novelty: 5/5&lt;/li&gt;&lt;li&gt;Significance: 4/5&lt;/li&gt;&lt;li&gt;Try-worthiness: Yes&lt;/li&gt;&lt;li&gt;Justification: The abstract is very clear, well-structured, and communicates the motivation, method, results, and implications effectively. The work is highly novel, as it addresses privacy attacks on federated tree-based models—a less explored area compared to neural networks. The attack (TimberStrike) is demonstrated across multiple frameworks and shows high reconstruction rates, indicating significant privacy risks. While the paper is a preprint and very recent (no citations yet), the topic is timely and relevant, especially as federated learning adoption grows. The findings are important for both researchers and practitioners, and the attack seems feasible to implement and test. No code repository is provided in the metadata.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Marco Di Gennaro', 'Giovanni De Lucia', 'Stefano Longari', 'Stefano Zanero', 'Michele Carminati']&lt;/li&gt;&lt;li&gt;Tags: ['privacy attack', 'federated learning', 'dataset reconstruction', 'tree-based models', 'differential privacy']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.07605</guid><pubDate>Fri, 13 Jun 2025 00:00:00 +0000</pubDate></item><item><title>DiffUMI: Training-Free Universal Model Inversion via Unconditional Diffusion for Face Recognition</title><link>https://arxiv.org/abs/2504.18015</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes DiffUMI, a training-free universal model inversion attack using diffusion models to reconstruct facial images from embeddings.&lt;/li&gt;&lt;li&gt;Demonstrates that face recognition embeddings are vulnerable to inversion attacks, challenging their privacy-preserving claims.&lt;/li&gt;&lt;li&gt;Shows that DiffUMI outperforms previous state-of-the-art inversion attacks on both standard and privacy-preserving face recognition systems.&lt;/li&gt;&lt;li&gt;Introduces a novel use of out-of-domain detection to distinguish facial from non-facial embeddings in the embedding space.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research Paper&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Quality Assessment&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Clarity: 5/5&lt;/li&gt;&lt;li&gt;Novelty: 5/5&lt;/li&gt;&lt;li&gt;Significance: 4/5&lt;/li&gt;&lt;li&gt;Try-worthiness: Yes&lt;/li&gt;&lt;li&gt;Justification: The abstract is well-written, clearly outlining the motivation, problem, and contributions of the work. The proposed DiffUMI method is described as the first training-free, universal model inversion attack using diffusion models, which is a novel approach in the context of face recognition privacy. The paper claims significant improvements over state-of-the-art attacks and introduces a new application of out-of-domain detection for embeddings. While the paper is very recent and has no citations yet, the topic is highly relevant and the claimed results are impactful, making it significant for both privacy and security research. The method's training-free nature and universality make it particularly worth trying for practitioners and researchers. No code repository is provided in the metadata.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Hanrui Wang', 'Shuo Wang', 'Chun-Shien Lu', 'Isao Echizen']&lt;/li&gt;&lt;li&gt;Tags: ['model inversion', 'privacy attack', 'face recognition', 'diffusion models', 'biometric security']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2504.18015</guid><pubDate>Fri, 13 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Improving LLM Safety Alignment with Dual-Objective Optimization</title><link>https://arxiv.org/abs/2503.03710</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Identifies limitations in current LLM safety alignment methods, specifically Direct Preference Optimization (DPO), regarding vulnerability to jailbreak attacks.&lt;/li&gt;&lt;li&gt;Proposes a dual-objective optimization approach that separates robust refusal training and targeted unlearning of harmful knowledge.&lt;/li&gt;&lt;li&gt;Demonstrates significant improvements in LLM robustness against various jailbreak attacks, including prefilling, suffix, and multi-turn attacks.&lt;/li&gt;&lt;li&gt;Introduces a reward-based token-level weighting mechanism to further enhance refusal learning and adversarial robustness.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research Paper&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Quality Assessment&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Clarity: 4/5&lt;/li&gt;&lt;li&gt;Novelty: 4/5&lt;/li&gt;&lt;li&gt;Significance: 4/5&lt;/li&gt;&lt;li&gt;Try-worthiness: Yes&lt;/li&gt;&lt;li&gt;Justification: The abstract is well-written and clearly outlines the motivation, limitations of existing methods (DPO), and the proposed dual-objective approach for improving LLM safety alignment. The work appears novel in its explicit disentanglement of refusal and unlearning objectives, as well as the introduction of token-level weighting for refusal learning. Given the ongoing challenges with jailbreak attacks and the importance of safety alignment in LLMs, the work is significant, especially as it addresses both theoretical and practical aspects. The presence of a code repository further increases its try-worthiness for practitioners and researchers interested in LLM safety. Although it is a recent preprint with no citations yet, the topic and approach are timely and relevant.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Xuandong Zhao', 'Will Cai', 'Tianneng Shi', 'David Huang', 'Licong Lin', 'Song Mei', 'Dawn Song']&lt;/li&gt;&lt;li&gt;Tags: ['LLM safety alignment', 'jailbreak attacks', 'robust refusal training', 'adversarial robustness', 'harmful content mitigation']&lt;/li&gt;&lt;li&gt;Code Repository: &lt;a href='https://github.com/wicai24/DOOR-Alignment'&gt;https://github.com/wicai24/DOOR-Alignment&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2503.03710</guid><pubDate>Fri, 13 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Breaking Distortion-free Watermarks in Large Language Models</title><link>https://arxiv.org/abs/2502.18608</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Analyzes the security of distortion-free watermarking schemes in large language models (LLMs).&lt;/li&gt;&lt;li&gt;Demonstrates that adaptive prompting and sorting-based algorithms can recover secret watermarking keys.&lt;/li&gt;&lt;li&gt;Shows that even advanced watermarking schemes are vulnerable to spoofing attacks, enabling adversaries to generate texts falsely attributed to watermarked LLMs.&lt;/li&gt;&lt;li&gt;Empirically evaluates attacks on several popular LLMs, challenging claims about the robustness of current watermarking techniques.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research Paper&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Quality Assessment&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Clarity: 4/5&lt;/li&gt;&lt;li&gt;Novelty: 5/5&lt;/li&gt;&lt;li&gt;Significance: 4/5&lt;/li&gt;&lt;li&gt;Try-worthiness: Yes&lt;/li&gt;&lt;li&gt;Justification: The abstract is well-written and clearly explains the motivation, prior work, and the new contribution: breaking distortion-free watermarking in LLMs, which is a timely and important topic. The novelty is high, as it targets a newer, less-explored watermarking scheme (distortion-free) rather than the more commonly attacked distribution-modifying methods. The significance is strong given the practical implications for AI safety and watermarking robustness, though the preprint status and lack of citations (due to recency) slightly temper this. The work is worth trying for researchers interested in LLM security, watermarking, or adversarial attacks. No code repository is listed in the metadata.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Shayleen Reynolds', 'Hengzhi He', 'Dung Daniel T. Ngo', 'Saheed Obitayo', 'Niccol\\`o Dalmasso', 'Guang Cheng', 'Vamsi K. Potluru', 'Manuela Veloso']&lt;/li&gt;&lt;li&gt;Tags: ['LLM watermarking', 'adversarial attacks', 'model security', 'prompt-based attacks', 'robustness evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2502.18608</guid><pubDate>Fri, 13 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Debiasing Watermarks for Large Language Models via Maximal Coupling</title><link>https://arxiv.org/abs/2411.11203</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a novel watermarking technique for large language models using a green/red list approach and maximal coupling.&lt;/li&gt;&lt;li&gt;Aims to distinguish between human and machine-generated text, supporting digital content integrity.&lt;/li&gt;&lt;li&gt;Demonstrates robust detection capabilities and resilience to targeted modifications.&lt;/li&gt;&lt;li&gt;Balances watermark detectability with minimal impact on text quality.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research Paper&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Quality Assessment&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Clarity: 4/5&lt;/li&gt;&lt;li&gt;Novelty: 5/5&lt;/li&gt;&lt;li&gt;Significance: 4/5&lt;/li&gt;&lt;li&gt;Try-worthiness: Yes&lt;/li&gt;&lt;li&gt;Justification: The abstract is well-written and clearly explains the motivation, method, and results of the paper, though some technical terms (e.g., maximal coupling) may require background knowledge. The approach appears novel, introducing a green/red list watermarking scheme with maximal coupling for bias correction, which is not standard in watermarking literature. The significance is high given the growing importance of watermarking for LLMs, and the paper claims robust detection and minimal text quality impact, which addresses key challenges in the field. Although it is a recent preprint with no citations yet, the described improvements over prior work and resilience to attacks make it worth experimenting with. No code repository is provided in the metadata.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yangxinyu Xie', 'Xiang Li', 'Tanwi Mallick', 'Weijie J. Su', 'Ruixun Zhang']&lt;/li&gt;&lt;li&gt;Tags: ['watermarking', 'LLM security', 'content authenticity', 'robust detection', 'text integrity']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2411.11203</guid><pubDate>Fri, 13 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Unveiling the Role of Randomization in Multiclass Adversarial Classification: Insights from Graph Theory</title><link>https://arxiv.org/abs/2503.14299</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Analyzes the impact of randomization on adversarial robustness in multiclass classification settings.&lt;/li&gt;&lt;li&gt;Utilizes graph theory to model adversarial risk minimization as set packing problems.&lt;/li&gt;&lt;li&gt;Identifies structural conditions under which randomization improves robustness against adversarial attacks.&lt;/li&gt;&lt;li&gt;Demonstrates cases where randomized solutions outperform deterministic ones in reducing adversarial risk.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research Paper&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Quality Assessment&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Clarity: 4/5&lt;/li&gt;&lt;li&gt;Novelty: 5/5&lt;/li&gt;&lt;li&gt;Significance: 4/5&lt;/li&gt;&lt;li&gt;Try-worthiness: Yes&lt;/li&gt;&lt;li&gt;Justification: The paper is clearly written, with a well-structured abstract that outlines the motivation, approach, and key findings. The focus on multiclass adversarial classification and the use of graph theory to analyze randomization is novel, especially since most prior work has focused on binary settings. The significance is high due to the theoretical advancement and potential practical implications for improving adversarial robustness in multiclass problems, though the impact is yet to be seen given the paper's recent release and preprint status. The work appears worth implementing or experimenting with, particularly for researchers interested in adversarial robustness and theoretical machine learning. No code repository is provided.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Lucas Gnecco-Heredia', 'Matteo Sammut', 'Muni Sreenivas Pydi', 'Rafael Pinot', 'Benjamin Negrevergne', 'Yann Chevaleyre']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial robustness', 'randomization', 'multiclass classification', 'theoretical analysis']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2503.14299</guid><pubDate>Fri, 13 Jun 2025 00:00:00 +0000</pubDate></item><item><title>A hierarchical approach for assessing the vulnerability of tree-based classification models to membership inference attack</title><link>https://arxiv.org/abs/2502.09396</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes efficient methods to assess the vulnerability of tree-based classification models to membership inference attacks (MIA).&lt;/li&gt;&lt;li&gt;Introduces ante-hoc (hyperparameter-based) and post-hoc (structural metric-based) analyses to filter out high-risk models before expensive MIA testing.&lt;/li&gt;&lt;li&gt;Finds that certain hyperparameter choices consistently increase disclosure risk across datasets, enabling interpretable risk rules.&lt;/li&gt;&lt;li&gt;Empirical results show that model accuracy does not correlate with privacy risk, highlighting the need for privacy-aware model selection.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research Paper&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Quality Assessment&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Clarity: 4/5&lt;/li&gt;&lt;li&gt;Novelty: 4/5&lt;/li&gt;&lt;li&gt;Significance: 3/5&lt;/li&gt;&lt;li&gt;Try-worthiness: Yes&lt;/li&gt;&lt;li&gt;Justification: The abstract is well-written and clearly explains the motivation, methods, and findings, though it is somewhat dense and could be more concise. The work is novel in proposing efficient, hierarchical filtering approaches (ante-hoc and post-hoc) for assessing MIA vulnerability in tree-based models, which addresses a practical pain point in privacy evaluation. The significance is moderate: while the problem is important and the approach is promising, the paper is a recent preprint on arXiv with no citations yet, so its impact is not yet established. The methods described (simple, interpretable rules and structural metrics for risk assessment) are practical and could be valuable for practitioners, making the paper worth trying out, especially for those working with tree-based models and privacy concerns. No code repository is provided in the metadata.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Richard J. Preen', 'Jim Smith']&lt;/li&gt;&lt;li&gt;Tags: ['membership inference attack', 'privacy attacks', 'model vulnerability assessment', 'tree-based models', 'risk mitigation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2502.09396</guid><pubDate>Fri, 13 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Free Record-Level Privacy Risk Evaluation Through Artifact-Based Methods</title><link>https://arxiv.org/abs/2411.05743</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a novel method (LT-IQR) for identifying training samples most vulnerable to membership inference attacks (MIAs) using artifacts from the training process.&lt;/li&gt;&lt;li&gt;Eliminates the need for computationally expensive shadow model training, making privacy risk evaluation more accessible.&lt;/li&gt;&lt;li&gt;Demonstrates high precision in identifying vulnerable samples across datasets and model architectures.&lt;/li&gt;&lt;li&gt;Highlights the potential for efficient, artifact-based privacy risk evaluation during model development.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research Paper&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Quality Assessment&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Clarity: 5/5&lt;/li&gt;&lt;li&gt;Novelty: 5/5&lt;/li&gt;&lt;li&gt;Significance: 4/5&lt;/li&gt;&lt;li&gt;Try-worthiness: Yes&lt;/li&gt;&lt;li&gt;Justification: The abstract is very well-written, clearly explaining the motivation, limitations of existing methods, and the proposed solution (LT-IQR). The approach is novel in that it leverages artifacts from the training process (per-sample loss trajectories) to identify privacy risks without the computational overhead of shadow models, which is a significant improvement over the state-of-the-art. The significance is high given the practical impact for privacy risk assessment in machine learning, though the paper is a preprint and has not yet accumulated citations or peer-reviewed validation. The method's strong reported performance and efficiency make it worth trying for practitioners concerned with privacy. No code repository is provided in the metadata.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Joseph Pollock', 'Igor Shilov', 'Euodia Dodd', 'Yves-Alexandre de Montjoye']&lt;/li&gt;&lt;li&gt;Tags: ['membership inference attacks', 'privacy risk evaluation', 'artifact-based methods', 'model vulnerability']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2411.05743</guid><pubDate>Fri, 13 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Differentially private and decentralized randomized power method</title><link>https://arxiv.org/abs/2411.01931</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes differentially private variants of the randomized power method for spectral analysis and recommendation tasks.&lt;/li&gt;&lt;li&gt;Introduces a refined privacy analysis that reduces the required noise for achieving differential privacy.&lt;/li&gt;&lt;li&gt;Adapts the method to a decentralized setting, enhancing privacy without sacrificing accuracy or efficiency.&lt;/li&gt;&lt;li&gt;Provides theoretical convergence bounds and empirical evaluation on real-world datasets.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research Paper&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Quality Assessment&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Clarity: 4/5&lt;/li&gt;&lt;li&gt;Novelty: 4/5&lt;/li&gt;&lt;li&gt;Significance: 3/5&lt;/li&gt;&lt;li&gt;Try-worthiness: Yes&lt;/li&gt;&lt;li&gt;Justification: The abstract is well-written and clearly explains the motivation, contributions, and results of the paper, though some technical details are omitted as is typical for an abstract. The work appears novel in two ways: reducing the noise required for differential privacy in the randomized power method, and adapting the method to a decentralized setting with improved privacy and low overhead. The significance is moderate: while the topic is important and the improvements are meaningful, the paper is a recent arXiv preprint with no citations yet and not peer-reviewed. However, the combination of privacy, decentralization, and practical spectral analysis makes it worth trying for researchers or practitioners in privacy-preserving machine learning or recommendation systems. No code repository is provided in the metadata.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Julien Nicolas', "C\\'esar Sabater", 'Mohamed Maouche', 'Sonia Ben Mokhtar', 'Mark Coates']&lt;/li&gt;&lt;li&gt;Tags: ['differential privacy', 'privacy-preserving machine learning', 'decentralized learning', 'privacy attacks']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2411.01931</guid><pubDate>Fri, 13 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Privacy-aware Berrut Approximated Coded Computing for Federated Learning</title><link>https://arxiv.org/abs/2405.01704</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a privacy-preserving technique for Federated Learning (FL) using Berrut Approximated Coded Computing adapted to Secret Sharing.&lt;/li&gt;&lt;li&gt;Addresses privacy vulnerabilities in FL, offering an alternative to Differential Privacy, Homomorphic Encryption, and Secure Multi-Party Computation.&lt;/li&gt;&lt;li&gt;Demonstrates applicability to non-linear functions and distributed matrix multiplication, improving scalability and efficiency.&lt;/li&gt;&lt;li&gt;Provides analysis of privacy guarantees and computational complexity, with empirical results showing a trade-off between privacy and precision.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research Paper&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Quality Assessment&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Clarity: 4/5&lt;/li&gt;&lt;li&gt;Novelty: 4/5&lt;/li&gt;&lt;li&gt;Significance: 3/5&lt;/li&gt;&lt;li&gt;Try-worthiness: Yes&lt;/li&gt;&lt;li&gt;Justification: The abstract is well-written and clearly outlines the motivation, limitations of existing privacy-preserving techniques in federated learning, and the proposed solution using Berrut Approximated Coded Computing. The approach appears novel, especially in adapting a coded computing technique for privacy in FL, and claims to address both non-linear functions and distributed matrix multiplication, which are challenging for traditional methods. While the paper is very recent and has no citations yet, the problem addressed is significant and the proposed solution could have broad applicability. The lack of a code repository is a minor drawback, but the described trade-off between privacy and precision and the claim of extensive numerical results make it worth experimenting with for researchers in privacy-preserving FL.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ["Xavier Mart\\'inez Lua\\~na", "Rebeca P. D\\'iaz Redondo", "Manuel Fern\\'andez Veiga"]&lt;/li&gt;&lt;li&gt;Tags: ['federated learning', 'privacy', 'secret sharing', 'coded computing', 'input privacy']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2405.01704</guid><pubDate>Fri, 13 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Assessing the Resilience of Automotive Intrusion Detection Systems to Adversarial Manipulation</title><link>https://arxiv.org/abs/2506.10620</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Investigates the vulnerability of automotive intrusion detection systems (IDSs) to adversarial attacks.&lt;/li&gt;&lt;li&gt;Analyzes gradient-based adversarial evasion attacks under white-box, grey-box, and black-box scenarios.&lt;/li&gt;&lt;li&gt;Evaluates attack effectiveness on state-of-the-art IDSs using real automotive datasets.&lt;/li&gt;&lt;li&gt;Assesses real-time feasibility of adversarial payloads and discusses the impact of adversarial perturbations.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research Paper&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Quality Assessment&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Clarity: 4/5&lt;/li&gt;&lt;li&gt;Novelty: 4/5&lt;/li&gt;&lt;li&gt;Significance: 4/5&lt;/li&gt;&lt;li&gt;Try-worthiness: Yes&lt;/li&gt;&lt;li&gt;Justification: The paper is clearly written, with a well-structured abstract that outlines the motivation, methodology, and findings. The focus on adversarial attacks against automotive IDSs, especially considering different attacker knowledge scenarios (white-box, grey-box, black-box), is a novel and timely extension in the automotive cybersecurity domain. The use of gradient-based attacks and evaluation on public datasets adds practical relevance. While the paper is a preprint and has no citations yet (which is expected given its recency), the topic is significant due to the increasing importance of vehicle security. The results and methodology suggest that the work is worth implementing or experimenting with, especially for researchers or practitioners in automotive security or adversarial machine learning. No code repository is provided in the metadata.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Stefano Longari', 'Paolo Cerracchio', 'Michele Carminati', 'Stefano Zanero']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial attacks', 'intrusion detection', 'automotive security', 'robustness', 'evasion attacks']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.10620</guid><pubDate>Fri, 13 Jun 2025 00:00:00 +0000</pubDate></item><item><title>A Crack in the Bark: Leveraging Public Knowledge to Remove Tree-Ring Watermarks</title><link>https://arxiv.org/abs/2506.10502</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a novel attack to remove Tree-Ring watermarks from images generated by diffusion models.&lt;/li&gt;&lt;li&gt;The attack leverages publicly available variational autoencoders, highlighting a new threat vector.&lt;/li&gt;&lt;li&gt;Demonstrates that the attack significantly degrades watermark detection performance while preserving image quality.&lt;/li&gt;&lt;li&gt;Raises concerns about current industry practices regarding the reuse of public autoencoders and the real-world reliability of watermark detectors.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research Paper&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Quality Assessment&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Clarity: 5/5&lt;/li&gt;&lt;li&gt;Novelty: 5/5&lt;/li&gt;&lt;li&gt;Significance: 4/5&lt;/li&gt;&lt;li&gt;Try-worthiness: Yes&lt;/li&gt;&lt;li&gt;Justification: The abstract is clearly written, with a well-structured explanation of the attack, its requirements, and its impact. The work is highly novel, as it introduces a new attack vector against Tree-Ring watermarking by exploiting the public availability of variational autoencoders—a threat not previously considered. The significance is high due to the practical implications for watermarking robustness and industry practices, though the impact is not yet validated by citations due to the paper's recency and preprint status. The dramatic reduction in detection metrics and outperforming of prior attacks make this work worth experimenting with, especially for those interested in diffusion model security and watermarking. No code repository is provided in the metadata.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Junhua Lin (University of Edinburgh)', 'Marc Juarez (University of Edinburgh)']&lt;/li&gt;&lt;li&gt;Tags: ['watermark removal', 'diffusion models', 'model security', 'adversarial attacks', 'AI system vulnerabilities']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.10502</guid><pubDate>Fri, 13 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Build the web for agents, not agents for the web</title><link>https://arxiv.org/abs/2506.10953</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Advocates for designing web interfaces specifically for AI agents rather than adapting agents to human-centric web interfaces.&lt;/li&gt;&lt;li&gt;Introduces the concept of an Agentic Web Interface (AWI) with guiding principles including safety, efficiency, and standardization.&lt;/li&gt;&lt;li&gt;Emphasizes the importance of safety and reliability in the design of web interfaces for autonomous agents.&lt;/li&gt;&lt;li&gt;Positions the work as a call for a paradigm shift to address limitations and risks in current web agent approaches.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Position Paper&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Quality Assessment&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Clarity: 5/5&lt;/li&gt;&lt;li&gt;Novelty: 5/5&lt;/li&gt;&lt;li&gt;Significance: 4/5&lt;/li&gt;&lt;li&gt;Try-worthiness: No&lt;/li&gt;&lt;li&gt;Justification: The paper is very clearly written, with a well-articulated motivation and proposal for a paradigm shift in web agent research. The idea of designing web interfaces specifically for agents (Agentic Web Interface, AWI) is highly novel and addresses a fundamental limitation in current approaches. While the significance is potentially high, as it could reshape how web automation is approached, the paper is a position piece rather than an empirical or technical contribution, and it does not present an implementation or experimental results. Therefore, it is not directly try-worthy in terms of implementation, but it is valuable for inspiring future research and discussion. No code repository is provided.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Xing Han L\\`u', 'Gaurav Kamath', 'Marius Mosbach', 'Siva Reddy']&lt;/li&gt;&lt;li&gt;Tags: ['AI safety', 'web agents', 'interface design', 'agentic web', 'standardization']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.10953</guid><pubDate>Fri, 13 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Lattice Climber Attack: Adversarial attacks for randomized mixtures of classifiers</title><link>https://arxiv.org/abs/2506.10888</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Analyzes the robustness of randomized mixtures of classifiers (ensembles) against adversarial attacks.&lt;/li&gt;&lt;li&gt;Identifies limitations of existing adversarial attacks when targeting such ensembles.&lt;/li&gt;&lt;li&gt;Proposes a new attack method, the 'lattice climber attack', with theoretical guarantees.&lt;/li&gt;&lt;li&gt;Demonstrates the effectiveness of the new attack through experiments on synthetic and real datasets.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research Paper&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Quality Assessment&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Clarity: 4/5&lt;/li&gt;&lt;li&gt;Novelty: 5/5&lt;/li&gt;&lt;li&gt;Significance: 4/5&lt;/li&gt;&lt;li&gt;Try-worthiness: Yes&lt;/li&gt;&lt;li&gt;Justification: The abstract is well-written and clearly outlines the motivation, problem, and contributions, though some technical terms may require background knowledge. The work is highly novel, introducing a new attack (lattice climber) specifically designed for randomized mixtures of classifiers, a setting where existing attacks are insufficient. The significance is high given the growing interest in robust machine learning and adversarial attacks, though the impact is yet to be seen due to its recent release and preprint status. The theoretical guarantees and experimental validation make it worth trying for researchers in adversarial ML. No code repository is provided in the metadata.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Lucas Gnecco-Heredia', 'Benjamin Negrevergne', 'Yann Chevaleyre']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial attacks', 'robustness', 'ensemble models', 'security evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.10888</guid><pubDate>Fri, 13 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Viability of Future Actions: Robust Safety in Reinforcement Learning via Entropy Regularization</title><link>https://arxiv.org/abs/2506.10871</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Investigates robust safety in reinforcement learning (RL) under unknown disturbances.&lt;/li&gt;&lt;li&gt;Analyzes how entropy regularization and constraint penalization can promote robust constraint satisfaction.&lt;/li&gt;&lt;li&gt;Demonstrates empirically that entropy regularization biases policies toward maximizing viable future actions, enhancing safety.&lt;/li&gt;&lt;li&gt;Proposes a reformulation that approximates constrained RL problems with unconstrained ones, improving resilience to disturbances.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research Paper&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Quality Assessment&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Clarity: 4/5&lt;/li&gt;&lt;li&gt;Novelty: 4/5&lt;/li&gt;&lt;li&gt;Significance: 3/5&lt;/li&gt;&lt;li&gt;Try-worthiness: Yes&lt;/li&gt;&lt;li&gt;Justification: The abstract is well-written and clearly explains the motivation, approach, and findings of the paper, though some technical terms may require RL background (clarity: 4). The work appears novel in its empirical and conceptual connection between entropy regularization and robust safety in RL, especially in the context of constraint satisfaction under disturbances (novelty: 4). As a recent arXiv preprint, it has no citations yet and is not peer-reviewed, but the topic is timely and relevant for safe RL research (significance: 3). The approach—using entropy regularization and constraint penalties for robust safety—seems practical and worth experimenting with, especially since it leverages standard RL techniques (try_worthiness: true). No code repository is provided in the metadata or abstract.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Pierre-Fran\\c{c}ois Massiani', 'Alexander von Rohr', 'Lukas Haverbeck', 'Sebastian Trimpe']&lt;/li&gt;&lt;li&gt;Tags: ['reinforcement learning', 'robustness', 'safety', 'entropy regularization', 'constraint satisfaction']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.10871</guid><pubDate>Fri, 13 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Detecting High-Stakes Interactions with Activation Probes</title><link>https://arxiv.org/abs/2506.10805</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Investigates the use of activation probes to detect 'high-stakes' interactions in LLM outputs that could lead to significant harm.&lt;/li&gt;&lt;li&gt;Compares probe architectures to LLM-based monitors, finding similar performance with much lower computational cost.&lt;/li&gt;&lt;li&gt;Proposes a hierarchical monitoring system where probes act as efficient filters for potentially harmful outputs.&lt;/li&gt;&lt;li&gt;Releases a synthetic dataset and codebase to support further research in LLM safety monitoring.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research Paper&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Quality Assessment&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Clarity: 5/5&lt;/li&gt;&lt;li&gt;Novelty: 4/5&lt;/li&gt;&lt;li&gt;Significance: 4/5&lt;/li&gt;&lt;li&gt;Try-worthiness: Yes&lt;/li&gt;&lt;li&gt;Justification: The abstract is well-written and clearly explains the motivation, methodology, and contributions of the paper, including the release of a novel dataset and codebase. The use of activation probes for detecting high-stakes interactions in LLMs is a relatively novel and underexplored area, especially with a focus on computational efficiency and hierarchical monitoring. While the paper is very new and has no citations yet, the topic is highly significant for safe LLM deployment. The reported results—robust generalization and massive computational savings—make it worth trying in practice, especially given the availability of code and data.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Alex McKenzie', 'Urja Pawar', 'Phil Blandfort', 'William Bankes', 'David Krueger', 'Ekdeep Singh Lubana', 'Dmitrii Krasheninnikov']&lt;/li&gt;&lt;li&gt;Tags: ['LLM safety', 'harmful output detection', 'monitoring', 'AI safety', 'activation probes']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.10805</guid><pubDate>Fri, 13 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Can We Infer Confidential Properties of Training Data from LLMs?</title><link>https://arxiv.org/abs/2506.10364</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces PropInfer, a benchmark for evaluating property inference attacks on LLMs fine-tuned with sensitive data.&lt;/li&gt;&lt;li&gt;Demonstrates that confidential dataset-level properties (e.g., demographics, disease prevalence) can be inferred from LLMs.&lt;/li&gt;&lt;li&gt;Proposes two novel attack methods: prompt-based generation and shadow-model attacks.&lt;/li&gt;&lt;li&gt;Empirically evaluates attacks on multiple LLMs, revealing a new privacy vulnerability.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Quality Assessment&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Clarity: 5/5&lt;/li&gt;&lt;li&gt;Novelty: 5/5&lt;/li&gt;&lt;li&gt;Significance: 4/5&lt;/li&gt;&lt;li&gt;Try-worthiness: Yes&lt;/li&gt;&lt;li&gt;Justification: The paper is very clearly written, with a well-structured abstract that outlines the motivation, gap in prior work, methodology, and findings. The work is highly novel, as it addresses property inference attacks in LLMs—a topic not previously explored in depth, especially compared to discriminative and generative models. The introduction of a benchmark (PropInfer) and new attack methods further adds to its novelty. The significance is high given the increasing use of LLMs in sensitive domains, though as a recent preprint, it has not yet accumulated citations or peer review. The work is worth trying, especially for researchers interested in LLM security and privacy, as it reveals a new class of vulnerabilities and provides concrete attack strategies. No code repository is listed in the metadata.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Penguin Huang', 'Chhavi Yadav', 'Ruihan Wu', 'Kamalika Chaudhuri']&lt;/li&gt;&lt;li&gt;Tags: ['property inference', 'privacy attacks', 'LLM security', 'benchmarking', 'data confidentiality']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.10364</guid><pubDate>Fri, 13 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Interior-Point Vanishing Problem in Semidefinite Relaxations for Neural Network Verification</title><link>https://arxiv.org/abs/2506.10269</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Identifies and analyzes the 'interior-point vanishing' problem in semidefinite programming (SDP) relaxations for neural network verification.&lt;/li&gt;&lt;li&gt;Demonstrates that this issue leads to loss of strict feasibility, impacting the stability and scalability of SDP-based verification for deep neural networks.&lt;/li&gt;&lt;li&gt;Proposes and evaluates five solutions to improve feasibility, significantly increasing the number of solvable verification problems.&lt;/li&gt;&lt;li&gt;Contributes to the reliability and security of systems using deep neural networks by improving verification techniques.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research Paper&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Quality Assessment&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Clarity: 5/5&lt;/li&gt;&lt;li&gt;Novelty: 5/5&lt;/li&gt;&lt;li&gt;Significance: 4/5&lt;/li&gt;&lt;li&gt;Try-worthiness: Yes&lt;/li&gt;&lt;li&gt;Justification: The abstract is very well-written, clearly explaining the problem (interior-point vanishing in SDP relaxations for neural network verification), its significance, and the proposed solutions. The identification of a fundamental limitation in SDP-based verification for deep networks is novel and important, especially as SDPs are widely used for tight verification bounds. The paper proposes five new solutions and provides both theoretical and empirical analysis, which is a strong contribution. While the paper is very recent and has no citations yet, the topic is highly relevant for the safety and reliability of deep learning systems. The work is worth trying for researchers and practitioners in neural network verification, especially those using or developing SDP-based methods. No code repository is provided in the metadata or abstract.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Ryota Ueda', 'Takami Sato', 'Ken Kobayashi', 'Kazuhide Nakata']&lt;/li&gt;&lt;li&gt;Tags: ['neural network verification', 'robustness', 'AI safety', 'semidefinite programming', 'deep learning security']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.10269</guid><pubDate>Fri, 13 Jun 2025 00:00:00 +0000</pubDate></item><item><title>SALAD: Systematic Assessment of Machine Unlearing on LLM-Aided Hardware Design</title><link>https://arxiv.org/abs/2506.02089</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces SALAD, a framework for systematically assessing machine unlearning in LLMs used for hardware design.&lt;/li&gt;&lt;li&gt;Addresses data security risks such as evaluation data contamination, IP leakage, and malicious code generation in LLM-aided Verilog code generation.&lt;/li&gt;&lt;li&gt;Demonstrates how machine unlearning can selectively remove sensitive or contaminated data from pre-trained LLMs without full retraining.&lt;/li&gt;&lt;li&gt;Presents case studies showing the effectiveness of unlearning techniques in mitigating security threats in hardware design automation.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research Paper&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Quality Assessment&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Clarity: 4/5&lt;/li&gt;&lt;li&gt;Novelty: 5/5&lt;/li&gt;&lt;li&gt;Significance: 4/5&lt;/li&gt;&lt;li&gt;Try-worthiness: Yes&lt;/li&gt;&lt;li&gt;Justification: The paper is clearly written, with a well-defined problem statement and proposed solution. The focus on machine unlearning for LLM-aided hardware design, specifically targeting data security issues such as IP leakage and malicious code, is highly novel and timely. While the paper is a preprint and very recent (hence no citations yet), the topic is significant given the growing use of LLMs in hardware design automation and the associated security risks. The approach of selective unlearning without full retraining is particularly promising. The lack of a code repository is a drawback, but the paper's novelty and relevance make it worth experimenting with, especially for researchers in secure AI and hardware design.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Zeng Wang', 'Minghao Shao', 'Rupesh Karn', 'Likhitha Mankali', 'Jitendra Bhandari', 'Ramesh Karri', 'Ozgur Sinanoglu', 'Muhammad Shafique', 'Johann Knechtel']&lt;/li&gt;&lt;li&gt;Tags: ['machine unlearning', 'data security', 'LLM security', 'intellectual property leakage', 'malicious code mitigation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.02089</guid><pubDate>Fri, 13 Jun 2025 00:00:00 +0000</pubDate></item><item><title>MAYA: Addressing Inconsistencies in Generative Password Guessing through a Unified Benchmark</title><link>https://arxiv.org/abs/2504.16651</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces MAYA, a unified benchmarking framework for evaluating generative password-guessing models.&lt;/li&gt;&lt;li&gt;Assesses six state-of-the-art generative models on eight real-world password datasets under various attack scenarios.&lt;/li&gt;&lt;li&gt;Finds that sequential generative models outperform others in password guessing, especially for complex passwords.&lt;/li&gt;&lt;li&gt;Aims to standardize and improve the evaluation of password-guessing models, facilitating research in password security.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Quality Assessment&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Clarity: 5/5&lt;/li&gt;&lt;li&gt;Novelty: 4/5&lt;/li&gt;&lt;li&gt;Significance: 4/5&lt;/li&gt;&lt;li&gt;Try-worthiness: Yes&lt;/li&gt;&lt;li&gt;Justification: The abstract is well-written, clearly outlining the motivation, methodology, and contributions of the paper. The introduction of MAYA as a unified, customizable benchmarking framework for generative password-guessing models addresses a clear gap in the field, as previous work suffered from inconsistent evaluation. The novelty is strong due to the systematic re-implementation and benchmarking of six state-of-the-art models across diverse datasets and scenarios, as well as the introduction of a multi-model attack strategy. While the paper is a preprint and very recent (hence no citations yet), the significance is high given the practical importance of password security and the lack of standardized benchmarks in this area. The public release of the MAYA framework further increases its impact and try-worthiness, making it a valuable resource for researchers and practitioners. The code repository is provided, facilitating immediate experimentation.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['William Corrias', 'Fabio De Gaspari', 'Dorjan Hitaj', 'Luigi V. Mancini']&lt;/li&gt;&lt;li&gt;Tags: ['password security', 'generative models', 'benchmarking', 'adversarial attacks', 'security evaluation']&lt;/li&gt;&lt;li&gt;Code Repository: &lt;a href='https://github.com/williamcorrias/MAYA-Password-Benchmarking'&gt;https://github.com/williamcorrias/MAYA-Password-Benchmarking&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2504.16651</guid><pubDate>Fri, 13 Jun 2025 00:00:00 +0000</pubDate></item><item><title>EgoNormia: Benchmarking Physical Social Norm Understanding</title><link>https://arxiv.org/abs/2502.20490</link><description>&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Other&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['MohammadHossein Rezaei', 'Yicheng Fu', 'Phil Cuvin', 'Caleb Ziems', 'Yanzhe Zhang', 'Hao Zhu', 'Diyi Yang']&lt;/li&gt;&lt;li&gt;Tags: []&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2502.20490</guid><pubDate>Fri, 13 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Obliviate: Efficient Unmemorization for Protecting Intellectual Property in Large Language Models</title><link>https://arxiv.org/abs/2502.15010</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes Obliviate, a post-training method to prevent LLMs from reproducing copyrighted or sensitive text sequences.&lt;/li&gt;&lt;li&gt;Targets model memorization and verbatim leakage, addressing privacy and intellectual property concerns.&lt;/li&gt;&lt;li&gt;Demonstrates minimal impact on downstream model performance while significantly reducing verbatim recall.&lt;/li&gt;&lt;li&gt;Benchmarks Obliviate against other unlearning and copyright protection techniques, showing superior performance.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research Paper&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Quality Assessment&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Clarity: 5/5&lt;/li&gt;&lt;li&gt;Novelty: 5/5&lt;/li&gt;&lt;li&gt;Significance: 4/5&lt;/li&gt;&lt;li&gt;Try-worthiness: Yes&lt;/li&gt;&lt;li&gt;Justification: The abstract is very well-written, clearly outlining the motivation, method, and results. The approach—surgically suppressing memorized sequences post-training with minimal utility loss—appears novel and highly relevant given current concerns about copyright and LLMs. The evaluation is thorough, using both synthetic and real copyrighted data, and the results are impressive (large reduction in verbatim recall with minimal accuracy loss). While the paper is very new and on arXiv (so not peer-reviewed yet), the topic is timely and significant for both research and industry. The lack of citations is expected due to its recency. No code repository is listed in the metadata or abstract, so implementation may require contacting the authors or waiting for a release.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Mark Russinovich', 'Ahmed Salem']&lt;/li&gt;&lt;li&gt;Tags: ['model unlearning', 'privacy', 'intellectual property', 'verbatim leakage', 'LLM security']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2502.15010</guid><pubDate>Fri, 13 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Great Models Think Alike and this Undermines AI Oversight</title><link>https://arxiv.org/abs/2502.04313</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Investigates the risks of using language models to oversee or evaluate other language models (AI oversight).&lt;/li&gt;&lt;li&gt;Introduces a new metric (CAPA) to measure similarity in model mistakes, highlighting the risk of correlated failures.&lt;/li&gt;&lt;li&gt;Finds that as models become more capable, their mistakes become more similar, undermining the effectiveness of AI oversight.&lt;/li&gt;&lt;li&gt;Emphasizes the need for reporting and correcting for model similarity to ensure robust oversight and safety.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research Paper&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Quality Assessment&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Clarity: 4/5&lt;/li&gt;&lt;li&gt;Novelty: 5/5&lt;/li&gt;&lt;li&gt;Significance: 4/5&lt;/li&gt;&lt;li&gt;Try-worthiness: Yes&lt;/li&gt;&lt;li&gt;Justification: The abstract is well-written and clearly explains the motivation, methodology (CAPA metric), and key findings, though some technical terms may require background knowledge. The work is highly novel, introducing a new metric (CAPA) and providing new insights into the risks of model similarity in AI oversight, which is a timely and underexplored topic. The significance is high given the growing reliance on LLMs for evaluation and supervision, though the impact is yet to be seen due to the paper's recency and preprint status. The findings are actionable and relevant for researchers and practitioners working on AI evaluation and oversight, making the paper worth experimenting with. No code repository is provided in the metadata.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Shashwat Goel', 'Joschka Struber', 'Ilze Amanda Auzina', 'Karuna K Chandra', 'Ponnurangam Kumaraguru', 'Douwe Kiela', 'Ameya Prabhu', 'Matthias Bethge', 'Jonas Geiping']&lt;/li&gt;&lt;li&gt;Tags: ['AI oversight', 'model similarity', 'AI safety', 'correlated failures', 'evaluation risks']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2502.04313</guid><pubDate>Fri, 13 Jun 2025 00:00:00 +0000</pubDate></item><item><title>SoK: Watermarking for AI-Generated Content</title><link>https://arxiv.org/abs/2411.18479</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Provides a comprehensive overview of watermarking techniques for AI-generated content.&lt;/li&gt;&lt;li&gt;Discusses the security and robustness of watermarking schemes against various attacks and threat models.&lt;/li&gt;&lt;li&gt;Explores the role of watermarking in enhancing AI safety, trustworthiness, and combating misinformation.&lt;/li&gt;&lt;li&gt;Reviews practical evaluation strategies, regulatory perspectives, and open challenges in watermarking for GenAI.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Survey/Review&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Quality Assessment&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Clarity: 5/5&lt;/li&gt;&lt;li&gt;Novelty: 3/5&lt;/li&gt;&lt;li&gt;Significance: 4/5&lt;/li&gt;&lt;li&gt;Try-worthiness: No&lt;/li&gt;&lt;li&gt;Justification: The paper is very clearly written, with a well-structured abstract that outlines the motivation, scope, and contributions. As a Systematization of Knowledge (SoK) paper, its novelty lies in synthesizing and formalizing the state of the art rather than presenting new algorithms or techniques. The topic is highly significant given the rapid adoption of generative AI and the societal need for reliable content attribution. However, as a survey/SoK paper, it is not directly try-worthy for implementation, but it is valuable for understanding the landscape and guiding future research. No code repository is provided, which is typical for survey papers.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Xuandong Zhao', 'Sam Gunn', 'Miranda Christ', 'Jaiden Fairoze', 'Andres Fabrega', 'Nicholas Carlini', 'Sanjam Garg', 'Sanghyun Hong', 'Milad Nasr', 'Florian Tramer', 'Somesh Jha', 'Lei Li', 'Yu-Xiang Wang', 'Dawn Song']&lt;/li&gt;&lt;li&gt;Tags: ['watermarking', 'AI-generated content', 'robustness', 'misinformation', 'AI safety']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2411.18479</guid><pubDate>Fri, 13 Jun 2025 00:00:00 +0000</pubDate></item><item><title>DAWN: Designing Distributed Agents in a Worldwide Network</title><link>https://arxiv.org/abs/2410.22339</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Presents DAWN, a framework for distributed LLM-based agents to collaborate globally.&lt;/li&gt;&lt;li&gt;Integrates a dedicated safety, security, and compliance layer to protect agent interactions and the network.&lt;/li&gt;&lt;li&gt;Addresses security and compliance standards for agent-based applications in diverse industries.&lt;/li&gt;&lt;li&gt;Enables secure registration, discovery, and coordination of agents through specialized gateway and principal agents.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research Paper&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Quality Assessment&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Clarity: 4/5&lt;/li&gt;&lt;li&gt;Novelty: 4/5&lt;/li&gt;&lt;li&gt;Significance: 3/5&lt;/li&gt;&lt;li&gt;Try-worthiness: Yes&lt;/li&gt;&lt;li&gt;Justification: The abstract is well-written and clearly outlines the motivation, architecture, and features of DAWN, making it accessible to readers (clarity: 4). The idea of a global, distributed framework for LLM-based agents with integrated safety, security, and compliance layers is relatively novel, especially with the explicit support for multiple operational modes and agent discovery (novelty: 4). As a very recent arXiv preprint, it has no citations yet and is not peer-reviewed, which limits its immediate significance, but the topic is timely and relevant (significance: 3). The framework's described capabilities and modularity make it worth experimenting with, especially for those interested in agent-based systems and LLM integration (try_worthiness: true). No code repository is provided in the metadata or abstract.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Zahra Aminiranjbar', 'Jianan Tang', 'Qiudan Wang', 'Shubha Pant', 'Mahesh Viswanathan']&lt;/li&gt;&lt;li&gt;Tags: ['AI security', 'agent safety', 'compliance', 'distributed systems', 'LLM agents']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2410.22339</guid><pubDate>Fri, 13 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Hey, That's My Model! Introducing Chain &amp; Hash, An LLM Fingerprinting Technique</title><link>https://arxiv.org/abs/2407.10887</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a novel fingerprinting technique (Chain &amp; Hash) for LLMs to prove model ownership and detect misuse.&lt;/li&gt;&lt;li&gt;Defines key properties for effective fingerprinting: transparency, efficiency, persistence, robustness, and unforgeability.&lt;/li&gt;&lt;li&gt;Addresses adversarial scenarios, including attempts to erase fingerprints and output distribution changes via meta-prompts.&lt;/li&gt;&lt;li&gt;Demonstrates robustness of the technique against both benign (fine-tuning) and adversarial modifications, including applicability to LoRA adapters.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research Paper&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Quality Assessment&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Clarity: 4/5&lt;/li&gt;&lt;li&gt;Novelty: 5/5&lt;/li&gt;&lt;li&gt;Significance: 4/5&lt;/li&gt;&lt;li&gt;Try-worthiness: Yes&lt;/li&gt;&lt;li&gt;Justification: The abstract is well-written and clearly outlines the motivation, contributions, and results of the paper, though some technical details are dense. The proposed Chain and Hash technique for LLM fingerprinting appears novel, especially in its cryptographic binding of prompts and responses and its robustness to adversarial and benign transformations. The work is significant given the growing importance of model ownership and misuse detection, and it addresses realistic threat models. While the paper is very recent and has no citations yet, its relevance and the practical security problem it tackles make it worth experimenting with. No code repository is provided in the metadata.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Mark Russinovich', 'Ahmed Salem']&lt;/li&gt;&lt;li&gt;Tags: ['model fingerprinting', 'LLM security', 'model ownership', 'adversarial robustness', 'model misuse detection']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2407.10887</guid><pubDate>Fri, 13 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Securing Large Language Models: Threats, Vulnerabilities and Responsible Practices</title><link>https://arxiv.org/abs/2403.12503</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Comprehensively investigates security and privacy concerns specific to large language models (LLMs).&lt;/li&gt;&lt;li&gt;Analyzes vulnerabilities of LLMs to adversarial attacks and potential misuse.&lt;/li&gt;&lt;li&gt;Discusses mitigation strategies and their limitations for securing LLMs.&lt;/li&gt;&lt;li&gt;Provides recommendations for future research to improve LLM security and risk management.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Survey/Review&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Quality Assessment&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Clarity: 4/5&lt;/li&gt;&lt;li&gt;Novelty: 3/5&lt;/li&gt;&lt;li&gt;Significance: 4/5&lt;/li&gt;&lt;li&gt;Try-worthiness: No&lt;/li&gt;&lt;li&gt;Justification: The abstract is well-written and clearly outlines the scope and structure of the paper, earning a high clarity score. However, the work appears to be a comprehensive survey or review of existing threats, vulnerabilities, and mitigation strategies for LLMs, rather than presenting new methods or empirical results, so the novelty is moderate. The significance is relatively high given the importance and timeliness of LLM security, but as a survey, its impact is more about synthesis than breakthrough. Since the paper does not propose new algorithms or systems, but rather reviews and recommends, it is not directly try-worthy for implementation or experimentation. No code repository is provided.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Sara Abdali', 'Richard Anarfi', 'CJ Barberan', 'Jia He', 'Erfan Shayegani']&lt;/li&gt;&lt;li&gt;Tags: ['LLM security', 'adversarial attacks', 'privacy', 'risk mitigation', 'responsible AI']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2403.12503</guid><pubDate>Fri, 13 Jun 2025 00:00:00 +0000</pubDate></item><item><title>IoTGeM: Generalizable Models for Behaviour-Based IoT Attack Detection</title><link>https://arxiv.org/abs/2401.01343</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes IoTGeM, a generalizable machine learning approach for detecting attacks in IoT networks.&lt;/li&gt;&lt;li&gt;Introduces improved feature extraction and selection methods to enhance model robustness and prevent overfitting.&lt;/li&gt;&lt;li&gt;Emphasizes rigorous evaluation using isolated datasets to avoid data leakage and ensure generalization to unseen attacks.&lt;/li&gt;&lt;li&gt;Utilizes explainable AI (SHAP) to interpret model decisions and build trust in attack detection.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research Paper&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Quality Assessment&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Clarity: 5/5&lt;/li&gt;&lt;li&gt;Novelty: 4/5&lt;/li&gt;&lt;li&gt;Significance: 3/5&lt;/li&gt;&lt;li&gt;Try-worthiness: Yes&lt;/li&gt;&lt;li&gt;Justification: The abstract is well-written, clearly outlining the motivation, methodology, and results of the work. The use of a rolling window for feature extraction, a genetic algorithm guided by exogenous feedback for feature selection, and strict train/test isolation are notable and suggest a thoughtful approach to generalizability in IoT attack detection. The use of SHAP for explainability is also a strong point. While the approach appears novel, especially in its focus on generalizability and rigorous evaluation, the venue (arXiv) and lack of citations (expected for a new preprint) limit the current significance. However, the strong reported results and methodological rigor make this paper worth trying for practitioners or researchers in IoT security.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Kahraman Kostas', 'Mike Just', 'Michael A. Lones']&lt;/li&gt;&lt;li&gt;Tags: ['IoT security', 'attack detection', 'machine learning robustness', 'explainable AI', 'network security']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2401.01343</guid><pubDate>Fri, 13 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Don't Lag, RAG: Training-Free Adversarial Detection Using RAG</title><link>https://arxiv.org/abs/2504.04858</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a training-free framework (VRAG) for detecting adversarial patch attacks in vision systems using Vision-Language Models (VLMs).&lt;/li&gt;&lt;li&gt;Utilizes retrieval-augmented generation to identify adversarial patches by comparing with a database of known attacks.&lt;/li&gt;&lt;li&gt;Demonstrates state-of-the-art performance on adversarial patch detection without requiring retraining or fine-tuning.&lt;/li&gt;&lt;li&gt;Evaluates both open-source and closed-source VLMs, showing high accuracy in detecting diverse adversarial attacks.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research Paper&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Quality Assessment&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Clarity: 5/5&lt;/li&gt;&lt;li&gt;Novelty: 4/5&lt;/li&gt;&lt;li&gt;Significance: 4/5&lt;/li&gt;&lt;li&gt;Try-worthiness: Yes&lt;/li&gt;&lt;li&gt;Justification: The abstract is well-written, clearly outlining the problem (adversarial patch attacks), the limitations of existing methods (need for retraining), and the proposed solution (a training-free, retrieval-augmented VLM-based framework). The approach of using retrieval-augmented generation with vision-language models for adversarial detection is novel, especially in a training-free context. The reported results are strong, with state-of-the-art performance on open-source models and competitive results with closed-source models. The significance is high given the practical importance of adversarial robustness and the use of large, modern VLMs. The paper is worth trying, especially since it claims minimal annotation and no retraining, making it practical for real-world use. However, no code repository is provided, which may limit immediate experimentation.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Roie Kazoom', 'Raz Lapid', 'Moshe Sipper', 'Ofer Hadar']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial attacks', 'adversarial detection', 'vision-language models', 'robustness', 'AI security']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2504.04858</guid><pubDate>Fri, 13 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Training-Free Safe Denoisers for Safe Use of Diffusion Models</title><link>https://arxiv.org/abs/2502.08011</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a novel, training-free method to prevent diffusion models from generating unsafe or unwanted content (e.g., NSFW, copyrighted, or privacy-sensitive data).&lt;/li&gt;&lt;li&gt;Directly modifies the sampling process using a 'negation set' to avoid specific regions of the data distribution, rather than relying on prompts or retraining.&lt;/li&gt;&lt;li&gt;Formally derives and implements a 'safe denoiser' algorithm that ensures generated samples are distant from unsafe or excluded data.&lt;/li&gt;&lt;li&gt;Demonstrates effectiveness in various image generation scenarios, improving the safety of diffusion model outputs.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research Paper&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Quality Assessment&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Clarity: 4/5&lt;/li&gt;&lt;li&gt;Novelty: 5/5&lt;/li&gt;&lt;li&gt;Significance: 4/5&lt;/li&gt;&lt;li&gt;Try-worthiness: Yes&lt;/li&gt;&lt;li&gt;Justification: The abstract is well-written and clearly explains the motivation, approach, and contributions of the paper, though some technical details are dense. The work is highly novel, proposing a training-free method for safe denoising in diffusion models, which is a significant departure from retraining or prompt-based approaches. The significance is high given the growing concerns about the misuse of diffusion models, though as a recent preprint, it has not yet accumulated citations or peer-reviewed validation. The method's practicality and broad applicability (text-conditional, class-conditional, unconditional) make it worth experimenting with, especially since it does not require retraining. No code repository is provided in the metadata.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Mingyu Kim', 'Dongjun Kim', 'Amman Yusuf', 'Stefano Ermon', 'Mijung Park']&lt;/li&gt;&lt;li&gt;Tags: ['diffusion models', 'safe generation', 'content filtering', 'AI safety', 'image generation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2502.08011</guid><pubDate>Fri, 13 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Monitoring Decomposition Attacks in LLMs with Lightweight Sequential Monitors</title><link>https://arxiv.org/abs/2506.10949</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Identifies and analyzes decomposition attacks on LLMs, where malicious goals are split into benign subtasks to evade safety mechanisms.&lt;/li&gt;&lt;li&gt;Curates a large, diverse dataset for evaluating decomposition attacks across question-answering, text-to-image, and agentic tasks.&lt;/li&gt;&lt;li&gt;Proposes a lightweight sequential monitoring framework that evaluates subtasks cumulatively to detect and mitigate decomposition attacks in real time.&lt;/li&gt;&lt;li&gt;Demonstrates that the proposed monitor outperforms existing reasoning models in defense success rate, cost, and latency.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research Paper&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Quality Assessment&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Clarity: 5/5&lt;/li&gt;&lt;li&gt;Novelty: 5/5&lt;/li&gt;&lt;li&gt;Significance: 4/5&lt;/li&gt;&lt;li&gt;Try-worthiness: Yes&lt;/li&gt;&lt;li&gt;Justification: The abstract is well-written, clearly outlining the problem (decomposition attacks on LLMs), the limitations of current defenses, and the proposed solution (lightweight sequential monitors). The work is novel, addressing a recently recognized and underexplored attack vector with a new defense mechanism and a large, diverse dataset. The significance is high given the prevalence of LLM safety concerns, though the paper is very new and only on arXiv, so broader impact and peer review are pending. The approach is practical, with strong reported results (93% defense success, cost and latency reductions), making it worth trying for those working on LLM safety. No code repository is provided in the metadata or abstract.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Chen Yueh-Han', 'Nitish Joshi', 'Yulin Chen', 'Maksym Andriushchenko', 'Rico Angell', 'He He']&lt;/li&gt;&lt;li&gt;Tags: ['LLM security', 'decomposition attacks', 'sequential monitoring', 'adversarial prompting', 'AI safety']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.10949</guid><pubDate>Fri, 13 Jun 2025 00:00:00 +0000</pubDate></item><item><title>GUARD: Guided Unlearning and Retention via Data Attribution for Large Language Models</title><link>https://arxiv.org/abs/2506.10946</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes GUARD, a framework for guided unlearning in large language models (LLMs) to address regulatory, copyright, and privacy concerns.&lt;/li&gt;&lt;li&gt;Introduces a data attribution metric to balance unlearning specific data while retaining valuable information, mitigating unintended forgetting.&lt;/li&gt;&lt;li&gt;Provides theoretical guarantees and empirical results showing improved retention and effective unlearning compared to prior methods.&lt;/li&gt;&lt;li&gt;Addresses compliance and privacy-driven data removal, which is directly relevant to AI security and safety.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research Paper&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Quality Assessment&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Clarity: 4/5&lt;/li&gt;&lt;li&gt;Novelty: 5/5&lt;/li&gt;&lt;li&gt;Significance: 4/5&lt;/li&gt;&lt;li&gt;Try-worthiness: Yes&lt;/li&gt;&lt;li&gt;Justification: The abstract is well-written and clearly explains the motivation, problem, and proposed solution (GUARD) for unlearning in LLMs, though some technical terms may require background knowledge. The work is highly novel, introducing a new data attribution-based approach to guide unlearning, which is a less-explored direction compared to architectural changes. The significance is high given the growing importance of unlearning for compliance and privacy, and the reported improvements on the TOFU benchmark suggest practical impact, though the paper is very new and not yet peer-reviewed or cited. The method appears promising and worth experimenting with, especially for researchers or practitioners concerned with data removal in LLMs. No code repository is provided in the metadata.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Evelyn Ma', 'Duo Zhou', 'Peizhi Niu', 'Huiting Zhou', 'Huan Zhang', 'Olgica Milenkovic', 'S. Rasoul Etesami']&lt;/li&gt;&lt;li&gt;Tags: ['unlearning', 'privacy', 'regulatory compliance', 'LLM security', 'data attribution']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.10946</guid><pubDate>Fri, 13 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Efficiency Robustness of Dynamic Deep Learning Systems</title><link>https://arxiv.org/abs/2506.10831</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Systematically explores the security vulnerabilities of Dynamic Deep Learning Systems (DDLSs) related to efficiency robustness.&lt;/li&gt;&lt;li&gt;Presents a comprehensive taxonomy of efficiency adversarial attacks targeting dynamic behaviors in DDLSs.&lt;/li&gt;&lt;li&gt;Analyzes adversarial strategies that degrade system performance by exploiting dynamic computation, inference iterations, and output production.&lt;/li&gt;&lt;li&gt;Evaluates existing defense mechanisms and highlights the need for novel mitigation strategies to secure adaptive DDLSs.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Survey/Review&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Quality Assessment&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Clarity: 4/5&lt;/li&gt;&lt;li&gt;Novelty: 5/5&lt;/li&gt;&lt;li&gt;Significance: 4/5&lt;/li&gt;&lt;li&gt;Try-worthiness: Yes&lt;/li&gt;&lt;li&gt;Justification: The abstract is well-written and clearly outlines the motivation, contributions, and findings of the paper, though some technical terms may require background knowledge. The work is highly novel, presenting the first comprehensive taxonomy of efficiency attacks on Dynamic Deep Learning Systems (DDLSs), a topic of growing importance as adaptive inference becomes more common. The significance is high due to the increasing deployment of DDLSs in real-world, resource-constrained environments, and the lack of prior systematic study on efficiency robustness. While the paper is very recent and has no citations yet, its relevance and the gap it addresses make it worth exploring and potentially implementing, especially for researchers or practitioners concerned with the security and robustness of adaptive AI systems. No code repository is provided.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Ravishka Rathnasuriya', 'Tingxi Li', 'Zexin Xu', 'Zihe Song', 'Mirazul Haque', 'Simin Chen', 'Wei Yang']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial attacks', 'deep learning security', 'efficiency robustness', 'dynamic systems', 'defense mechanisms']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.10831</guid><pubDate>Fri, 13 Jun 2025 00:00:00 +0000</pubDate></item><item><title>ME: Trigger Element Combination Backdoor Attack on Copyright Infringement</title><link>https://arxiv.org/abs/2506.10776</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a new backdoor attack method (Multi-Element attack) targeting generative diffusion models like Stable Diffusion.&lt;/li&gt;&lt;li&gt;Focuses on copyright infringement attacks by injecting poisoned image-text pairs to cause models to replicate copyrighted content.&lt;/li&gt;&lt;li&gt;Introduces new datasets for evaluating such attacks and demonstrates improved attack effectiveness and stealthiness using Discrete Cosine Transform.&lt;/li&gt;&lt;li&gt;Benchmarks the proposed method against existing attacks, showing superior performance in low-data poisoning scenarios.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research Paper&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Quality Assessment&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Clarity: 3/5&lt;/li&gt;&lt;li&gt;Novelty: 4/5&lt;/li&gt;&lt;li&gt;Significance: 3/5&lt;/li&gt;&lt;li&gt;Try-worthiness: Yes&lt;/li&gt;&lt;li&gt;Justification: The abstract is somewhat dense and contains grammatical errors, which affects clarity (score 3). However, the paper proposes a novel Multi-Element (ME) attack method for backdooring generative diffusion models, building on and improving over SilentBadDiffusion (SBD), and introduces new datasets for this line of research, which is a notable contribution (novelty 4). The significance is moderate (score 3) as the topic is timely and relevant to the security of generative models, but the impact is not yet established due to the paper's recency and preprint status. The results suggest measurable improvements over prior work, making it worth trying for researchers interested in model security and copyright attacks. No code repository is provided in the metadata.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Feiyu Yang', 'Siyuan Liang', 'Aishan Liu', 'Dacheng Tao']&lt;/li&gt;&lt;li&gt;Tags: ['backdoor attacks', 'data poisoning', 'generative models', 'copyright infringement', 'diffusion models']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.10776</guid><pubDate>Fri, 13 Jun 2025 00:00:00 +0000</pubDate></item><item><title>TED-LaST: Towards Robust Backdoor Defense Against Adaptive Attacks</title><link>https://arxiv.org/abs/2506.10722</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes TED-LaST, a novel defense strategy to enhance robustness against adaptive backdoor attacks in deep neural networks.&lt;/li&gt;&lt;li&gt;Introduces label-supervised dynamics tracking and adaptive layer emphasis to improve detection of stealthy backdoors.&lt;/li&gt;&lt;li&gt;Reviews and classifies data poisoning techniques in adaptive attacks and proposes an enhanced adaptive attack with target mapping.&lt;/li&gt;&lt;li&gt;Demonstrates effectiveness of TED-LaST against advanced backdoor attacks across multiple datasets and architectures.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research Paper&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Quality Assessment&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Clarity: 4/5&lt;/li&gt;&lt;li&gt;Novelty: 4/5&lt;/li&gt;&lt;li&gt;Significance: 4/5&lt;/li&gt;&lt;li&gt;Try-worthiness: Yes&lt;/li&gt;&lt;li&gt;Justification: The abstract is well-written and clearly explains the motivation, contributions, and results of the paper, though some technical terms (e.g., 'topological evolution dynamics') may require prior knowledge. The work appears novel, introducing TED-LaST with new mechanisms (label-supervised dynamics tracking and adaptive layer emphasis) to address adaptive backdoor attacks, which is a current and challenging problem in DNN security. The significance is high given the comprehensive evaluation on multiple datasets and architectures, and the focus on adaptive attacks, which are a major concern in the field. Although the paper is very recent and has no citations yet, its relevance and the thoroughness of the experiments make it worth trying for researchers and practitioners interested in robust backdoor defenses. No code repository is provided in the metadata.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Xiaoxing Mo', 'Yuxuan Cheng', 'Nan Sun', 'Leo Yu Zhang', 'Wei Luo', 'Shang Gao']&lt;/li&gt;&lt;li&gt;Tags: ['backdoor attacks', 'data poisoning', 'adversarial defense', 'deep neural networks', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.10722</guid><pubDate>Fri, 13 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Data Shifts Hurt CoT: A Theoretical Study</title><link>https://arxiv.org/abs/2506.10647</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;The paper theoretically analyzes the impact of data shifts, including distribution shifts and data poisoning, on Chain of Thought (CoT) reasoning in large language models.&lt;/li&gt;&lt;li&gt;It rigorously studies how these shifts degrade model performance, particularly on the k-parity problem.&lt;/li&gt;&lt;li&gt;The work provides mechanistic explanations for why CoT can be more vulnerable to such shifts compared to direct prediction.&lt;/li&gt;&lt;li&gt;It highlights the risks of relying on idealized assumptions (identical distributions, clean data) in real-world AI deployments.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research Paper&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Quality Assessment&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Clarity: 4/5&lt;/li&gt;&lt;li&gt;Novelty: 5/5&lt;/li&gt;&lt;li&gt;Significance: 4/5&lt;/li&gt;&lt;li&gt;Try-worthiness: Yes&lt;/li&gt;&lt;li&gt;Justification: The abstract is well-written and clearly outlines the motivation, problem, and contributions of the paper, though some technical terms may require background knowledge. The work is highly novel, being the first to rigorously analyze the impact of data shifts (distribution shift and data poisoning) on Chain of Thought (CoT) reasoning in transformers, especially for the k-parity problem. The significance is high due to the foundational nature of the findings and the relevance to real-world robustness of LLMs, though the impact is yet to be seen given its recency and preprint status. The surprising result that CoT can hurt performance under data shifts is important for practitioners and theorists, making the paper worth further exploration and experimentation. No code repository is provided.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Lang Yin', 'Debangshu Banerjee', 'Gagandeep Singh']&lt;/li&gt;&lt;li&gt;Tags: ['data poisoning', 'distribution shift', 'robustness', 'AI safety', 'Chain of Thought']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.10647</guid><pubDate>Fri, 13 Jun 2025 00:00:00 +0000</pubDate></item><item><title>SoK: Evaluating Jailbreak Guardrails for Large Language Models</title><link>https://arxiv.org/abs/2506.10597</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Presents a comprehensive systematization of knowledge (SoK) on jailbreak guardrails for Large Language Models (LLMs).&lt;/li&gt;&lt;li&gt;Proposes a novel taxonomy categorizing guardrails along six key dimensions.&lt;/li&gt;&lt;li&gt;Introduces a Security-Efficiency-Utility evaluation framework for assessing guardrail effectiveness.&lt;/li&gt;&lt;li&gt;Analyzes strengths, limitations, and universality of current guardrail approaches against jailbreak attacks.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Survey/Review&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Quality Assessment&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Clarity: 5/5&lt;/li&gt;&lt;li&gt;Novelty: 4/5&lt;/li&gt;&lt;li&gt;Significance: 4/5&lt;/li&gt;&lt;li&gt;Try-worthiness: Yes&lt;/li&gt;&lt;li&gt;Justification: The paper is very clearly written, with a well-structured abstract that outlines the motivation, contributions, and methodology. It introduces a novel taxonomy and evaluation framework for LLM guardrails, which is a timely and important topic given the rise of jailbreak attacks. While systematization papers are not always groundbreaking in terms of new algorithms, this work fills a clear gap by providing the first holistic analysis and structured foundation for future research. Its significance is high due to the relevance of LLM safety and the lack of unified frameworks in this area. The presence of a code repository further increases its practical value and try-worthiness for researchers and practitioners interested in LLM security.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Xunguang Wang', 'Zhenlan Ji', 'Wenxuan Wang', 'Zongjie Li', 'Daoyuan Wu', 'Shuai Wang']&lt;/li&gt;&lt;li&gt;Tags: ['LLM security', 'jailbreaking', 'guardrails', 'adversarial prompting', 'safety evaluation']&lt;/li&gt;&lt;li&gt;Code Repository: &lt;a href='https://github.com/xunguangwang/SoK4JailbreakGuardrails'&gt;https://github.com/xunguangwang/SoK4JailbreakGuardrails&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.10597</guid><pubDate>Fri, 13 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Specification and Evaluation of Multi-Agent LLM Systems -- Prototype and Cybersecurity Applications</title><link>https://arxiv.org/abs/2506.10467</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a specification and evaluation framework for multi-agent LLM systems.&lt;/li&gt;&lt;li&gt;Demonstrates the application of multi-agent LLMs to cybersecurity tasks, including server and network security.&lt;/li&gt;&lt;li&gt;Presents a prototype system and evaluates its performance on cybersecurity-related question answering and task completion.&lt;/li&gt;&lt;li&gt;Highlights the feasibility of using LLM-based agents for complex reasoning and security applications.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research Paper&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Quality Assessment&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Clarity: 4/5&lt;/li&gt;&lt;li&gt;Novelty: 4/5&lt;/li&gt;&lt;li&gt;Significance: 3/5&lt;/li&gt;&lt;li&gt;Try-worthiness: Yes&lt;/li&gt;&lt;li&gt;Justification: The abstract is well-written and clearly outlines the motivation, approach, and results of the paper, though some sentences are dense. The work is novel in its focus on the joint specification and evaluation of multi-agent LLM systems, especially for cybersecurity applications, which is not widely explored. The significance is moderate: while the topic is timely and relevant, the paper is a preprint on arXiv with no citations yet, and its impact is not yet established. The prototype and evaluation in cybersecurity tasks suggest practical value and make it worth trying for researchers or practitioners interested in multi-agent LLM systems. No code repository is provided in the metadata.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Felix H\\"arer']&lt;/li&gt;&lt;li&gt;Tags: ['multi-agent LLMs', 'cybersecurity', 'AI security evaluation', 'LLM applications', 'system specification']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.10467</guid><pubDate>Fri, 13 Jun 2025 00:00:00 +0000</pubDate></item><item><title>SOFT: Selective Data Obfuscation for Protecting LLM Fine-tuning against Membership Inference Attacks</title><link>https://arxiv.org/abs/2506.10424</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Presents a comprehensive study on the vulnerability of fine-tuned LLMs to membership inference attacks (MIAs), a key privacy threat.&lt;/li&gt;&lt;li&gt;Demonstrates empirically how MIAs exploit loss reduction during fine-tuning to reveal membership information.&lt;/li&gt;&lt;li&gt;Introduces SOFT, a selective data obfuscation technique to defend against MIAs in LLM fine-tuning.&lt;/li&gt;&lt;li&gt;Shows through experiments that SOFT reduces privacy risks while preserving model utility across multiple domains and LLM architectures.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research Paper&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Quality Assessment&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Clarity: 4/5&lt;/li&gt;&lt;li&gt;Novelty: 5/5&lt;/li&gt;&lt;li&gt;Significance: 4/5&lt;/li&gt;&lt;li&gt;Try-worthiness: Yes&lt;/li&gt;&lt;li&gt;Justification: The abstract is well-written and clearly explains the motivation, methodology, and results of the paper, though some technical details are omitted (clarity: 4). The work claims to be the first comprehensive study of membership inference attacks (MIAs) on fine-tuned LLMs and introduces a novel defense (SOFT), which appears to be a new contribution (novelty: 5). The significance is high due to the relevance of privacy in LLM fine-tuning, but as an arXiv preprint with no citations yet, it is not yet fully validated by the community (significance: 4). The practical focus, extensive experiments, and the importance of the problem make it worth trying for researchers and practitioners (try_worthiness: true). No code repository is provided in the metadata.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Kaiyuan Zhang', 'Siyuan Cheng', 'Hanxi Guo', 'Yuetian Chen', 'Zian Su', 'Shengwei An', 'Yuntao Du', 'Charles Fleming', 'Ashish Kundu', 'Xiangyu Zhang', 'Ninghui Li']&lt;/li&gt;&lt;li&gt;Tags: ['membership inference', 'privacy attacks', 'LLM security', 'defense mechanisms', 'fine-tuning']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.10424</guid><pubDate>Fri, 13 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Prompt Attacks Reveal Superficial Knowledge Removal in Unlearning Methods</title><link>https://arxiv.org/abs/2506.10236</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Demonstrates that prompt attacks can recover supposedly unlearned knowledge from models subjected to machine unlearning methods.&lt;/li&gt;&lt;li&gt;Systematically evaluates eight unlearning techniques across three model families using output-based, logit-based, and probe analysis.&lt;/li&gt;&lt;li&gt;Finds that some unlearning methods (e.g., ELM) are vulnerable to prompt attacks, revealing superficial knowledge removal.&lt;/li&gt;&lt;li&gt;Proposes an evaluation framework for reliably testing the effectiveness of unlearning and distinguishing true knowledge removal from output suppression.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research Paper&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Quality Assessment&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Clarity: 4/5&lt;/li&gt;&lt;li&gt;Novelty: 4/5&lt;/li&gt;&lt;li&gt;Significance: 4/5&lt;/li&gt;&lt;li&gt;Try-worthiness: Yes&lt;/li&gt;&lt;li&gt;Justification: The paper is clearly written, with a well-structured abstract that outlines the motivation, methodology, and key findings. The work is novel in systematically evaluating prompt attacks against multiple unlearning methods and revealing vulnerabilities, which is a timely and underexplored area. Its significance is high given the growing importance of machine unlearning for privacy and compliance, and the results challenge current assumptions in the field. The public release of an evaluation framework further increases its practical value. Although it is a preprint and very recent (hence no citations yet), the topic and findings make it worth experimenting with, especially for researchers or practitioners concerned with model unlearning robustness.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yeonwoo Jang', 'Shariqah Hossain', 'Ashwin Sreevatsa', 'Diogo Cruz']&lt;/li&gt;&lt;li&gt;Tags: ['prompt attacks', 'machine unlearning', 'model robustness', 'AI security', 'evaluation framework']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.10236</guid><pubDate>Fri, 13 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Disclosure Audits for LLM Agents</title><link>https://arxiv.org/abs/2506.10171</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes an auditing framework (CMPL) to assess privacy risks in LLM agents through iterative, multi-turn probing.&lt;/li&gt;&lt;li&gt;Focuses on conversational privacy leakage and the ability of LLM agents to inadvertently disclose sensitive information.&lt;/li&gt;&lt;li&gt;Introduces quantifiable risk metrics and an open benchmark for evaluating privacy vulnerabilities in conversational agents.&lt;/li&gt;&lt;li&gt;Demonstrates that multi-turn interactions can reveal privacy risks not caught by single-turn defenses.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research Paper&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Quality Assessment&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Clarity: 5/5&lt;/li&gt;&lt;li&gt;Novelty: 4/5&lt;/li&gt;&lt;li&gt;Significance: 4/5&lt;/li&gt;&lt;li&gt;Try-worthiness: Yes&lt;/li&gt;&lt;li&gt;Justification: The abstract is well-written and clearly explains the motivation, methodology, and contributions of the paper, earning a high clarity score. The proposed CMPL framework for multi-turn privacy auditing in LLM agents appears novel, as most prior work focuses on single-turn disclosures. The significance is high given the growing deployment of LLM agents in sensitive domains, though the impact is yet to be seen due to the paper's recency and preprint status. The introduction of an open benchmark and quantifiable risk metrics further increases its potential value. Despite being a preprint with no citations yet, the topic and approach make it worth experimenting with, especially for those concerned with LLM privacy and safety.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Saswat Das', 'Jameson Sandler', 'Ferdinando Fioretto']&lt;/li&gt;&lt;li&gt;Tags: ['privacy attacks', 'LLM red teaming', 'security auditing', 'risk assessment', 'benchmarking']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.10171</guid><pubDate>Fri, 13 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Safeguarding Multimodal Knowledge Copyright in the RAG-as-a-Service Environment</title><link>https://arxiv.org/abs/2506.10030</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes AQUA, a watermarking framework for protecting image knowledge copyright in Multimodal Retrieval-Augmented Generation (RAG) systems.&lt;/li&gt;&lt;li&gt;Introduces semantic watermarking techniques (acronym-based triggers and spatial relationship cues) for synthetic images.&lt;/li&gt;&lt;li&gt;Demonstrates that the watermark signals are robust, stealthy, and survive propagation through RAG pipelines.&lt;/li&gt;&lt;li&gt;Addresses a key gap in copyright protection for multimodal (text and image) knowledge bases in RAG-as-a-Service environments.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research Paper&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Quality Assessment&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Clarity: 4/5&lt;/li&gt;&lt;li&gt;Novelty: 5/5&lt;/li&gt;&lt;li&gt;Significance: 4/5&lt;/li&gt;&lt;li&gt;Try-worthiness: Yes&lt;/li&gt;&lt;li&gt;Justification: The paper is clearly written, with a well-structured abstract that explains the motivation, problem, and proposed solution (AQUA) for watermarking image knowledge in multimodal RAG systems. The novelty is high, as it claims to be the first framework addressing image watermarking in this context, using innovative semantic embedding techniques. The significance is strong given the growing importance of copyright protection in RAG-as-a-Service environments, though the impact is yet to be seen due to its recent publication and preprint status. The approach appears practical and addresses a real gap, making it worth experimenting with. No code repository is provided in the metadata.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Tianyu Chen', 'Jian Lou', 'Wenjie Wang']&lt;/li&gt;&lt;li&gt;Tags: ['watermarking', 'copyright protection', 'multimodal security', 'RAG', 'image security']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.10030</guid><pubDate>Fri, 13 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Evaluation empirique de la s\'ecurisation et de l'alignement de ChatGPT et Gemini: analyse comparative des vuln\'erabilit\'es par exp\'erimentations de jailbreaks</title><link>https://arxiv.org/abs/2506.10029</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Empirically evaluates the security and alignment of ChatGPT and Gemini.&lt;/li&gt;&lt;li&gt;Conducts comparative experiments focused on jailbreak vulnerabilities.&lt;/li&gt;&lt;li&gt;Presents a taxonomy of jailbreak techniques targeting LLMs.&lt;/li&gt;&lt;li&gt;Discusses cybersecurity challenges such as prompt injection and circumvention of safeguards.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research Paper&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Quality Assessment&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Clarity: 4/5&lt;/li&gt;&lt;li&gt;Novelty: 4/5&lt;/li&gt;&lt;li&gt;Significance: 4/5&lt;/li&gt;&lt;li&gt;Try-worthiness: Yes&lt;/li&gt;&lt;li&gt;Justification: The paper is clearly written, with a well-defined focus on the empirical evaluation of security and alignment in ChatGPT and Gemini, two leading LLMs. The comparative analysis and taxonomy of jailbreak techniques are timely and relevant, given the rapid deployment of LLMs and the ongoing concerns about their vulnerabilities. While the topic of LLM security is not entirely new, direct empirical comparisons between major models and systematic experimentation with jailbreaks are still relatively rare, lending novelty to the work. The significance is high due to the practical implications for AI safety and the widespread use of these models. The paper is worth experimenting with, especially for researchers or practitioners interested in LLM security and alignment. No code repository is provided.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Rafa\\"el Nouailles (GdR)']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'jailbreaking', 'prompt injection', 'AI security', 'alignment']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.10029</guid><pubDate>Fri, 13 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Private Memorization Editing: Turning Memorization into a Defense to Strengthen Data Privacy in Large Language Models</title><link>https://arxiv.org/abs/2506.10024</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces Private Memorization Editing (PME), a method to mitigate memorization of Personally Identifiable Information (PII) in Large Language Models (LLMs).&lt;/li&gt;&lt;li&gt;PME detects and edits memorized PII in LLMs to prevent private data leakage.&lt;/li&gt;&lt;li&gt;Demonstrates that PME increases robustness against privacy Training Data Extraction attacks without degrading model performance.&lt;/li&gt;&lt;li&gt;Shows empirical results where PME significantly reduces or eliminates successful privacy attacks.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research Paper&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Quality Assessment&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Clarity: 4/5&lt;/li&gt;&lt;li&gt;Novelty: 4/5&lt;/li&gt;&lt;li&gt;Significance: 4/5&lt;/li&gt;&lt;li&gt;Try-worthiness: Yes&lt;/li&gt;&lt;li&gt;Justification: The paper is clearly written, with a well-defined problem statement and a concise description of the proposed solution (Private Memorization Editing, PME). The approach is novel in that it leverages the memorization property of LLMs as a defense mechanism, rather than simply treating it as a vulnerability. The significance is high given the increasing concern over privacy and PII leakage in LLMs, and the abstract claims strong results (even reducing attack accuracy to zero in some cases). While the paper is very recent and has no citations yet, the topic is timely and relevant. The lack of a code repository is a minor drawback, but the idea appears promising and worth experimenting with.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Elena Sofia Ruzzetti', 'Giancarlo A. Xompero', 'Davide Venditti', 'Fabio Massimo Zanzotto']&lt;/li&gt;&lt;li&gt;Tags: ['privacy attacks', 'training data extraction', 'LLM security', 'data privacy', 'defense mechanisms']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.10024</guid><pubDate>Fri, 13 Jun 2025 00:00:00 +0000</pubDate></item><item><title>LLMs Caught in the Crossfire: Malware Requests and Jailbreak Challenges</title><link>https://arxiv.org/abs/2506.10022</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces MalwareBench, a benchmark dataset of 3,520 jailbreaking prompts for malicious code generation.&lt;/li&gt;&lt;li&gt;Evaluates the robustness of mainstream LLMs against jailbreak attacks specifically targeting code generation.&lt;/li&gt;&lt;li&gt;Finds that LLMs have limited ability to reject malicious code-generation requests, especially when multiple jailbreak methods are combined.&lt;/li&gt;&lt;li&gt;Highlights ongoing security challenges in preventing LLMs from generating harmful code.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Quality Assessment&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Clarity: 5/5&lt;/li&gt;&lt;li&gt;Novelty: 4/5&lt;/li&gt;&lt;li&gt;Significance: 4/5&lt;/li&gt;&lt;li&gt;Try-worthiness: Yes&lt;/li&gt;&lt;li&gt;Justification: The abstract is well-written, clearly outlining the motivation, methodology, and findings. The introduction of MalwareBench, a benchmark specifically for jailbreaking prompts in malicious code generation, addresses a gap in current LLM security research, making the work novel. The significance is high given the growing concern over LLM misuse, though the impact is yet to be measured due to the paper's recency and preprint status. The benchmark and findings are likely valuable for researchers and practitioners working on LLM safety, making the paper worth experimenting with. No code repository is provided in the metadata.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Haoyang Li', 'Huan Gao', 'Zhiyuan Zhao', 'Zhiyu Lin', 'Junyu Gao', 'Xuelong Li']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'jailbreaking', 'malicious code generation', 'security evaluation', 'benchmarking']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.10022</guid><pubDate>Fri, 13 Jun 2025 00:00:00 +0000</pubDate></item><item><title>From Threat to Tool: Leveraging Refusal-Aware Injection Attacks for Safety Alignment</title><link>https://arxiv.org/abs/2506.10020</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces Refusal-Aware Adaptive Injection (RAAI), a framework that uses LLM attack techniques for generating synthetic data.&lt;/li&gt;&lt;li&gt;Demonstrates how RAAI can jailbreak LLMs by adaptively injecting phrases to elicit harmful completions.&lt;/li&gt;&lt;li&gt;Shows that fine-tuning with RAAI-generated data improves LLM robustness against harmful prompts while maintaining general performance.&lt;/li&gt;&lt;li&gt;Reframes adversarial attack methodologies as tools for scalable safety alignment in LLMs.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research Paper&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Quality Assessment&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Clarity: 5/5&lt;/li&gt;&lt;li&gt;Novelty: 4/5&lt;/li&gt;&lt;li&gt;Significance: 4/5&lt;/li&gt;&lt;li&gt;Try-worthiness: Yes&lt;/li&gt;&lt;li&gt;Justification: The abstract is well-written, clearly outlining the motivation, method (RAAI), results, and implications. The idea of leveraging LLM attack techniques (specifically refusal-aware injection attacks) as a tool for generating synthetic data for safety alignment is novel and creative, especially as it reframes a threat into a constructive tool. The significance is promising, as improving safety alignment with less reliance on costly human data is a major challenge in LLM development. The reported results (substantial increase in harmful response rate for data generation, and improved robustness after fine-tuning) suggest practical impact. As a recent arXiv preprint, it has no citations yet, but the approach is worth experimenting with for those working on LLM safety. No code repository is provided in the metadata.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Kyubyung Chae', 'Hyunbin Jin', 'Taesup Kim']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'prompt injection', 'safety alignment', 'jailbreaking', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.10020</guid><pubDate>Fri, 13 Jun 2025 00:00:00 +0000</pubDate></item><item><title>The Alignment Trap: Complexity Barriers</title><link>https://arxiv.org/abs/2506.10304</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Analyzes computational complexity barriers to verifying AI safety as system capabilities increase.&lt;/li&gt;&lt;li&gt;Proves that safety verification becomes exponentially hard and coNP-complete beyond a certain capability threshold.&lt;/li&gt;&lt;li&gt;Demonstrates that robust safety properties are extremely rare in neural network policy spaces.&lt;/li&gt;&lt;li&gt;Presents a strategic trilemma for AI development: constrain complexity, accept unverifiable risks, or invent new safety paradigms.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research Paper&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Quality Assessment&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Clarity: 4/5&lt;/li&gt;&lt;li&gt;Novelty: 5/5&lt;/li&gt;&lt;li&gt;Significance: 4/5&lt;/li&gt;&lt;li&gt;Try-worthiness: No&lt;/li&gt;&lt;li&gt;Justification: The paper is clearly written, with a well-structured abstract that outlines the main results and their implications. The work appears highly novel, providing the first systematic complexity-theoretic analysis of AI alignment and introducing new formal results about the intractability of safety verification as AI systems scale. Its significance is high given the importance of AI safety, though as a recent arXiv preprint with no citations yet, its impact is not fully established. However, the paper is primarily theoretical, focusing on complexity barriers rather than proposing new algorithms or practical methods, so it is not directly try-worthy for implementation or experimentation. No code repository is provided, though a formal Lean4 verification is mentioned as in progress.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jasper Yao']&lt;/li&gt;&lt;li&gt;Tags: ['AI safety', 'alignment', 'verification complexity', 'robustness', 'risk assessment']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.10304</guid><pubDate>Fri, 13 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Towards Responsible AI: Advances in Safety, Fairness, and Accountability of Autonomous Systems</title><link>https://arxiv.org/abs/2506.10192</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Extends deterministic shielding techniques to improve safety in autonomous systems, particularly under delayed observations.&lt;/li&gt;&lt;li&gt;Implements and validates safety shields in simulated autonomous vehicles to prevent collisions.&lt;/li&gt;&lt;li&gt;Introduces fairness shields to enforce group fairness in sequential decision-making with minimal intervention.&lt;/li&gt;&lt;li&gt;Proposes formal frameworks and metrics for transparency and accountability, including retrospective analysis of intention in autonomous agents.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research Paper&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Quality Assessment&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Clarity: 4/5&lt;/li&gt;&lt;li&gt;Novelty: 4/5&lt;/li&gt;&lt;li&gt;Significance: 4/5&lt;/li&gt;&lt;li&gt;Try-worthiness: Yes&lt;/li&gt;&lt;li&gt;Justification: The abstract is well-written and clearly outlines the contributions in safety, fairness, transparency, and accountability for autonomous AI systems. The introduction of 'fairness shields' and formal metrics for agency and intention quotient appear novel, especially in the context of sequential decision-making and retrospective responsibility analysis. The work is significant as it addresses pressing issues in trustworthy AI, and the practical validation in simulated autonomous vehicles adds to its relevance. Although it is a preprint with no citations yet (expected for its age), the breadth and integration of the proposed frameworks make it worth experimenting with, especially for researchers or practitioners interested in responsible AI. No code repository is provided in the metadata.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Filip Cano']&lt;/li&gt;&lt;li&gt;Tags: ['AI safety', 'autonomous systems', 'safety shields', 'accountability', 'fairness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.10192</guid><pubDate>Fri, 13 Jun 2025 00:00:00 +0000</pubDate></item></channel></rss>