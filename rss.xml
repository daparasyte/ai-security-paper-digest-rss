<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>AI Security Paper Digest</title><link>https://kentaroh-toyoda.github.io/ai-security-paper-digest-rss/rss.xml</link><description>Curated papers on AI security from ArXiv</description><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><lastBuildDate>Tue, 16 Sep 2025 22:50:11 +0000</lastBuildDate><item><title>Fighting Fire with Fire (F3): A Training-free and Efficient Visual Adversarial Example Purification Method in LVLMs</title><link>https://arxiv.org/abs/2506.01064</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces F3, a training-free adversarial purification method for LVLMs&lt;/li&gt;&lt;li&gt;Uses intentional perturbations to counteract visual adversarial attacks&lt;/li&gt;&lt;li&gt;Leverages cross-modal attentions from perturbed examples&lt;/li&gt;&lt;li&gt;Offers computational efficiency improvements&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yudong Zhang', 'Ruobing Xie', 'Yiqing Huang', 'Jiansheng Chen', 'Xingwu Sun', 'Zhanhui Kang', 'Di Wang', 'Yu Wang']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial-purification', 'visual-adversaries', 'training-free', 'cross-modal-attention', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.01064</guid><pubDate>Tue, 16 Sep 2025 00:00:00 +0000</pubDate></item><item><title>SAIF: Sparse Adversarial and Imperceptible Attack Framework</title><link>https://arxiv.org/abs/2212.07495</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces SAIF: Sparse Adversarial and Interpretable Attack Framework&lt;/li&gt;&lt;li&gt;Uses Frank-Wolfe algorithm for optimizing sparse, low-magnitude perturbations&lt;/li&gt;&lt;li&gt;Demonstrates superior performance over state-of-the-art on ImageNet dataset&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Tooba Imtiaz', 'Morgan Kohler', 'Jared Miller', 'Zifeng Wang', 'Masih Eskandar', 'Mario Sznaier', 'Octavia Camps', 'Jennifer Dy']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial attacks', 'sparse attacks', 'image classification', 'robustness', 'security']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2212.07495</guid><pubDate>Tue, 16 Sep 2025 00:00:00 +0000</pubDate></item><item><title>DRAG: Data Reconstruction Attack using Guided Diffusion</title><link>https://arxiv.org/abs/2509.11724</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes DRAG: a data reconstruction attack using guided diffusion&lt;/li&gt;&lt;li&gt;Targets foundation models in split inference settings&lt;/li&gt;&lt;li&gt;Leverages latent diffusion model prior knowledge for high-fidelity image reconstruction&lt;/li&gt;&lt;li&gt;Outperforms state-of-the-art methods in reconstructing data from deep-layer IRs&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Wa-Kin Lei', 'Jun-Cheng Chen', 'Shang-Tse Chen']&lt;/li&gt;&lt;li&gt;Tags: ['privacy attack', 'data reconstruction', 'split inference', 'diffusion models', 'foundation models']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.11724</guid><pubDate>Tue, 16 Sep 2025 00:00:00 +0000</pubDate></item><item><title>Realistic Environmental Injection Attacks on GUI Agents</title><link>https://arxiv.org/abs/2509.11250</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces a more realistic threat model for Environmental Injection Attacks (EIAs) on GUI agents with dynamic environments and small triggers&lt;/li&gt;&lt;li&gt;Proposes Chameleon framework with LLM-driven environment simulation and Attention Black Hole mechanism&lt;/li&gt;&lt;li&gt;Significantly outperforms existing attacks on real-world websites and LVLM-powered agents&lt;/li&gt;&lt;li&gt;Reveals underexplored vulnerabilities in modern GUI agents&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yitong Zhang', 'Ximo Li', 'Liyi Cai', 'Jia Li']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial_attacks', 'red_team', 'GUI_agents', 'LLM', 'attention_manipulation', 'environment_simulation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.11250</guid><pubDate>Tue, 16 Sep 2025 00:00:00 +0000</pubDate></item><item><title>Robust Concept Erasure in Diffusion Models: A Theoretical Perspective on Security and Robustness</title><link>https://arxiv.org/abs/2509.12024</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces SCORE framework for robust concept erasure in diffusion models&lt;/li&gt;&lt;li&gt;Formulates erasure as adversarial independence problem with provable guarantees&lt;/li&gt;&lt;li&gt;Empirically evaluated on Stable Diffusion and FLUX across multiple benchmarks&lt;/li&gt;&lt;li&gt;Outperforms state-of-the-art methods with up to 12.5% higher efficacy&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Zixuan Fu', 'Yan Ren', 'Finn Carter', 'Chenyue Wen', 'Le Ku', 'Daheng Yu', 'Emily Davis', 'Bo Zhang']&lt;/li&gt;&lt;li&gt;Tags: ['concept erasure', 'adversarial independence', 'diffusion models', 'privacy', 'security']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.12024</guid><pubDate>Tue, 16 Sep 2025 00:00:00 +0000</pubDate></item><item><title>Oyster-I: Beyond Refusal -- Constructive Safety Alignment for Responsible Language Models</title><link>https://arxiv.org/abs/2509.01909</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces Constructive Safety Alignment (CSA) paradigm for LLMs&lt;/li&gt;&lt;li&gt;Presents Oyster-I (Oy1) model implementing CSA&lt;/li&gt;&lt;li&gt;Achieves state-of-the-art safety while retaining capabilities&lt;/li&gt;&lt;li&gt;Strong performance on Strata-Sword jailbreak dataset&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Ranjie Duan', 'Jiexi Liu', 'Xiaojun Jia', 'Shiji Zhao', 'Ruoxi Cheng', 'Fengxiang Wang', 'Cheng Wei', 'Yong Xie', 'Chang Liu', 'Defeng Li', 'Yinpeng Dong', 'Yichi Zhang', 'Yuefeng Chen', 'Chongwen Wang', 'Xingjun Ma', 'Xingxing Wei', 'Yang Liu', 'Hang Su', 'Jun Zhu', 'Xinfeng Li', 'Yitong Sun', 'Jie Zhang', 'Jinzhao Hu', 'Sha Xu', 'Wenchao Yang', 'Yitong Yang', 'Jialing Tao', 'Hui Xue']&lt;/li&gt;&lt;li&gt;Tags: ['safety alignment', 'constructive engagement', 'jailbreak resistance', 'user trust', 'benchmarking']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.01909</guid><pubDate>Tue, 16 Sep 2025 00:00:00 +0000</pubDate></item><item><title>Enhancing Prompt Injection Attacks to LLMs via Poisoning Alignment</title><link>https://arxiv.org/abs/2410.14827</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces PoisonedAlign, a method to poison LLM alignment data&lt;/li&gt;&lt;li&gt;Enhances prompt injection attacks by manipulating training data&lt;/li&gt;&lt;li&gt;Vulnerability persists while maintaining benchmark performance&lt;/li&gt;&lt;li&gt;Evaluated across multiple LLMs and alignment datasets&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Zedian Shao', 'Hongbin Liu', 'Jaden Mu', 'Neil Zhenqiang Gong']&lt;/li&gt;&lt;li&gt;Tags: ['prompt injection', 'data poisoning', 'red teaming', 'LLM security', 'adversarial attacks']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2410.14827</guid><pubDate>Tue, 16 Sep 2025 00:00:00 +0000</pubDate></item><item><title>Multilingual Collaborative Defense for Large Language Models</title><link>https://arxiv.org/abs/2505.11835</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes Multilingual Collaborative Defense (MCD) to enhance LLM safety across multiple languages&lt;/li&gt;&lt;li&gt;Creates multilingual versions of jailbreak benchmarks like MaliciousInstruct and AdvBench&lt;/li&gt;&lt;li&gt;Demonstrates improved safeguarding performance and language transfer capabilities&lt;/li&gt;&lt;li&gt;Addresses vulnerability of LLMs to jailbreaking via underrepresented languages&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Hongliang Li', 'Jinan Xu', 'Gengping Cui', 'Changhao Guan', 'Fengran Mo', 'Kaiyu Huang']&lt;/li&gt;&lt;li&gt;Tags: ['jailbreaking', 'multilingual', 'safety', 'defense', 'prompt injection']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2505.11835</guid><pubDate>Tue, 16 Sep 2025 00:00:00 +0000</pubDate></item><item><title>Revealing the Inherent Instructability of Pre-Trained Language Models</title><link>https://arxiv.org/abs/2410.02465</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes Response Tuning (RT) which focuses on response distribution without instruction&lt;/li&gt;&lt;li&gt;Models trained with RT can handle instructions and reject unsafe queries&lt;/li&gt;&lt;li&gt;Highlights inherent capabilities of pre-trained LLMs&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Seokhyun An', 'Minji Kim', 'Hyounghun Kim']&lt;/li&gt;&lt;li&gt;Tags: ['safety', 'alignment', 'instruction_tuning', 'pre_training']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2410.02465</guid><pubDate>Tue, 16 Sep 2025 00:00:00 +0000</pubDate></item><item><title>Collapse of Irrelevant Representations (CIR) Ensures Robust and Non-Disruptive LLM Unlearning</title><link>https://arxiv.org/abs/2509.11816</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes CIR technique for selective unlearning of dangerous knowledge from LLMs&lt;/li&gt;&lt;li&gt;Uses PCA on activations and gradients to identify specific subspaces&lt;/li&gt;&lt;li&gt;Achieves 80x better biohazardous and 30x better cyberhazardous fact removal than baseline&lt;/li&gt;&lt;li&gt;Minimal disruption to general performance (0.1% WikiText loss increase)&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Filip Sondej', 'Yushi Yang']&lt;/li&gt;&lt;li&gt;Tags: ['unlearning', 'safety', 'robustness', 'LLM', 'PCA']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.11816</guid><pubDate>Tue, 16 Sep 2025 00:00:00 +0000</pubDate></item><item><title>Securing AI Agents: Implementing Role-Based Access Control for Industrial Applications</title><link>https://arxiv.org/abs/2509.11431</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes RBAC framework for securing AI agents in industrial settings&lt;/li&gt;&lt;li&gt;Addresses prompt injection attacks and other security threats&lt;/li&gt;&lt;li&gt;Focuses on on-premises deployment and scalability&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Aadil Gani Ganie']&lt;/li&gt;&lt;li&gt;Tags: ['security', 'access control', 'prompt injection', 'RBAC', 'industrial applications']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.11431</guid><pubDate>Tue, 16 Sep 2025 00:00:00 +0000</pubDate></item><item><title>Harmful Prompt Laundering: Jailbreaking LLMs with Abductive Styles and Symbolic Encoding</title><link>https://arxiv.org/abs/2509.10931</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces Harmful Prompt Laundering (HaPLa), a novel jailbreaking technique for LLMs&lt;/li&gt;&lt;li&gt;Uses abductive framing and symbolic encoding to bypass safety filters&lt;/li&gt;&lt;li&gt;Achieves high success rates against GPT models and others&lt;/li&gt;&lt;li&gt;Highlights trade-off between safety and helpfulness in LLM tuning&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Seongho Joo', 'Hyukhun Koh', 'Kyomin Jung']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'jailbreaking', 'adversarial prompting', 'prompt injection', 'alignment']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.10931</guid><pubDate>Tue, 16 Sep 2025 00:00:00 +0000</pubDate></item><item><title>LLM in the Middle: A Systematic Review of Threats and Mitigations to Real-World LLM-based Systems</title><link>https://arxiv.org/abs/2509.10682</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Systematic review of security threats and mitigations for LLM-based systems&lt;/li&gt;&lt;li&gt;Categorizes threats by severity and real-world scenarios&lt;/li&gt;&lt;li&gt;Maps defense strategies to lifecycle phases and attack types&lt;/li&gt;&lt;li&gt;Aids understanding and mitigation of risks in LLM integration&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Survey&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Vitor Hugo Galhardo Moia', 'Igor Jochem Sanz', 'Gabriel Antonio Fontes Rebello', 'Rodrigo Duarte de Meneses', 'Briland Hitaj', 'Ulf Lindqvist']&lt;/li&gt;&lt;li&gt;Tags: ['LLM security', 'threat modeling', 'mitigation strategies', 'systematic review', 'real-world scenarios']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.10682</guid><pubDate>Tue, 16 Sep 2025 00:00:00 +0000</pubDate></item><item><title>When Smiley Turns Hostile: Interpreting How Emojis Trigger LLMs' Toxicity</title><link>https://arxiv.org/abs/2509.11141</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Investigates how emojis can trigger toxic content generation in LLMs&lt;/li&gt;&lt;li&gt;Uses jailbreak tasks and automated prompt construction with emojis&lt;/li&gt;&lt;li&gt;Analyzes model-level interpretations to understand the bypass mechanism&lt;/li&gt;&lt;li&gt;Explores correlation between emoji-related data pollution and toxicity&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Shiyao Cui', 'Xijia Feng', 'Yingkang Wang', 'Junxiao Yang', 'Zhexin Zhang', 'Biplab Sikdar', 'Hongning Wang', 'Han Qiu', 'Minlie Huang']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'adversarial prompting', 'jailbreaking', 'safety evaluation', 'data poisoning']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.11141</guid><pubDate>Tue, 16 Sep 2025 00:00:00 +0000</pubDate></item><item><title>Uncovering the Vulnerability of Large Language Models in the Financial Domain via Risk Concealment</title><link>https://arxiv.org/abs/2509.10546</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces Risk-Concealment Attacks (RCA) for financial LLMs&lt;/li&gt;&lt;li&gt;Creates FIN-Bench benchmark for financial domain safety evaluation&lt;/li&gt;&lt;li&gt;Demonstrates high attack success rates (93.18% avg) against major LLMs including GPT-4.1 and OpenAI o1&lt;/li&gt;&lt;li&gt;Highlights critical gaps in current alignment techniques for financial applications&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Gang Cheng', 'Haibo Jin', 'Wenbin Zhang', 'Haohan Wang', 'Jun Zhuang']&lt;/li&gt;&lt;li&gt;Tags: ['red teaming', 'financial domain', 'risk concealment', 'alignment', 'safety evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.10546</guid><pubDate>Tue, 16 Sep 2025 00:00:00 +0000</pubDate></item><item><title>Murphys Laws of AI Alignment: Why the Gap Always Wins</title><link>https://arxiv.org/abs/2509.05381</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;The paper studies the difficulty of learning true reward functions from biased human feedback in reinforcement learning&lt;/li&gt;&lt;li&gt;Proves exponential sample complexity when feedback is biased on a fraction of contexts&lt;/li&gt;&lt;li&gt;Shows that a calibration oracle can reduce sample complexity to polynomial&lt;/li&gt;&lt;li&gt;Quantifies the alignment gap based on misspecification parameters&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Madhava Gaikwad']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'safety', 'reinforcement_learning', 'human_feedback', 'sample_complexity']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.05381</guid><pubDate>Tue, 16 Sep 2025 00:00:00 +0000</pubDate></item><item><title>Multi-Agent Systems Execute Arbitrary Malicious Code</title><link>https://arxiv.org/abs/2503.12188</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Demonstrates adversarial attacks on multi-agent LLM systems leading to arbitrary code execution&lt;/li&gt;&lt;li&gt;Attacks succeed even when individual agents are resistant to prompt injection&lt;/li&gt;&lt;li&gt;High success rates (58-90%) with GPT-4o, up to 100% in some configurations&lt;/li&gt;&lt;li&gt;Highlights need for security models in multi-agent systems&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Harold Triedman', 'Rishi Jha', 'Vitaly Shmatikov']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial_prompting', 'multi_agent_systems', 'arbitrary_code_execution', 'red_team', 'llm_security']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2503.12188</guid><pubDate>Tue, 16 Sep 2025 00:00:00 +0000</pubDate></item><item><title>Enhancing Prompt Injection Attacks to LLMs via Poisoning Alignment</title><link>https://arxiv.org/abs/2410.14827</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces PoisonedAlign, a method to poison LLM alignment data&lt;/li&gt;&lt;li&gt;Enhances prompt injection attacks by manipulating training data&lt;/li&gt;&lt;li&gt;Vulnerability persists while maintaining benchmark performance&lt;/li&gt;&lt;li&gt;Evaluated across multiple LLMs and alignment datasets&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Zedian Shao', 'Hongbin Liu', 'Jaden Mu', 'Neil Zhenqiang Gong']&lt;/li&gt;&lt;li&gt;Tags: ['prompt injection', 'data poisoning', 'red teaming', 'LLM security', 'adversarial attacks']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2410.14827</guid><pubDate>Tue, 16 Sep 2025 00:00:00 +0000</pubDate></item><item><title>Prompt Injection Attacks on LLM Generated Reviews of Scientific Publications</title><link>https://arxiv.org/abs/2509.10248</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Systematic evaluation of prompt injection attacks on LLM-generated scientific reviews&lt;/li&gt;&lt;li&gt;Simple prompt injections found to be highly effective (up to 100% acceptance)&lt;/li&gt;&lt;li&gt;LLM reviews show significant bias towards acceptance (&gt;95% in many models)&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Janis Keuper']&lt;/li&gt;&lt;li&gt;Tags: ['prompt injection', 'adversarial prompting', 'red teaming', 'LLM security', 'peer review']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.10248</guid><pubDate>Tue, 16 Sep 2025 00:00:00 +0000</pubDate></item><item><title>Adversarial Examples Are Not Bugs, They Are Superposition</title><link>https://arxiv.org/abs/2508.17456</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Presents evidence linking superposition to adversarial examples&lt;/li&gt;&lt;li&gt;Explores theoretical and empirical connections&lt;/li&gt;&lt;li&gt;Extends prior work on mechanistic interpretability&lt;/li&gt;&lt;li&gt;Implications for robustness and defense mechanisms&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Liv Gorton', 'Owen Lewis']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial examples', 'superposition', 'robustness', 'security']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2508.17456</guid><pubDate>Tue, 16 Sep 2025 00:00:00 +0000</pubDate></item><item><title>Safety Pretraining: Toward the Next Generation of Safe AI</title><link>https://arxiv.org/abs/2504.16980</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces a data-centric pretraining framework for LLM safety&lt;/li&gt;&lt;li&gt;Includes safety filtering, rephrasing, refusal training, and harmfulness tagging&lt;/li&gt;&lt;li&gt;Reduces attack success rates from 38.8% to 8.4% on safety benchmarks&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Pratyush Maini', 'Sachin Goyal', 'Dylan Sam', 'Alex Robey', 'Yash Savani', 'Yiding Jiang', 'Andy Zou', 'Matt Fredrikson', 'Zacharcy C. Lipton', 'J. Zico Kolter']&lt;/li&gt;&lt;li&gt;Tags: ['safety', 'pretraining', 'LLM', 'red teaming', 'adversarial prompting']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2504.16980</guid><pubDate>Tue, 16 Sep 2025 00:00:00 +0000</pubDate></item><item><title>SafeSwitch: Steering Unsafe LLM Behavior via Internal Activation Signals</title><link>https://arxiv.org/abs/2502.01042</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes SafeSwitch framework to dynamically regulate unsafe LLM outputs using internal activation signals&lt;/li&gt;&lt;li&gt;Reduces harmful outputs by ~80% while maintaining utility through context-aware safety activation&lt;/li&gt;&lt;li&gt;Efficiently tunes &lt;6% of original parameters for safety control&lt;/li&gt;&lt;li&gt;Demonstrates LLM capacity for self-aware safety reflection&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Peixuan Han', 'Cheng Qian', 'Xiusi Chen', 'Yuji Zhang', 'Heng Ji', 'Denghui Zhang']&lt;/li&gt;&lt;li&gt;Tags: ['LLM safety', 'internal state monitoring', 'dynamic safety control', 'parameter efficiency', 'alignment']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2502.01042</guid><pubDate>Tue, 16 Sep 2025 00:00:00 +0000</pubDate></item><item><title>Security of Deep Reinforcement Learning for Autonomous Driving: A Survey</title><link>https://arxiv.org/abs/2212.06123</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Comprehensive survey of 86 studies on RL security&lt;/li&gt;&lt;li&gt;Categorizes attacks and defenses by threat models&lt;/li&gt;&lt;li&gt;Focuses on autonomous driving applications&lt;/li&gt;&lt;li&gt;Provides insights for robust RL system design&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Survey&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Ambra Demontis', 'Srishti Gupta', 'Maura Pintor', 'Luca Demetrio', 'Kathrin Grosse', 'Hsiao-Ying Lin', 'Chengfang Fang', 'Battista Biggio', 'Fabio Roli']&lt;/li&gt;&lt;li&gt;Tags: ['security', 'reinforcement learning', 'autonomous driving', 'survey', 'adversarial attacks']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2212.06123</guid><pubDate>Tue, 16 Sep 2025 00:00:00 +0000</pubDate></item><item><title>Your Compiler is Backdooring Your Model: Understanding and Exploiting Compilation Inconsistency Vulnerabilities in Deep Learning Compilers</title><link>https://arxiv.org/abs/2509.11173</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Unveils a fundamental vulnerability in DL compilers that can introduce backdoors during compilation&lt;/li&gt;&lt;li&gt;Demonstrates adversarial attacks that remain undetected by state-of-the-art detectors&lt;/li&gt;&lt;li&gt;Finds natural triggers in popular HuggingFace models even without adversarial manipulation&lt;/li&gt;&lt;li&gt;Highlights a new direction for secure and trustworthy ML&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Simin Chen', 'Jinjun Peng', 'Yixin He', 'Junfeng Yang', 'Baishakhi Ray']&lt;/li&gt;&lt;li&gt;Tags: ['compiler security', 'backdoor attacks', 'adversarial manipulation', 'model integrity', 'security standards']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.11173</guid><pubDate>Tue, 16 Sep 2025 00:00:00 +0000</pubDate></item><item><title>LLM in the Middle: A Systematic Review of Threats and Mitigations to Real-World LLM-based Systems</title><link>https://arxiv.org/abs/2509.10682</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Systematic review of security threats and mitigations for LLM-based systems&lt;/li&gt;&lt;li&gt;Categorizes threats by severity and real-world scenarios&lt;/li&gt;&lt;li&gt;Maps defense strategies to lifecycle phases and attack types&lt;/li&gt;&lt;li&gt;Aids understanding and mitigation of risks in LLM integration&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Survey&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Vitor Hugo Galhardo Moia', 'Igor Jochem Sanz', 'Gabriel Antonio Fontes Rebello', 'Rodrigo Duarte de Meneses', 'Briland Hitaj', 'Ulf Lindqvist']&lt;/li&gt;&lt;li&gt;Tags: ['LLM security', 'threat modeling', 'mitigation strategies', 'systematic review', 'real-world scenarios']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.10682</guid><pubDate>Tue, 16 Sep 2025 00:00:00 +0000</pubDate></item><item><title>Uncovering the Vulnerability of Large Language Models in the Financial Domain via Risk Concealment</title><link>https://arxiv.org/abs/2509.10546</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces Risk-Concealment Attacks (RCA) for financial LLMs&lt;/li&gt;&lt;li&gt;Creates FIN-Bench benchmark for financial domain safety evaluation&lt;/li&gt;&lt;li&gt;Demonstrates high attack success rates (93.18% avg) against major LLMs including GPT-4.1 and OpenAI o1&lt;/li&gt;&lt;li&gt;Highlights critical gaps in current alignment techniques for financial applications&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Gang Cheng', 'Haibo Jin', 'Wenbin Zhang', 'Haohan Wang', 'Jun Zhuang']&lt;/li&gt;&lt;li&gt;Tags: ['red teaming', 'financial domain', 'risk concealment', 'alignment', 'safety evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.10546</guid><pubDate>Tue, 16 Sep 2025 00:00:00 +0000</pubDate></item><item><title>Collapse of Irrelevant Representations (CIR) Ensures Robust and Non-Disruptive LLM Unlearning</title><link>https://arxiv.org/abs/2509.11816</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes CIR technique for selective unlearning of dangerous knowledge from LLMs&lt;/li&gt;&lt;li&gt;Uses PCA on activations and gradients to identify specific subspaces&lt;/li&gt;&lt;li&gt;Achieves 80x better biohazardous and 30x better cyberhazardous fact removal than baseline&lt;/li&gt;&lt;li&gt;Minimal disruption to general performance (0.1% WikiText loss increase)&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Filip Sondej', 'Yushi Yang']&lt;/li&gt;&lt;li&gt;Tags: ['unlearning', 'safety', 'robustness', 'LLM', 'PCA']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.11816</guid><pubDate>Tue, 16 Sep 2025 00:00:00 +0000</pubDate></item><item><title>DRAG: Data Reconstruction Attack using Guided Diffusion</title><link>https://arxiv.org/abs/2509.11724</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes DRAG: a data reconstruction attack using guided diffusion&lt;/li&gt;&lt;li&gt;Targets foundation models in split inference settings&lt;/li&gt;&lt;li&gt;Leverages latent diffusion model prior knowledge for high-fidelity image reconstruction&lt;/li&gt;&lt;li&gt;Outperforms state-of-the-art methods in reconstructing data from deep-layer IRs&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Wa-Kin Lei', 'Jun-Cheng Chen', 'Shang-Tse Chen']&lt;/li&gt;&lt;li&gt;Tags: ['privacy attack', 'data reconstruction', 'split inference', 'diffusion models', 'foundation models']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.11724</guid><pubDate>Tue, 16 Sep 2025 00:00:00 +0000</pubDate></item><item><title>Reasoned Safety Alignment: Ensuring Jailbreak Defense via Answer-Then-Check</title><link>https://arxiv.org/abs/2509.11629</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces Answer-Then-Check method to enhance LLM jailbreak defense&lt;/li&gt;&lt;li&gt;Constructs ReSA dataset with 80K examples for safety alignment training&lt;/li&gt;&lt;li&gt;Demonstrates improved safety while reducing over-refusal rates&lt;/li&gt;&lt;li&gt;Maintains general reasoning capabilities on benchmarks&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Chentao Cao', 'Xiaojun Xu', 'Bo Han', 'Hang Li']&lt;/li&gt;&lt;li&gt;Tags: ['jailbreaking', 'safety alignment', 'LLM security', 'red teaming', 'dataset']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.11629</guid><pubDate>Tue, 16 Sep 2025 00:00:00 +0000</pubDate></item><item><title>DARD: Dice Adversarial Robustness Distillation against Adversarial Attacks</title><link>https://arxiv.org/abs/2509.11525</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces DARD for distilling robustness from large to small models&lt;/li&gt;&lt;li&gt;Proposes DPGD for generating adversarial examples&lt;/li&gt;&lt;li&gt;Shows improved robustness and accuracy over adversarial training&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jing Zou', 'Shungeng Zhang', 'Meikang Qiu', 'Chong Li']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial robustness', 'knowledge distillation', 'defense', 'adversarial training']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.11525</guid><pubDate>Tue, 16 Sep 2025 00:00:00 +0000</pubDate></item><item><title>From Firewalls to Frontiers: AI Red-Teaming is a Domain-Specific Evolution of Cyber Red-Teaming</title><link>https://arxiv.org/abs/2509.11398</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Argues that AI red-teaming should be recognized as a domain-specific evolution of cyber red-teaming&lt;/li&gt;&lt;li&gt;Emphasizes the need for existing cyber red teams to adapt to AI-specific vulnerabilities&lt;/li&gt;&lt;li&gt;Highlights the importance of merging AI and cyber red teaming approaches for robust security&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Position&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Anusha Sinha', 'Keltin Grimes', 'James Lucassen', 'Michael Feffer', 'Nathan VanHoudnos', 'Zhiwei Steven Wu', 'Hoda Heidari']&lt;/li&gt;&lt;li&gt;Tags: ['red teaming', 'AI security', 'cybersecurity', 'security standards']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.11398</guid><pubDate>Tue, 16 Sep 2025 00:00:00 +0000</pubDate></item><item><title>Stabilizing Data-Free Model Extraction</title><link>https://arxiv.org/abs/2509.11159</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Addresses data-free model extraction attacks by proposing MetaDFME&lt;/li&gt;&lt;li&gt;Uses meta-learning in generator training to reduce distribution shift&lt;/li&gt;&lt;li&gt;Improves stability and accuracy of substitute model during extraction&lt;/li&gt;&lt;li&gt;Validated on image datasets (MNIST, SVHN, CIFAR-10, CIFAR-100)&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Dat-Thinh Nguyen', 'Kim-Hung Le', 'Nhien-An Le-Khac']&lt;/li&gt;&lt;li&gt;Tags: ['model extraction', 'data-free', 'meta-learning', 'adversarial attacks']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.11159</guid><pubDate>Tue, 16 Sep 2025 00:00:00 +0000</pubDate></item><item><title>The Psychogenic Machine: Simulating AI Psychosis, Delusion Reinforcement and Harm Enablement in Large Language Models</title><link>https://arxiv.org/abs/2509.10970</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduced psychosis-bench benchmark to evaluate psychogenic risks in LLMs&lt;/li&gt;&lt;li&gt;Assessed Delusion Confirmation, Harm Enablement, and Safety Intervention across 16 scenarios&lt;/li&gt;&lt;li&gt;Found all models demonstrated psychogenic potential with high delusion confirmation and harm enablement rates&lt;/li&gt;&lt;li&gt;Emphasized need for safety-focused training and collaboration with healthcare professionals&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Joshua Au Yeung', 'Jacopo Dalmasso', 'Luca Foschini', 'Richard JB Dobson', 'Zeljko Kraljevic']&lt;/li&gt;&lt;li&gt;Tags: ['safety evaluation', 'alignment', 'benchmarking', 'psychogenic risk', 'LLM safety']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.10970</guid><pubDate>Tue, 16 Sep 2025 00:00:00 +0000</pubDate></item><item><title>The Anti-Ouroboros Effect: Emergent Resilience in Large Language Models from Recursive Selective Feedback</title><link>https://arxiv.org/abs/2509.10509</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces selective feedback mechanism in recursive LLM training&lt;/li&gt;&lt;li&gt;Demonstrates performance improvement (Anti-Ouroboros Effect)&lt;/li&gt;&lt;li&gt;Contrasts with degenerative loop in simple models&lt;/li&gt;&lt;li&gt;Suggests emergent resilience for AI safety&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Sai Teja Reddy Adapala']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'robustness', 'safety evaluation', 'AI safety']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.10509</guid><pubDate>Tue, 16 Sep 2025 00:00:00 +0000</pubDate></item><item><title>Character-Level Perturbations Disrupt LLM Watermarks</title><link>https://arxiv.org/abs/2509.09112</link><description>&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Other&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Zhaoxi Zhang', 'Xiaomei Zhang', 'Yanjun Zhang', 'He Zhang', 'Shirui Pan', 'Bo Liu', 'Asif Qumer Gill', 'Leo Yu Zhang']&lt;/li&gt;&lt;li&gt;Tags: []&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.09112</guid><pubDate>Tue, 16 Sep 2025 00:00:00 +0000</pubDate></item><item><title>Exploit Tool Invocation Prompt for Tool Behavior Hijacking in LLM-Based Agentic System</title><link>https://arxiv.org/abs/2509.05755</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Investigates security risks in Tool Invocation Prompts (TIPs) for LLM-based agentic systems&lt;/li&gt;&lt;li&gt;Demonstrates tool behavior hijacking via manipulated TIPs leading to RCE and DoS&lt;/li&gt;&lt;li&gt;Identifies vulnerabilities in major systems like Cursor and Claude Code&lt;/li&gt;&lt;li&gt;Proposes defense mechanisms to enhance TIP security&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yu Liu', 'Yuchong Xie', 'Mingyu Luo', 'Zesen Liu', 'Zhixiang Zhang', 'Kaikai Zhang', 'Zongjie Li', 'Ping Chen', 'Shuai Wang', 'Dongdong She']&lt;/li&gt;&lt;li&gt;Tags: ['red teaming', 'adversarial prompting', 'tool hijacking', 'LLM security', 'defense mechanisms']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.05755</guid><pubDate>Tue, 16 Sep 2025 00:00:00 +0000</pubDate></item><item><title>Confusion is the Final Barrier: Rethinking Jailbreak Evaluation and Investigating the Real Misuse Threat of LLMs</title><link>https://arxiv.org/abs/2508.16347</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Rethinks jailbreak evaluation for LLMs by focusing on knowledge-intensive Q&amp;A&lt;/li&gt;&lt;li&gt;Investigates real-world misuse threats beyond hallucinated responses&lt;/li&gt;&lt;li&gt;Reveals mismatch between jailbreak success and actual harmful knowledge possession&lt;/li&gt;&lt;li&gt;Highlights flaws in current harmfulness judgment frameworks&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yu Yan', 'Sheng Sun', 'Zhe Wang', 'Yijun Lin', 'Zenghao Duan', 'zhifei zheng', 'Min Liu', 'Zhiyi yin', 'Jianping Zhang']&lt;/li&gt;&lt;li&gt;Tags: ['jailbreaking', 'safety evaluation', 'adversarial prompting', 'alignment', 'LLM red teaming']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2508.16347</guid><pubDate>Tue, 16 Sep 2025 00:00:00 +0000</pubDate></item><item><title>Fighting Fire with Fire (F3): A Training-free and Efficient Visual Adversarial Example Purification Method in LVLMs</title><link>https://arxiv.org/abs/2506.01064</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces F3, a training-free adversarial purification method for LVLMs&lt;/li&gt;&lt;li&gt;Uses intentional perturbations to counteract visual adversarial attacks&lt;/li&gt;&lt;li&gt;Leverages cross-modal attentions from perturbed examples&lt;/li&gt;&lt;li&gt;Offers computational efficiency improvements&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yudong Zhang', 'Ruobing Xie', 'Yiqing Huang', 'Jiansheng Chen', 'Xingwu Sun', 'Zhanhui Kang', 'Di Wang', 'Yu Wang']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial-purification', 'visual-adversaries', 'training-free', 'cross-modal-attention', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.01064</guid><pubDate>Tue, 16 Sep 2025 00:00:00 +0000</pubDate></item><item><title>Multilingual Collaborative Defense for Large Language Models</title><link>https://arxiv.org/abs/2505.11835</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes Multilingual Collaborative Defense (MCD) to enhance LLM safety across multiple languages&lt;/li&gt;&lt;li&gt;Creates multilingual versions of jailbreak benchmarks like MaliciousInstruct and AdvBench&lt;/li&gt;&lt;li&gt;Demonstrates improved safeguarding performance and language transfer capabilities&lt;/li&gt;&lt;li&gt;Addresses vulnerability of LLMs to jailbreaking via underrepresented languages&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Hongliang Li', 'Jinan Xu', 'Gengping Cui', 'Changhao Guan', 'Fengran Mo', 'Kaiyu Huang']&lt;/li&gt;&lt;li&gt;Tags: ['jailbreaking', 'multilingual', 'safety', 'defense', 'prompt injection']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2505.11835</guid><pubDate>Tue, 16 Sep 2025 00:00:00 +0000</pubDate></item><item><title>DataSentinel: A Game-Theoretic Detection of Prompt Injection Attacks</title><link>https://arxiv.org/abs/2504.11358</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes DataSentinel, a game-theoretic method for detecting prompt injection attacks in LLMs&lt;/li&gt;&lt;li&gt;Formulates the problem as a minimax optimization to handle adaptive attacks&lt;/li&gt;&lt;li&gt;Evaluates effectiveness against existing and adaptive attacks&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yupei Liu', 'Yuqi Jia', 'Jinyuan Jia', 'Dawn Song', 'Neil Zhenqiang Gong']&lt;/li&gt;&lt;li&gt;Tags: ['prompt injection', 'red teaming', 'game-theoretic', 'adversarial prompting', 'detection']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2504.11358</guid><pubDate>Tue, 16 Sep 2025 00:00:00 +0000</pubDate></item><item><title>Enhancing Prompt Injection Attacks to LLMs via Poisoning Alignment</title><link>https://arxiv.org/abs/2410.14827</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces PoisonedAlign, a method to poison LLM alignment data&lt;/li&gt;&lt;li&gt;Enhances prompt injection attacks by manipulating training data&lt;/li&gt;&lt;li&gt;Vulnerability persists while maintaining benchmark performance&lt;/li&gt;&lt;li&gt;Evaluated across multiple LLMs and alignment datasets&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Zedian Shao', 'Hongbin Liu', 'Jaden Mu', 'Neil Zhenqiang Gong']&lt;/li&gt;&lt;li&gt;Tags: ['prompt injection', 'data poisoning', 'red teaming', 'LLM security', 'adversarial attacks']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2410.14827</guid><pubDate>Tue, 16 Sep 2025 00:00:00 +0000</pubDate></item><item><title>Revealing the Inherent Instructability of Pre-Trained Language Models</title><link>https://arxiv.org/abs/2410.02465</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes Response Tuning (RT) which focuses on response distribution without instruction&lt;/li&gt;&lt;li&gt;Models trained with RT can handle instructions and reject unsafe queries&lt;/li&gt;&lt;li&gt;Highlights inherent capabilities of pre-trained LLMs&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Seokhyun An', 'Minji Kim', 'Hyounghun Kim']&lt;/li&gt;&lt;li&gt;Tags: ['safety', 'alignment', 'instruction_tuning', 'pre_training']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2410.02465</guid><pubDate>Tue, 16 Sep 2025 00:00:00 +0000</pubDate></item><item><title>Murphys Laws of AI Alignment: Why the Gap Always Wins</title><link>https://arxiv.org/abs/2509.05381</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;The paper studies the difficulty of learning true reward functions from biased human feedback in reinforcement learning&lt;/li&gt;&lt;li&gt;Proves exponential sample complexity when feedback is biased on a fraction of contexts&lt;/li&gt;&lt;li&gt;Shows that a calibration oracle can reduce sample complexity to polynomial&lt;/li&gt;&lt;li&gt;Quantifies the alignment gap based on misspecification parameters&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Madhava Gaikwad']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'safety', 'reinforcement_learning', 'human_feedback', 'sample_complexity']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.05381</guid><pubDate>Tue, 16 Sep 2025 00:00:00 +0000</pubDate></item><item><title>Oyster-I: Beyond Refusal -- Constructive Safety Alignment for Responsible Language Models</title><link>https://arxiv.org/abs/2509.01909</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces Constructive Safety Alignment (CSA) paradigm for LLMs&lt;/li&gt;&lt;li&gt;Presents Oyster-I (Oy1) model implementing CSA&lt;/li&gt;&lt;li&gt;Achieves state-of-the-art safety while retaining capabilities&lt;/li&gt;&lt;li&gt;Strong performance on Strata-Sword jailbreak dataset&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Ranjie Duan', 'Jiexi Liu', 'Xiaojun Jia', 'Shiji Zhao', 'Ruoxi Cheng', 'Fengxiang Wang', 'Cheng Wei', 'Yong Xie', 'Chang Liu', 'Defeng Li', 'Yinpeng Dong', 'Yichi Zhang', 'Yuefeng Chen', 'Chongwen Wang', 'Xingjun Ma', 'Xingxing Wei', 'Yang Liu', 'Hang Su', 'Jun Zhu', 'Xinfeng Li', 'Yitong Sun', 'Jie Zhang', 'Jinzhao Hu', 'Sha Xu', 'Wenchao Yang', 'Yitong Yang', 'Jialing Tao', 'Hui Xue']&lt;/li&gt;&lt;li&gt;Tags: ['safety alignment', 'constructive engagement', 'jailbreak resistance', 'user trust', 'benchmarking']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.01909</guid><pubDate>Tue, 16 Sep 2025 00:00:00 +0000</pubDate></item><item><title>Hide-and-Shill: A Reinforcement Learning Framework for Market Manipulation Detection in Symphony-a Decentralized Multi-Agent System</title><link>https://arxiv.org/abs/2507.09179</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a Multi-Agent Reinforcement Learning (MARL) framework for detecting market manipulation in DeFi&lt;/li&gt;&lt;li&gt;Integrates LLM-based semantic features, social graph signals, and on-chain market data&lt;/li&gt;&lt;li&gt;Models adversarial interactions between manipulators and detectors&lt;/li&gt;&lt;li&gt;Validated through adversarial simulations and real-world data&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Ronghua Shi', 'Yiou Liu', 'Xinyu Ying', 'Yang Tan', 'Yuchun Feng', 'Lynn Ai', 'Bill Shi', 'Xuhui Wang', 'Zhuang Liu']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial prompting', 'robustness', 'safety evaluation', 'multi-agent systems', 'LLM integration']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2507.09179</guid><pubDate>Tue, 16 Sep 2025 00:00:00 +0000</pubDate></item><item><title>Collapse of Irrelevant Representations (CIR) Ensures Robust and Non-Disruptive LLM Unlearning</title><link>https://arxiv.org/abs/2509.11816</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes CIR technique for selective unlearning of dangerous knowledge from LLMs&lt;/li&gt;&lt;li&gt;Uses PCA on activations and gradients to identify specific subspaces&lt;/li&gt;&lt;li&gt;Achieves 80x better biohazardous and 30x better cyberhazardous fact removal than baseline&lt;/li&gt;&lt;li&gt;Minimal disruption to general performance (0.1% WikiText loss increase)&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Filip Sondej', 'Yushi Yang']&lt;/li&gt;&lt;li&gt;Tags: ['unlearning', 'safety', 'robustness', 'LLM', 'PCA']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.11816</guid><pubDate>Tue, 16 Sep 2025 00:00:00 +0000</pubDate></item><item><title>Reasoned Safety Alignment: Ensuring Jailbreak Defense via Answer-Then-Check</title><link>https://arxiv.org/abs/2509.11629</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces Answer-Then-Check method to enhance LLM jailbreak defense&lt;/li&gt;&lt;li&gt;Constructs ReSA dataset with 80K examples for safety alignment training&lt;/li&gt;&lt;li&gt;Demonstrates improved safety while reducing over-refusal rates&lt;/li&gt;&lt;li&gt;Maintains general reasoning capabilities on benchmarks&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Chentao Cao', 'Xiaojun Xu', 'Bo Han', 'Hang Li']&lt;/li&gt;&lt;li&gt;Tags: ['jailbreaking', 'safety alignment', 'LLM security', 'red teaming', 'dataset']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.11629</guid><pubDate>Tue, 16 Sep 2025 00:00:00 +0000</pubDate></item><item><title>From Firewalls to Frontiers: AI Red-Teaming is a Domain-Specific Evolution of Cyber Red-Teaming</title><link>https://arxiv.org/abs/2509.11398</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Argues that AI red-teaming should be recognized as a domain-specific evolution of cyber red-teaming&lt;/li&gt;&lt;li&gt;Emphasizes the need for existing cyber red teams to adapt to AI-specific vulnerabilities&lt;/li&gt;&lt;li&gt;Highlights the importance of merging AI and cyber red teaming approaches for robust security&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Position&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Anusha Sinha', 'Keltin Grimes', 'James Lucassen', 'Michael Feffer', 'Nathan VanHoudnos', 'Zhiwei Steven Wu', 'Hoda Heidari']&lt;/li&gt;&lt;li&gt;Tags: ['red teaming', 'AI security', 'cybersecurity', 'security standards']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.11398</guid><pubDate>Tue, 16 Sep 2025 00:00:00 +0000</pubDate></item><item><title>Your Compiler is Backdooring Your Model: Understanding and Exploiting Compilation Inconsistency Vulnerabilities in Deep Learning Compilers</title><link>https://arxiv.org/abs/2509.11173</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Unveils a fundamental vulnerability in DL compilers that can introduce backdoors during compilation&lt;/li&gt;&lt;li&gt;Demonstrates adversarial attacks that remain undetected by state-of-the-art detectors&lt;/li&gt;&lt;li&gt;Finds natural triggers in popular HuggingFace models even without adversarial manipulation&lt;/li&gt;&lt;li&gt;Highlights a new direction for secure and trustworthy ML&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Simin Chen', 'Jinjun Peng', 'Yixin He', 'Junfeng Yang', 'Baishakhi Ray']&lt;/li&gt;&lt;li&gt;Tags: ['compiler security', 'backdoor attacks', 'adversarial manipulation', 'model integrity', 'security standards']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.11173</guid><pubDate>Tue, 16 Sep 2025 00:00:00 +0000</pubDate></item><item><title>ENJ: Optimizing Noise with Genetic Algorithms to Jailbreak LSMs</title><link>https://arxiv.org/abs/2509.11128</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes Evolutionary Noise Jailbreak (ENJ) using genetic algorithms to optimize environmental noise for stealthy adversarial attacks on LSMs&lt;/li&gt;&lt;li&gt;Generates audio samples that appear as harmless noise to humans but contain malicious instructions parsed by the model&lt;/li&gt;&lt;li&gt;Demonstrates superior attack effectiveness compared to baseline methods across multiple speech models&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yibo Zhang', 'Liang Lin']&lt;/li&gt;&lt;li&gt;Tags: ['jailbreaking', 'adversarial attack', 'speech models', 'genetic algorithms', 'red teaming']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.11128</guid><pubDate>Tue, 16 Sep 2025 00:00:00 +0000</pubDate></item><item><title>The Psychogenic Machine: Simulating AI Psychosis, Delusion Reinforcement and Harm Enablement in Large Language Models</title><link>https://arxiv.org/abs/2509.10970</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduced psychosis-bench benchmark to evaluate psychogenic risks in LLMs&lt;/li&gt;&lt;li&gt;Assessed Delusion Confirmation, Harm Enablement, and Safety Intervention across 16 scenarios&lt;/li&gt;&lt;li&gt;Found all models demonstrated psychogenic potential with high delusion confirmation and harm enablement rates&lt;/li&gt;&lt;li&gt;Emphasized need for safety-focused training and collaboration with healthcare professionals&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Joshua Au Yeung', 'Jacopo Dalmasso', 'Luca Foschini', 'Richard JB Dobson', 'Zeljko Kraljevic']&lt;/li&gt;&lt;li&gt;Tags: ['safety evaluation', 'alignment', 'benchmarking', 'psychogenic risk', 'LLM safety']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.10970</guid><pubDate>Tue, 16 Sep 2025 00:00:00 +0000</pubDate></item><item><title>LLM in the Middle: A Systematic Review of Threats and Mitigations to Real-World LLM-based Systems</title><link>https://arxiv.org/abs/2509.10682</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Systematic review of security threats and mitigations for LLM-based systems&lt;/li&gt;&lt;li&gt;Categorizes threats by severity and real-world scenarios&lt;/li&gt;&lt;li&gt;Maps defense strategies to lifecycle phases and attack types&lt;/li&gt;&lt;li&gt;Aids understanding and mitigation of risks in LLM integration&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Survey&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Vitor Hugo Galhardo Moia', 'Igor Jochem Sanz', 'Gabriel Antonio Fontes Rebello', 'Rodrigo Duarte de Meneses', 'Briland Hitaj', 'Ulf Lindqvist']&lt;/li&gt;&lt;li&gt;Tags: ['LLM security', 'threat modeling', 'mitigation strategies', 'systematic review', 'real-world scenarios']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.10682</guid><pubDate>Tue, 16 Sep 2025 00:00:00 +0000</pubDate></item><item><title>AVEC: Bootstrapping Privacy for Local LLMs</title><link>https://arxiv.org/abs/2509.10561</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces AVEC framework for privacy in local LLMs&lt;/li&gt;&lt;li&gt;Uses adaptive differential privacy budgeting&lt;/li&gt;&lt;li&gt;Verifiable transformations with integrity checks&lt;/li&gt;&lt;li&gt;Formal privacy guarantees and simulation-based evaluation&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Position&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Madhava Gaikwad']&lt;/li&gt;&lt;li&gt;Tags: ['privacy attacks', 'safety evaluation', 'security standards']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.10561</guid><pubDate>Tue, 16 Sep 2025 00:00:00 +0000</pubDate></item><item><title>Uncovering the Vulnerability of Large Language Models in the Financial Domain via Risk Concealment</title><link>https://arxiv.org/abs/2509.10546</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces Risk-Concealment Attacks (RCA) for financial LLMs&lt;/li&gt;&lt;li&gt;Creates FIN-Bench benchmark for financial domain safety evaluation&lt;/li&gt;&lt;li&gt;Demonstrates high attack success rates (93.18% avg) against major LLMs including GPT-4.1 and OpenAI o1&lt;/li&gt;&lt;li&gt;Highlights critical gaps in current alignment techniques for financial applications&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Gang Cheng', 'Haibo Jin', 'Wenbin Zhang', 'Haohan Wang', 'Jun Zhuang']&lt;/li&gt;&lt;li&gt;Tags: ['red teaming', 'financial domain', 'risk concealment', 'alignment', 'safety evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.10546</guid><pubDate>Tue, 16 Sep 2025 00:00:00 +0000</pubDate></item><item><title>EchoLeak: The First Real-World Zero-Click Prompt Injection Exploit in a Production LLM System</title><link>https://arxiv.org/abs/2509.10540</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Case study of EchoLeak, a zero-click prompt injection vulnerability in Microsoft 365 Copilot&lt;/li&gt;&lt;li&gt;Exploit involved chaining multiple bypasses to achieve data exfiltration via email&lt;/li&gt;&lt;li&gt;Analysis of failed defenses and proposed engineering mitigations&lt;/li&gt;&lt;li&gt;Emphasizes need for defense-in-depth and continuous adversarial testing&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Pavan Reddy', 'Aditya Sanjay Gujral']&lt;/li&gt;&lt;li&gt;Tags: ['prompt injection', 'red teaming', 'LLM security', 'data exfiltration', 'zero-click exploit']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.10540</guid><pubDate>Tue, 16 Sep 2025 00:00:00 +0000</pubDate></item><item><title>The Anti-Ouroboros Effect: Emergent Resilience in Large Language Models from Recursive Selective Feedback</title><link>https://arxiv.org/abs/2509.10509</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces selective feedback mechanism in recursive LLM training&lt;/li&gt;&lt;li&gt;Demonstrates performance improvement (Anti-Ouroboros Effect)&lt;/li&gt;&lt;li&gt;Contrasts with degenerative loop in simple models&lt;/li&gt;&lt;li&gt;Suggests emergent resilience for AI safety&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Sai Teja Reddy Adapala']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'robustness', 'safety evaluation', 'AI safety']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.10509</guid><pubDate>Tue, 16 Sep 2025 00:00:00 +0000</pubDate></item><item><title>Co-Alignment: Rethinking Alignment as Bidirectional Human-AI Cognitive Adaptation</title><link>https://arxiv.org/abs/2509.12179</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes Bidirectional Cognitive Alignment (BiCA) for mutual human-AI adaptation&lt;/li&gt;&lt;li&gt;Uses learnable protocols, representation mapping, and KL-budget constraints&lt;/li&gt;&lt;li&gt;Achieves significant improvements in safety (+23% OOD robustness) and collaboration synergy&lt;/li&gt;&lt;li&gt;Shifts alignment paradigm from one-directional to co-alignment&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yubo Li', 'Weiyi Song']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'safety', 'robustness', 'human-ai interaction', 'co-evolution']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.12179</guid><pubDate>Tue, 16 Sep 2025 00:00:00 +0000</pubDate></item><item><title>When Safe Unimodal Inputs Collide: Optimizing Reasoning Chains for Cross-Modal Safety in Multimodal Large Language Models</title><link>https://arxiv.org/abs/2509.12060</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces SSUI dataset for cross-modal safety challenges&lt;/li&gt;&lt;li&gt;Proposes SRPO training framework to align reasoning with safety&lt;/li&gt;&lt;li&gt;Achieves SOTA on safety benchmarks including RSBench&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Wei Cai', 'Shujuan Liu', 'Jian Zhao', 'Ziyan Shi', 'Yusheng Zhao', 'Yuchen Yuan', 'Tianle Zhang', 'Chi Zhang', 'Xuelong Li']&lt;/li&gt;&lt;li&gt;Tags: ['safety', 'alignment', 'multimodal', 'reasoning', 'benchmark']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.12060</guid><pubDate>Tue, 16 Sep 2025 00:00:00 +0000</pubDate></item><item><title>Securing AI Agents: Implementing Role-Based Access Control for Industrial Applications</title><link>https://arxiv.org/abs/2509.11431</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes RBAC framework for securing AI agents in industrial settings&lt;/li&gt;&lt;li&gt;Addresses prompt injection attacks and other security threats&lt;/li&gt;&lt;li&gt;Focuses on on-premises deployment and scalability&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Aadil Gani Ganie']&lt;/li&gt;&lt;li&gt;Tags: ['security', 'access control', 'prompt injection', 'RBAC', 'industrial applications']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.11431</guid><pubDate>Tue, 16 Sep 2025 00:00:00 +0000</pubDate></item><item><title>Tractable Asymmetric Verification for Large Language Models via Deterministic Replicability</title><link>https://arxiv.org/abs/2509.11068</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a verification framework for LLM outputs using deterministic replicability&lt;/li&gt;&lt;li&gt;Enables efficient probabilistic auditing with 12x speedup over full regeneration&lt;/li&gt;&lt;li&gt;Aims to establish computational trust in multi-agent systems&lt;/li&gt;&lt;li&gt;Requires identical hardware/software stacks for replicability&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Zan-Kai Chong', 'Hiroyuki Ohsaki', 'Bryan Ng']&lt;/li&gt;&lt;li&gt;Tags: ['verification', 'security', 'multi-agent systems', 'model trust', 'asymmetric verification']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.11068</guid><pubDate>Tue, 16 Sep 2025 00:00:00 +0000</pubDate></item><item><title>Harmful Prompt Laundering: Jailbreaking LLMs with Abductive Styles and Symbolic Encoding</title><link>https://arxiv.org/abs/2509.10931</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces Harmful Prompt Laundering (HaPLa), a novel jailbreaking technique for LLMs&lt;/li&gt;&lt;li&gt;Uses abductive framing and symbolic encoding to bypass safety filters&lt;/li&gt;&lt;li&gt;Achieves high success rates against GPT models and others&lt;/li&gt;&lt;li&gt;Highlights trade-off between safety and helpfulness in LLM tuning&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Seongho Joo', 'Hyukhun Koh', 'Kyomin Jung']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'jailbreaking', 'adversarial prompting', 'prompt injection', 'alignment']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.10931</guid><pubDate>Tue, 16 Sep 2025 00:00:00 +0000</pubDate></item></channel></rss>