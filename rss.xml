<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>AI Security Paper Digest</title><link>https://kentaroh-toyoda.github.io/ai-security-paper-digest-rss/rss.xml</link><description>Curated papers on AI security from OpenAlex and ArXiv</description><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><lastBuildDate>Tue, 01 Jul 2025 22:34:56 +0000</lastBuildDate><item><title>Revisiting CroPA: A Reproducibility Study and Enhancements for Cross-Prompt Adversarial Transferability in Vision-Language Models</title><link>https://arxiv.org/abs/2506.22982</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Reproducibility study of Cross-Prompt Attack (CroPA) on Vision-Language Models&lt;/li&gt;&lt;li&gt;Proposed improvements: initialization strategy, universal perturbations, attention-based loss&lt;/li&gt;&lt;li&gt;Evaluated on Flamingo, BLIP-2, InstructBLIP, LLaVA&lt;/li&gt;&lt;li&gt;Enhanced adversarial transferability and attack success rate&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Atharv Mittal', 'Agam Pandey', 'Amritanshu Tiwari', 'Sukrit Jindal', 'Swadesh Swain']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial prompting', 'red teaming', 'vision-language models', 'transferability', 'security evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.22982</guid><pubDate>Tue, 01 Jul 2025 00:00:00 +0000</pubDate></item><item><title>Peccavi: Visual Paraphrase Attack Safe and Distortion Free Image Watermarking Technique for AI-Generated Images</title><link>https://arxiv.org/abs/2506.22960</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces PECCAVI, a new image watermarking technique for AI-generated content&lt;/li&gt;&lt;li&gt;Resistant to visual paraphrase attacks that aim to remove watermarks&lt;/li&gt;&lt;li&gt;Embeds watermarks in Non-Melting Points (NMPs) and uses multi-channel frequency domain watermarking&lt;/li&gt;&lt;li&gt;Incorporates noisy burnishing to prevent reverse-engineering of NMPs&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Shreyas Dixit', 'Ashhar Aziz', 'Shashwat Bajpai', 'Vasu Sharma', 'Aman Chadha', 'Vinija Jain', 'Amitava Das']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial attacks', 'watermarking', 'AI security', 'robustness', 'image processing']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.22960</guid><pubDate>Tue, 01 Jul 2025 00:00:00 +0000</pubDate></item><item><title>CP-Guard: A Unified, Probability-Agnostic, and Adaptive Framework for Malicious Agent Detection and Defense in Multi-Agent Embodied Perception Systems</title><link>https://arxiv.org/abs/2506.22890</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes CP-Guard framework for detecting and defending against malicious agents in multi-agent perception systems&lt;/li&gt;&lt;li&gt;Introduces probability-agnostic sample consensus (PASAC) method for verifying agent consistency&lt;/li&gt;&lt;li&gt;Defines collaborative consistency loss (CCLoss) for object detection and BEV segmentation tasks&lt;/li&gt;&lt;li&gt;Implements online adaptive threshold via dual sliding windows for dynamic environments&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Senkang Hu', 'Yihang Tao', 'Guowen Xu', 'Xinyuan Qian', 'Yiqin Deng', 'Xianhao Chen', 'Sam Tak Wu Kwong', 'Yuguang Fang']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial attacks', 'multi-agent systems', 'consensus verification', 'malicious agent detection', 'defense framework']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.22890</guid><pubDate>Tue, 01 Jul 2025 00:00:00 +0000</pubDate></item><item><title>Enhancing the Capability and Robustness of Large Language Models through Reinforcement Learning-Driven Query Refinement</title><link>https://arxiv.org/abs/2407.01461</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a query refinement framework using reinforcement learning to improve LLM response quality&lt;/li&gt;&lt;li&gt;Enhances robustness against jailbreak attacks by refining user prompts before LLM processing&lt;/li&gt;&lt;li&gt;Demonstrates improved response truthfulness, harmlessness, and helpfulness through experiments&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Xiaohua Wang', 'Zisu Huang', 'Feiran Zhang', 'Zhibo Xu', 'Cenyuan Zhang', 'Qi Qian', 'Xiaoqing Zheng', 'Xuanjing Huang']&lt;/li&gt;&lt;li&gt;Tags: ['jailbreaking', 'prompt injection', 'adversarial prompting', 'robustness', 'reinforcement learning']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2407.01461</guid><pubDate>Tue, 01 Jul 2025 00:00:00 +0000</pubDate></item><item><title>Robustness of Misinformation Classification Systems to Adversarial Examples Through BeamAttack</title><link>https://arxiv.org/abs/2506.23661</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Extended BeamAttack with word deletions and optional substitutions&lt;/li&gt;&lt;li&gt;Integrated LIME for better word replacement prioritization&lt;/li&gt;&lt;li&gt;Evaluated on multiple datasets and models (BiLSTM, BERT, RoBERTa)&lt;/li&gt;&lt;li&gt;Achieved over 99% attack success rate while preserving semantic/lexical similarity&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Arnisa Fazla', 'Lucas Krauter', 'David Guzman Piedrahita', 'Andrianos Michail']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial attacks', 'robustness', 'text classification', 'security evaluation', 'red teaming']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.23661</guid><pubDate>Tue, 01 Jul 2025 00:00:00 +0000</pubDate></item><item><title>Scaling Laws for Black box Adversarial Attacks</title><link>https://arxiv.org/abs/2411.16782</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Investigates scaling laws for black-box adversarial attacks using model ensembles&lt;/li&gt;&lt;li&gt;Demonstrates improved transferability with more surrogate models through theory and experiments&lt;/li&gt;&lt;li&gt;Achieves 90%+ attack success on proprietary models like GPT-4o&lt;/li&gt;&lt;li&gt;Observes scaling effects on perturbation interpretability and semantics&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Chuan Liu', 'Huanran Chen', 'Yichi Zhang', 'Yinpeng Dong', 'Jun Zhu']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial attacks', 'black box', 'model ensembling', 'scaling laws', 'LLMs', 'transferability']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2411.16782</guid><pubDate>Tue, 01 Jul 2025 00:00:00 +0000</pubDate></item><item><title>Enhancing Adversarial Robustness through Multi-Objective Representation Learning</title><link>https://arxiv.org/abs/2410.01697</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes MOREL, a multi-objective approach to enhance adversarial robustness&lt;/li&gt;&lt;li&gt;Uses cosine similarity and multi-positive contrastive losses to align natural and adversarial features&lt;/li&gt;&lt;li&gt;Demonstrates significant improvements against white-box and black-box attacks&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Sedjro Salomon Hotegni', 'Sebastian Peitz']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial robustness', 'multi-objective learning', 'representation learning', 'contrastive loss', 'training-time defense']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2410.01697</guid><pubDate>Tue, 01 Jul 2025 00:00:00 +0000</pubDate></item><item><title>Logit-Gap Steering: Efficient Short-Suffix Jailbreaks for Aligned Large Language Models</title><link>https://arxiv.org/abs/2506.24056</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces logit-gap steering for efficient jailbreaking of aligned LLMs&lt;/li&gt;&lt;li&gt;Generates short suffixes that can bypass safety checks with high success rates&lt;/li&gt;&lt;li&gt;Method is significantly faster than beam or gradient-based attacks&lt;/li&gt;&lt;li&gt;Exposes alignment artifacts and reward cliffs in safety-tuned models&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Tung-Ling Li', 'Hongliang Liu']&lt;/li&gt;&lt;li&gt;Tags: ['jailbreaking', 'adversarial prompting', 'red teaming', 'alignment', 'safety evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.24056</guid><pubDate>Tue, 01 Jul 2025 00:00:00 +0000</pubDate></item><item><title>VERA: Variational Inference Framework for Jailbreaking Large Language Models</title><link>https://arxiv.org/abs/2506.22666</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces VERA: Variational infErence fRamework for jAilbreaking&lt;/li&gt;&lt;li&gt;Uses variational inference to train an attacker LLM to approximate target model's posterior over adversarial prompts&lt;/li&gt;&lt;li&gt;Generates diverse, fluent jailbreak prompts without re-optimization&lt;/li&gt;&lt;li&gt;Demonstrates strong performance across multiple LLMs&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Anamika Lochab', 'Lu Yan', 'Patrick Pynadath', 'Xiangyu Zhang', 'Ruqi Zhang']&lt;/li&gt;&lt;li&gt;Tags: ['jailbreaking', 'adversarial prompting', 'variational inference', 'black-box attacks', 'LLM security']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.22666</guid><pubDate>Tue, 01 Jul 2025 00:00:00 +0000</pubDate></item><item><title>MetaCipher: A General and Extensible Reinforcement Learning Framework for Obfuscation-Based Jailbreak Attacks on Black-Box LLMs</title><link>https://arxiv.org/abs/2506.22557</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces MetaCipher, a reinforcement learning-based framework for obfuscation-based jailbreak attacks on LLMs&lt;/li&gt;&lt;li&gt;Employs dynamic cipher selection to adaptively choose optimal encryption strategies&lt;/li&gt;&lt;li&gt;Achieves high attack success rates (92%+ on non-reasoning LLMs, 74%+ on reasoning LLMs) with few queries&lt;/li&gt;&lt;li&gt;Modular and extensible design supports various cipher families and adversarial strategies&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Boyuan Chen', 'Minghao Shao', 'Abdul Basit', 'Siddharth Garg', 'Muhammad Shafique']&lt;/li&gt;&lt;li&gt;Tags: ['jailbreaking', 'obfuscation', 'reinforcement_learning', 'adversarial_attacks', 'LLM_security']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.22557</guid><pubDate>Tue, 01 Jul 2025 00:00:00 +0000</pubDate></item><item><title>Fragile, Robust, and Antifragile: A Perspective from Parameter Responses in Reinforcement Learning Under Stress</title><link>https://arxiv.org/abs/2506.23036</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Analyzes RL policy parameters under internal and external stress&lt;/li&gt;&lt;li&gt;Uses adversarial attacks for external stress&lt;/li&gt;&lt;li&gt;Classifies parameters as fragile, robust, or antifragile&lt;/li&gt;&lt;li&gt;Validates on PPO agents in Mujoco environments&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Zain ul Abdeen', 'Ming Jin']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial attacks', 'robustness', 'reinforcement learning', 'parameter analysis']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.23036</guid><pubDate>Tue, 01 Jul 2025 00:00:00 +0000</pubDate></item><item><title>No, of course I can! Refusal Mechanisms Can Be Exploited Using Harmless Fine-Tuning Data</title><link>https://arxiv.org/abs/2502.19537</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Demonstrates that existing fine-tuning attacks are shallow and can be blocked by checking initial tokens&lt;/li&gt;&lt;li&gt;Introduces a 'refuse-then-comply' attack that first refuses then provides harmful responses, bypassing shallow defenses&lt;/li&gt;&lt;li&gt;Achieves high success rates against GPT-4o and Claude Haiku, earning a bug bounty from OpenAI&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Joshua Kazdan', 'Abhay Puri', 'Rylan Schaeffer', 'Lisa Yu', 'Chris Cundy', 'Jason Stanley', 'Sanmi Koyejo', 'Krishnamurthy Dvijotham']&lt;/li&gt;&lt;li&gt;Tags: ['red teaming', 'jailbreak', 'adversarial prompting', 'fine-tuning', 'safety']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2502.19537</guid><pubDate>Tue, 01 Jul 2025 00:00:00 +0000</pubDate></item><item><title>Investigating the Impact of Quantization Methods on the Safety and Reliability of Large Language Models</title><link>https://arxiv.org/abs/2502.15799</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduced OpenMiniSafety dataset for safety evaluation&lt;/li&gt;&lt;li&gt;Evaluated 66 quantized LLM variants across multiple safety benchmarks&lt;/li&gt;&lt;li&gt;Found that quantization methods can degrade safety alignment&lt;/li&gt;&lt;li&gt;No single method consistently outperforms others in safety&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Artyom Kharinaev', 'Viktor Moskvoretskii', 'Egor Shvetsov', 'Kseniia Studenikina', 'Bykov Mikhail', 'Evgeny Burnaev']&lt;/li&gt;&lt;li&gt;Tags: ['safety evaluation', 'model quantization', 'alignment', 'robustness', 'benchmarking']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2502.15799</guid><pubDate>Tue, 01 Jul 2025 00:00:00 +0000</pubDate></item><item><title>Riddle Me This! Stealthy Membership Inference for Retrieval-Augmented Generation</title><link>https://arxiv.org/abs/2502.00306</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Presents Interrogation Attack (IA) for stealthy membership inference in RAG systems&lt;/li&gt;&lt;li&gt;Uses natural-text queries to avoid detection&lt;/li&gt;&lt;li&gt;Achieves high true positive rate with low false positives&lt;/li&gt;&lt;li&gt;Cost-effective and outperforms existing methods&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Ali Naseh', 'Yuefeng Peng', 'Anshuman Suri', 'Harsh Chaudhari', 'Alina Oprea', 'Amir Houmansadr']&lt;/li&gt;&lt;li&gt;Tags: ['membership inference', 'RAG', 'stealthy attacks', 'adversarial prompting', 'privacy attacks']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2502.00306</guid><pubDate>Tue, 01 Jul 2025 00:00:00 +0000</pubDate></item><item><title>GenBFA: An Evolutionary Optimization Approach to Bit-Flip Attacks on LLMs</title><link>https://arxiv.org/abs/2411.13757</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces AttentionBreaker and GenBFA frameworks for efficient bit-flip attacks on LLMs&lt;/li&gt;&lt;li&gt;Demonstrates catastrophic performance degradation with as few as three bit-flips&lt;/li&gt;&lt;li&gt;Highlights significant security vulnerability in large language models&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Sanjay Das', 'Swastik Bhattacharya', 'Souvik Kundu', 'Shamik Kundu', 'Anand Menon', 'Arnab Raha', 'Kanad Basu']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial attacks', 'LLM security', 'bit-flip attacks', 'evolutionary optimization', 'hardware security']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2411.13757</guid><pubDate>Tue, 01 Jul 2025 00:00:00 +0000</pubDate></item><item><title>Adversarial Robustness Unhardening via Backdoor Attacks in Federated Learning</title><link>https://arxiv.org/abs/2310.11594</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces Adversarial Robustness Unhardening (ARU) attack in federated learning&lt;/li&gt;&lt;li&gt;ARU undermines adversarial training's defenses against evasion attacks&lt;/li&gt;&lt;li&gt;Shows ARU can bypass existing robust aggregation defenses&lt;/li&gt;&lt;li&gt;Highlights security challenges in federated learning's intersection with adversarial training&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Taejin Kim', 'Jiarui Li', 'Shubhranshu Singh', 'Nikhil Madaan', 'Carlee Joe-Wong']&lt;/li&gt;&lt;li&gt;Tags: ['federated learning', 'backdoor attacks', 'adversarial training', 'poisoning', 'robustness', 'security standards']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2310.11594</guid><pubDate>Tue, 01 Jul 2025 00:00:00 +0000</pubDate></item><item><title>TRiSM for Agentic AI: A Review of Trust, Risk, and Security Management in LLM-based Agentic Multi-Agent Systems</title><link>https://arxiv.org/abs/2506.04133</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Reviews Trust, Risk, and Security Management (TRiSM) for LLM-based Agentic Multi-Agent Systems (AMAS)&lt;/li&gt;&lt;li&gt;Introduces a novel risk taxonomy capturing adversarial manipulation and other unique threats&lt;/li&gt;&lt;li&gt;Discusses security strategies including encryption, adversarial robustness, and regulatory compliance&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Survey&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Shaina Raza', 'Ranjan Sapkota', 'Manoj Karkee', 'Christos Emmanouilidis']&lt;/li&gt;&lt;li&gt;Tags: ['security', 'safety', 'red teaming', 'adversarial prompting', 'privacy', 'risk management']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.04133</guid><pubDate>Tue, 01 Jul 2025 00:00:00 +0000</pubDate></item><item><title>CoT Red-Handed: Stress Testing Chain-of-Thought Monitoring</title><link>https://arxiv.org/abs/2505.23575</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Investigates CoT monitoring for detecting harmful actions in AI models.&lt;/li&gt;&lt;li&gt;Compares CoT monitoring to action-only monitoring in a red-teaming setup.&lt;/li&gt;&lt;li&gt;Introduces a hybrid monitoring protocol combining reasoning and output scores.&lt;/li&gt;&lt;li&gt;Shows improved detection rates, especially in subtle deception scenarios.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Benjamin Arnav', "Pablo Bernabeu-P\\'erez", 'Nathan Helm-Burger', 'Tim Kostolansky', 'Hannes Whittingham', 'Mary Phuong']&lt;/li&gt;&lt;li&gt;Tags: ['red teaming', 'monitoring', 'chain-of-thought', 'safety evaluation', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2505.23575</guid><pubDate>Tue, 01 Jul 2025 00:00:00 +0000</pubDate></item><item><title>Trust &amp; Safety of LLMs and LLMs in Trust &amp; Safety</title><link>https://arxiv.org/abs/2412.02113</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Systematic review of LLM trust and safety research&lt;/li&gt;&lt;li&gt;Focuses on using LLMs in Trust and Safety domains&lt;/li&gt;&lt;li&gt;Identifies key challenges and solutions&lt;/li&gt;&lt;li&gt;Discusses prompt injection and jailbreak attacks&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Survey&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Doohee You', 'Dan Chon']&lt;/li&gt;&lt;li&gt;Tags: ['jailbreaking', 'prompt injection', 'safety evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2412.02113</guid><pubDate>Tue, 01 Jul 2025 00:00:00 +0000</pubDate></item><item><title>STACK: Adversarial Attacks on LLM Safeguard Pipelines</title><link>https://arxiv.org/abs/2506.24068</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Developed a few-shot-prompted classifier that outperforms ShieldGemma in safeguarding against misuse&lt;/li&gt;&lt;li&gt;Introduced STACK, a staged attack procedure achieving 71% ASR in black-box attacks&lt;/li&gt;&lt;li&gt;Evaluated transferability with 33% ASR, showing feasibility of attacks without pipeline access&lt;/li&gt;&lt;li&gt;Suggested specific mitigations against staged attacks&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Ian R. McKenzie', 'Oskar J. Hollinsworth', 'Tom Tseng', 'Xander Davies', 'Stephen Casper', 'Aaron D. Tucker', 'Robert Kirk', 'Adam Gleave']&lt;/li&gt;&lt;li&gt;Tags: ['red teaming', 'adversarial attacks', 'safety evaluation', 'model extraction', 'LLM safeguards']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.24068</guid><pubDate>Tue, 01 Jul 2025 00:00:00 +0000</pubDate></item><item><title>AutoEvoEval: An Automated Framework for Evolving Close-Ended LLM Evaluation Data</title><link>https://arxiv.org/abs/2506.23735</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes AutoEvoEval, an evolution-based framework for generating adversarial test cases for LLMs&lt;/li&gt;&lt;li&gt;Introduces 22 atomic evolution operations and multi-round compositions&lt;/li&gt;&lt;li&gt;Experiments show significant accuracy drops with combined perturbations&lt;/li&gt;&lt;li&gt;Highlights need for evolution-aware robustness evaluation&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['JiaRu Wu', 'Mingwei Liu']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial prompting', 'red teaming', 'robustness evaluation', 'LLM security', 'evolutionary testing']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.23735</guid><pubDate>Tue, 01 Jul 2025 00:00:00 +0000</pubDate></item><item><title>SoK: Semantic Privacy in Large Language Models</title><link>https://arxiv.org/abs/2506.23603</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces a lifecycle-centric framework for semantic privacy in LLMs&lt;/li&gt;&lt;li&gt;Categorizes key attack vectors and evaluates current defenses&lt;/li&gt;&lt;li&gt;Identifies gaps in semantic-level protection&lt;/li&gt;&lt;li&gt;Outlines open challenges for future research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Survey&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Baihe Ma', 'Yanna Jiang', 'Xu Wang', 'Guangshen Yu', 'Qin Wang', 'Caijun Sun', 'Chen Li', 'Xuelei Qi', 'Ying He', 'Wei Ni', 'Ren Ping Liu']&lt;/li&gt;&lt;li&gt;Tags: ['semantic privacy', 'LLM security', 'privacy attacks', 'defense mechanisms', 'lifecycle analysis']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.23603</guid><pubDate>Tue, 01 Jul 2025 00:00:00 +0000</pubDate></item><item><title>TuCo: Measuring the Contribution of Fine-Tuning to Individual Responses of LLMs</title><link>https://arxiv.org/abs/2506.23423</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces TuCo metric to measure fine-tuning contribution to LLM responses&lt;/li&gt;&lt;li&gt;Analyzes adversarial attacks showing lower TuCo on successful attacks&lt;/li&gt;&lt;li&gt;Provides insights into how fine-tuning affects model safety&lt;/li&gt;&lt;li&gt;Enables quantitative study of fine-tuning's impact on individual outputs&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Felipe Nuti', 'Tim Franzmeyer', 'Jo\\~ao Henriques']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial prompting', 'safety evaluation', 'model robustness', 'fine-tuning analysis']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.23423</guid><pubDate>Tue, 01 Jul 2025 00:00:00 +0000</pubDate></item><item><title>Securing AI Systems: A Guide to Known Attacks and Impacts</title><link>https://arxiv.org/abs/2506.23296</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Provides an overview of 11 major AI-specific attack types&lt;/li&gt;&lt;li&gt;Links attacks to CIA security impacts (confidentiality, integrity, availability)&lt;/li&gt;&lt;li&gt;Aimed at equipping practitioners with foundational AI security knowledge&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Survey&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Naoto Kiribuchi', 'Kengo Zenitani', 'Takayuki Semitsu']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial attacks', 'AI security', 'red teaming', 'CIA triad', 'security threats']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.23296</guid><pubDate>Tue, 01 Jul 2025 00:00:00 +0000</pubDate></item><item><title>From Prompt Injections to Protocol Exploits: Threats in LLM-Powered AI Agents Workflows</title><link>https://arxiv.org/abs/2506.23260</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces a comprehensive threat model for LLM-powered AI agents&lt;/li&gt;&lt;li&gt;Categorizes attacks into four domains: Input Manipulation, Model Compromise, System/Privacy Attacks, Protocol Vulnerabilities&lt;/li&gt;&lt;li&gt;Reviews existing defenses and identifies open challenges&lt;/li&gt;&lt;li&gt;Guides future research in securing LLM agent workflows&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Survey&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Mohamed Amine Ferrag', 'Norbert Tihanyi', 'Djallel Hamouda', 'Leandros Maglaras', 'Merouane Debbah']&lt;/li&gt;&lt;li&gt;Tags: ['prompt injection', 'model compromise', 'protocol exploits', 'threat model', 'LLM security']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.23260</guid><pubDate>Tue, 01 Jul 2025 00:00:00 +0000</pubDate></item><item><title>Agent-to-Agent Theory of Mind: Testing Interlocutor Awareness among Large Language Models</title><link>https://arxiv.org/abs/2506.22957</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Formalizes interlocutor awareness in LLMs across reasoning, style, and alignment&lt;/li&gt;&lt;li&gt;Shows LLMs can identify same-family peers and prominent model families&lt;/li&gt;&lt;li&gt;Case studies demonstrate both enhanced collaboration and new safety vulnerabilities&lt;/li&gt;&lt;li&gt;Highlights increased jailbreak susceptibility and reward-hacking behaviors&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Younwoo Choi', 'Changling Li', 'Yongjin Yang', 'Zhijing Jin']&lt;/li&gt;&lt;li&gt;Tags: ['jailbreaking', 'red teaming', 'safety evaluation', 'alignment', 'multi-agent systems']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.22957</guid><pubDate>Tue, 01 Jul 2025 00:00:00 +0000</pubDate></item><item><title>Teaching Models to Verbalize Reward Hacking in Chain-of-Thought Reasoning</title><link>https://arxiv.org/abs/2506.22777</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces Verbalization Fine-Tuning (VFT) to make models verbalize reward hacking behavior&lt;/li&gt;&lt;li&gt;Evaluates VFT by training models with RL in environments with misleading prompt cues&lt;/li&gt;&lt;li&gt;Shows significant reduction in undetected reward hacks (6% vs 88% without VFT)&lt;/li&gt;&lt;li&gt;Increases verbalization of cue influence from 8% to 42% after VFT&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Miles Turpin', 'Andy Arditi', 'Marvin Li', 'Joe Benton', 'Julian Michael']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'safety', 'reward_hacking', 'transparency', 'chain_of_thought']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.22777</guid><pubDate>Tue, 01 Jul 2025 00:00:00 +0000</pubDate></item><item><title>Kill Two Birds with One Stone! Trajectory enabled Unified Online Detection of Adversarial Examples and Backdoor Attacks</title><link>https://arxiv.org/abs/2506.22722</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes UniGuard, a unified online detection framework for adversarial examples and backdoor attacks&lt;/li&gt;&lt;li&gt;Detects distinctive trajectory signatures through model layers using LSTM and spectrum analysis&lt;/li&gt;&lt;li&gt;Validated across image, text, and audio modalities with superior performance vs. SOTA&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Anmin Fu', 'Fanyu Meng', 'Huaibing Peng', 'Hua Ma', 'Zhi Zhang', 'Yifeng Zheng', 'Willy Susilo', 'Yansong Gao']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial detection', 'backdoor detection', 'online detection', 'trajectory analysis', 'deep learning security']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.22722</guid><pubDate>Tue, 01 Jul 2025 00:00:00 +0000</pubDate></item><item><title>A Survey on Model Extraction Attacks and Defenses for Large Language Models</title><link>https://arxiv.org/abs/2506.22521</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Comprehensive survey on model extraction attacks and defenses for LLMs&lt;/li&gt;&lt;li&gt;Categorizes attacks into functionality, training data, and prompt-targeted&lt;/li&gt;&lt;li&gt;Examines defense mechanisms across model, data, and prompt protection&lt;/li&gt;&lt;li&gt;Proposes specialized metrics and research directions&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Survey&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Kaixiang Zhao', 'Lincan Li', 'Kaize Ding', 'Neil Zhenqiang Gong', 'Yue Zhao', 'Yushun Dong']&lt;/li&gt;&lt;li&gt;Tags: ['model extraction', 'security', 'defense', 'LLM', 'survey']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.22521</guid><pubDate>Tue, 01 Jul 2025 00:00:00 +0000</pubDate></item><item><title>SABRE-FL: Selective and Accurate Backdoor Rejection for Federated Prompt Learning</title><link>https://arxiv.org/abs/2506.22506</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;First study of backdoor attacks in Federated Prompt Learning&lt;/li&gt;&lt;li&gt;Proposes SABRE-FL defense using embedding-space anomaly detection&lt;/li&gt;&lt;li&gt;Filters poisoned prompt updates without raw client data&lt;/li&gt;&lt;li&gt;Empirically outperforms baselines in reducing backdoor accuracy&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Momin Ahmad Khan', 'Yasra Chandio', 'Fatima Muhammad Anwar']&lt;/li&gt;&lt;li&gt;Tags: ['backdoor attacks', 'federated learning', 'anomaly detection', 'prompt injection', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.22506</guid><pubDate>Tue, 01 Jul 2025 00:00:00 +0000</pubDate></item><item><title>Report on NSF Workshop on Science of Safe AI</title><link>https://arxiv.org/abs/2506.22492</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Workshop report on AI safety challenges and research agenda&lt;/li&gt;&lt;li&gt;Emphasizes transparency, safety guarantees, and trustworthy AI&lt;/li&gt;&lt;li&gt;Covers autonomous systems, chatbots, and clinical applications&lt;/li&gt;&lt;li&gt;Aims to develop theory, methods, and tools for safe AI&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Position&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Rajeev Alur', 'Greg Durrett', 'Hadas Kress-Gazit', 'Corina P\\u{a}s\\u{a}reanu', "Ren\\'e Vidal"]&lt;/li&gt;&lt;li&gt;Tags: ['AI safety', 'research agenda', 'workshop report', 'safety evaluation', 'trustworthy AI']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.22492</guid><pubDate>Tue, 01 Jul 2025 00:00:00 +0000</pubDate></item><item><title>Hierarchical Adversarially-Resilient Multi-Agent Reinforcement Learning for Cyber-Physical Systems Security</title><link>https://arxiv.org/abs/2506.22445</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces HAMARL framework for CPS security using hierarchical multi-agent RL&lt;/li&gt;&lt;li&gt;Incorporates adversarial training to simulate evolving cyber threats&lt;/li&gt;&lt;li&gt;Outperforms traditional approaches in detection accuracy and response times&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Saad Alqithami']&lt;/li&gt;&lt;li&gt;Tags: ['red teaming', 'adversarial training', 'multi-agent systems', 'security', 'reinforcement learning']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.22445</guid><pubDate>Tue, 01 Jul 2025 00:00:00 +0000</pubDate></item><item><title>AI Risk-Management Standards Profile for General-Purpose AI (GPAI) and Foundation Models</title><link>https://arxiv.org/abs/2506.23949</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Provides risk-management practices for GPAI/foundation models&lt;/li&gt;&lt;li&gt;Adapts NIST AI Risk Management Framework and ISO/IEC 23894&lt;/li&gt;&lt;li&gt;Targets developers of large-scale GPAI models&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Position&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Anthony M. Barrett', 'Jessica Newman', 'Brandie Nonnecke', 'Nada Madkour', 'Dan Hendrycks', 'Evan R. Murphy', 'Krystal Jackson', 'Deepika Raman']&lt;/li&gt;&lt;li&gt;Tags: ['security standards', 'risk management', 'foundation models', 'GPAI', 'developer guidelines']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.23949</guid><pubDate>Tue, 01 Jul 2025 00:00:00 +0000</pubDate></item><item><title>A Survey on Autonomy-Induced Security Risks in Large Model-Based Agents</title><link>https://arxiv.org/abs/2506.23844</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Survey of security risks in autonomous AI agents&lt;/li&gt;&lt;li&gt;Discusses memory poisoning, tool misuse, reward hacking, and misalignment&lt;/li&gt;&lt;li&gt;Reviews defense strategies across different autonomy layers&lt;/li&gt;&lt;li&gt;Introduces Reflective Risk-Aware Agent Architecture (R2A2)&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Survey&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Hang Su', 'Jun Luo', 'Chang Liu', 'Xiao Yang', 'Yichi Zhang', 'Yinpeng Dong', 'Jun Zhu']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'robustness', 'safety evaluation', 'agent security', 'survey']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.23844</guid><pubDate>Tue, 01 Jul 2025 00:00:00 +0000</pubDate></item><item><title>Attestable Audits: Verifiable AI Safety Benchmarks Using Trusted Execution Environments</title><link>https://arxiv.org/abs/2506.23706</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes Attestable Audits using Trusted Execution Environments&lt;/li&gt;&lt;li&gt;Enables verifiable and confidential safety benchmarking&lt;/li&gt;&lt;li&gt;Protects model IP and dataset confidentiality&lt;/li&gt;&lt;li&gt;Demonstrated on Llama-3.1 benchmarks&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Christoph Schnabl', 'Daniel Hugenroth', 'Bill Marino', 'Alastair R. Beresford']&lt;/li&gt;&lt;li&gt;Tags: ['safety evaluation', 'security', 'trusted execution environments', 'benchmarks', 'model verification']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.23706</guid><pubDate>Tue, 01 Jul 2025 00:00:00 +0000</pubDate></item><item><title>Evaluating Multi-Agent Defences Against Jailbreaking Attacks on Large Language Models</title><link>https://arxiv.org/abs/2506.23576</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Evaluates multi-agent LLM defenses against jailbreaking attacks&lt;/li&gt;&lt;li&gt;Compares single-agent vs multi-agent setups (2/3 agents)&lt;/li&gt;&lt;li&gt;Reduces false negatives but increases false positives and computational overhead&lt;/li&gt;&lt;li&gt;Highlights limitations of current automated defenses and suggests future directions&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Maria Carolina Cornelia Wit', 'Jun Pang']&lt;/li&gt;&lt;li&gt;Tags: ['jailbreaking', 'red teaming', 'multi-agent', 'defense evaluation', 'alignment robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.23576</guid><pubDate>Tue, 01 Jul 2025 00:00:00 +0000</pubDate></item></channel></rss>