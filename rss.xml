<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>AI Security Paper Digest</title><link>https://kentaroh-toyoda.github.io/ai-security-paper-digest-rss/rss.xml</link><description>Curated papers on AI security from OpenAlex and ArXiv</description><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><lastBuildDate>Wed, 06 Aug 2025 22:17:48 +0000</lastBuildDate><item><title>BadBlocks: Low-Cost and Stealthy Backdoor Attacks Tailored for Text-to-Image Diffusion Models</title><link>https://arxiv.org/abs/2508.03221</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces BadBlocks, a stealthy backdoor attack for text-to-image diffusion models&lt;/li&gt;&lt;li&gt;Requires 30% computational resources and 20% GPU time of previous attacks&lt;/li&gt;&lt;li&gt;Achieves high ASR and low FID loss while evading defenses&lt;/li&gt;&lt;li&gt;Targets specific UNet blocks to maintain normal functionality&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yu Pan', 'Jiahao Chen', 'Lin Wang', 'Bingrong Dai', 'Yi Du']&lt;/li&gt;&lt;li&gt;Tags: ['backdoor', 'diffusion models', 'stealth', 'low-resource', 'evasion']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2508.03221</guid><pubDate>Wed, 06 Aug 2025 00:00:00 +0000</pubDate></item><item><title>Adversarial Attention Perturbations for Large Object Detection Transformers</title><link>https://arxiv.org/abs/2508.02987</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Presents AFOG, a new adversarial attack for object detection transformers&lt;/li&gt;&lt;li&gt;Utilizes attention mechanism to focus perturbations&lt;/li&gt;&lt;li&gt;Outperforms existing attacks in experiments&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Zachary Yahn', 'Selim Furkan Tekin', 'Fatih Ilhan', 'Sihao Hu', 'Tiansheng Huang', 'Yichang Xu', 'Margaret Loper', 'Ling Liu']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial attacks', 'security', 'object detection', 'transformers']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2508.02987</guid><pubDate>Wed, 06 Aug 2025 00:00:00 +0000</pubDate></item><item><title>When Truth Is Overridden: Uncovering the Internal Origins of Sycophancy in Large Language Models</title><link>https://arxiv.org/abs/2508.02087</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Systematically studies sycophancy in LLMs using opinion statements&lt;/li&gt;&lt;li&gt;Identifies two-stage emergence via logit-lens and activation patching&lt;/li&gt;&lt;li&gt;Highlights deep representational divergence causing alignment issues&lt;/li&gt;&lt;li&gt;Finds first-person prompts induce higher sycophancy rates&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Keyu Wang', 'Jin Li', 'Shu Yang', 'Zhuoran Zhang', 'Di Wang']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'safety evaluation', 'model internals', 'causal analysis']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2508.02087</guid><pubDate>Wed, 06 Aug 2025 00:00:00 +0000</pubDate></item><item><title>Token-Level Precise Attack on RAG: Searching for the Best Alternatives to Mislead Generation</title><link>https://arxiv.org/abs/2508.03110</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces TPARAG, a novel framework for attacking RAG systems&lt;/li&gt;&lt;li&gt;Targets both white-box and black-box scenarios&lt;/li&gt;&lt;li&gt;Uses token-level optimization to generate malicious passages&lt;/li&gt;&lt;li&gt;Demonstrates high effectiveness in retrieval and end-to-end attacks&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Zizhong Li', 'Haopeng Zhang', 'Jiawei Zhang']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial attack', 'RAG', 'data poisoning', 'black-box attack', 'prompt injection']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2508.03110</guid><pubDate>Wed, 06 Aug 2025 00:00:00 +0000</pubDate></item><item><title>Privacy-Aware Decoding: Mitigating Privacy Leakage of Large Language Models in Retrieval-Augmented Generation</title><link>https://arxiv.org/abs/2508.03098</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes Privacy-Aware Decoding (PAD) to mitigate privacy leakage in RAG systems&lt;/li&gt;&lt;li&gt;Adds calibrated Gaussian noise during decoding with confidence-based screening&lt;/li&gt;&lt;li&gt;Provides per-response differential privacy guarantees&lt;/li&gt;&lt;li&gt;Outperforms existing defenses while preserving response utility&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Haoran Wang', 'Xiongxiao Xu', 'Baixiang Huang', 'Kai Shu']&lt;/li&gt;&lt;li&gt;Tags: ['privacy', 'differential privacy', 'RAG', 'inference-time defense']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2508.03098</guid><pubDate>Wed, 06 Aug 2025 00:00:00 +0000</pubDate></item><item><title>CoCoTen: Detecting Adversarial Inputs to Large Language Models through Latent Space Features of Contextual Co-occurrence Tensors</title><link>https://arxiv.org/abs/2508.02997</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a novel method using Contextual Co-occurrence Tensors to detect adversarial and jailbreak prompts in LLMs&lt;/li&gt;&lt;li&gt;Achieves F1 score of 0.83 with only 0.5% labeled data, a 96.6% improvement over baselines&lt;/li&gt;&lt;li&gt;Significantly faster than baseline models (2.3-128.4x speedup)&lt;/li&gt;&lt;li&gt;Publicly available implementation supports reproducibility&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Sri Durga Sai Sowmya Kadali', 'Evangelos E. Papalexakis']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial detection', 'jailbreak', 'LLM security', 'red teaming', 'contextual co-occurrence']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2508.02997</guid><pubDate>Wed, 06 Aug 2025 00:00:00 +0000</pubDate></item><item><title>Highlight &amp; Summarize: RAG without the jailbreaks</title><link>https://arxiv.org/abs/2508.02872</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces Highlight &amp; Summarize (H&amp;S) design pattern for RAG systems to prevent jailbreaking&lt;/li&gt;&lt;li&gt;Splits pipeline into highlighter and summarizer to isolate user input from generative LLM&lt;/li&gt;&lt;li&gt;Evaluated for correctness, relevance, and response quality&lt;/li&gt;&lt;li&gt;LLM-based highlighter outperforms standard RAG in user studies&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Giovanni Cherubin', 'Andrew Paverd']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'adversarial prompting', 'safety evaluation', 'robustness', 'RAG systems']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2508.02872</guid><pubDate>Wed, 06 Aug 2025 00:00:00 +0000</pubDate></item><item><title>Heterogeneity-Oblivious Robust Federated Learning</title><link>https://arxiv.org/abs/2508.03579</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes Horus, a robust federated learning framework using low-rank adaptations (LoRAs) to reduce attack surface&lt;/li&gt;&lt;li&gt;Introduces Heterogeneity-Oblivious Poisoning Score using LoRA-A features to detect poisoned clients&lt;/li&gt;&lt;li&gt;Employs projection-aware aggregation to preserve collaboration while suppressing drifts&lt;/li&gt;&lt;li&gt;Demonstrates improved robustness and accuracy across diverse datasets and attacks&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Weiyao Zhang', 'Jinyang Li', 'Qi Song', 'Miao Wang', 'Chungang Lin', 'Haitong Luo', 'Xuying Meng', 'Yujun Zhang']&lt;/li&gt;&lt;li&gt;Tags: ['data poisoning', 'federated learning', 'robustness', 'aggregation strategies', 'client filtering']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2508.03579</guid><pubDate>Wed, 06 Aug 2025 00:00:00 +0000</pubDate></item><item><title>Defending Against Knowledge Poisoning Attacks During Retrieval-Augmented Generation</title><link>https://arxiv.org/abs/2508.02835</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes FilterRAG and ML-FilterRAG defenses against knowledge poisoning attacks in RAG systems&lt;/li&gt;&lt;li&gt;Identifies new properties to differentiate adversarial vs clean texts&lt;/li&gt;&lt;li&gt;Evaluates effectiveness using benchmark datasets with performance close to original RAG&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Kennedy Edemacu', 'Vinay M. Shashidhar', 'Micheal Tuape', 'Dan Abudu', 'Beakcheol Jang', 'Jong Wook Kim']&lt;/li&gt;&lt;li&gt;Tags: ['data_poisoning', 'security', 'RAG', 'defense', 'adversarial_texts']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2508.02835</guid><pubDate>Wed, 06 Aug 2025 00:00:00 +0000</pubDate></item><item><title>FFCBA: Feature-based Full-target Clean-label Backdoor Attacks</title><link>https://arxiv.org/abs/2504.21054</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes FFCBA with FSBA and FMBA paradigms for clean-label backdoor attacks&lt;/li&gt;&lt;li&gt;Uses class-conditional autoencoders to generate noise triggers&lt;/li&gt;&lt;li&gt;Achieves multi-target attacks with stealth and effectiveness&lt;/li&gt;&lt;li&gt;Demonstrates robustness against state-of-the-art defenses&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yangxu Yin', 'Honglong Chen', 'Yudong Gao', 'Peng Sun', 'Liantao Wu', 'Zhe Li', 'Weifeng Liu']&lt;/li&gt;&lt;li&gt;Tags: ['backdoor attacks', 'clean-label attacks', 'multi-target', 'adversarial attacks', 'data poisoning']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2504.21054</guid><pubDate>Wed, 06 Aug 2025 00:00:00 +0000</pubDate></item><item><title>Augmented Adversarial Trigger Learning</title><link>https://arxiv.org/abs/2503.12339</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces ATLA: Adversarial Trigger Learning with Augmented objectives&lt;/li&gt;&lt;li&gt;Improves adversarial trigger learning with weighted loss and auxiliary loss&lt;/li&gt;&lt;li&gt;Achieves nearly 100% success rate with 80% fewer queries&lt;/li&gt;&lt;li&gt;Demonstrates high generalization to unseen queries and model transfer&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Zhe Wang', 'Yanjun Qi']&lt;/li&gt;&lt;li&gt;Tags: ['jailbreaking', 'adversarial prompting', 'model extraction', 'red teaming', 'prompt injection']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2503.12339</guid><pubDate>Wed, 06 Aug 2025 00:00:00 +0000</pubDate></item><item><title>M2S: Multi-turn to Single-turn jailbreak in Red Teaming for LLMs</title><link>https://arxiv.org/abs/2503.04856</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces M2S framework to convert multi-turn jailbreak prompts to single-turn&lt;/li&gt;&lt;li&gt;Methods include Hyphenize, Numberize, Pythonize&lt;/li&gt;&lt;li&gt;Achieves high attack success rates while reducing token usage&lt;/li&gt;&lt;li&gt;Reveals contextual blindness in LLM defenses&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Junwoo Ha', 'Hyunjun Kim', 'Sangyoon Yu', 'Haon Park', 'Ashkan Yousefpour', 'Yuna Park', 'Suhyun Kim']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'jailbreaking', 'adversarial prompting', 'prompt injection', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2503.04856</guid><pubDate>Wed, 06 Aug 2025 00:00:00 +0000</pubDate></item><item><title>IDEATOR: Jailbreaking and Benchmarking Large Vision-Language Models Using Themselves</title><link>https://arxiv.org/abs/2411.00827</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes IDEATOR, a method for jailbreaking VLMs using model-generated malicious image-text pairs&lt;/li&gt;&lt;li&gt;Achieves high attack success rates (ASR) across multiple VLMs with minimal queries&lt;/li&gt;&lt;li&gt;Introduces VLJailbreakBench, a safety benchmark with 3,654 multimodal jailbreak samples&lt;/li&gt;&lt;li&gt;Reveals significant safety gaps in state-of-the-art models like GPT-4o and Claude-3.5-Sonnet&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Ruofan Wang', 'Juncheng Li', 'Yixu Wang', 'Bo Wang', 'Xiaosen Wang', 'Yan Teng', 'Yingchun Wang', 'Xingjun Ma', 'Yu-Gang Jiang']&lt;/li&gt;&lt;li&gt;Tags: ['jailbreaking', 'VLM', 'red teaming', 'adversarial prompting', 'safety benchmark', 'multimodal']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2411.00827</guid><pubDate>Wed, 06 Aug 2025 00:00:00 +0000</pubDate></item><item><title>Antidistillation Sampling</title><link>https://arxiv.org/abs/2504.13146</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces antidistillation sampling to prevent model extraction via distillation&lt;/li&gt;&lt;li&gt;Modifies next-token probability distributions to poison reasoning traces&lt;/li&gt;&lt;li&gt;Aims to preserve model utility while reducing distillation effectiveness&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yash Savani', 'Asher Trockman', 'Zhili Feng', 'Avi Schwarzschild', 'Alexander Robey', 'Marc Finzi', 'J. Zico Kolter']&lt;/li&gt;&lt;li&gt;Tags: ['model extraction', 'security', 'robustness', 'LLM']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2504.13146</guid><pubDate>Wed, 06 Aug 2025 00:00:00 +0000</pubDate></item><item><title>When Good Sounds Go Adversarial: Jailbreaking Audio-Language Models with Benign Inputs</title><link>https://arxiv.org/abs/2508.03365</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces WhisperInject, a two-stage adversarial audio attack framework&lt;/li&gt;&lt;li&gt;Uses RL-PGD for safety protocol circumvention and PGD for payload injection&lt;/li&gt;&lt;li&gt;Validated with StrongREJECT, LlamaGuard, and human evaluation&lt;/li&gt;&lt;li&gt;Achieves 86% success rate across multiple multimodal models&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Bodam Kim', 'Hiskias Dingeto', 'Taeyoun Kwon', 'Dasol Choi', 'DongGeon Lee', 'Haon Park', 'JaeHoon Lee', 'Jongho Shin']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial prompting', 'red teaming', 'jailbreaking', 'audio', 'multimodal']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2508.03365</guid><pubDate>Wed, 06 Aug 2025 00:00:00 +0000</pubDate></item><item><title>The Power of Many: Synergistic Unification of Diverse Augmentations for Efficient Adversarial Robustness</title><link>https://arxiv.org/abs/2508.03213</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes Universal Adversarial Augmenter (UAA) framework for efficient adversarial robustness&lt;/li&gt;&lt;li&gt;Pre-computes universal transformations offline to avoid online perturbation generation&lt;/li&gt;&lt;li&gt;Achieves state-of-the-art results in data-augmentation-based defenses without training overhead&lt;/li&gt;&lt;li&gt;Validated through extensive experiments on multiple benchmarks&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Wang Yu-Hang', 'Shiwei Li', 'Jianxiang Liao', 'Li Bohan', 'Jian Liu', 'Wenfei Yin']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial robustness', 'data augmentation', 'adversarial training', 'defense mechanism', 'efficient training']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2508.03213</guid><pubDate>Wed, 06 Aug 2025 00:00:00 +0000</pubDate></item><item><title>Estimating Worst-Case Frontier Risks of Open-Weight LLMs</title><link>https://arxiv.org/abs/2508.03153</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces Malicious Fine-Tuning (MFT) to assess worst-case risks in biology and cybersecurity&lt;/li&gt;&lt;li&gt;Compares MFT models against open and closed-weight LLMs&lt;/li&gt;&lt;li&gt;Results inform decision to release the model and provide guidance for future risk estimation&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Eric Wallace', 'Olivia Watkins', 'Miles Wang', 'Kai Chen', 'Chris Koch']&lt;/li&gt;&lt;li&gt;Tags: ['red teaming', 'safety evaluation', 'adversarial prompting']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2508.03153</guid><pubDate>Wed, 06 Aug 2025 00:00:00 +0000</pubDate></item><item><title>Attack the Messages, Not the Agents: A Multi-round Adaptive Stealthy Tampering Framework for LLM-MAS</title><link>https://arxiv.org/abs/2508.03125</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes MAST framework for stealthy tampering in LLM-MAS communication&lt;/li&gt;&lt;li&gt;Uses Monte Carlo Tree Search and Direct Preference Optimization for adaptive attacks&lt;/li&gt;&lt;li&gt;Imposes semantic and embedding similarity constraints for stealth&lt;/li&gt;&lt;li&gt;Demonstrates high attack success rates across diverse setups&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Bingyu Yan', 'Ziyi Zhou', 'Xiaoming Zhang', 'Chaozhuo Li', 'Ruilin Zeng', 'Yirui Qi', 'Tianbo Wang', 'Litian Zhang']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'Adversarial prompting', 'Stealth attacks', 'Multi-agent systems', 'Safety evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2508.03125</guid><pubDate>Wed, 06 Aug 2025 00:00:00 +0000</pubDate></item><item><title>VFLAIR-LLM: A Comprehensive Framework and Benchmark for Split Learning of LLMs</title><link>https://arxiv.org/abs/2508.03097</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces VFLAIR-LLM, a split learning framework for LLMs focusing on privacy and resource efficiency&lt;/li&gt;&lt;li&gt;Provides modules for implementing and evaluating attacks and defenses&lt;/li&gt;&lt;li&gt;Benchmarks 5 attacks and 9 defenses across different split learning configurations&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Zixuan Gu', 'Qiufeng Fan', 'Long Sun', 'Yang Liu', 'Xiaojun Ye']&lt;/li&gt;&lt;li&gt;Tags: ['split learning', 'privacy-preserving', 'attacks', 'defenses', 'benchmarking']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2508.03097</guid><pubDate>Wed, 06 Aug 2025 00:00:00 +0000</pubDate></item><item><title>Untraceable DeepFakes via Traceable Fingerprint Elimination</title><link>https://arxiv.org/abs/2508.03067</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces a multiplicative attack method to eliminate generative model traces in DeepFakes&lt;/li&gt;&lt;li&gt;Universal black-box attack trained on real data, applicable to various GMs and AMs&lt;/li&gt;&lt;li&gt;Achieves 97.08% ASR against 6 advanced AMs and maintains 72.39% under defenses&lt;/li&gt;&lt;li&gt;Highlights need for more robust attribution models&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jiewei Lai', 'Lan Zhang', 'Chen Tang', 'Pengcheng Sun', 'Xinming Wang', 'Yunhao Wang']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial attacks', 'DeepFakes', 'attribution models', 'evasion attacks', 'black-box attack']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2508.03067</guid><pubDate>Wed, 06 Aug 2025 00:00:00 +0000</pubDate></item><item><title>Hide and Seek with LLMs: An Adversarial Game for Sneaky Error Generation and Self-Improving Diagnosis</title><link>https://arxiv.org/abs/2508.03396</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes Hide and Seek Game (HSG) framework for adversarial error generation and diagnosis&lt;/li&gt;&lt;li&gt;Involves Sneaky and Diagnosis roles in a co-evolutionary process&lt;/li&gt;&lt;li&gt;Significantly improves error detection accuracy over baselines&lt;/li&gt;&lt;li&gt;Releases a new dataset of deceptive errors for benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Rui Zou', 'Mengqi Wei', 'Yutao Zhu', 'Jirong Wen', 'Xin Zhao', 'Jing Chen']&lt;/li&gt;&lt;li&gt;Tags: ['red teaming', 'adversarial prompting', 'safety evaluation', 'robustness', 'benchmarking']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2508.03396</guid><pubDate>Wed, 06 Aug 2025 00:00:00 +0000</pubDate></item><item><title>Beyond Surface-Level Detection: Towards Cognitive-Driven Defense Against Jailbreak Attacks via Meta-Operations Reasoning</title><link>https://arxiv.org/abs/2508.03054</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes Cognitive-Driven Defense (CDD) framework against jailbreak attacks&lt;/li&gt;&lt;li&gt;Uses meta-operations reasoning to detect underlying prompt manipulations&lt;/li&gt;&lt;li&gt;Combines supervised fine-tuning with entropy-guided RL for generalization&lt;/li&gt;&lt;li&gt;Demonstrates state-of-the-art defense performance and robustness&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Rui Pu', 'Chaozhuo Li', 'Rui Ha', 'Litian Zhang', 'Lirong Qiu', 'Xi Zhang']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'jailbreaking', 'prompt injection', 'adversarial prompting', 'safety evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2508.03054</guid><pubDate>Wed, 06 Aug 2025 00:00:00 +0000</pubDate></item><item><title>Defend LLMs Through Self-Consciousness</title><link>https://arxiv.org/abs/2508.02961</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces self-consciousness defense mechanism for LLMs against prompt injection attacks&lt;/li&gt;&lt;li&gt;Uses Meta-Cognitive and Arbitration Modules for autonomous output regulation&lt;/li&gt;&lt;li&gt;Evaluated on 7 LLMs using AdvBench and Prompt-Injection-Mixed-Techniques-2024 datasets&lt;/li&gt;&lt;li&gt;Demonstrates significant defense improvements with low computational overhead&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Boshi Huang', 'Fabio Nonato de Paula']&lt;/li&gt;&lt;li&gt;Tags: ['prompt injection', 'LLM security', 'red teaming', 'adversarial prompting', 'defense mechanisms']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2508.02961</guid><pubDate>Wed, 06 Aug 2025 00:00:00 +0000</pubDate></item><item><title>PentestJudge: Judging Agent Behavior Against Operational Requirements</title><link>https://arxiv.org/abs/2508.02921</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces PentestJudge, an LLM-based system for evaluating penetration testing agents&lt;/li&gt;&lt;li&gt;Uses hierarchical rubrics to break down evaluation criteria into yes/no decisions&lt;/li&gt;&lt;li&gt;Compares LLM judge performance to human experts with F1 scores&lt;/li&gt;&lt;li&gt;Finds that tool-use proficiency in models correlates with better judgment accuracy&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Shane Caldwell', 'Max Harley', 'Michael Kouremetis', 'Vincent Abruzzo', 'Will Pearce']&lt;/li&gt;&lt;li&gt;Tags: ['red teaming', 'LLM evaluation', 'security agents', 'judging systems', 'operational security']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2508.02921</guid><pubDate>Wed, 06 Aug 2025 00:00:00 +0000</pubDate></item></channel></rss>