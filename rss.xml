<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>AI Security Paper Digest</title><link>https://kentaroh-toyoda.github.io/ai-security-paper-digest-rss/rss.xml</link><description>Curated papers on AI security from OpenAlex and ArXiv</description><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><lastBuildDate>Thu, 26 Jun 2025 05:33:45 +0000</lastBuildDate><item><title>Bounding-box Watermarking: Defense against Model Extraction Attacks on Object Detectors</title><link>https://arxiv.org/abs/2411.13047</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a defense against model extraction attacks on object detectors using bounding-box watermarking&lt;/li&gt;&lt;li&gt;Injects a backdoor via API responses by stealthily modifying detected bounding boxes&lt;/li&gt;&lt;li&gt;Achieves 100% accuracy in identifying extracted models across multiple datasets&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Satoru Koda', 'Ikuya Morikawa']&lt;/li&gt;&lt;li&gt;Tags: ['model extraction', 'backdoor', 'watermarking', 'object detection', 'security']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2411.13047</guid><pubDate>Thu, 26 Jun 2025 00:00:00 +0000</pubDate></item><item><title>FactCheckmate: Preemptively Detecting and Mitigating Hallucinations in LMs</title><link>https://arxiv.org/abs/2410.02899</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces FactCheckmate, a system that preemptively detects and mitigates hallucinations in LMs using hidden state analysis&lt;/li&gt;&lt;li&gt;Achieves over 70% detection accuracy and 34.4% improvement in factual outputs&lt;/li&gt;&lt;li&gt;Evaluated across multiple model families (Llama, Mistral, Qwen, Gemma) and QA datasets&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Deema Alnuhait', 'Neeraja Kirtane', 'Muhammad Khalifa', 'Hao Peng']&lt;/li&gt;&lt;li&gt;Tags: ['hallucination mitigation', 'safety', 'preemptive detection', 'model intervention', 'hidden states']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2410.02899</guid><pubDate>Thu, 26 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Model Editing as a Double-Edged Sword: Steering Agent Ethical Behavior Toward Beneficence or Harm</title><link>https://arxiv.org/abs/2506.20606</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces BehaviorBench for evaluating and editing agent ethical behavior&lt;/li&gt;&lt;li&gt;Demonstrates model editing can steer agents toward beneficence or harm&lt;/li&gt;&lt;li&gt;Highlights safety and ethical risks of LLM-based agents&lt;/li&gt;&lt;li&gt;Provides insights into both promise and perils of Behavior Editing&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Baixiang Huang', 'Zhen Tan', 'Haoran Wang', 'Zijie Liu', 'Dawei Li', 'Ali Payani', 'Huan Liu', 'Tianlong Chen', 'Kai Shu']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'safety evaluation', 'benchmarking', 'ethical behavior', 'model editing']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.20606</guid><pubDate>Thu, 26 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Probing AI Safety with Source Code</title><link>https://arxiv.org/abs/2506.20471</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces Code of Thought (CoDoT) prompting strategy to evaluate LLM safety&lt;/li&gt;&lt;li&gt;Transforms natural language prompts into code-like structures&lt;/li&gt;&lt;li&gt;Demonstrates significant toxicity increases in multiple state-of-the-art LLMs&lt;/li&gt;&lt;li&gt;Highlights critical need for first-principles safety evaluation&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Ujwal Narayan', 'Shreyas Chaudhari', 'Ashwin Kalyan', 'Tanmay Rajpurohit', 'Karthik Narasimhan', 'Ameet Deshpande', 'Vishvak Murahari']&lt;/li&gt;&lt;li&gt;Tags: ['red teaming', 'adversarial prompting', 'safety evaluation', 'LLM safety', 'prompt injection']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.20471</guid><pubDate>Thu, 26 Jun 2025 00:00:00 +0000</pubDate></item><item><title>FORTRESS: Frontier Risk Evaluation for National Security and Public Safety</title><link>https://arxiv.org/abs/2506.14922</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces FORTRESS, a benchmark for evaluating LLM security against national security risks&lt;/li&gt;&lt;li&gt;Contains 500 adversarial prompts across three domains with corresponding rubrics&lt;/li&gt;&lt;li&gt;Includes benign prompts to test for over-refusals&lt;/li&gt;&lt;li&gt;Demonstrates trade-offs between risk and usability across different models&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Christina Q. Knight', 'Kaustubh Deshpande', 'Ved Sirdeshmukh', 'Meher Mankikar', 'Scale Red Team', 'SEAL Research Team', 'Julian Michael']&lt;/li&gt;&lt;li&gt;Tags: ['red teaming', 'adversarial prompting', 'safety evaluation', 'benchmarking']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.14922</guid><pubDate>Thu, 26 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Autonomous Cyber Resilience via a Co-Evolutionary Arms Race within a Fortified Digital Twin Sandbox</title><link>https://arxiv.org/abs/2506.20102</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces ARC framework for autonomous cyber resilience using co-evolutionary red/blue agents&lt;/li&gt;&lt;li&gt;Red Agent uses DRL to discover stealthy attack paths while Blue Agent defends via adversarial training&lt;/li&gt;&lt;li&gt;Validated on TEP and SWaT testbeds with comprehensive ablation studies&lt;/li&gt;&lt;li&gt;Emphasizes dynamic, self-improving security through perpetual co-evolution&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Malikussaid', 'Sutiyo']&lt;/li&gt;&lt;li&gt;Tags: ['red teaming', 'adversarial training', 'AI security', 'co-evolution', 'digital twin']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.20102</guid><pubDate>Thu, 26 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Diffusion-based Task-oriented Semantic Communications with Model Inversion Attack</title><link>https://arxiv.org/abs/2506.19886</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes DiffSem, a diffusion-based semantic communication framework with self-referential label embedding&lt;/li&gt;&lt;li&gt;Introduces a new metric to evaluate semantic fidelity in model inversion attacks&lt;/li&gt;&lt;li&gt;Improves classification accuracy by 10.03% on MNIST while maintaining robustness under dynamic channels&lt;/li&gt;&lt;li&gt;Highlights discrepancy between traditional image quality metrics and semantic information leakage&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Xuesong Wang', 'Mo Li', 'Xingyan Shi', 'Zhaoqian Liu', 'Shenghao Yang']&lt;/li&gt;&lt;li&gt;Tags: ['model inversion', 'privacy', 'semantic communication', 'diffusion models', 'security evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.19886</guid><pubDate>Thu, 26 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Hear No Evil: Detecting Gradient Leakage by Malicious Servers in Federated Learning</title><link>https://arxiv.org/abs/2506.20651</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Analyzes malicious gradient leakage attacks in federated learning&lt;/li&gt;&lt;li&gt;Reveals trade-off between attack effectiveness and stealth&lt;/li&gt;&lt;li&gt;Proposes client-side detection mechanism for suspicious model updates&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Fei Wang', 'Baochun Li']&lt;/li&gt;&lt;li&gt;Tags: ['federated_learning', 'privacy_attacks', 'security', 'detection', 'malicious_server']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.20651</guid><pubDate>Thu, 26 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Adversarial Reasoning at Jailbreaking Time</title><link>https://arxiv.org/abs/2502.01633</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces adversarial reasoning approach for automatic jailbreaking of LLMs&lt;/li&gt;&lt;li&gt;Leverages loss signal to guide test-time compute for improved attack success&lt;/li&gt;&lt;li&gt;Achieves state-of-the-art attack rates against aligned LLMs&lt;/li&gt;&lt;li&gt;Aims to enhance robustness and trustworthiness of AI systems&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Mahdi Sabbaghi', 'Paul Kassianik', 'George Pappas', 'Yaron Singer', 'Amin Karbasi', 'Hamed Hassani']&lt;/li&gt;&lt;li&gt;Tags: ['jailbreaking', 'red teaming', 'adversarial prompting', 'LLM security', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2502.01633</guid><pubDate>Thu, 26 Jun 2025 00:00:00 +0000</pubDate></item><item><title>AgentBreeder: Mitigating the AI Safety Impact of Multi-Agent Scaffolds via Self-Improvement</title><link>https://arxiv.org/abs/2502.00757</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces AgentBreeder, a framework for evolutionary search over multi-agent scaffolds&lt;/li&gt;&lt;li&gt;Evaluates safety and capability improvements in 'blue' mode (79.4% safety uplift)&lt;/li&gt;&lt;li&gt;Identifies adversarial weak points in 'red' mode during capability optimization&lt;/li&gt;&lt;li&gt;Demonstrates risks and mitigation strategies for multi-agent LLM systems&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['J Rosser', 'Jakob Nicolaus Foerster']&lt;/li&gt;&lt;li&gt;Tags: ['multi-agent systems', 'safety evaluation', 'red teaming', 'evolutionary algorithms', 'scaffolding']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2502.00757</guid><pubDate>Thu, 26 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Towards Backdoor Stealthiness in Model Parameter Space</title><link>https://arxiv.org/abs/2501.05928</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces Grond, a supply-chain backdoor attack using Adversarial Backdoor Injection (ABI)&lt;/li&gt;&lt;li&gt;Focuses on parameter space stealthiness to evade existing defenses&lt;/li&gt;&lt;li&gt;Outperforms 12 backdoor attacks against 17 defenses on image datasets&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Xiaoyun Xu', 'Zhuoran Liu', 'Stefanos Koffas', 'Stjepan Picek']&lt;/li&gt;&lt;li&gt;Tags: ['data poisoning', 'red teaming', 'adversarial attacks', 'model parameters']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2501.05928</guid><pubDate>Thu, 26 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Fuzz-Testing Meets LLM-Based Agents: An Automated and Efficient Framework for Jailbreaking Text-To-Image Generation Models</title><link>https://arxiv.org/abs/2408.00523</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces JailFuzzer, a fuzzing framework for jailbreaking text-to-image models&lt;/li&gt;&lt;li&gt;Uses LLM-based agents for generating natural and semantically meaningful prompts&lt;/li&gt;&lt;li&gt;Components include seed pool, guided mutation engine, and oracle function&lt;/li&gt;&lt;li&gt;Demonstrates high success rates with minimal queries in black-box settings&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yingkai Dong', 'Xiangtao Meng', 'Ning Yu', 'Zheng Li', 'Shanqing Guo']&lt;/li&gt;&lt;li&gt;Tags: ['jailbreaking', 'adversarial prompting', 'text-to-image', 'fuzzing', 'red teaming']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2408.00523</guid><pubDate>Thu, 26 Jun 2025 00:00:00 +0000</pubDate></item><item><title>$C^3$-Bench: The Things Real Disturbing LLM based Agent in Multi-Tasking</title><link>https://arxiv.org/abs/2505.18746</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces $C^3$-Bench, a benchmark for evaluating LLM-based agents in multi-tasking scenarios&lt;/li&gt;&lt;li&gt;Designs three challenges: tool relationships, hidden information, dynamic decisions&lt;/li&gt;&lt;li&gt;Conducts experiments on 49 agents, revealing significant shortcomings&lt;/li&gt;&lt;li&gt;Aims to expose model vulnerabilities and improve interpretability&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Peijie Yu', 'Yifan Yang', 'Jinjian Li', 'Zelong Zhang', 'Haorui Wang', 'Xiao Feng', 'Feng Zhang']&lt;/li&gt;&lt;li&gt;Tags: ['robustness', 'safety evaluation', 'benchmarking', 'adversarial testing', 'red teaming']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2505.18746</guid><pubDate>Thu, 26 Jun 2025 00:00:00 +0000</pubDate></item><item><title>RefPentester: A Knowledge-Informed Self-Reflective Penetration Testing Framework Based on Large Language Models</title><link>https://arxiv.org/abs/2505.07089</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces RefPentester, a knowledge-informed self-reflective penetration testing framework using LLMs&lt;/li&gt;&lt;li&gt;Models the PT process as a seven-state Stage Machine for effective integration&lt;/li&gt;&lt;li&gt;Demonstrates 16.7% better performance than GPT-4o in credential discovery on Hack The Box&lt;/li&gt;&lt;li&gt;Shows superior success rates across PT stage transitions&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Hanzheng Dai', 'Yuanliang Li', 'Jun Yan', 'Zhibo Zhang']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'penetration testing', 'self-reflection', 'stage machine', 'automated ethical hacking']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2505.07089</guid><pubDate>Thu, 26 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Retrieval-Confused Generation is a Good Defender for Privacy Violation Attack of Large Language Models</title><link>https://arxiv.org/abs/2506.19889</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a retrieval-confused generation (RCG) defense against privacy violation attacks (PVA) on LLMs&lt;/li&gt;&lt;li&gt;Uses paraphrasing prompts to rewrite user comments and create a disturbed database&lt;/li&gt;&lt;li&gt;Employs a most irrelevant retrieval strategy to fetch incorrect user data&lt;/li&gt;&lt;li&gt;Replaces data comments with retrieved data to mislead attackers&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Wanli Peng', 'Xin Chen', 'Hang Fu', 'XinYu He', 'Xue Yiming', 'Juan Wen']&lt;/li&gt;&lt;li&gt;Tags: ['privacy attack', 'defense mechanism', 'retrieval', 'paraphrasing', 'LLM security']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.19889</guid><pubDate>Thu, 26 Jun 2025 00:00:00 +0000</pubDate></item><item><title>An Attack Method for Medical Insurance Claim Fraud Detection based on Generative Adversarial Network</title><link>https://arxiv.org/abs/2506.19871</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a GAN-based adversarial attack on insurance fraud detection&lt;/li&gt;&lt;li&gt;Achieves 99% attack success rate without model knowledge&lt;/li&gt;&lt;li&gt;Highlights need for robustness in fraud detection systems&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yining Pang', 'Chenghan Li']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial attacks', 'red teaming', 'security', 'robustness', 'data poisoning']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.19871</guid><pubDate>Thu, 26 Jun 2025 00:00:00 +0000</pubDate></item></channel></rss>