<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>AI Security Paper Digest</title><link>https://kentaroh-toyoda.github.io/ai-security-paper-digest-rss/rss.xml</link><description>Curated papers on AI security from OpenAlex and ArXiv</description><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><lastBuildDate>Wed, 09 Jul 2025 22:18:30 +0000</lastBuildDate><item><title>ScoreAdv: Score-based Targeted Generation of Natural Adversarial Examples via Diffusion Models</title><link>https://arxiv.org/abs/2507.06078</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces ScoreAdv, a novel method for generating natural adversarial examples using diffusion models&lt;/li&gt;&lt;li&gt;Employs adversarial guidance and saliency maps to inject visual information&lt;/li&gt;&lt;li&gt;Validated on ImageNet and CelebA datasets against 10 models in both black-box and white-box settings&lt;/li&gt;&lt;li&gt;Achieves state-of-the-art attack success rates and image quality&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Chihan Huang', 'Hao Tang']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial attacks', 'diffusion models', 'unrestricted adversarial examples', 'red teaming', 'model robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2507.06078</guid><pubDate>Wed, 09 Jul 2025 00:00:00 +0000</pubDate></item><item><title>MEF: A Capability-Aware Multi-Encryption Framework for Evaluating Vulnerabilities in Black-Box Large Language Models</title><link>https://arxiv.org/abs/2505.23404</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes MEF, a multi-encryption framework for evaluating LLM vulnerabilities&lt;/li&gt;&lt;li&gt;Classifies LLMs into Type I/II based on comprehension capabilities&lt;/li&gt;&lt;li&gt;Uses adaptive attack strategies with semantic mutations and dual encryption&lt;/li&gt;&lt;li&gt;Achieves 98.9% jailbreak success on GPT-4o&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Mingyu Yu', 'Wei Wang', 'Yanjie Wei', 'Sujuan Qin', 'Fei Gao', 'Wenmin Li']&lt;/li&gt;&lt;li&gt;Tags: ['jailbreak', 'red teaming', 'LLM security', 'prompt injection', 'model evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2505.23404</guid><pubDate>Wed, 09 Jul 2025 00:00:00 +0000</pubDate></item><item><title>One Surrogate to Fool Them All: Universal, Transferable, and Targeted Adversarial Attacks with CLIP</title><link>https://arxiv.org/abs/2505.19840</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces UnivIntruder, a novel framework for generating universal, transferable adversarial attacks using CLIP&lt;/li&gt;&lt;li&gt;Achieves high ASR rates (85% on ImageNet, 99% on CIFAR-10) without target model queries or training data&lt;/li&gt;&lt;li&gt;Demonstrates real-world vulnerabilities in image search engines (Google, Baidu) and VLMs (GPT-4, Claude-3.5)&lt;/li&gt;&lt;li&gt;Highlights need for new security paradigms in AI applications&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Binyan Xu', 'Xilin Dai', 'Di Tang', 'Kehuan Zhang']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial attacks', 'transferability', 'CLIP', 'universal attacks', 'red teaming']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2505.19840</guid><pubDate>Wed, 09 Jul 2025 00:00:00 +0000</pubDate></item><item><title>From Counterfactuals to Trees: Competitive Analysis of Model Extraction Attacks</title><link>https://arxiv.org/abs/2502.05325</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Formalizes model extraction attack risks using competitive analysis&lt;/li&gt;&lt;li&gt;Focuses on tree-based models (decision trees, gradient boosting, random forests)&lt;/li&gt;&lt;li&gt;Introduces reconstruction algorithms with provable fidelity and anytime performance&lt;/li&gt;&lt;li&gt;Provides theoretical bounds on query complexity for model extraction&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Awa Khouna', 'Julien Ferry', 'Thibaut Vidal']&lt;/li&gt;&lt;li&gt;Tags: ['model extraction', 'competitive analysis', 'tree-based models', 'security', 'red teaming']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2502.05325</guid><pubDate>Wed, 09 Jul 2025 00:00:00 +0000</pubDate></item><item><title>Mind the Cost of Scaffold! Benign Clients May Even Become Accomplices of Backdoor Attack</title><link>https://arxiv.org/abs/2411.16167</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces BadSFL, a backdoor attack on Scaffold in federated learning&lt;/li&gt;&lt;li&gt;Manipulates control variates to turn benign clients into accomplices&lt;/li&gt;&lt;li&gt;Uses GAN-enhanced poisoning for stealth&lt;/li&gt;&lt;li&gt;Demonstrates long-lasting attack persistence&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Xingshuo Han', 'Xuanye Zhang', 'Xiang Lan', 'Haozhao Wang', 'Shengmin Xu', 'Shen Ren', 'Jason Zeng', 'Ming Wu', 'Michael Heinrich', 'Tianwei Zhang']&lt;/li&gt;&lt;li&gt;Tags: ['federated learning', 'backdoor attack', 'data poisoning', 'scaffold', 'gan', 'security']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2411.16167</guid><pubDate>Wed, 09 Jul 2025 00:00:00 +0000</pubDate></item><item><title>Detecting and Mitigating Reward Hacking in Reinforcement Learning Systems: A Comprehensive Empirical Study</title><link>https://arxiv.org/abs/2507.05619</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Large-scale empirical study of reward hacking in RL systems&lt;/li&gt;&lt;li&gt;Detection framework for 6 categories of reward hacking with 78.4% precision and 81.7% recall&lt;/li&gt;&lt;li&gt;Mitigation techniques reducing hacking frequency by up to 54.6%&lt;/li&gt;&lt;li&gt;Analysis across 15 environments and 5 algorithms&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Ibne Farabi Shihab', 'Sanjeda Akter', 'Anuj Sharma']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'robustness', 'safety evaluation', 'empirical study', 'RL safety']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2507.05619</guid><pubDate>Wed, 09 Jul 2025 00:00:00 +0000</pubDate></item><item><title>The Landscape of Memorization in LLMs: Mechanisms, Measurement, and Mitigation</title><link>https://arxiv.org/abs/2507.05578</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Explores memorization in LLMs and its privacy implications&lt;/li&gt;&lt;li&gt;Discusses detection methods like adversarial prompting&lt;/li&gt;&lt;li&gt;Reviews mitigation strategies including differential privacy&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Survey&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Alexander Xiong', 'Xuandong Zhao', 'Aneesh Pappu', 'Dawn Song']&lt;/li&gt;&lt;li&gt;Tags: ['privacy', 'adversarial prompting', 'memorization', 'data poisoning']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2507.05578</guid><pubDate>Wed, 09 Jul 2025 00:00:00 +0000</pubDate></item><item><title>Feint and Attack: Attention-Based Strategies for Jailbreaking and Protecting LLMs</title><link>https://arxiv.org/abs/2410.16327</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces attention-based metrics (Attn_SensWords, Attn_DepScore, Attn_Entropy) to analyze LLM vulnerabilities&lt;/li&gt;&lt;li&gt;Proposes Attention-Based Attack (ABA) using nested prompts to divert attention&lt;/li&gt;&lt;li&gt;Presents Attention-Based Defense (ABD) to calibrate attention distribution&lt;/li&gt;&lt;li&gt;Demonstrates effectiveness through comparative experiments&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Rui Pu', 'Chaozhuo Li', 'Rui Ha', 'Zejian Chen', 'Litian Zhang', 'Zheng Liu', 'Lirong Qiu', 'Zaisheng Ye']&lt;/li&gt;&lt;li&gt;Tags: ['jailbreaking', 'adversarial prompting', 'attention mechanisms', 'red teaming', 'defense strategies']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2410.16327</guid><pubDate>Wed, 09 Jul 2025 00:00:00 +0000</pubDate></item><item><title>AgentSafe: Safeguarding Large Language Model-based Multi-agent Systems via Hierarchical Data Management</title><link>https://arxiv.org/abs/2503.04392</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces AgentSafe framework for securing LLM-based multi-agent systems&lt;/li&gt;&lt;li&gt;Components include ThreatSieve for communication security and HierarCache for memory protection&lt;/li&gt;&lt;li&gt;Experiments show over 80% defense success against adversarial attacks&lt;/li&gt;&lt;li&gt;Demonstrates scalability with increasing agents and data complexity&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Junyuan Mao', 'Fanci Meng', 'Yifan Duan', 'Miao Yu', 'Xiaojun Jia', 'Junfeng Fang', 'Yuxuan Liang', 'Kun Wang', 'Qingsong Wen']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'data poisoning', 'security', 'privacy attacks', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2503.04392</guid><pubDate>Wed, 09 Jul 2025 00:00:00 +0000</pubDate></item><item><title>Agents Are All You Need for LLM Unlearning</title><link>https://arxiv.org/abs/2502.00406</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces ALU, a multi-agent framework for inference-time LLM unlearning&lt;/li&gt;&lt;li&gt;Achieves effective unlearning while preserving utility without retraining&lt;/li&gt;&lt;li&gt;Demonstrates robustness against jailbreaking techniques and scales to 1000+ targets&lt;/li&gt;&lt;li&gt;Evaluates on established benchmarks (TOFU, WMDP, WPU) and various jailbreaking methods&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Debdeep Sanyal', 'Murari Mandal']&lt;/li&gt;&lt;li&gt;Tags: ['LLM unlearning', 'Safety', 'Robustness', 'Jailbreaking', 'Privacy']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2502.00406</guid><pubDate>Wed, 09 Jul 2025 00:00:00 +0000</pubDate></item><item><title>Hidden Prompts in Manuscripts Exploit AI-Assisted Peer Review</title><link>https://arxiv.org/abs/2507.06185</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Discovery of hidden prompts in academic manuscripts to manipulate AI-assisted peer review&lt;/li&gt;&lt;li&gt;Analysis of four types of hidden prompts used, including positive review commands and evaluation frameworks&lt;/li&gt;&lt;li&gt;Examination of author defenses and publisher policies regarding AI use in peer review&lt;/li&gt;&lt;li&gt;Call for technical screening and harmonized policies to address the vulnerability&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Position&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Zhicheng Lin']&lt;/li&gt;&lt;li&gt;Tags: ['prompt injection', 'adversarial prompting', 'safety evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2507.06185</guid><pubDate>Wed, 09 Jul 2025 00:00:00 +0000</pubDate></item><item><title>CAVGAN: Unifying Jailbreak and Defense of LLMs via Generative Adversarial Attacks on their Internal Representations</title><link>https://arxiv.org/abs/2507.06043</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces CAVGAN framework combining jailbreak attack and defense for LLMs using GANs&lt;/li&gt;&lt;li&gt;Achieves 88.85% jailbreak success and 84.17% defense success across three LLMs&lt;/li&gt;&lt;li&gt;Leverages linear separability of LLM internal representations&lt;/li&gt;&lt;li&gt;Provides code and data for reproducibility&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Xiaohu Li', 'Yunfeng Ning', 'Zepeng Bao', 'Mayi Xu', 'Jianhao Chen', 'Tieyun Qian']&lt;/li&gt;&lt;li&gt;Tags: ['LLM', 'red teaming', 'jailbreak', 'defense', 'GAN']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2507.06043</guid><pubDate>Wed, 09 Jul 2025 00:00:00 +0000</pubDate></item><item><title>TuneShield: Mitigating Toxicity in Conversational AI while Fine-tuning on Untrusted Data</title><link>https://arxiv.org/abs/2507.05660</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces TuneShield, a defense framework to mitigate toxicity during LLM fine-tuning on untrusted data&lt;/li&gt;&lt;li&gt;Uses LLM-based toxicity classification and generates 'healing data' to counteract toxic samples&lt;/li&gt;&lt;li&gt;Includes alignment process to reinforce desired behavior&lt;/li&gt;&lt;li&gt;Demonstrates resilience against adaptive adversarial and jailbreak attacks&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Aravind Cheruvu', 'Shravya Kanchi', 'Sifat Muhammad Abdullah', 'Nicholas Kong', 'Daphne Yao', 'Murtuza Jadliwala', 'Bimal Viswanath']&lt;/li&gt;&lt;li&gt;Tags: ['red teaming', 'jailbreaking', 'adversarial prompting', 'safety evaluation', 'data poisoning']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2507.05660</guid><pubDate>Wed, 09 Jul 2025 00:00:00 +0000</pubDate></item><item><title>How Not to Detect Prompt Injections with an LLM</title><link>https://arxiv.org/abs/2507.05630</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Unveils a structural vulnerability in known-answer detection (KAD) defenses against prompt injection attacks&lt;/li&gt;&lt;li&gt;Introduces DataFlip, an adaptive attack that reliably evades KAD with minimal detection (1.5%) and high success rates (88%)&lt;/li&gt;&lt;li&gt;Attack works without white-box access or optimization, highlighting significant security gaps in LLM defenses&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Sarthak Choudhary', 'Divyam Anshumaan', 'Nils Palumbo', 'Somesh Jha']&lt;/li&gt;&lt;li&gt;Tags: ['prompt injection', 'red teaming', 'adversarial prompting', 'LLM security', 'KAD vulnerability']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2507.05630</guid><pubDate>Wed, 09 Jul 2025 00:00:00 +0000</pubDate></item><item><title>AI Agent Smart Contract Exploit Generation</title><link>https://arxiv.org/abs/2507.05558</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces A1, an LLM-based system for autonomous smart contract exploit generation&lt;/li&gt;&lt;li&gt;Evaluates success rates across real-world contracts and analyzes economic implications&lt;/li&gt;&lt;li&gt;Highlights asymmetry between attacker/defender profitability using AI agents&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Arthur Gervais', 'Liyi Zhou']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'smart contract security', 'exploit generation', 'blockchain', 'economic analysis']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2507.05558</guid><pubDate>Wed, 09 Jul 2025 00:00:00 +0000</pubDate></item><item><title>A Survey on Proactive Defense Strategies Against Misinformation in Large Language Models</title><link>https://arxiv.org/abs/2507.05288</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces a Three Pillars framework for proactive defense against misinformation in LLMs&lt;/li&gt;&lt;li&gt;Pillars include Knowledge Credibility, Inference Reliability, and Input Robustness&lt;/li&gt;&lt;li&gt;Survey of existing techniques shows 63% improvement over conventional methods&lt;/li&gt;&lt;li&gt;Emphasizes co-designing robust knowledge, reasoning certification, and attack-resistant interfaces&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Survey&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Shuliang Liu', 'Hongyi Liu', 'Aiwei Liu', 'Bingchen Duan', 'Qi Zheng', 'Yibo Yan', 'He Geng', 'Peijie Jiang', 'Jia Liu', 'Xuming Hu']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial prompting', 'data poisoning', 'robustness', 'alignment', 'safety evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2507.05288</guid><pubDate>Wed, 09 Jul 2025 00:00:00 +0000</pubDate></item><item><title>OpenAgentSafety: A Comprehensive Framework for Evaluating Real-World AI Agent Safety</title><link>https://arxiv.org/abs/2507.06134</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces OpenAgentSafety framework for evaluating AI agent safety&lt;/li&gt;&lt;li&gt;Tests agents in real-world scenarios with real tools&lt;/li&gt;&lt;li&gt;Covers eight risk categories and adversarial strategies&lt;/li&gt;&lt;li&gt;Found significant unsafe behavior in tested LLMs&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Sanidhya Vijayvargiya', 'Aditya Bharat Soni', 'Xuhui Zhou', 'Zora Zhiruo Wang', 'Nouha Dziri', 'Graham Neubig', 'Maarten Sap']&lt;/li&gt;&lt;li&gt;Tags: ['safety evaluation', 'adversarial strategies', 'real-world testing', 'agent safety', 'red teaming']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2507.06134</guid><pubDate>Wed, 09 Jul 2025 00:00:00 +0000</pubDate></item><item><title>Red Teaming AI Red Teaming</title><link>https://arxiv.org/abs/2507.05538</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Critiques current AI red teaming focus on model-level flaws&lt;/li&gt;&lt;li&gt;Proposes macro and micro-level red teaming framework&lt;/li&gt;&lt;li&gt;Emphasizes multifunctional teams and sociotechnical risks&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Position&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Subhabrata Majumdar', 'Brian Pendleton', 'Abhishek Gupta']&lt;/li&gt;&lt;li&gt;Tags: ['red teaming', 'AI security', 'sociotechnical systems', 'model vulnerabilities', 'framework']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2507.05538</guid><pubDate>Wed, 09 Jul 2025 00:00:00 +0000</pubDate></item></channel></rss>