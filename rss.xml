<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>AI Security Paper Digest</title><link>https://kentaroh-toyoda.github.io/ai-security-paper-digest-rss/rss.xml</link><description>Curated papers on AI security from ArXiv</description><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><lastBuildDate>Mon, 25 Aug 2025 22:27:12 +0000</lastBuildDate><item><title>A Curious Case of Remarkable Resilience to Gradient Attacks via Fully Convolutional and Differentiable Front End with a Skip Connection</title><link>https://arxiv.org/abs/2402.17018</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces a front-end architecture with gradient masking to resist adversarial attacks&lt;/li&gt;&lt;li&gt;Demonstrates resilience against APGD and FAB-T attacks from AutoAttack&lt;/li&gt;&lt;li&gt;Discusses randomized ensembling as a practical defense strategy&lt;/li&gt;&lt;li&gt;Evaluates on CIFAR10, CIFAR100, and ImageNet datasets&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Leonid Boytsov', 'Ameya Joshi', 'Filipe Condessa']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial attacks', 'gradient masking', 'robustness', 'defense mechanisms', 'adversarial training']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2402.17018</guid><pubDate>Mon, 25 Aug 2025 00:00:00 +0000</pubDate></item><item><title>PromptFlare: Prompt-Generalized Defense via Cross-Attention Decoy in Diffusion-Based Inpainting</title><link>https://arxiv.org/abs/2508.16217</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes PromptFlare, a defense against malicious image modifications using diffusion-based inpainting&lt;/li&gt;&lt;li&gt;Leverages cross-attention decoys to suppress prompt influence&lt;/li&gt;&lt;li&gt;Achieves state-of-the-art performance with reduced computational overhead&lt;/li&gt;&lt;li&gt;Targets shared tokens in prompt embeddings to inject adversarial noise&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Hohyun Na', 'Seunghoo Hong', 'Simon S. Woo']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial defense', 'prompt injection', 'diffusion models', 'image manipulation', 'cross-attention']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2508.16217</guid><pubDate>Mon, 25 Aug 2025 00:00:00 +0000</pubDate></item><item><title>PoisonSwarm: Universal Harmful Information Synthesis via Model Crowdsourcing</title><link>https://arxiv.org/abs/2505.21184</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces PoisonSwarm, a model crowdsourcing framework for generating diverse harmful data&lt;/li&gt;&lt;li&gt;Decomposes templates into semantic units and applies unit-by-unit toxification with dynamic model switching&lt;/li&gt;&lt;li&gt;Achieves state-of-the-art performance in synthesizing harmful data with high diversity and scalability&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yu Yan', 'Sheng Sun', 'Zhifei Zheng', 'Ziji Hao', 'Teli Liu', 'Min Liu']&lt;/li&gt;&lt;li&gt;Tags: ['red teaming', 'data poisoning', 'adversarial testing', 'model crowdsourcing', 'LLM security']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2505.21184</guid><pubDate>Mon, 25 Aug 2025 00:00:00 +0000</pubDate></item><item><title>from Benign import Toxic: Jailbreaking the Language Model via Adversarial Metaphors</title><link>https://arxiv.org/abs/2503.00038</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces AVATAR framework for jailbreaking LLMs using adversarial metaphors&lt;/li&gt;&lt;li&gt;Starts with benign metaphors and calibrates them into harmful content&lt;/li&gt;&lt;li&gt;Achieves state-of-the-art attack success rates across multiple LLMs&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yu Yan', 'Sheng Sun', 'Zenghao Duan', 'Teli Liu', 'Min Liu', 'Zhiyi Yin', 'Jiangyu Lei', 'Qi Li']&lt;/li&gt;&lt;li&gt;Tags: ['jailbreaking', 'adversarial prompting', 'LLM security', 'metaphors', 'red teaming']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2503.00038</guid><pubDate>Mon, 25 Aug 2025 00:00:00 +0000</pubDate></item><item><title>Soteria: Language-Specific Functional Parameter Steering for Multilingual Safety Alignment</title><link>https://arxiv.org/abs/2502.11244</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces Soteria for multilingual safety alignment&lt;/li&gt;&lt;li&gt;Presents XThreatBench dataset for safety evaluation&lt;/li&gt;&lt;li&gt;Demonstrates improved safety metrics across languages&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Somnath Banerjee', 'Sayan Layek', 'Pratyush Chatterjee', 'Animesh Mukherjee', 'Rima Hazra']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'safety evaluation', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2502.11244</guid><pubDate>Mon, 25 Aug 2025 00:00:00 +0000</pubDate></item><item><title>Retrieval-Augmented Defense: Adaptive and Controllable Jailbreak Prevention for Large Language Models</title><link>https://arxiv.org/abs/2508.16406</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes Retrieval-Augmented Defense (RAD) framework for jailbreak detection in LLMs&lt;/li&gt;&lt;li&gt;Uses retrieval-augmented generation to infer malicious queries from known attack examples&lt;/li&gt;&lt;li&gt;Enables training-free updates and controllable safety-utility trade-off&lt;/li&gt;&lt;li&gt;Demonstrates effectiveness against PAP and PAIR attacks while maintaining low false positives&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Guangyu Yang', 'Jinghong Chen', 'Jingbiao Mei', 'Weizhe Lin', 'Bill Byrne']&lt;/li&gt;&lt;li&gt;Tags: ['jailbreak prevention', 'red teaming', 'adversarial prompting', 'safety evaluation', 'LLM security']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2508.16406</guid><pubDate>Mon, 25 Aug 2025 00:00:00 +0000</pubDate></item><item><title>Self-Disguise Attack: Induce the LLM to disguise itself for AIGT detection evasion</title><link>https://arxiv.org/abs/2508.15848</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes Self-Disguise Attack (SDA) to help LLMs generate text that evades AIGT detectors&lt;/li&gt;&lt;li&gt;Uses adversarial feature extraction and context example optimization to improve human-likeness&lt;/li&gt;&lt;li&gt;Reduces detection accuracy while maintaining text quality across multiple LLMs&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yinghan Zhou', 'Juan Wen', 'Wanli Peng', 'Zhengxian Wu', 'Ziwei Zhang', 'Yiming Xue']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial prompting', 'red teaming', 'AI security', 'AIGC detection evasion', 'text generation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2508.15848</guid><pubDate>Mon, 25 Aug 2025 00:00:00 +0000</pubDate></item><item><title>HAMSA: Hijacking Aligned Compact Models via Stealthy Automation</title><link>https://arxiv.org/abs/2508.16484</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces HAMSA, an automated red-teaming framework for generating stealthy jailbreak prompts&lt;/li&gt;&lt;li&gt;Uses multi-stage evolutionary search with temperature-controlled variability&lt;/li&gt;&lt;li&gt;Evaluates on English and Arabic benchmarks for multilingual assessment&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Alexey Krylov', 'Iskander Vagizov', 'Dmitrii Korzh', 'Maryam Douiba', 'Azidine Guezzaz', 'Vladimir Kokh', 'Sergey D. Erokhin', 'Elena V. Tutubalina', 'Oleg Y. Rogov']&lt;/li&gt;&lt;li&gt;Tags: ['red teaming', 'jailbreaking', 'adversarial prompting', 'multilingual', 'evolutionary algorithms']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2508.16484</guid><pubDate>Mon, 25 Aug 2025 00:00:00 +0000</pubDate></item><item><title>LLMSymGuard: A Symbolic Safety Guardrail Framework Leveraging Interpretable Jailbreak Concepts</title><link>https://arxiv.org/abs/2508.16325</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces LLMSymGuard, a framework for defending against jailbreak attacks on LLMs&lt;/li&gt;&lt;li&gt;Uses Sparse Autoencoders to identify interpretable jailbreak concepts in model internals&lt;/li&gt;&lt;li&gt;Builds symbolic, logical safety guardrails without additional fine-tuning&lt;/li&gt;&lt;li&gt;Leverages mechanistic interpretability for transparent and robust defenses&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Darpan Aswal', "C\\'eline Hudelot"]&lt;/li&gt;&lt;li&gt;Tags: ['jailbreak defense', 'symbolic guardrails', 'sparse autoencoders', 'mechanistic interpretability', 'safety framework']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2508.16325</guid><pubDate>Mon, 25 Aug 2025 00:00:00 +0000</pubDate></item><item><title>Political Ideology Shifts in Large Language Models</title><link>https://arxiv.org/abs/2508.16013</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Investigates ideological shifts in LLMs when adopting synthetic personas&lt;/li&gt;&lt;li&gt;Larger models show more polarization and susceptibility to ideological cues&lt;/li&gt;&lt;li&gt;Right-authoritarian priming has stronger effects than left-libertarian&lt;/li&gt;&lt;li&gt;Persona content can systematically shift ideological responses, amplifying with model size&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Pietro Bernardelle', 'Stefano Civelli', 'Leon Fr\\"ohling', 'Riccardo Lunardi', 'Kevin Roitero', 'Gianluca Demartini']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'safety evaluation', 'adversarial prompting', 'model extraction', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2508.16013</guid><pubDate>Mon, 25 Aug 2025 00:00:00 +0000</pubDate></item><item><title>Mechanistic Exploration of Backdoored Large Language Model Attention Patterns</title><link>https://arxiv.org/abs/2508.15847</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;This study uses mechanistic interpretability to detect backdoor signatures in LLM attention patterns&lt;/li&gt;&lt;li&gt;Compares clean vs poisoned models with single/multi-token triggers&lt;/li&gt;&lt;li&gt;Finds distinct attention pattern changes in later layers&lt;/li&gt;&lt;li&gt;Shows potential for detection based on trigger complexity&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Mohammed Abu Baker', 'Lakshmi Babu-Saheer']&lt;/li&gt;&lt;li&gt;Tags: ['data poisoning', 'safety evaluation', 'adversarial prompting']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2508.15847</guid><pubDate>Mon, 25 Aug 2025 00:00:00 +0000</pubDate></item><item><title>A Review of Developmental Interpretability in Large Language Models</title><link>https://arxiv.org/abs/2508.15841</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Reviews the field of developmental interpretability in LLMs&lt;/li&gt;&lt;li&gt;Focuses on understanding the training process dynamically&lt;/li&gt;&lt;li&gt;Highlights key findings on learning strategies and emergent abilities&lt;/li&gt;&lt;li&gt;Emphasizes importance for proactive AI safety and alignment&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Survey&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Ihor Kendiukhov']&lt;/li&gt;&lt;li&gt;Tags: ['interpretability', 'safety', 'alignment', 'LLM', 'training process']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2508.15841</guid><pubDate>Mon, 25 Aug 2025 00:00:00 +0000</pubDate></item><item><title>Persuasiveness and Bias in LLM: Investigating the Impact of Persuasiveness and Reinforcement of Bias in Language Models</title><link>https://arxiv.org/abs/2508.15798</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces a convincer-skeptic framework to test LLM persuasive impact and bias reinforcement&lt;/li&gt;&lt;li&gt;Uses sycophantic adversarial prompts to probe for bias in persuasive models&lt;/li&gt;&lt;li&gt;Highlights risks of LLM misuse for spreading misinformation and stereotypes&lt;/li&gt;&lt;li&gt;Calls for safety guardrails and alignment-focused policies&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Saumya Roy']&lt;/li&gt;&lt;li&gt;Tags: ['LLM safety', 'Bias', 'Adversarial prompting', 'Red teaming', 'Safety evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2508.15798</guid><pubDate>Mon, 25 Aug 2025 00:00:00 +0000</pubDate></item><item><title>PoisonSwarm: Universal Harmful Information Synthesis via Model Crowdsourcing</title><link>https://arxiv.org/abs/2505.21184</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces PoisonSwarm, a model crowdsourcing framework for generating diverse harmful data&lt;/li&gt;&lt;li&gt;Decomposes templates into semantic units and applies unit-by-unit toxification with dynamic model switching&lt;/li&gt;&lt;li&gt;Achieves state-of-the-art performance in synthesizing harmful data with high diversity and scalability&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yu Yan', 'Sheng Sun', 'Zhifei Zheng', 'Ziji Hao', 'Teli Liu', 'Min Liu']&lt;/li&gt;&lt;li&gt;Tags: ['red teaming', 'data poisoning', 'adversarial testing', 'model crowdsourcing', 'LLM security']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2505.21184</guid><pubDate>Mon, 25 Aug 2025 00:00:00 +0000</pubDate></item><item><title>Robustness of deep learning classification to adversarial input on GPUs: asynchronous parallel accumulation is a source of vulnerability</title><link>https://arxiv.org/abs/2503.17173</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Identifies GPU floating-point non-associativity and async parallelism as a source of adversarial vulnerability&lt;/li&gt;&lt;li&gt;Develops a black-box attack using Bayesian optimization to exploit GPU instruction scheduling&lt;/li&gt;&lt;li&gt;Introduces a learnable permutation (LP) method for efficient worst-case robustness estimation&lt;/li&gt;&lt;li&gt;Highlights the need to consider machine-level details in adversarial robustness assessments&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Sanjif Shanmugavelu', 'Mathieu Taillefumier', 'Christopher Culver', 'Vijay Ganesh', 'Oscar Hernandez', 'Ada Sedova']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial_robustness', 'GPU_vulnerabilities', 'floating_point_precision', 'security', 'model_safety']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2503.17173</guid><pubDate>Mon, 25 Aug 2025 00:00:00 +0000</pubDate></item><item><title>A Curious Case of Remarkable Resilience to Gradient Attacks via Fully Convolutional and Differentiable Front End with a Skip Connection</title><link>https://arxiv.org/abs/2402.17018</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces a front-end architecture with gradient masking to resist adversarial attacks&lt;/li&gt;&lt;li&gt;Demonstrates resilience against APGD and FAB-T attacks from AutoAttack&lt;/li&gt;&lt;li&gt;Discusses randomized ensembling as a practical defense strategy&lt;/li&gt;&lt;li&gt;Evaluates on CIFAR10, CIFAR100, and ImageNet datasets&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Leonid Boytsov', 'Ameya Joshi', 'Filipe Condessa']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial attacks', 'gradient masking', 'robustness', 'defense mechanisms', 'adversarial training']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2402.17018</guid><pubDate>Mon, 25 Aug 2025 00:00:00 +0000</pubDate></item><item><title>LLM-GUARD: Large Language Model-Based Detection and Repair of Bugs and Security Vulnerabilities in C++ and Python</title><link>https://arxiv.org/abs/2508.16419</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Evaluates leading LLMs on detecting bugs and security vulnerabilities in C++ and Python code&lt;/li&gt;&lt;li&gt;Uses a benchmark combining SEED Labs, OpenSSL, and PyBugHive datasets&lt;/li&gt;&lt;li&gt;Finds models excel at syntactic/semantic issues but struggle with complex security flaws&lt;/li&gt;&lt;li&gt;Highlights promise and limitations of LLMs in code analysis&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Akshay Mhatre', 'Noujoud Nader', 'Patrick Diehl', 'Deepti Gupta']&lt;/li&gt;&lt;li&gt;Tags: ['safety evaluation', 'robustness', 'model evaluation', 'code analysis', 'security vulnerabilities']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2508.16419</guid><pubDate>Mon, 25 Aug 2025 00:00:00 +0000</pubDate></item><item><title>Mechanistic Exploration of Backdoored Large Language Model Attention Patterns</title><link>https://arxiv.org/abs/2508.15847</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;This study uses mechanistic interpretability to detect backdoor signatures in LLM attention patterns&lt;/li&gt;&lt;li&gt;Compares clean vs poisoned models with single/multi-token triggers&lt;/li&gt;&lt;li&gt;Finds distinct attention pattern changes in later layers&lt;/li&gt;&lt;li&gt;Shows potential for detection based on trigger complexity&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Mohammed Abu Baker', 'Lakshmi Babu-Saheer']&lt;/li&gt;&lt;li&gt;Tags: ['data poisoning', 'safety evaluation', 'adversarial prompting']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2508.15847</guid><pubDate>Mon, 25 Aug 2025 00:00:00 +0000</pubDate></item><item><title>A Review of Developmental Interpretability in Large Language Models</title><link>https://arxiv.org/abs/2508.15841</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Reviews the field of developmental interpretability in LLMs&lt;/li&gt;&lt;li&gt;Focuses on understanding the training process dynamically&lt;/li&gt;&lt;li&gt;Highlights key findings on learning strategies and emergent abilities&lt;/li&gt;&lt;li&gt;Emphasizes importance for proactive AI safety and alignment&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Survey&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Ihor Kendiukhov']&lt;/li&gt;&lt;li&gt;Tags: ['interpretability', 'safety', 'alignment', 'LLM', 'training process']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2508.15841</guid><pubDate>Mon, 25 Aug 2025 00:00:00 +0000</pubDate></item><item><title>Benchmarking the Robustness of Agentic Systems to Adversarially-Induced Harms</title><link>https://arxiv.org/abs/2508.16481</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a taxonomy of harms for agentic systems&lt;/li&gt;&lt;li&gt;Introduces BAD-ACTS benchmark with 4 implementations and 188 harmful action examples&lt;/li&gt;&lt;li&gt;Evaluates robustness against attacks where one agent is compromised&lt;/li&gt;&lt;li&gt;Demonstrates high attack success rate but potential defense via message monitoring&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jonathan N\\"other', 'Adish Singla', 'Goran Radanovic']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial prompting', 'robustness', 'benchmarking', 'agentic systems', 'red teaming']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2508.16481</guid><pubDate>Mon, 25 Aug 2025 00:00:00 +0000</pubDate></item><item><title>PoisonSwarm: Universal Harmful Information Synthesis via Model Crowdsourcing</title><link>https://arxiv.org/abs/2505.21184</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces PoisonSwarm, a model crowdsourcing framework for generating diverse harmful data&lt;/li&gt;&lt;li&gt;Decomposes templates into semantic units and applies unit-by-unit toxification with dynamic model switching&lt;/li&gt;&lt;li&gt;Achieves state-of-the-art performance in synthesizing harmful data with high diversity and scalability&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yu Yan', 'Sheng Sun', 'Zhifei Zheng', 'Ziji Hao', 'Teli Liu', 'Min Liu']&lt;/li&gt;&lt;li&gt;Tags: ['red teaming', 'data poisoning', 'adversarial testing', 'model crowdsourcing', 'LLM security']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2505.21184</guid><pubDate>Mon, 25 Aug 2025 00:00:00 +0000</pubDate></item><item><title>from Benign import Toxic: Jailbreaking the Language Model via Adversarial Metaphors</title><link>https://arxiv.org/abs/2503.00038</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces AVATAR framework for jailbreaking LLMs using adversarial metaphors&lt;/li&gt;&lt;li&gt;Starts with benign metaphors and calibrates them into harmful content&lt;/li&gt;&lt;li&gt;Achieves state-of-the-art attack success rates across multiple LLMs&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yu Yan', 'Sheng Sun', 'Zenghao Duan', 'Teli Liu', 'Min Liu', 'Zhiyi Yin', 'Jiangyu Lei', 'Qi Li']&lt;/li&gt;&lt;li&gt;Tags: ['jailbreaking', 'adversarial prompting', 'LLM security', 'metaphors', 'red teaming']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2503.00038</guid><pubDate>Mon, 25 Aug 2025 00:00:00 +0000</pubDate></item><item><title>Soteria: Language-Specific Functional Parameter Steering for Multilingual Safety Alignment</title><link>https://arxiv.org/abs/2502.11244</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces Soteria for multilingual safety alignment&lt;/li&gt;&lt;li&gt;Presents XThreatBench dataset for safety evaluation&lt;/li&gt;&lt;li&gt;Demonstrates improved safety metrics across languages&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Somnath Banerjee', 'Sayan Layek', 'Pratyush Chatterjee', 'Animesh Mukherjee', 'Rima Hazra']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'safety evaluation', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2502.11244</guid><pubDate>Mon, 25 Aug 2025 00:00:00 +0000</pubDate></item><item><title>A Curious Case of Remarkable Resilience to Gradient Attacks via Fully Convolutional and Differentiable Front End with a Skip Connection</title><link>https://arxiv.org/abs/2402.17018</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces a front-end architecture with gradient masking to resist adversarial attacks&lt;/li&gt;&lt;li&gt;Demonstrates resilience against APGD and FAB-T attacks from AutoAttack&lt;/li&gt;&lt;li&gt;Discusses randomized ensembling as a practical defense strategy&lt;/li&gt;&lt;li&gt;Evaluates on CIFAR10, CIFAR100, and ImageNet datasets&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Leonid Boytsov', 'Ameya Joshi', 'Filipe Condessa']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial attacks', 'gradient masking', 'robustness', 'defense mechanisms', 'adversarial training']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2402.17018</guid><pubDate>Mon, 25 Aug 2025 00:00:00 +0000</pubDate></item><item><title>Confusion is the Final Barrier: Rethinking Jailbreak Evaluation and Investigating the Real Misuse Threat of LLMs</title><link>https://arxiv.org/abs/2508.16347</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Rethinks jailbreak evaluation for LLMs by focusing on knowledge-intensive Q&amp;A&lt;/li&gt;&lt;li&gt;Investigates whether LLMs possess real harmful knowledge vs. just generating toxic patterns&lt;/li&gt;&lt;li&gt;Finds a mismatch between jailbreak success and actual dangerous knowledge&lt;/li&gt;&lt;li&gt;Highlights issues with LLM-as-a-judge frameworks relying on toxic language patterns&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yu Yan', 'Sheng Sun', 'Zhe Wang', 'Yijun Lin', 'Zenghao Duan', 'zhifei zheng', 'Min Liu', 'Zhiyi yin', 'Jianping Zhang']&lt;/li&gt;&lt;li&gt;Tags: ['jailbreaking', 'red teaming', 'safety evaluation', 'alignment', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2508.16347</guid><pubDate>Mon, 25 Aug 2025 00:00:00 +0000</pubDate></item><item><title>LLMSymGuard: A Symbolic Safety Guardrail Framework Leveraging Interpretable Jailbreak Concepts</title><link>https://arxiv.org/abs/2508.16325</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces LLMSymGuard, a framework for defending against jailbreak attacks on LLMs&lt;/li&gt;&lt;li&gt;Uses Sparse Autoencoders to identify interpretable jailbreak concepts in model internals&lt;/li&gt;&lt;li&gt;Builds symbolic, logical safety guardrails without additional fine-tuning&lt;/li&gt;&lt;li&gt;Leverages mechanistic interpretability for transparent and robust defenses&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Darpan Aswal', "C\\'eline Hudelot"]&lt;/li&gt;&lt;li&gt;Tags: ['jailbreak defense', 'symbolic guardrails', 'sparse autoencoders', 'mechanistic interpretability', 'safety framework']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2508.16325</guid><pubDate>Mon, 25 Aug 2025 00:00:00 +0000</pubDate></item><item><title>CIA+TA Risk Assessment for AI Reasoning Vulnerabilities</title><link>https://arxiv.org/abs/2508.15839</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Establishes cognitive cybersecurity as a discipline&lt;/li&gt;&lt;li&gt;Introduces CIA+TA framework extending traditional security&lt;/li&gt;&lt;li&gt;Presents quantitative risk assessment methodology&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yuksel Aydin']&lt;/li&gt;&lt;li&gt;Tags: ['security', 'risk assessment', 'framework', 'red teaming', 'standards']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2508.15839</guid><pubDate>Mon, 25 Aug 2025 00:00:00 +0000</pubDate></item><item><title>Persuasiveness and Bias in LLM: Investigating the Impact of Persuasiveness and Reinforcement of Bias in Language Models</title><link>https://arxiv.org/abs/2508.15798</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces a convincer-skeptic framework to test LLM persuasive impact and bias reinforcement&lt;/li&gt;&lt;li&gt;Uses sycophantic adversarial prompts to probe for bias in persuasive models&lt;/li&gt;&lt;li&gt;Highlights risks of LLM misuse for spreading misinformation and stereotypes&lt;/li&gt;&lt;li&gt;Calls for safety guardrails and alignment-focused policies&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Saumya Roy']&lt;/li&gt;&lt;li&gt;Tags: ['LLM safety', 'Bias', 'Adversarial prompting', 'Red teaming', 'Safety evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2508.15798</guid><pubDate>Mon, 25 Aug 2025 00:00:00 +0000</pubDate></item></channel></rss>