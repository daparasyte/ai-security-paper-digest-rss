<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>AI Security Paper Digest</title><link>https://kentaroh-toyoda.github.io/ai-security-paper-digest-rss/rss.xml</link><description>Summarized AI security papers from arXiv</description><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><lastBuildDate>Tue, 27 May 2025 22:13:52 +0000</lastBuildDate><item><title>The Power of Iterative Filtering for Supervised Learning with (Heavy) Contamination</title><link>https://arxiv.org/abs/2505.20177</link><description>• The paper presents algorithms for supervised learning that are robust to adversarial contamination of training data, including scenarios where a large fraction of the data is adversarially corrupted. It addresses learning under 'nasty noise' and heavy contamination, which are directly related to data poisoning attacks in AI security.&lt;br/&gt;&lt;br/&gt;Tags: robustness, adversarial attacks, data poisoning, contamination, supervised learning&lt;br/&gt;Authors: Adam R. Klivans, Konstantinos Stavropoulos, Kevin Tian, Arsen Vasilyan&lt;br/&gt;Relevance: 4 / 5&lt;br/&gt;&lt;br/&gt;&lt;a href='https://arxiv.org/abs/2505.20177'&gt;Read on arXiv&lt;/a&gt;</description><pubDate>Tue, 27 May 2025 22:13:52 +0000</pubDate></item><item><title>GraphemeAug: A Systematic Approach to Synthesized Hard Negative Keyword Spotting Examples</title><link>https://arxiv.org/abs/2505.14814</link><description>• The paper proposes a method to systematically generate adversarial examples for keyword spotting models by manipulating graphemes, aiming to create hard negative samples near the decision boundary. This approach is evaluated for its impact on model robustness and performance.&lt;br/&gt;&lt;br/&gt;Tags: adversarial examples, robustness, keyword spotting, audio, hard negatives&lt;br/&gt;Authors: Harry Zhang, Kurt Partridge, Pai Zhu, Neng Chen, Hyun Jin Park, Dhruuv Agarwal, Quan Wang&lt;br/&gt;Relevance: 4 / 5&lt;br/&gt;&lt;br/&gt;&lt;a href='https://arxiv.org/abs/2505.14814'&gt;Read on arXiv&lt;/a&gt;</description><pubDate>Tue, 27 May 2025 22:13:52 +0000</pubDate></item><item><title>Jailbreak-AudioBench: In-Depth Evaluation and Analysis of Jailbreak Threats for Large Audio Language Models</title><link>https://arxiv.org/abs/2501.13772</link><description>• This paper introduces Jailbreak-AudioBench, a benchmark and toolkit for evaluating jailbreak threats in Large Audio-Language Models (LALMs). It focuses on how audio inputs can be used to circumvent safety mechanisms, providing datasets and tools for testing and analyzing such vulnerabilities. The work aims to expose and understand jailbreak attacks in audio modalities and to support the development of defenses.&lt;br/&gt;&lt;br/&gt;Tags: jailbreak, LLM misuse, audio-language models, robustness under attack, red teaming, AI security&lt;br/&gt;Authors: Hao Cheng, Erjia Xiao, Jing Shao, Yichi Wang, Le Yang, Chao Sheng, Philip Torr, Jindong Gu, Renjing Xu&lt;br/&gt;Relevance: 5 / 5&lt;br/&gt;&lt;br/&gt;&lt;a href='https://arxiv.org/abs/2501.13772'&gt;Read on arXiv&lt;/a&gt;</description><pubDate>Tue, 27 May 2025 22:13:52 +0000</pubDate></item><item><title>PITCH: AI-assisted Tagging of Deepfake Audio Calls using Challenge-Response</title><link>https://arxiv.org/abs/2402.18085</link><description>• The paper addresses the security threat posed by AI-generated real-time deepfake audio calls, which enable voice impersonation attacks that bypass traditional authentication systems.&lt;br/&gt;• It proposes PITCH, a challenge-response method for detecting and tagging deepfake audio calls, enhancing both machine and human detection capabilities.&lt;br/&gt;• The work is directly concerned with defending against adversarial attacks using AI-generated content, a core topic in AI security.&lt;br/&gt;&lt;br/&gt;Tags: deepfake detection, adversarial attacks, voice cloning, authentication security, challenge-response, AI misuse&lt;br/&gt;Authors: Govind Mittal, Arthur Jakobsson, Kelly O. Marshall, Chinmay Hegde, Nasir Memon&lt;br/&gt;Relevance: 5 / 5&lt;br/&gt;&lt;br/&gt;&lt;a href='https://arxiv.org/abs/2402.18085'&gt;Read on arXiv&lt;/a&gt;</description><pubDate>Tue, 27 May 2025 22:13:52 +0000</pubDate></item><item><title>ASVspoof2019 vs. ASVspoof5: Assessment and Comparison</title><link>https://arxiv.org/abs/2505.15911</link><description>• The paper compares two ASVspoof challenges, which focus on spoofing attacks against automatic speaker verification (ASV) systems and the robustness of countermeasures. It analyzes how changes in database conditions affect the difficulty of detecting spoofed versus genuine speech, directly addressing adversarial attacks and robustness under attack in the context of AI-driven speaker verification.&lt;br/&gt;&lt;br/&gt;Tags: spoofing, adversarial attacks, robustness, automatic speaker verification, AI security&lt;br/&gt;Authors: Avishai Weizman, Yehuda Ben-Shimol, Itshak Lapidot&lt;br/&gt;Relevance: 4 / 5&lt;br/&gt;&lt;br/&gt;&lt;a href='https://arxiv.org/abs/2505.15911'&gt;Read on arXiv&lt;/a&gt;</description><pubDate>Tue, 27 May 2025 22:13:52 +0000</pubDate></item><item><title>EnvSDD: Benchmarking Environmental Sound Deepfake Detection</title><link>https://arxiv.org/abs/2505.19203</link><description>• This paper introduces EnvSDD, a large-scale dataset and benchmark for detecting deepfakes in environmental sounds. It also proposes a detection system based on a pre-trained audio foundation model and evaluates its robustness against diverse and unseen generative models. The work directly addresses the detection of AI-generated (deepfake) audio, which is a relevant AI security concern.&lt;br/&gt;&lt;br/&gt;Tags: deepfake detection, audio security, environmental sound, robustness, AI-generated content&lt;br/&gt;Authors: Han Yin, Yang Xiao, Rohan Kumar Das, Jisheng Bai, Haohe Liu, Wenwu Wang, Mark D Plumbley&lt;br/&gt;Relevance: 4 / 5&lt;br/&gt;&lt;br/&gt;&lt;a href='https://arxiv.org/abs/2505.19203'&gt;Read on arXiv&lt;/a&gt;</description><pubDate>Tue, 27 May 2025 22:13:52 +0000</pubDate></item><item><title>CloneShield: A Framework for Universal Perturbation Against Zero-Shot Voice Cloning</title><link>https://arxiv.org/abs/2505.19119</link><description>• This paper presents CloneShield, a framework that generates universal adversarial perturbations to defend against zero-shot voice cloning attacks. The method aims to protect users' vocal identity by degrading the effectiveness of voice cloning systems while preserving the perceptual quality of the original audio. The approach is evaluated against multiple state-of-the-art TTS systems and demonstrates robust protection across speakers and utterances.&lt;br/&gt;&lt;br/&gt;Tags: adversarial defense, voice cloning, robustness, privacy, audio security&lt;br/&gt;Authors: Renyuan Li, Zhibo Liang, Haichuan Zhang, Tianyu Shi, Zhiyuan Cheng, Jia Shi, Carl Yang, Mingjie Tang&lt;br/&gt;Relevance: 5 / 5&lt;br/&gt;&lt;br/&gt;&lt;a href='https://arxiv.org/abs/2505.19119'&gt;Read on arXiv&lt;/a&gt;</description><pubDate>Tue, 27 May 2025 22:13:52 +0000</pubDate></item><item><title>ATMM-SAGA: Alternating Training for Multi-Module with Score-Aware Gated Attention SASV system</title><link>https://arxiv.org/abs/2505.18273</link><description>• The paper proposes a spoofing-robust automatic speaker verification (SASV) system using a score-aware gated attention fusion scheme to integrate countermeasure scores and speaker embeddings, aiming to improve robustness against spoofing attacks in ASV systems.&lt;br/&gt;&lt;br/&gt;Tags: spoofing, robustness, adversarial attacks, speaker verification, countermeasures&lt;br/&gt;Authors: Amro Asali, Yehuda Ben-Shimol, Itshak Lapidot&lt;br/&gt;Relevance: 4 / 5&lt;br/&gt;&lt;br/&gt;&lt;a href='https://arxiv.org/abs/2505.18273'&gt;Read on arXiv&lt;/a&gt;</description><pubDate>Tue, 27 May 2025 22:13:52 +0000</pubDate></item><item><title>Towards Generalized Proactive Defense against Face Swappingwith Contour-Hybrid Watermark</title><link>https://arxiv.org/abs/2505.19081</link><description>• This paper proposes a proactive defense mechanism against AI-driven face swapping attacks by embedding a robust, contour-based hybrid watermark into facial images. The method aims to generalize detection and prevention of face swapping without prior knowledge of specific attack techniques, addressing privacy and security concerns related to AI-generated content manipulation.&lt;br/&gt;&lt;br/&gt;Tags: face swapping, AI-generated content, watermarking, proactive defense, robustness, forgery detection, AI security&lt;br/&gt;Authors: Ruiyang Xia, Dawei Zhou, Decheng Liu, Lin Yuan, Jie Li, Nannan Wang, Xinbo Gao&lt;br/&gt;Relevance: 4 / 5&lt;br/&gt;&lt;br/&gt;&lt;a href='https://arxiv.org/abs/2505.19081'&gt;Read on arXiv&lt;/a&gt;</description><pubDate>Tue, 27 May 2025 22:13:52 +0000</pubDate></item><item><title>StyleGuard: Preventing Text-to-Image-Model-based Style Mimicry Attacks by Style Perturbations</title><link>https://arxiv.org/abs/2505.18766</link><description>• The paper addresses security concerns related to text-to-image diffusion models, specifically focusing on defending against style mimicry attacks (a form of model misuse and adversarial attack). It proposes StyleGuard, a method to perturb images to prevent unauthorized style extraction and mimicry, and evaluates its robustness against various attack and purification methods.&lt;br/&gt;&lt;br/&gt;Tags: adversarial attacks, model misuse, robustness, text-to-image models, intellectual property protection&lt;br/&gt;Authors: Yanjie Li, Wenxuan Zhang, Xinqi Lyu, Yihao Liu, Bin Xiao&lt;br/&gt;Relevance: 4 / 5&lt;br/&gt;&lt;br/&gt;&lt;a href='https://arxiv.org/abs/2505.18766'&gt;Read on arXiv&lt;/a&gt;</description><pubDate>Tue, 27 May 2025 22:13:52 +0000</pubDate></item><item><title>HyperFake: Hyperspectral Reconstruction and Attention-Guided Analysis for Advanced Deepfake Detection</title><link>https://arxiv.org/abs/2505.18587</link><description>• The paper introduces HyperFake, a novel deepfake detection pipeline that reconstructs hyperspectral data from RGB videos to improve detection of manipulated media. It addresses the challenge of generalizing deepfake detection across various manipulation techniques and datasets, proposing new methods for revealing hidden traces of manipulation. The work is directly related to AI security as it aims to counteract adversarial uses of generative AI for creating deceptive media.&lt;br/&gt;&lt;br/&gt;Tags: deepfake detection, AI security, media forensics, adversarial attacks, robustness&lt;br/&gt;Authors: Pavan C Shekar, Pawan Soni, Vivek Kanhangad&lt;br/&gt;Relevance: 4 / 5&lt;br/&gt;&lt;br/&gt;&lt;a href='https://arxiv.org/abs/2505.18587'&gt;Read on arXiv&lt;/a&gt;</description><pubDate>Tue, 27 May 2025 22:13:52 +0000</pubDate></item><item><title>ACSE-Eval: Can LLMs threat model real-world cloud infrastructure?</title><link>https://arxiv.org/abs/2505.11565</link><description>• The paper introduces a dataset (ACSE-Eval) for evaluating large language models (LLMs) in the context of cloud security threat modeling.&lt;br/&gt;• It assesses LLMs' abilities to identify security risks, analyze attack vectors, and propose mitigations in real-world AWS cloud deployments.&lt;br/&gt;• The work directly addresses the use of LLMs for identifying and modeling security threats, which is a core aspect of AI security.&lt;br/&gt;&lt;br/&gt;Tags: LLM security, threat modeling, cloud security, attack vectors, AI robustness&lt;br/&gt;Authors: Sarthak Munshi, Swapnil Pathak, Sonam Ghatode, Thenuga Priyadarshini, Dhivya Chandramouleeswaran, Ashutosh Rana&lt;br/&gt;Relevance: 5 / 5&lt;br/&gt;&lt;br/&gt;&lt;a href='https://arxiv.org/abs/2505.11565'&gt;Read on arXiv&lt;/a&gt;</description><pubDate>Tue, 27 May 2025 22:13:52 +0000</pubDate></item><item><title>Toward Malicious Clients Detection in Federated Learning</title><link>https://arxiv.org/abs/2505.09110</link><description>• This paper addresses the detection of malicious clients in federated learning, focusing on identifying and mitigating poisoning attacks. It proposes a new algorithm, SafeFL, to accurately distinguish between benign and malicious participants, thereby improving the security and robustness of federated learning systems against adversarial threats.&lt;br/&gt;&lt;br/&gt;Tags: federated learning, malicious clients, poisoning attacks, adversarial detection, robustness, AI security&lt;br/&gt;Authors: Zhihao Dou, Jiaqi Wang, Wei Sun, Zhuqing Liu, Minghong Fang&lt;br/&gt;Relevance: 5 / 5&lt;br/&gt;&lt;br/&gt;&lt;a href='https://arxiv.org/abs/2505.09110'&gt;Read on arXiv&lt;/a&gt;</description><pubDate>Tue, 27 May 2025 22:13:52 +0000</pubDate></item><item><title>MixBridge: Heterogeneous Image-to-Image Backdoor Attack through Mixture of Schr\"odinger Bridges</title><link>https://arxiv.org/abs/2505.08809</link><description>• The paper proposes MixBridge, a framework for implanting multiple heterogeneous backdoor triggers in diffusion models, specifically addressing backdoor attacks in image-to-image tasks. It discusses methods for injecting backdoors, overcoming conflicts in multi-trigger scenarios, and enhancing stealthiness, with empirical validation.&lt;br/&gt;&lt;br/&gt;Tags: backdoor attacks, model poisoning, diffusion models, AI security, robustness&lt;br/&gt;Authors: Shixi Qin, Zhiyong Yang, Shilong Bao, Shi Wang, Qianqian Xu, Qingming Huang&lt;br/&gt;Relevance: 5 / 5&lt;br/&gt;&lt;br/&gt;&lt;a href='https://arxiv.org/abs/2505.08809'&gt;Read on arXiv&lt;/a&gt;</description><pubDate>Tue, 27 May 2025 22:13:52 +0000</pubDate></item><item><title>Sugar-Coated Poison: Benign Generation Unlocks LLM Jailbreaking</title><link>https://arxiv.org/abs/2504.05652</link><description>• This paper introduces a new jailbreak attack paradigm (Sugar-Coated Poison) that exploits benign content generation in LLMs to bypass safety mechanisms. It analyzes the Defense Threshold Decay phenomenon and proposes both an attack (SCP) and a defense (POSD) method. The work is directly focused on adversarial attacks, jailbreaks, and LLM misuse, making it highly relevant to AI security.&lt;br/&gt;&lt;br/&gt;Tags: LLM jailbreak, adversarial attacks, prompt injection, AI safety, robustness, defense mechanisms&lt;br/&gt;Authors: Yu-Hang Wu, Yu-Jie Xiong, Hao Zhang, Jia-Chen Zhang, Zheng Zhou&lt;br/&gt;Relevance: 5 / 5&lt;br/&gt;&lt;br/&gt;&lt;a href='https://arxiv.org/abs/2504.05652'&gt;Read on arXiv&lt;/a&gt;</description><pubDate>Tue, 27 May 2025 22:13:52 +0000</pubDate></item><item><title>Beyond the Tip of Efficiency: Uncovering the Submerged Threats of Jailbreak Attacks in Small Language Models</title><link>https://arxiv.org/abs/2502.19883</link><description>• This paper presents a comprehensive empirical study on the security vulnerabilities of small language models (SLMs) against jailbreak attacks. It evaluates 13 SLMs, demonstrates their susceptibility to such attacks, assesses the effectiveness of defense methods, and analyzes how various SLM techniques impact security. The work directly addresses AI security concerns, particularly jailbreaks and robustness under attack.&lt;br/&gt;&lt;br/&gt;Tags: jailbreak attacks, small language models, AI security, robustness, defenses&lt;br/&gt;Authors: Sibo Yi, Tianshuo Cong, Xinlei He, Qi Li, Jiaxing Song&lt;br/&gt;Relevance: 5 / 5&lt;br/&gt;&lt;br/&gt;&lt;a href='https://arxiv.org/abs/2502.19883'&gt;Read on arXiv&lt;/a&gt;</description><pubDate>Tue, 27 May 2025 22:13:52 +0000</pubDate></item><item><title>QueryAttack: Jailbreaking Aligned Large Language Models Using Structured Non-natural Query Language</title><link>https://arxiv.org/abs/2502.09723</link><description>• The paper introduces QueryAttack, a novel jailbreak attack on aligned large language models (LLMs) using structured non-natural query language to bypass safety mechanisms. It evaluates the attack's effectiveness across mainstream LLMs, demonstrates high attack success rates, and proposes a defense method to mitigate the attack.&lt;br/&gt;&lt;br/&gt;Tags: jailbreak, LLM misuse, adversarial attacks, robustness under attack, defense methods&lt;br/&gt;Authors: Qingsong Zou, Jingyu Xiao, Qing Li, Zhi Yan, Yuhang Wang, Li Xu, Wenxuan Wang, Kuofeng Gao, Ruoyu Li, Yong Jiang&lt;br/&gt;Relevance: 5 / 5&lt;br/&gt;&lt;br/&gt;&lt;a href='https://arxiv.org/abs/2502.09723'&gt;Read on arXiv&lt;/a&gt;</description><pubDate>Tue, 27 May 2025 22:13:52 +0000</pubDate></item><item><title>MELON: Provable Indirect Prompt Injection Defense via Masked Re-execution and Tool Comparison</title><link>https://arxiv.org/abs/2502.05174</link><description>• This paper introduces MELON, a defense mechanism specifically designed to protect LLM agents from indirect prompt injection (IPI) attacks. It proposes a novel detection method based on masked re-execution and tool comparison, and demonstrates effectiveness against sophisticated IPI attacks. The work includes extensive evaluation, ablation studies, and comparisons with state-of-the-art defenses.&lt;br/&gt;&lt;br/&gt;Tags: prompt injection, LLM security, adversarial attacks, defense mechanisms, robustness&lt;br/&gt;Authors: Kaijie Zhu, Xianjun Yang, Jindong Wang, Wenbo Guo, William Yang Wang&lt;br/&gt;Relevance: 5 / 5&lt;br/&gt;&lt;br/&gt;&lt;a href='https://arxiv.org/abs/2502.05174'&gt;Read on arXiv&lt;/a&gt;</description><pubDate>Tue, 27 May 2025 22:13:52 +0000</pubDate></item><item><title>Firewalls to Secure Dynamic LLM Agentic Networks</title><link>https://arxiv.org/abs/2502.01822</link><description>• This paper addresses security challenges in networks of communicating LLM agents, focusing on vulnerabilities such as prompt injection, jailbreaks, and manipulation.&lt;br/&gt;• It proposes a firewall-inspired framework that automatically derives task-specific security rules to control agent communication and protect against attacks.&lt;br/&gt;• The framework is evaluated against diverse attacks and is shown to reduce privacy and security vulnerabilities while maintaining adaptability.&lt;br/&gt;&lt;br/&gt;Tags: LLM security, prompt injection, jailbreak prevention, agent communication, firewall, privacy, robustness&lt;br/&gt;Authors: Sahar Abdelnabi, Amr Gomaa, Eugene Bagdasarian, Per Ola Kristensson, Reza Shokri&lt;br/&gt;Relevance: 5 / 5&lt;br/&gt;&lt;br/&gt;&lt;a href='https://arxiv.org/abs/2502.01822'&gt;Read on arXiv&lt;/a&gt;</description><pubDate>Tue, 27 May 2025 22:13:52 +0000</pubDate></item><item><title>Data Overvaluation Attack and Truthful Data Valuation in Federated Learning</title><link>https://arxiv.org/abs/2502.00494</link><description>• The paper introduces a novel 'data overvaluation attack' in federated learning, where clients can manipulate data valuation mechanisms to exaggerate their data's contribution, potentially undermining the integrity and security of collaborative machine learning. It also proposes a new robust data valuation metric to mitigate this attack.&lt;br/&gt;&lt;br/&gt;Tags: federated learning, data valuation, adversarial attacks, robustness, AI security&lt;br/&gt;Authors: Shuyuan Zheng, Sudong Cai, Chuan Xiao, Yang Cao, Jianbin Qin, Masatoshi Yoshikawa, Makoto Onizuka&lt;br/&gt;Relevance: 4 / 5&lt;br/&gt;&lt;br/&gt;&lt;a href='https://arxiv.org/abs/2502.00494'&gt;Read on arXiv&lt;/a&gt;</description><pubDate>Tue, 27 May 2025 22:13:52 +0000</pubDate></item><item><title>Latent-space adversarial training with post-aware calibration for defending large language models against jailbreak attacks</title><link>https://arxiv.org/abs/2501.10639</link><description>• This paper proposes a new defense framework (LATPC) for large language models specifically targeting jailbreak attacks, a form of adversarial attack that bypasses safety measures. The method involves latent-space adversarial training and calibration to improve robustness against such attacks while minimizing over-refusal. The work is evaluated against multiple jailbreak attack types and demonstrates improved safety-utility trade-offs.&lt;br/&gt;&lt;br/&gt;Tags: jailbreak attacks, adversarial training, LLM security, robustness, defense mechanisms&lt;br/&gt;Authors: Xin Yi, Yue Li, dongsheng Shi, Linlin Wang, Xiaoling Wang, Liang He&lt;br/&gt;Relevance: 5 / 5&lt;br/&gt;&lt;br/&gt;&lt;a href='https://arxiv.org/abs/2501.10639'&gt;Read on arXiv&lt;/a&gt;</description><pubDate>Tue, 27 May 2025 22:13:52 +0000</pubDate></item><item><title>PentestAgent: Incorporating LLM Agents to Automated Penetration Testing</title><link>https://arxiv.org/abs/2411.05185</link><description>• The paper introduces PentestAgent, a framework that uses large language models (LLMs) to automate penetration testing tasks such as intelligence gathering, vulnerability analysis, and exploitation. The work focuses on leveraging LLMs for security testing, addressing challenges in knowledge and automation, and demonstrates improved performance in automated penetration testing.&lt;br/&gt;&lt;br/&gt;Tags: LLM, penetration testing, AI security, automation, vulnerability analysis&lt;br/&gt;Authors: Xiangmin Shen, Lingzhi Wang, Zhenyuan Li, Yan Chen, Wencheng Zhao, Dawei Sun, Jiashui Wang, Wei Ruan&lt;br/&gt;Relevance: 4 / 5&lt;br/&gt;&lt;br/&gt;&lt;a href='https://arxiv.org/abs/2411.05185'&gt;Read on arXiv&lt;/a&gt;</description><pubDate>Tue, 27 May 2025 22:13:52 +0000</pubDate></item><item><title>From ML to LLM: Evaluating the Robustness of Phishing Webpage Detection Models against Adversarial Attacks</title><link>https://arxiv.org/abs/2407.20361</link><description>• The paper investigates the robustness of phishing webpage detection models, including multimodal large language models (MLLMs), against adversarial attacks. It introduces PhishOracle, a tool for generating adversarial phishing webpages, and evaluates how these attacks impact detection rates. The study finds that both traditional and LLM-based detectors are vulnerable to adversarially crafted phishing webpages, highlighting security weaknesses and the need for more robust AI-based detection methods.&lt;br/&gt;&lt;br/&gt;Tags: adversarial attacks, robustness, phishing detection, LLM security, evasion&lt;br/&gt;Authors: Aditya Kulkarni, Vivek Balachandran, Dinil Mon Divakaran, Tamal Das&lt;br/&gt;Relevance: 5 / 5&lt;br/&gt;&lt;br/&gt;&lt;a href='https://arxiv.org/abs/2407.20361'&gt;Read on arXiv&lt;/a&gt;</description><pubDate>Tue, 27 May 2025 22:13:52 +0000</pubDate></item><item><title>Chain-of-Scrutiny: Detecting Backdoor Attacks for Large Language Models</title><link>https://arxiv.org/abs/2406.05948</link><description>• This paper proposes Chain-of-Scrutiny (CoS), a method for detecting backdoor attacks in large language models (LLMs), particularly those accessed via APIs. The approach leverages LLM reasoning to identify inconsistencies indicative of backdoor triggers, addressing the challenge of defending API-accessible LLMs against adversarial threats. The method is validated experimentally and is designed to be user-friendly for non-experts.&lt;br/&gt;&lt;br/&gt;Tags: backdoor attacks, LLM security, adversarial attacks, robustness, API-accessible LLMs&lt;br/&gt;Authors: Xi Li, Ruofan Mao, Yusen Zhang, Renze Lou, Chen Wu, Jiaqi Wang&lt;br/&gt;Relevance: 5 / 5&lt;br/&gt;&lt;br/&gt;&lt;a href='https://arxiv.org/abs/2406.05948'&gt;Read on arXiv&lt;/a&gt;</description><pubDate>Tue, 27 May 2025 22:13:52 +0000</pubDate></item><item><title>Jailbreaking Prompt Attack: A Controllable Adversarial Attack against Diffusion Models</title><link>https://arxiv.org/abs/2404.02928</link><description>• The paper introduces the Jailbreaking Prompt Attack (JPA), a controllable adversarial attack targeting diffusion-based text-to-image models. JPA enables the generation of harmful or NSFW content by bypassing safety mechanisms in both open-source and closed-source T2I models. The attack does not require access to the target model and is faster than previous methods, making it a practical tool for evaluating and challenging the robustness and security of generative AI systems.&lt;br/&gt;&lt;br/&gt;Tags: jailbreaking, adversarial attacks, prompt injection, LLM misuse, robustness, diffusion models, AI security&lt;br/&gt;Authors: Jiachen Ma, Yijiang Li, Zhiqing Xiao, Anda Cao, Jie Zhang, Chao Ye, Junbo Zhao&lt;br/&gt;Relevance: 5 / 5&lt;br/&gt;&lt;br/&gt;&lt;a href='https://arxiv.org/abs/2404.02928'&gt;Read on arXiv&lt;/a&gt;</description><pubDate>Tue, 27 May 2025 22:13:52 +0000</pubDate></item><item><title>JailbreakRadar: Comprehensive Assessment of Jailbreak Attacks Against LLMs</title><link>https://arxiv.org/abs/2402.05668</link><description>• This paper presents a comprehensive assessment of jailbreak attacks against large language models (LLMs), including a taxonomy of attacks, large-scale evaluation across multiple models and defenses, and analysis of attack effectiveness and mitigation. It directly addresses the security of LLMs in the context of adversarial misuse and defense strategies.&lt;br/&gt;&lt;br/&gt;Tags: jailbreak attacks, LLM security, adversarial attacks, defenses, robustness, red teaming&lt;br/&gt;Authors: Junjie Chu, Yugeng Liu, Ziqing Yang, Xinyue Shen, Michael Backes, Yang Zhang&lt;br/&gt;Relevance: 5 / 5&lt;br/&gt;&lt;br/&gt;&lt;a href='https://arxiv.org/abs/2402.05668'&gt;Read on arXiv&lt;/a&gt;</description><pubDate>Tue, 27 May 2025 22:13:52 +0000</pubDate></item><item><title>BlackboxBench: A Comprehensive Benchmark of Black-box Adversarial Attacks</title><link>https://arxiv.org/abs/2312.16979</link><description>• This paper introduces BlackboxBench, a comprehensive benchmark for evaluating black-box adversarial attacks on deep neural networks. It implements and compares a wide range of attack algorithms, providing extensive evaluations and analysis of model vulnerabilities in scenarios where attackers do not have access to model internals. The work directly addresses the security of AI systems under adversarial attack.&lt;br/&gt;&lt;br/&gt;Tags: adversarial attacks, black-box attacks, AI robustness, benchmarking, model security&lt;br/&gt;Authors: Meixi Zheng, Xuanchen Yan, Zihao Zhu, Hongrui Chen, Baoyuan Wu&lt;br/&gt;Relevance: 5 / 5&lt;br/&gt;&lt;br/&gt;&lt;a href='https://arxiv.org/abs/2312.16979'&gt;Read on arXiv&lt;/a&gt;</description><pubDate>Tue, 27 May 2025 22:13:52 +0000</pubDate></item><item><title>FedCC: Robust Federated Learning against Model Poisoning Attacks</title><link>https://arxiv.org/abs/2212.01976</link><description>• This paper proposes FedCC, a defense algorithm for federated learning that mitigates model poisoning and backdoor attacks, especially in non-IID data settings. It uses Centered Kernel Alignment similarity of penultimate layer representations to cluster and filter out malicious clients, improving robustness against adversarial attacks in federated learning.&lt;br/&gt;&lt;br/&gt;Tags: model poisoning, federated learning, backdoor attacks, robustness, adversarial attacks, AI security&lt;br/&gt;Authors: Hyejun Jeong, Hamin Son, Seohu Lee, Jayun Hyun, Tai-Myoung Chung&lt;br/&gt;Relevance: 5 / 5&lt;br/&gt;&lt;br/&gt;&lt;a href='https://arxiv.org/abs/2212.01976'&gt;Read on arXiv&lt;/a&gt;</description><pubDate>Tue, 27 May 2025 22:13:52 +0000</pubDate></item><item><title>TrojanStego: Your Language Model Can Secretly Be A Steganographic Privacy Leaking Agent</title><link>https://arxiv.org/abs/2505.20118</link><description>• The paper introduces TrojanStego, a threat model where adversaries fine-tune large language models (LLMs) to covertly leak sensitive information using linguistic steganography. It presents a taxonomy of risk factors, demonstrates a practical encoding scheme, and shows that compromised models can exfiltrate secrets with high accuracy while evading detection. This work highlights a new class of covert data exfiltration attacks on LLMs.&lt;br/&gt;&lt;br/&gt;Tags: LLM misuse, data exfiltration, steganography, model compromise, AI security&lt;br/&gt;Authors: Dominik Meier, Jan Philip Wahle, Paul R\"ottger, Terry Ruas, Bela Gipp&lt;br/&gt;Relevance: 5 / 5&lt;br/&gt;&lt;br/&gt;&lt;a href='https://arxiv.org/abs/2505.20118'&gt;Read on arXiv&lt;/a&gt;</description><pubDate>Tue, 27 May 2025 22:13:52 +0000</pubDate></item><item><title>Novel Loss-Enhanced Universal Adversarial Patches for Sustainable Speaker Privacy</title><link>https://arxiv.org/abs/2505.19951</link><description>• The paper proposes novel methods for generating universal adversarial patches (UAPs) to anonymize speakers and protect personal data in deep learning voice models. It addresses adversarial attacks on speaker identification systems, focusing on improving the effectiveness and imperceptibility of such attacks to enhance privacy and security.&lt;br/&gt;&lt;br/&gt;Tags: adversarial attacks, speaker anonymization, voice biometrics, privacy, robustness&lt;br/&gt;Authors: Elvir Karimov, Alexander Varlamov, Danil Ivanov, Dmitrii Korzh, Oleg Y. Rogov&lt;br/&gt;Relevance: 4 / 5&lt;br/&gt;&lt;br/&gt;&lt;a href='https://arxiv.org/abs/2505.19951'&gt;Read on arXiv&lt;/a&gt;</description><pubDate>Tue, 27 May 2025 22:13:52 +0000</pubDate></item><item><title>What Really Matters in Many-Shot Attacks? An Empirical Study of Long-Context Vulnerabilities in LLMs</title><link>https://arxiv.org/abs/2505.19773</link><description>• This paper empirically studies vulnerabilities in large language models (LLMs) related to long-context processing, specifically focusing on many-shot jailbreaking attacks. It analyzes how context length affects the effectiveness of adversarial attacks and demonstrates that even simple or repetitive prompts can bypass safety mechanisms in LLMs as context length increases. The findings highlight fundamental safety gaps and the need for improved security mechanisms in LLMs with expanded context windows.&lt;br/&gt;&lt;br/&gt;Tags: LLM security, jailbreaking, adversarial attacks, context length vulnerabilities, model robustness, AI safety&lt;br/&gt;Authors: Sangyeop Kim, Yohan Lee, Yongwoo Song, Kimin Lee&lt;br/&gt;Relevance: 5 / 5&lt;br/&gt;&lt;br/&gt;&lt;a href='https://arxiv.org/abs/2505.19773'&gt;Read on arXiv&lt;/a&gt;</description><pubDate>Tue, 27 May 2025 22:13:52 +0000</pubDate></item><item><title>Structure Disruption: Subverting Malicious Diffusion-Based Inpainting via Self-Attention Query Perturbation</title><link>https://arxiv.org/abs/2505.19425</link><description>• The paper introduces Structure Disruption Attack (SDA), a method to protect sensitive image regions from malicious inpainting/editing by adversaries using diffusion models. It targets the self-attention mechanism to disrupt structural generation, thereby preventing coherent image manipulation. The work is motivated by the societal risks of adversarial misuse of image editing AI and demonstrates robust protection against such attacks.&lt;br/&gt;&lt;br/&gt;Tags: adversarial attacks, diffusion models, image inpainting, robustness, AI misuse&lt;br/&gt;Authors: Yuhao He, Jinyu Tian, Haiwei Wu, Jianqing Li&lt;br/&gt;Relevance: 5 / 5&lt;br/&gt;&lt;br/&gt;&lt;a href='https://arxiv.org/abs/2505.19425'&gt;Read on arXiv&lt;/a&gt;</description><pubDate>Tue, 27 May 2025 22:13:52 +0000</pubDate></item><item><title>Lifelong Safety Alignment for Language Models</title><link>https://arxiv.org/abs/2505.20259</link><description>• This paper proposes a lifelong safety alignment framework for language models (LLMs) to continuously adapt to new and evolving jailbreaking attacks.&lt;br/&gt;• It introduces a competitive setup between a Meta-Attacker (to discover novel jailbreak strategies) and a Defender (to resist them), directly addressing LLM misuse and robustness under attack.&lt;br/&gt;• The work focuses on improving LLM robustness against prompt injection and jailbreaks, which are core AI security concerns.&lt;br/&gt;&lt;br/&gt;Tags: jailbreaking, LLM misuse, robustness, prompt injection, AI security&lt;br/&gt;Authors: Haoyu Wang, Zeyu Qin, Yifei Zhao, Chao Du, Min Lin, Xueqian Wang, Tianyu Pang&lt;br/&gt;Relevance: 5 / 5&lt;br/&gt;&lt;br/&gt;&lt;a href='https://arxiv.org/abs/2505.20259'&gt;Read on arXiv&lt;/a&gt;</description><pubDate>Tue, 27 May 2025 22:13:52 +0000</pubDate></item><item><title>CPA-RAG:Covert Poisoning Attacks on Retrieval-Augmented Generation in Large Language Models</title><link>https://arxiv.org/abs/2505.19864</link><description>• This paper introduces CPA-RAG, a black-box adversarial framework for conducting covert poisoning attacks on Retrieval-Augmented Generation (RAG) systems in large language models.&lt;br/&gt;• The method generates query-relevant adversarial texts to manipulate retrieval and induce target answers, demonstrating high attack success rates and outperforming existing baselines.&lt;br/&gt;• The paper includes experiments on multiple datasets and LLMs, as well as a real-world attack on a commercial RAG system, highlighting practical security threats and the need for robust defenses.&lt;br/&gt;&lt;br/&gt;Tags: poisoning attacks, adversarial attacks, retrieval-augmented generation, large language models, AI security, robustness&lt;br/&gt;Authors: Chunyang Li, Junwei Zhang, Anda Cheng, Zhuo Ma, Xinghua Li, Jianfeng Ma&lt;br/&gt;Relevance: 5 / 5&lt;br/&gt;&lt;br/&gt;&lt;a href='https://arxiv.org/abs/2505.19864'&gt;Read on arXiv&lt;/a&gt;</description><pubDate>Tue, 27 May 2025 22:13:52 +0000</pubDate></item><item><title>One Surrogate to Fool Them All: Universal, Transferable, and Targeted Adversarial Attacks with CLIP</title><link>https://arxiv.org/abs/2505.19840</link><description>• This paper introduces UnivIntruder, a new attack framework that uses a single, publicly available CLIP model to generate universal, transferable, and targeted adversarial perturbations. The method does not require access to the target model's training data or excessive queries, making it practical for real-world attacks. The paper demonstrates high attack success rates against major datasets and real-world systems, including image search engines and vision-language models like GPT-4 and Claude-3.5. The work highlights significant vulnerabilities and the need to reassess AI security paradigms.&lt;br/&gt;&lt;br/&gt;Tags: adversarial attacks, transferability, model robustness, vision-language models, AI security&lt;br/&gt;Authors: Binyan Xu, Xilin Dai, Di Tang, Kehuan Zhang&lt;br/&gt;Relevance: 5 / 5&lt;br/&gt;&lt;br/&gt;&lt;a href='https://arxiv.org/abs/2505.19840'&gt;Read on arXiv&lt;/a&gt;</description><pubDate>Tue, 27 May 2025 22:13:52 +0000</pubDate></item><item><title>Poison in the Well: Feature Embedding Disruption in Backdoor Attacks</title><link>https://arxiv.org/abs/2505.19821</link><description>• This paper introduces ShadowPrint, a novel backdoor attack targeting feature embeddings in neural networks. The attack is designed to be highly stealthy, effective at low poison rates, and less reliant on training data access. The work demonstrates high attack success rates and stability across various scenarios, highlighting new challenges for AI security and the need for advanced defenses against feature space manipulations.&lt;br/&gt;&lt;br/&gt;Tags: backdoor attacks, feature embedding, poisoning, AI security, robustness, adversarial attacks&lt;br/&gt;Authors: Zhou Feng, Jiahao Chen, Chunyi Zhou, Yuwen Pu, Qingming Li, Shouling Ji&lt;br/&gt;Relevance: 5 / 5&lt;br/&gt;&lt;br/&gt;&lt;a href='https://arxiv.org/abs/2505.19821'&gt;Read on arXiv&lt;/a&gt;</description><pubDate>Tue, 27 May 2025 22:13:52 +0000</pubDate></item><item><title>VADER: A Human-Evaluated Benchmark for Vulnerability Assessment, Detection, Explanation, and Remediation</title><link>https://arxiv.org/abs/2505.19395</link><description>• The paper introduces VADER, a benchmark for evaluating large language models (LLMs) on their ability to assess, detect, explain, and remediate software vulnerabilities. It focuses on LLMs' performance in identifying and fixing real-world software flaws, which is directly related to AI security, particularly in the context of LLMs being used for vulnerability assessment and remediation. The benchmark and results provide insights into the robustness and security capabilities of current LLMs.&lt;br/&gt;&lt;br/&gt;Tags: LLM security, vulnerability detection, robustness, benchmarking, software security&lt;br/&gt;Authors: Ethan TS. Liu, Austin Wang, Spencer Mateega, Carlos Georgescu, Danny Tang&lt;br/&gt;Relevance: 4 / 5&lt;br/&gt;&lt;br/&gt;&lt;a href='https://arxiv.org/abs/2505.19395'&gt;Read on arXiv&lt;/a&gt;</description><pubDate>Tue, 27 May 2025 22:13:52 +0000</pubDate></item><item><title>RADEP: A Resilient Adaptive Defense Framework Against Model Extraction Attacks</title><link>https://arxiv.org/abs/2505.19364</link><description>• The paper introduces RADEP, a defense framework specifically designed to protect machine learning models deployed via MLaaS from model extraction attacks. It details mechanisms such as adversarial training, malicious query detection, adaptive response, and ownership verification to counteract adversarial attempts to steal or misuse models. The framework is evaluated for its effectiveness against adaptive adversaries and its minimal impact on legitimate users.&lt;br/&gt;&lt;br/&gt;Tags: model extraction, adversarial attacks, MLaaS security, defense mechanisms, ownership verification, robustness under attack&lt;br/&gt;Authors: Amit Chakraborty, Sayyed Farid Ahamed, Sandip Roy, Soumya Banerjee, Kevin Choi, Abdul Rahman, Alison Hu, Edward Bowen, Sachin Shetty&lt;br/&gt;Relevance: 5 / 5&lt;br/&gt;&lt;br/&gt;&lt;a href='https://arxiv.org/abs/2505.19364'&gt;Read on arXiv&lt;/a&gt;</description><pubDate>Tue, 27 May 2025 22:13:52 +0000</pubDate></item><item><title>A Novel Zero-Trust Identity Framework for Agentic AI: Decentralized Authentication and Fine-Grained Access Control</title><link>https://arxiv.org/abs/2505.19301</link><description>• The paper proposes a novel identity and access management (IAM) framework specifically for agentic AI and multi-agent systems, addressing the inadequacy of traditional IAM protocols for dynamic AI agents.&lt;br/&gt;• It introduces decentralized authentication, fine-grained access control, and privacy-preserving mechanisms (e.g., Zero-Knowledge Proofs) to enhance security, trust, and accountability in AI agent ecosystems.&lt;br/&gt;• Security considerations are explicitly discussed, focusing on foundational trust and real-time policy enforcement for AI agents, which are core aspects of AI security.&lt;br/&gt;&lt;br/&gt;Tags: AI security, identity and access management, multi-agent systems, decentralized authentication, access control, zero-trust, zero-knowledge proofs&lt;br/&gt;Authors: Ken Huang, Vineeth Sai Narajala, John Yeoh, Ramesh Raskar, Youssef Harkati, Jerry Huang, Idan Habler, Chris Hughes&lt;br/&gt;Relevance: 4 / 5&lt;br/&gt;&lt;br/&gt;&lt;a href='https://arxiv.org/abs/2505.19301'&gt;Read on arXiv&lt;/a&gt;</description><pubDate>Tue, 27 May 2025 22:13:52 +0000</pubDate></item><item><title>ALRPHFS: Adversarially Learned Risk Patterns with Hierarchical Fast \&amp; Slow Reasoning for Robust Agent Defense</title><link>https://arxiv.org/abs/2505.19260</link><description>• This paper introduces ALRPHFS, a defense framework for LLM agents that uses adversarial self-learning to identify risk patterns and a hierarchical reasoning engine to detect and mitigate harmful inputs and unsafe behaviors. The approach aims to bridge the gap between traditional safety checks and real-world semantic risks, enhancing robustness against adversarial attacks and misuse.&lt;br/&gt;&lt;br/&gt;Tags: AI security, adversarial attacks, robustness, LLM defense, risk detection&lt;br/&gt;Authors: Shiyu Xiang, Tong Zhang, Ronghao Chen&lt;br/&gt;Relevance: 5 / 5&lt;br/&gt;&lt;br/&gt;&lt;a href='https://arxiv.org/abs/2505.19260'&gt;Read on arXiv&lt;/a&gt;</description><pubDate>Tue, 27 May 2025 22:13:52 +0000</pubDate></item><item><title>Exemplifying Emerging Phishing: QR-based Browser-in-The-Browser (BiTB) Attack</title><link>https://arxiv.org/abs/2505.18944</link><description>• The paper presents a novel phishing attack that leverages Large Language Models (LLMs), specifically Google Gemini, to facilitate QR-based Browser-in-The-Browser (BiTB) attacks. It discusses the use of malicious prompts to exploit LLMs in the context of phishing, demonstrating a clear misuse scenario involving AI systems.&lt;br/&gt;&lt;br/&gt;Tags: LLM misuse, phishing, prompt injection, adversarial attacks, AI security&lt;br/&gt;Authors: Muhammad Wahid Akram, Keshav Sood, Muneeb Ul Hassan, Basant Subba&lt;br/&gt;Relevance: 4 / 5&lt;br/&gt;&lt;br/&gt;&lt;a href='https://arxiv.org/abs/2505.18944'&gt;Read on arXiv&lt;/a&gt;</description><pubDate>Tue, 27 May 2025 22:13:52 +0000</pubDate></item><item><title>Security Concerns for Large Language Models: A Survey</title><link>https://arxiv.org/abs/2505.18889</link><description>• This survey paper provides a comprehensive overview of security concerns specific to Large Language Models (LLMs), including prompt injection, jailbreaking, adversarial attacks, data poisoning, misuse for malicious purposes, and risks associated with autonomous LLM agents. It reviews recent studies, analyzes defenses, and discusses open challenges in securing LLM-based applications.&lt;br/&gt;&lt;br/&gt;Tags: LLM security, adversarial attacks, prompt injection, jailbreaking, misuse, data poisoning, robustness, autonomous agents, AI safety&lt;br/&gt;Authors: Miles Q. Li, Benjamin C. M. Fung&lt;br/&gt;Relevance: 5 / 5&lt;br/&gt;&lt;br/&gt;&lt;a href='https://arxiv.org/abs/2505.18889'&gt;Read on arXiv&lt;/a&gt;</description><pubDate>Tue, 27 May 2025 22:13:52 +0000</pubDate></item><item><title>Mal-D2GAN: Double-Detector based GAN for Malware Generation</title><link>https://arxiv.org/abs/2505.18806</link><description>• This paper proposes Mal-D2GAN, a GAN-based approach for generating adversarial malware examples to test and improve the robustness of machine learning-based malware detectors. It discusses vulnerabilities of ML models to intentional attacks and evaluates the effectiveness of adversarial examples in reducing detection accuracy, directly addressing adversarial attacks and robustness in AI security.&lt;br/&gt;&lt;br/&gt;Tags: adversarial attacks, robustness, malware, GAN, AI security&lt;br/&gt;Authors: Nam Hoang Thanh, Trung Pham Duy, Lam Bui Thu&lt;br/&gt;Relevance: 5 / 5&lt;br/&gt;&lt;br/&gt;&lt;a href='https://arxiv.org/abs/2505.18806'&gt;Read on arXiv&lt;/a&gt;</description><pubDate>Tue, 27 May 2025 22:13:52 +0000</pubDate></item><item><title>Strong Membership Inference Attacks on Massive Datasets and (Moderately) Large Language Models</title><link>https://arxiv.org/abs/2505.18773</link><description>• The paper investigates strong membership inference attacks (MIAs) on large language models (LLMs), scaling attacks to models with up to 1B parameters and large datasets. It evaluates the effectiveness of these attacks, providing insights into privacy risks and vulnerabilities of LLMs under adversarial scrutiny.&lt;br/&gt;&lt;br/&gt;Tags: membership inference, privacy attacks, LLM security, adversarial attacks, model vulnerability&lt;br/&gt;Authors: Jamie Hayes, Ilia Shumailov, Christopher A. Choquette-Choo, Matthew Jagielski, George Kaissis, Katherine Lee, Milad Nasr, Sahra Ghalebikesabi, Niloofar Mireshghallah, Meenatchi Sundaram Mutu Selva Annamalai, Igor Shilov, Matthieu Meeus, Yves-Alexandre de Montjoye, Franziska Boenisch, Adam Dziedzic, A. Feder Cooper&lt;br/&gt;Relevance: 5 / 5&lt;br/&gt;&lt;br/&gt;&lt;a href='https://arxiv.org/abs/2505.18773'&gt;Read on arXiv&lt;/a&gt;</description><pubDate>Tue, 27 May 2025 22:13:52 +0000</pubDate></item><item><title>$PD^3F$: A Pluggable and Dynamic DoS-Defense Framework Against Resource Consumption Attacks Targeting Large Language Models</title><link>https://arxiv.org/abs/2505.18680</link><description>• This paper addresses denial-of-service (DoS) attacks targeting Large Language Models (LLMs), a clear AI security concern. It proposes a framework ($PD^3F$) to defend against resource consumption attacks that can degrade or crash LLM servers. The work includes mitigation strategies and experimental validation, directly contributing to the robustness and security of LLM deployments under adversarial conditions.&lt;br/&gt;&lt;br/&gt;Tags: DoS attacks, resource consumption, LLM security, adversarial robustness, AI security&lt;br/&gt;Authors: Yuanhe Zhang, Xinyue Wang, Haoran Gao, Zhenhong Zhou, Fanyu Meng, Yuyao Zhang, Sen Su&lt;br/&gt;Relevance: 5 / 5&lt;br/&gt;&lt;br/&gt;&lt;a href='https://arxiv.org/abs/2505.18680'&gt;Read on arXiv&lt;/a&gt;</description><pubDate>Tue, 27 May 2025 22:13:52 +0000</pubDate></item><item><title>LAMDA: A Longitudinal Android Malware Benchmark for Concept Drift Analysis</title><link>https://arxiv.org/abs/2505.18551</link><description>• This paper introduces LAMDA, a large-scale, temporally diverse Android malware benchmark designed for analyzing concept drift in machine learning-based malware detection systems. It addresses the challenges posed by evolving adversarial threats and distributional shifts in malware and benign samples, which are central to the robustness and security of AI systems deployed for malware detection.&lt;br/&gt;&lt;br/&gt;Tags: malware detection, concept drift, adversarial attacks, robustness, AI security&lt;br/&gt;Authors: Md Ahsanul Haque, Ismail Hossain, Md Mahmuduzzaman Kamol, Md Jahangir Alam, Suresh Kumar Amalapuram, Sajedul Talukder, Mohammad Saidur Rahman&lt;br/&gt;Relevance: 4 / 5&lt;br/&gt;&lt;br/&gt;&lt;a href='https://arxiv.org/abs/2505.18551'&gt;Read on arXiv&lt;/a&gt;</description><pubDate>Tue, 27 May 2025 22:13:52 +0000</pubDate></item><item><title>Benchmarking Poisoning Attacks against Retrieval-Augmented Generation</title><link>https://arxiv.org/abs/2505.18543</link><description>• This paper presents a comprehensive benchmark for evaluating poisoning attacks against Retrieval-Augmented Generation (RAG) systems, assessing the effectiveness of various attack and defense methods across multiple datasets and RAG architectures. The study highlights the vulnerability of RAG systems to poisoning attacks and the insufficiency of current defense mechanisms.&lt;br/&gt;&lt;br/&gt;Tags: poisoning attacks, retrieval-augmented generation, RAG, AI security, robustness, defenses&lt;br/&gt;Authors: Baolei Zhang, Haoran Xin, Jiatong Li, Dongzhe Zhang, Minghong Fang, Zhuqing Liu, Lihai Nie, Zheli Liu&lt;br/&gt;Relevance: 5 / 5&lt;br/&gt;&lt;br/&gt;&lt;a href='https://arxiv.org/abs/2505.18543'&gt;Read on arXiv&lt;/a&gt;</description><pubDate>Tue, 27 May 2025 22:13:52 +0000</pubDate></item><item><title>AI/ML for 5G and Beyond Cybersecurity</title><link>https://arxiv.org/abs/2505.18402</link><description>• The paper discusses the security threats and vulnerabilities introduced by AI/ML-based 5G and IoT systems, emphasizing the need to investigate attack vectors specific to AI/ML.&lt;br/&gt;• It highlights the challenges in defining and understanding attacks on AI/ML systems, differentiating them from traditional cybersecurity threats.&lt;br/&gt;• The study reviews the role of AI/ML in both enabling and defending against security threats in 5G and beyond, and provides perspectives on predicting and mitigating such threats.&lt;br/&gt;&lt;br/&gt;Tags: AI security, adversarial attacks, AI/ML threats, 5G security, IoT security, robustness&lt;br/&gt;Authors: Sandeep Pirbhulal, Habtamu Abie, Martin Jullum, Didrik Nielsen, Anders L{\o}land&lt;br/&gt;Relevance: 4 / 5&lt;br/&gt;&lt;br/&gt;&lt;a href='https://arxiv.org/abs/2505.18402'&gt;Read on arXiv&lt;/a&gt;</description><pubDate>Tue, 27 May 2025 22:13:52 +0000</pubDate></item><item><title>Dynamic Risk Assessments for Offensive Cybersecurity Agents</title><link>https://arxiv.org/abs/2505.18384</link><description>• The paper discusses the risks associated with autonomous agents powered by foundation models being used for offensive cybersecurity operations. It highlights how adversaries can iteratively improve such agents, even with limited resources, and argues for dynamic risk assessments that consider adversarial degrees of freedom. The study demonstrates significant improvements in agent capability through adversarial iteration, underscoring the need for robust evaluation of AI agents in cybersecurity contexts.&lt;br/&gt;&lt;br/&gt;Tags: AI security, offensive cybersecurity, adversarial attacks, risk assessment, autonomous agents&lt;br/&gt;Authors: Boyi Wei, Benedikt Stroebl, Jiacen Xu, Joie Zhang, Zhou Li, Peter Henderson&lt;br/&gt;Relevance: 5 / 5&lt;br/&gt;&lt;br/&gt;&lt;a href='https://arxiv.org/abs/2505.18384'&gt;Read on arXiv&lt;/a&gt;</description><pubDate>Tue, 27 May 2025 22:13:52 +0000</pubDate></item><item><title>A Critical Evaluation of Defenses against Prompt Injection Attacks</title><link>https://arxiv.org/abs/2505.18333</link><description>• This paper critically evaluates defenses against prompt injection attacks on Large Language Models (LLMs), proposing a principled methodology for assessing their effectiveness and utility. It demonstrates that existing defenses are less effective than previously reported and provides resources for future research in this area.&lt;br/&gt;&lt;br/&gt;Tags: prompt injection, LLM security, defense evaluation, adversarial attacks, robustness&lt;br/&gt;Authors: Yuqi Jia, Zedian Shao, Yupei Liu, Jinyuan Jia, Dawn Song, Neil Zhenqiang Gong&lt;br/&gt;Relevance: 5 / 5&lt;br/&gt;&lt;br/&gt;&lt;a href='https://arxiv.org/abs/2505.18333'&gt;Read on arXiv&lt;/a&gt;</description><pubDate>Tue, 27 May 2025 22:13:52 +0000</pubDate></item><item><title>An Attack to Break Permutation-Based Private Third-Party Inference Schemes for LLMs</title><link>https://arxiv.org/abs/2505.18332</link><description>• The paper presents a novel attack that breaks permutation-based privacy schemes for LLM inference, demonstrating the ability to reconstruct original prompts from obfuscated hidden states.&lt;br/&gt;• It analyzes the insecurity of several recent privacy-preserving inference schemes and critiques prior theoretical security proofs.&lt;br/&gt;• The work directly addresses vulnerabilities and attacks in LLM privacy mechanisms, emphasizing the need for rigorous security analysis.&lt;br/&gt;&lt;br/&gt;Tags: adversarial attacks, LLM security, privacy attacks, model inversion, robustness under attack&lt;br/&gt;Authors: Rahul Thomas, Louai Zahran, Erica Choi, Akilesh Potti, Micah Goldblum, Arka Pal&lt;br/&gt;Relevance: 5 / 5&lt;br/&gt;&lt;br/&gt;&lt;a href='https://arxiv.org/abs/2505.18332'&gt;Read on arXiv&lt;/a&gt;</description><pubDate>Tue, 27 May 2025 22:13:52 +0000</pubDate></item><item><title>Architectural Backdoors for Within-Batch Data Stealing and Model Inference Manipulation</title><link>https://arxiv.org/abs/2505.18323</link><description>• This paper introduces a new class of architectural backdoors in neural networks that exploit batched inference to enable large-scale user data manipulation and theft. The attacks allow information leakage and manipulation between concurrent user requests, posing significant threats to user privacy and system integrity. The authors also propose a deterministic mitigation strategy based on Information Flow Control to formally guarantee protection against these attacks, and conduct a large-scale analysis revealing widespread vulnerabilities in real-world models.&lt;br/&gt;&lt;br/&gt;Tags: backdoors, information leakage, model manipulation, data theft, robustness, AI security, privacy, mitigation&lt;br/&gt;Authors: Nicolas K\"uchler, Ivan Petrov, Conrad Grobler, Ilia Shumailov&lt;br/&gt;Relevance: 5 / 5&lt;br/&gt;&lt;br/&gt;&lt;a href='https://arxiv.org/abs/2505.18323'&gt;Read on arXiv&lt;/a&gt;</description><pubDate>Tue, 27 May 2025 22:13:52 +0000</pubDate></item><item><title>GenAI Security: Outsmarting the Bots with a Proactive Testing Framework</title><link>https://arxiv.org/abs/2505.18172</link><description>• The paper introduces a proactive security framework for Generative AI (GenAI) systems, specifically targeting risks such as adversarial attacks and prompt injection.&lt;br/&gt;• It empirically evaluates the framework using a prompt injection dataset, demonstrating its effectiveness against malicious exploitation.&lt;br/&gt;• The work emphasizes the need for proactive, rather than reactive, security measures in GenAI deployment.&lt;br/&gt;&lt;br/&gt;Tags: GenAI security, adversarial attacks, prompt injection, proactive defense, LLM security, robustness&lt;br/&gt;Authors: Sunil Kumar Jang Bahadur, Gopala Dhar, Lavi Nigam&lt;br/&gt;Relevance: 5 / 5&lt;br/&gt;&lt;br/&gt;&lt;a href='https://arxiv.org/abs/2505.18172'&gt;Read on arXiv&lt;/a&gt;</description><pubDate>Tue, 27 May 2025 22:13:52 +0000</pubDate></item><item><title>InjectLab: A Tactical Framework for Adversarial Threat Modeling Against Large Language Models</title><link>https://arxiv.org/abs/2505.18156</link><description>• The paper introduces InjectLab, a tactical framework for adversarial threat modeling against Large Language Models (LLMs).&lt;br/&gt;• It focuses on prompt-based attacks, including instruction override, identity swapping, and multi-agent exploitation.&lt;br/&gt;• InjectLab provides a structured matrix of real-world adversarial techniques, detection guidance, mitigation strategies, and simulation tools.&lt;br/&gt;• The framework is inspired by MITRE ATT&amp;CK and aims to help secure LLMs against prompt injection and related threats.&lt;br/&gt;&lt;br/&gt;Tags: prompt injection, adversarial attacks, LLM security, threat modeling, robustness, mitigation strategies&lt;br/&gt;Authors: Austin Howard&lt;br/&gt;Relevance: 5 / 5&lt;br/&gt;&lt;br/&gt;&lt;a href='https://arxiv.org/abs/2505.18156'&gt;Read on arXiv&lt;/a&gt;</description><pubDate>Tue, 27 May 2025 22:13:52 +0000</pubDate></item><item><title>An Embarrassingly Simple Defense Against LLM Abliteration Attacks</title><link>https://arxiv.org/abs/2505.19056</link><description>• The paper addresses a specific adversarial attack on large language models (LLMs) called 'abliteration', which suppresses refusal behavior and enables unethical outputs. It proposes and evaluates a defense mechanism involving fine-tuning on an extended-refusal dataset to neutralize this attack while maintaining model utility.&lt;br/&gt;&lt;br/&gt;Tags: LLM security, adversarial attacks, robustness, defense, model alignment&lt;br/&gt;Authors: Harethah Abu Shairah, Hasan Abed Al Kader Hammoud, Bernard Ghanem, George Turkiyyah&lt;br/&gt;Relevance: 5 / 5&lt;br/&gt;&lt;br/&gt;&lt;a href='https://arxiv.org/abs/2505.19056'&gt;Read on arXiv&lt;/a&gt;</description><pubDate>Tue, 27 May 2025 22:13:52 +0000</pubDate></item><item><title>CRMArena-Pro: Holistic Assessment of LLM Agents Across Diverse Business Scenarios and Interactions</title><link>https://arxiv.org/abs/2505.18878</link><description>• The paper introduces CRMArena-Pro, a benchmark for evaluating LLM agents in business scenarios, with a focus that includes confidentiality awareness assessments. The findings highlight that current LLMs have near-zero inherent confidentiality awareness, which is a key aspect of AI security, particularly regarding information leakage and misuse. The paper discusses the impact of prompting on confidentiality and the trade-offs with task performance, directly relating to secure deployment and robustness against inadvertent data exposure.&lt;br/&gt;&lt;br/&gt;Tags: confidentiality, LLM misuse, robustness, enterprise security&lt;br/&gt;Authors: Kung-Hsiang Huang, Akshara Prabhakar, Onkar Thorat, Divyansh Agarwal, Prafulla Kumar Choubey, Yixin Mao, Silvio Savarese, Caiming Xiong, Chien-Sheng Wu&lt;br/&gt;Relevance: 3 / 5&lt;br/&gt;&lt;br/&gt;&lt;a href='https://arxiv.org/abs/2505.18878'&gt;Read on arXiv&lt;/a&gt;</description><pubDate>Tue, 27 May 2025 22:13:52 +0000</pubDate></item><item><title>Audio Jailbreak Attacks: Exposing Vulnerabilities in SpeechGPT in a White-Box Framework</title><link>https://arxiv.org/abs/2505.18864</link><description>• This paper presents a novel adversarial attack (audio jailbreak) against SpeechGPT, a voice-enabled multimodal large language model. The attack exploits vulnerabilities in the speech input modality to bypass alignment safeguards and induce prohibited outputs. The work demonstrates high attack success rates and highlights the security risks unique to voice-based AI systems.&lt;br/&gt;&lt;br/&gt;Tags: audio jailbreak, adversarial attacks, LLM misuse, robustness under attack, multimodal models, SpeechGPT, voice modality&lt;br/&gt;Authors: Binhao Ma, Hanqing Guo, Zhengping Jay Luo, Rui Duan&lt;br/&gt;Relevance: 5 / 5&lt;br/&gt;&lt;br/&gt;&lt;a href='https://arxiv.org/abs/2505.18864'&gt;Read on arXiv&lt;/a&gt;</description><pubDate>Tue, 27 May 2025 22:13:52 +0000</pubDate></item><item><title>Robustness in Large Language Models: A Survey of Mitigation Strategies and Evaluation Metrics</title><link>https://arxiv.org/abs/2505.18658</link><description>• This survey paper provides a comprehensive overview of robustness in Large Language Models (LLMs), including sources of non-robustness such as adversarial factors, and reviews mitigation strategies and evaluation metrics. It discusses both intrinsic and external vulnerabilities that can compromise model reliability, which are central to AI security concerns.&lt;br/&gt;&lt;br/&gt;Tags: LLM robustness, adversarial attacks, mitigation strategies, evaluation metrics, AI security&lt;br/&gt;Authors: Pankaj Kumar, Subhankar Mishra&lt;br/&gt;Relevance: 5 / 5&lt;br/&gt;&lt;br/&gt;&lt;a href='https://arxiv.org/abs/2505.18658'&gt;Read on arXiv&lt;/a&gt;</description><pubDate>Tue, 27 May 2025 22:13:52 +0000</pubDate></item><item><title>Safety Alignment via Constrained Knowledge Unlearning</title><link>https://arxiv.org/abs/2505.18588</link><description>• This paper addresses the vulnerability of large language models (LLMs) to jailbreak attacks and proposes a new method, Constrained Knowledge Unlearning (CKU), to improve safety alignment by selectively unlearning harmful knowledge while retaining useful information. The approach is evaluated for its effectiveness in mitigating harmful outputs and enhancing model safety.&lt;br/&gt;&lt;br/&gt;Tags: jailbreaks, LLM misuse, robustness under attack, safety alignment, knowledge unlearning&lt;br/&gt;Authors: Zesheng Shi, Yucheng Zhou, Jing Li&lt;br/&gt;Relevance: 5 / 5&lt;br/&gt;&lt;br/&gt;&lt;a href='https://arxiv.org/abs/2505.18588'&gt;Read on arXiv&lt;/a&gt;</description><pubDate>Tue, 27 May 2025 22:13:52 +0000</pubDate></item><item><title>Exploring the Vulnerability of the Content Moderation Guardrail in Large Language Models via Intent Manipulation</title><link>https://arxiv.org/abs/2505.18556</link><description>• This paper investigates vulnerabilities in content moderation guardrails of large language models (LLMs) by manipulating intent detection mechanisms.&lt;br/&gt;• It introduces a novel intent-based prompt-refinement framework (IntentPrompt) to enhance jailbreak success rates for red-teaming purposes.&lt;br/&gt;• The work demonstrates that current intent-aware guardrails can be bypassed using advanced prompt engineering, highlighting weaknesses in LLM safety mechanisms.&lt;br/&gt;&lt;br/&gt;Tags: jailbreaks, prompt injection, LLM misuse, red teaming, robustness under attack, content moderation, intent manipulation&lt;br/&gt;Authors: Jun Zhuang, Haibo Jin, Ye Zhang, Zhengjian Kang, Wenbin Zhang, Gaby G. Dagher, Haohan Wang&lt;br/&gt;Relevance: 5 / 5&lt;br/&gt;&lt;br/&gt;&lt;a href='https://arxiv.org/abs/2505.18556'&gt;Read on arXiv&lt;/a&gt;</description><pubDate>Tue, 27 May 2025 22:13:52 +0000</pubDate></item><item><title>CoMet: Metaphor-Driven Covert Communication for Multi-Agent Language Games</title><link>https://arxiv.org/abs/2505.18218</link><description>• The paper introduces CoMet, a framework for enabling LLM-based agents to use metaphors for covert communication and semantic evasion in multi-agent language games. It evaluates the system on games emphasizing covert communication and adversarial evasion, which are relevant to AI security concerns such as evasion and misuse.&lt;br/&gt;&lt;br/&gt;Tags: covert communication, semantic evasion, LLM misuse, adversarial games, multi-agent systems&lt;br/&gt;Authors: Shuhang Xu, Fangwei Zhong&lt;br/&gt;Relevance: 4 / 5&lt;br/&gt;&lt;br/&gt;&lt;a href='https://arxiv.org/abs/2505.18218'&gt;Read on arXiv&lt;/a&gt;</description><pubDate>Tue, 27 May 2025 22:13:52 +0000</pubDate></item><item><title>Does Representation Intervention Really Identify Desired Concepts and Elicit Alignment?</title><link>https://arxiv.org/abs/2505.18672</link><description>• This paper investigates representation intervention techniques in LLMs for safety alignment, specifically focusing on erasing harmful concepts and improving robustness against adversarial prompts and jailbreaks. It introduces a new method (COCA) to enhance the effectiveness of such interventions and demonstrates reduced jailbreak success rates while maintaining model performance.&lt;br/&gt;&lt;br/&gt;Tags: LLM security, jailbreak robustness, adversarial prompts, representation intervention, model alignment&lt;br/&gt;Authors: Hongzheng Yang, Yongqiang Chen, Zeyu Qin, Tongliang Liu, Chaowei Xiao, Kun Zhang, Bo Han&lt;br/&gt;Relevance: 5 / 5&lt;br/&gt;&lt;br/&gt;&lt;a href='https://arxiv.org/abs/2505.18672'&gt;Read on arXiv&lt;/a&gt;</description><pubDate>Tue, 27 May 2025 22:13:52 +0000</pubDate></item><item><title>A Robust PPO-optimized Tabular Transformer Framework for Intrusion Detection in Industrial IoT Systems</title><link>https://arxiv.org/abs/2505.18234</link><description>• The paper presents a robust, reinforcement learning-enhanced intrusion detection system (NIDS) for Industrial IoT, focusing on detecting network attacks, including rare and few-shot attack scenarios. It leverages TabTransformer and PPO to improve detection robustness and generalization, specifically targeting adversarial threats in network environments.&lt;br/&gt;&lt;br/&gt;Tags: intrusion detection, robustness, adversarial attacks, industrial IoT, reinforcement learning, network security&lt;br/&gt;Authors: Yuanya She&lt;br/&gt;Relevance: 4 / 5&lt;br/&gt;&lt;br/&gt;&lt;a href='https://arxiv.org/abs/2505.18234'&gt;Read on arXiv&lt;/a&gt;</description><pubDate>Tue, 27 May 2025 22:13:52 +0000</pubDate></item><item><title>Origin Tracer: A Method for Detecting LoRA Fine-Tuning Origins in LLMs</title><link>https://arxiv.org/abs/2505.19466</link><description>• The paper introduces Origin-Tracer, a method for detecting whether a large language model has been fine-tuned from a specific base model, even under obfuscation techniques. This addresses concerns about model provenance, transparency, and potential misuse, which are important aspects of AI security, particularly regarding model theft and unauthorized fine-tuning.&lt;br/&gt;&lt;br/&gt;Tags: model verification, model provenance, model theft, fine-tuning detection, AI security&lt;br/&gt;Authors: Hongyu Liang, Yuting Zheng, Yihan Li, Yiran Zhang, Shiyu Liang&lt;br/&gt;Relevance: 4 / 5&lt;br/&gt;&lt;br/&gt;&lt;a href='https://arxiv.org/abs/2505.19466'&gt;Read on arXiv&lt;/a&gt;</description><pubDate>Tue, 27 May 2025 22:13:52 +0000</pubDate></item><item><title>Stronger Enforcement of Instruction Hierarchy via Augmented Intermediate Representations</title><link>https://arxiv.org/abs/2505.18907</link><description>• The paper addresses prompt injection attacks, a key AI security vulnerability in large language models (LLMs).&lt;br/&gt;• It proposes a novel defense mechanism that augments intermediate token representations with layer-specific trainable embeddings to better enforce instruction hierarchy and privilege levels.&lt;br/&gt;• The method demonstrates significant reductions in attack success rates for prompt injection attacks, indicating improved robustness against this form of adversarial misuse.&lt;br/&gt;&lt;br/&gt;Tags: prompt injection, LLM security, adversarial attacks, robustness, defense mechanisms&lt;br/&gt;Authors: Sanjay Kariyappa, G. Edward Suh&lt;br/&gt;Relevance: 5 / 5&lt;br/&gt;&lt;br/&gt;&lt;a href='https://arxiv.org/abs/2505.18907'&gt;Read on arXiv&lt;/a&gt;</description><pubDate>Tue, 27 May 2025 22:13:52 +0000</pubDate></item><item><title>$C^3$-Bench: The Things Real Disturbing LLM based Agent in Multi-Tasking</title><link>https://arxiv.org/abs/2505.18746</link><description>• The paper introduces C^3-Bench, a benchmark designed to evaluate the robustness and vulnerabilities of LLM-based agents in complex multi-tasking scenarios. It integrates attack concepts and aims to expose model vulnerabilities, particularly in handling tool dependencies, hidden information, and dynamic decision paths. The benchmark is intended to drive research into agent robustness and interpretability, with a focus on identifying weaknesses that could be exploited.&lt;br/&gt;&lt;br/&gt;Tags: LLM robustness, agent vulnerabilities, benchmark, attack concepts, AI security&lt;br/&gt;Authors: Peijie Yu, Yifan Yang, Jinjian Li, Zelong Zhang, Haorui Wang, Xiao Feng, Feng Zhang&lt;br/&gt;Relevance: 4 / 5&lt;br/&gt;&lt;br/&gt;&lt;a href='https://arxiv.org/abs/2505.18746'&gt;Read on arXiv&lt;/a&gt;</description><pubDate>Tue, 27 May 2025 22:13:52 +0000</pubDate></item><item><title>EdgeAgentX: A Novel Framework for Agentic AI at the Edge in Military Communication Networks</title><link>https://arxiv.org/abs/2505.18457</link><description>• The paper presents EdgeAgentX, a framework for agentic AI at the edge in military networks, which integrates adversarial defense mechanisms to robustly withstand adversarial disruptions. The focus on adversarial defense and robustness under attack in AI systems directly addresses AI security concerns.&lt;br/&gt;&lt;br/&gt;Tags: adversarial defense, robustness, military AI, federated learning, multi-agent reinforcement learning&lt;br/&gt;Authors: Abir Ray&lt;br/&gt;Relevance: 4 / 5&lt;br/&gt;&lt;br/&gt;&lt;a href='https://arxiv.org/abs/2505.18457'&gt;Read on arXiv&lt;/a&gt;</description><pubDate>Tue, 27 May 2025 22:13:52 +0000</pubDate></item><item><title>Understanding and Mitigating Overrefusal in LLMs from an Unveiling Perspective of Safety Decision Boundary</title><link>https://arxiv.org/abs/2505.18325</link><description>• This paper investigates the phenomenon of overrefusal in large language models (LLMs), where models refuse to answer legitimate queries due to over-conservative safety alignment. It analyzes the safety decision boundaries that cause this behavior and introduces an automated framework (RASS) to generate and select prompts near these boundaries for better mitigation. The work provides insights into model safety decisions and proposes tools for robust assessment of safety and helpfulness, including a multilingual benchmark.&lt;br/&gt;&lt;br/&gt;Tags: LLM safety, decision boundaries, overrefusal, robustness, alignment&lt;br/&gt;Authors: Licheng Pan, Yongqi Tong, Xin Zhang, Xiaolu Zhang, Jun Zhou, Zhixuan Chu&lt;br/&gt;Relevance: 3 / 5&lt;br/&gt;&lt;br/&gt;&lt;a href='https://arxiv.org/abs/2505.18325'&gt;Read on arXiv&lt;/a&gt;</description><pubDate>Tue, 27 May 2025 22:13:52 +0000</pubDate></item></channel></rss>