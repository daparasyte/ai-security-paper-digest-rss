<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>AI Security Paper Digest</title><link>https://kentaroh-toyoda.github.io/ai-security-paper-digest-rss/rss.xml</link><description>Curated papers on AI security from OpenAlex and ArXiv</description><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><lastBuildDate>Mon, 18 Aug 2025 22:18:19 +0000</lastBuildDate><item><title>Towards Physically Realizable Adversarial Attacks in Embodied Vision Navigation</title><link>https://arxiv.org/abs/2409.10071</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a practical adversarial attack method for embodied vision navigation using learnable adversarial patches&lt;/li&gt;&lt;li&gt;Employs multi-view optimization for cross-viewpoint effectiveness&lt;/li&gt;&lt;li&gt;Introduces two-stage opacity optimization for naturalness&lt;/li&gt;&lt;li&gt;Reduces navigation success rate by 22.39% on average&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Meng Chen', 'Jiawei Tu', 'Chao Qi', 'Yonghao Dang', 'Feng Zhou', 'Wei Wei', 'Jianqin Yin']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial_attacks', 'physical_attacks', 'embodied_navigation', 'red_teaming', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2409.10071</guid><pubDate>Mon, 18 Aug 2025 00:00:00 +0000</pubDate></item><item><title>IPG: Incremental Patch Generation for Generalized Adversarial Patch Training</title><link>https://arxiv.org/abs/2508.10946</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces IPG for generating adversarial patches more efficiently&lt;/li&gt;&lt;li&gt;Demonstrates effectiveness with YOLO experiments and adversarial training&lt;/li&gt;&lt;li&gt;Potential applications in autonomous vehicles, security, and medical imaging&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Wonho Lee', 'Hyunsik Na', 'Jisu Lee', 'Daeseon Choi']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial patches', 'robustness', 'security', 'red teaming']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2508.10946</guid><pubDate>Mon, 18 Aug 2025 00:00:00 +0000</pubDate></item><item><title>SafeConstellations: Steering LLM Safety to Reduce Over-Refusals Through Task-Specific Trajectory</title><link>https://arxiv.org/abs/2508.11290</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Addresses over-refusal in LLMs where safety mechanisms reject benign instructions&lt;/li&gt;&lt;li&gt;Introduces SafeConstellations, an inference-time trajectory-shifting approach&lt;/li&gt;&lt;li&gt;Tracks task-specific patterns in embedding space to guide non-refusal paths&lt;/li&gt;&lt;li&gt;Reduces over-refusal rates by up to 73% with minimal utility impact&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Utsav Maskey', 'Sumit Yadav', 'Mark Dras', 'Usman Naseem']&lt;/li&gt;&lt;li&gt;Tags: ['LLM safety', 'over-refusal mitigation', 'embedding trajectories', 'inference-time adjustment', 'task-specific safety']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2508.11290</guid><pubDate>Mon, 18 Aug 2025 00:00:00 +0000</pubDate></item><item><title>Hell or High Water: Evaluating Agentic Recovery from External Failures</title><link>https://arxiv.org/abs/2508.11027</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces a benchmark for evaluating language model agents' ability to recover from external failures&lt;/li&gt;&lt;li&gt;Tests agents' capacity to adapt when functions become unavailable during planning&lt;/li&gt;&lt;li&gt;Finds that current models struggle to formulate backup plans and adapt to environment feedback&lt;/li&gt;&lt;li&gt;Analyzes the impact of search space size and model scaling on performance&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Andrew Wang', 'Sophia Hager', 'Adi Asija', 'Daniel Khashabi', 'Nicholas Andrews']&lt;/li&gt;&lt;li&gt;Tags: ['safety evaluation', 'robustness', 'agentic planning', 'failure recovery']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2508.11027</guid><pubDate>Mon, 18 Aug 2025 00:00:00 +0000</pubDate></item><item><title>Pr$\epsilon\epsilon$mpt: Sanitizing Sensitive Prompts for LLMs</title><link>https://arxiv.org/abs/2504.05147</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces Prεεmpt, a system for sanitizing sensitive prompts in LLMs&lt;/li&gt;&lt;li&gt;Categorizes sensitive tokens into format-dependent (FPE) and value-dependent (mDP) types&lt;/li&gt;&lt;li&gt;Demonstrates high privacy protection while maintaining response quality&lt;/li&gt;&lt;li&gt;Outperforms prior methods in utility&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Amrita Roy Chowdhury', 'David Glukhov', 'Divyam Anshumaan', 'Prasad Chalasani', 'Nicolas Papernot', 'Somesh Jha', 'Mihir Bellare']&lt;/li&gt;&lt;li&gt;Tags: ['privacy', 'prompt_sanitization', 'differential_privacy', 'encryption', 'LLM_security']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2504.05147</guid><pubDate>Mon, 18 Aug 2025 00:00:00 +0000</pubDate></item><item><title>Chasing Moving Targets with Online Self-Play Reinforcement Learning for Safer Language Models</title><link>https://arxiv.org/abs/2506.07468</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces Self-RedTeam: online self-play RL for co-evolving attacker/defender agents&lt;/li&gt;&lt;li&gt;Uses two-player zero-sum game framework with theoretical safety guarantees&lt;/li&gt;&lt;li&gt;Empirically shows improved attack diversity and defense robustness&lt;/li&gt;&lt;li&gt;Incorporates hidden Chain-of-Thought for better planning and reduced over-refusals&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Mickel Liu', 'Liwei Jiang', 'Yancheng Liang', 'Simon Shaolei Du', 'Yejin Choi', 'Tim Althoff', 'Natasha Jaques']&lt;/li&gt;&lt;li&gt;Tags: ['red teaming', 'adversarial prompting', 'safety', 'reinforcement learning', 'MARL']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.07468</guid><pubDate>Mon, 18 Aug 2025 00:00:00 +0000</pubDate></item><item><title>JMA: a General Algorithm to Craft Nearly Optimal Targeted Adversarial Example</title><link>https://arxiv.org/abs/2401.01199</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces JMA, a targeted adversarial attack algorithm using Jacobian-induced Mahalanobis distance&lt;/li&gt;&lt;li&gt;Optimizes latent space movement with Wolfe duality and NNLS&lt;/li&gt;&lt;li&gt;Effective in multi-label classification scenarios&lt;/li&gt;&lt;li&gt;More efficient than existing methods&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Benedetta Tondi', 'Wei Guo', 'Niccol\\`o Pancino', 'Mauro Barni']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial attacks', 'red teaming', 'security', 'robustness', 'multi-label']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2401.01199</guid><pubDate>Mon, 18 Aug 2025 00:00:00 +0000</pubDate></item><item><title>ORFuzz: Fuzzing the "Other Side" of LLM Safety -- Testing Over-Refusal</title><link>https://arxiv.org/abs/2508.11222</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces ORFuzz, an evolutionary testing framework for detecting LLM over-refusal&lt;/li&gt;&lt;li&gt;Features safety category-aware seed selection, adaptive mutator optimization, and OR-Judge for evaluation&lt;/li&gt;&lt;li&gt;Demonstrates 6.98% average over-refusal rate, double that of baselines&lt;/li&gt;&lt;li&gt;Releases ORFuzzSet, a benchmark of 1,855 test cases with 63.56% average over-refusal rate across 10 LLMs&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Haonan Zhang', 'Dongxia Wang', 'Yi Liu', 'Kexin Chen', 'Jiashui Wang', 'Xinlei Ying', 'Long Liu', 'Wenhai Wang']&lt;/li&gt;&lt;li&gt;Tags: ['safety', 'testing', 'LLM', 'benchmark', 'red teaming']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2508.11222</guid><pubDate>Mon, 18 Aug 2025 00:00:00 +0000</pubDate></item><item><title>MCP-Guard: A Defense Framework for Model Context Protocol Integrity in Large Language Model Applications</title><link>https://arxiv.org/abs/2508.10991</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes MCP-Guard, a three-stage defense framework against prompt injection and other attacks in LLM-tool interactions&lt;/li&gt;&lt;li&gt;Introduces MCP-AttackBench, a comprehensive benchmark with 70,000+ adversarial samples&lt;/li&gt;&lt;li&gt;Features a fine-tuned E5 model achieving 96.01% accuracy in adversarial prompt detection&lt;/li&gt;&lt;li&gt;Balances efficiency with accuracy through static scanning, neural detection, and LLM arbitration&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Wenpeng Xing', 'Zhonghao Qi', 'Yupeng Qin', 'Yilin Li', 'Caini Chang', 'Jiahui Yu', 'Changting Lin', 'Zhenzhen Xie', 'Meng Han']&lt;/li&gt;&lt;li&gt;Tags: ['red teaming', 'prompt injection', 'adversarial prompting', 'defense framework', 'benchmark']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2508.10991</guid><pubDate>Mon, 18 Aug 2025 00:00:00 +0000</pubDate></item></channel></rss>