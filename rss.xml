<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>AI Security Paper Digest</title><link>https://kentaroh-toyoda.github.io/ai-security-paper-digest-rss/rss.xml</link><description>Curated papers on AI security from ArXiv</description><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><lastBuildDate>Mon, 01 Sep 2025 22:16:10 +0000</lastBuildDate><item><title>WebInject: Prompt Injection Attack to Web Agents</title><link>https://arxiv.org/abs/2505.11717</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces WebInject, a prompt injection attack targeting MLLM-based web agents&lt;/li&gt;&lt;li&gt;Manipulates webpage pixel values to induce agent actions&lt;/li&gt;&lt;li&gt;Overcomes non-differentiable rendering with neural network approximation&lt;/li&gt;&lt;li&gt;Demonstrates high effectiveness against multiple datasets&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Xilong Wang', 'John Bloch', 'Zedian Shao', 'Yuepeng Hu', 'Shuyan Zhou', 'Neil Zhenqiang Gong']&lt;/li&gt;&lt;li&gt;Tags: ['prompt injection', 'adversarial attack', 'web agent', 'multi-modal', 'security']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2505.11717</guid><pubDate>Mon, 01 Sep 2025 00:00:00 +0000</pubDate></item><item><title>WebInject: Prompt Injection Attack to Web Agents</title><link>https://arxiv.org/abs/2505.11717</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces WebInject, a prompt injection attack targeting MLLM-based web agents&lt;/li&gt;&lt;li&gt;Manipulates webpage pixel values to induce agent actions&lt;/li&gt;&lt;li&gt;Overcomes non-differentiable rendering with neural network approximation&lt;/li&gt;&lt;li&gt;Demonstrates high effectiveness against multiple datasets&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Xilong Wang', 'John Bloch', 'Zedian Shao', 'Yuepeng Hu', 'Shuyan Zhou', 'Neil Zhenqiang Gong']&lt;/li&gt;&lt;li&gt;Tags: ['prompt injection', 'adversarial attack', 'web agent', 'multi-modal', 'security']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2505.11717</guid><pubDate>Mon, 01 Sep 2025 00:00:00 +0000</pubDate></item><item><title>Refusal Tokens: A Simple Way to Calibrate Refusals in Large Language Models</title><link>https://arxiv.org/abs/2412.06748</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces refusal tokens to calibrate model refusals without retraining&lt;/li&gt;&lt;li&gt;Enables per-category refusal rate adjustment during inference&lt;/li&gt;&lt;li&gt;Aims to enhance model safety and alignment with user preferences&lt;/li&gt;&lt;li&gt;Reduces computational costs by eliminating multiple model trainings&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Neel Jain', 'Aditya Shrivastava', 'Chenyang Zhu', 'Daben Liu', 'Alfy Samuel', 'Ashwinee Panda', 'Anoop Kumar', 'Micah Goldblum', 'Tom Goldstein']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'safety', 'refusal', 'model-steering', 'inference-time-control']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2412.06748</guid><pubDate>Mon, 01 Sep 2025 00:00:00 +0000</pubDate></item><item><title>FedSEA-LLaMA: A Secure, Efficient and Adaptive Federated Splitting Framework for Large Language Models</title><link>https://arxiv.org/abs/2505.15683</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Injects Gaussian noise into hidden states for secure transmission&lt;/li&gt;&lt;li&gt;Reduces communication overhead with attention-mask compression and KV cache collaboration&lt;/li&gt;&lt;li&gt;Allows dynamic partition points for task-specific adaptability&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Zishuai Zhang', 'Hainan zhang', 'Weihua Li', 'Qinnan zhang', 'jin Dong', 'Yongxin Tong', 'Zhiming Zheng']&lt;/li&gt;&lt;li&gt;Tags: ['federated learning', 'security', 'privacy', 'LLM', 'adaptability']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2505.15683</guid><pubDate>Mon, 01 Sep 2025 00:00:00 +0000</pubDate></item><item><title>Going over Fine Web with a Fine-Tooth Comb: Technical Report of Indexing Fine Web for Problematic Content Search and Retrieval</title><link>https://arxiv.org/abs/2508.21788</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Presents an ElasticSearch-based framework for indexing and analyzing LLM training datasets&lt;/li&gt;&lt;li&gt;Applies to FineWeb-2 (1.5TB, four languages) with millisecond query performance&lt;/li&gt;&lt;li&gt;Enables real-time detection of problematic content in training data&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ["In\\'es Altemir Marinas", 'Anastasiia Kucherenko', 'Andrei Kucharavy']&lt;/li&gt;&lt;li&gt;Tags: ['data_poisoning', 'safety_evaluation', 'robustness', 'training_data_analysis']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2508.21788</guid><pubDate>Mon, 01 Sep 2025 00:00:00 +0000</pubDate></item><item><title>SAGA: A Security Architecture for Governing AI Agentic Systems</title><link>https://arxiv.org/abs/2504.21034</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes SAGA, a security architecture for governing AI agentic systems&lt;/li&gt;&lt;li&gt;Introduces cryptographic access control tokens with formal security guarantees&lt;/li&gt;&lt;li&gt;Evaluated with agents using different LLMs and geolocations, showing minimal overhead&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Georgios Syros', 'Anshuman Suri', 'Jacob Ginesin', 'Cristina Nita-Rotaru', 'Alina Oprea']&lt;/li&gt;&lt;li&gt;Tags: ['security architecture', 'access control', 'agent governance', 'cryptographic tokens', 'LLM security']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2504.21034</guid><pubDate>Mon, 01 Sep 2025 00:00:00 +0000</pubDate></item><item><title>WebInject: Prompt Injection Attack to Web Agents</title><link>https://arxiv.org/abs/2505.11717</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces WebInject, a prompt injection attack targeting MLLM-based web agents&lt;/li&gt;&lt;li&gt;Manipulates webpage pixel values to induce agent actions&lt;/li&gt;&lt;li&gt;Overcomes non-differentiable rendering with neural network approximation&lt;/li&gt;&lt;li&gt;Demonstrates high effectiveness against multiple datasets&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Xilong Wang', 'John Bloch', 'Zedian Shao', 'Yuepeng Hu', 'Shuyan Zhou', 'Neil Zhenqiang Gong']&lt;/li&gt;&lt;li&gt;Tags: ['prompt injection', 'adversarial attack', 'web agent', 'multi-modal', 'security']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2505.11717</guid><pubDate>Mon, 01 Sep 2025 00:00:00 +0000</pubDate></item><item><title>Refusal Tokens: A Simple Way to Calibrate Refusals in Large Language Models</title><link>https://arxiv.org/abs/2412.06748</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces refusal tokens to calibrate model refusals without retraining&lt;/li&gt;&lt;li&gt;Enables per-category refusal rate adjustment during inference&lt;/li&gt;&lt;li&gt;Aims to enhance model safety and alignment with user preferences&lt;/li&gt;&lt;li&gt;Reduces computational costs by eliminating multiple model trainings&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Neel Jain', 'Aditya Shrivastava', 'Chenyang Zhu', 'Daben Liu', 'Alfy Samuel', 'Ashwinee Panda', 'Anoop Kumar', 'Micah Goldblum', 'Tom Goldstein']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'safety', 'refusal', 'model-steering', 'inference-time-control']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2412.06748</guid><pubDate>Mon, 01 Sep 2025 00:00:00 +0000</pubDate></item><item><title>I Stolenly Swear That I Am Up to (No) Good: Design and Evaluation of Model Stealing Attacks</title><link>https://arxiv.org/abs/2508.21654</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Standardizes design and evaluation of model stealing attacks&lt;/li&gt;&lt;li&gt;Proposes comprehensive threat model&lt;/li&gt;&lt;li&gt;Analyzes attack setups and provides best practices&lt;/li&gt;&lt;li&gt;Focuses on image classification models&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Daryna Oliynyk', 'Rudolf Mayer', 'Kathrin Grosse', 'Andreas Rauber']&lt;/li&gt;&lt;li&gt;Tags: ['model stealing', 'security', 'evaluation', 'threat model', 'image classification']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2508.21654</guid><pubDate>Mon, 01 Sep 2025 00:00:00 +0000</pubDate></item><item><title>FedSEA-LLaMA: A Secure, Efficient and Adaptive Federated Splitting Framework for Large Language Models</title><link>https://arxiv.org/abs/2505.15683</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Injects Gaussian noise into hidden states for secure transmission&lt;/li&gt;&lt;li&gt;Reduces communication overhead with attention-mask compression and KV cache collaboration&lt;/li&gt;&lt;li&gt;Allows dynamic partition points for task-specific adaptability&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Zishuai Zhang', 'Hainan zhang', 'Weihua Li', 'Qinnan zhang', 'jin Dong', 'Yongxin Tong', 'Zhiming Zheng']&lt;/li&gt;&lt;li&gt;Tags: ['federated learning', 'security', 'privacy', 'LLM', 'adaptability']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2505.15683</guid><pubDate>Mon, 01 Sep 2025 00:00:00 +0000</pubDate></item><item><title>WebInject: Prompt Injection Attack to Web Agents</title><link>https://arxiv.org/abs/2505.11717</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces WebInject, a prompt injection attack targeting MLLM-based web agents&lt;/li&gt;&lt;li&gt;Manipulates webpage pixel values to induce agent actions&lt;/li&gt;&lt;li&gt;Overcomes non-differentiable rendering with neural network approximation&lt;/li&gt;&lt;li&gt;Demonstrates high effectiveness against multiple datasets&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Xilong Wang', 'John Bloch', 'Zedian Shao', 'Yuepeng Hu', 'Shuyan Zhou', 'Neil Zhenqiang Gong']&lt;/li&gt;&lt;li&gt;Tags: ['prompt injection', 'adversarial attack', 'web agent', 'multi-modal', 'security']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2505.11717</guid><pubDate>Mon, 01 Sep 2025 00:00:00 +0000</pubDate></item><item><title>SAGA: A Security Architecture for Governing AI Agentic Systems</title><link>https://arxiv.org/abs/2504.21034</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes SAGA, a security architecture for governing AI agentic systems&lt;/li&gt;&lt;li&gt;Introduces cryptographic access control tokens with formal security guarantees&lt;/li&gt;&lt;li&gt;Evaluated with agents using different LLMs and geolocations, showing minimal overhead&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Georgios Syros', 'Anshuman Suri', 'Jacob Ginesin', 'Cristina Nita-Rotaru', 'Alina Oprea']&lt;/li&gt;&lt;li&gt;Tags: ['security architecture', 'access control', 'agent governance', 'cryptographic tokens', 'LLM security']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2504.21034</guid><pubDate>Mon, 01 Sep 2025 00:00:00 +0000</pubDate></item><item><title>RevPRAG: Revealing Poisoning Attacks in Retrieval-Augmented Generation through LLM Activation Analysis</title><link>https://arxiv.org/abs/2411.18948</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces RevPRAG, a detection pipeline for RAG poisoning attacks using LLM activation analysis&lt;/li&gt;&lt;li&gt;Identifies distinct activation patterns between correct and poisoned responses&lt;/li&gt;&lt;li&gt;Achieves 98% true positive rate with &lt;1% false positive rate on benchmarks&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Xue Tan', 'Hao Luan', 'Mingyu Luo', 'Xiaoyan Sun', 'Ping Chen', 'Jun Dai']&lt;/li&gt;&lt;li&gt;Tags: ['data_poisoning', 'security', 'detection', 'rag', 'llm_activation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2411.18948</guid><pubDate>Mon, 01 Sep 2025 00:00:00 +0000</pubDate></item><item><title>Going over Fine Web with a Fine-Tooth Comb: Technical Report of Indexing Fine Web for Problematic Content Search and Retrieval</title><link>https://arxiv.org/abs/2508.21788</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Presents an ElasticSearch-based framework for indexing and analyzing LLM training datasets&lt;/li&gt;&lt;li&gt;Applies to FineWeb-2 (1.5TB, four languages) with millisecond query performance&lt;/li&gt;&lt;li&gt;Enables real-time detection of problematic content in training data&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ["In\\'es Altemir Marinas", 'Anastasiia Kucherenko', 'Andrei Kucharavy']&lt;/li&gt;&lt;li&gt;Tags: ['data_poisoning', 'safety_evaluation', 'robustness', 'training_data_analysis']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2508.21788</guid><pubDate>Mon, 01 Sep 2025 00:00:00 +0000</pubDate></item><item><title>zkLoRA: Fine-Tuning Large Language Models with Verifiable Security via Zero-Knowledge Proofs</title><link>https://arxiv.org/abs/2508.21393</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Presents zkLoRA for secure LLM fine-tuning with ZKPs&lt;/li&gt;&lt;li&gt;Ensures privacy and verifiability of training process&lt;/li&gt;&lt;li&gt;Uses advanced cryptographic verification methods&lt;/li&gt;&lt;li&gt;Validated on LLaMA models with up to 13B parameters&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Guofu Liao', 'Taotao Wang', 'Shengli Zhang', 'Jiqun Zhang', 'Shi Long', 'Dacheng Tao']&lt;/li&gt;&lt;li&gt;Tags: ['security', 'privacy', 'zero-knowledge', 'LLM', 'fine-tuning']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2508.21393</guid><pubDate>Mon, 01 Sep 2025 00:00:00 +0000</pubDate></item><item><title>RoboInspector: Unveiling the Unreliability of Policy Code for LLM-enabled Robotic Manipulation</title><link>https://arxiv.org/abs/2508.21378</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces RoboInspector pipeline to evaluate policy code reliability for LLM-enabled robotic manipulation&lt;/li&gt;&lt;li&gt;Identifies four main unreliable behaviors causing manipulation failures&lt;/li&gt;&lt;li&gt;Proposes refinement approach using failure feedback improving reliability by up to 35%&lt;/li&gt;&lt;li&gt;Conducts experiments across 168 task-instruction-LLM combinations in simulation and real-world&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Chenduo Ying', 'Linkang Du', 'Peng Cheng', 'Yuanchao Shu']&lt;/li&gt;&lt;li&gt;Tags: ['safety', 'LLM', 'robotics', 'policy code', 'reliability']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2508.21378</guid><pubDate>Mon, 01 Sep 2025 00:00:00 +0000</pubDate></item></channel></rss>