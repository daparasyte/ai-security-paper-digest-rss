<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>AI Security Paper Digest</title><link>https://kentaroh-toyoda.github.io/ai-security-paper-digest-rss/rss.xml</link><description>Curated papers on AI security from OpenAlex and ArXiv</description><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><lastBuildDate>Tue, 22 Jul 2025 02:15:54 +0000</lastBuildDate><item><title>Sparsification Under Siege: Defending Against Poisoning Attacks in Communication-Efficient Federated Learning</title><link>https://arxiv.org/abs/2505.01454</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Addresses poisoning attacks in sparsified federated learning&lt;/li&gt;&lt;li&gt;Proposes FLARE framework with sparse index mask inspection and model update sign analysis&lt;/li&gt;&lt;li&gt;Demonstrates superior defense performance while maintaining communication efficiency&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Zhiyong Jin', 'Runhua Xu', 'Chao Li', 'Yizhong Liu', 'Jianxin Li']&lt;/li&gt;&lt;li&gt;Tags: ['data poisoning', 'federated learning', 'defense mechanism', 'sparsification']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2505.01454</guid><pubDate>Tue, 22 Jul 2025 00:00:00 +0000</pubDate></item><item><title>DiffGradCAM: A Universal Class Activation Map Resistant to Adversarial Training</title><link>https://arxiv.org/abs/2506.08514</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Existing CAM methods like GradCAM are vulnerable to adversarial manipulation through passive fooling&lt;/li&gt;&lt;li&gt;Introduces SHAMs as a benchmark for entropy-aware passive fooling attacks&lt;/li&gt;&lt;li&gt;Proposes DiffGradCAM, a contrastive CAM approach resistant to passive fooling while maintaining standard CAM performance&lt;/li&gt;&lt;li&gt;Validates robustness across multi-class tasks with varying numbers of classes&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jacob Piland', 'Chris Sweet', 'Adam Czajka']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial_training', 'robustness', 'security', 'explainability', 'CNN']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.08514</guid><pubDate>Tue, 22 Jul 2025 00:00:00 +0000</pubDate></item><item><title>An Adversarial-Driven Experimental Study on Deep Learning for RF Fingerprinting</title><link>https://arxiv.org/abs/2507.14109</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Systematically investigates security risks in DL-based RF fingerprinting&lt;/li&gt;&lt;li&gt;Identifies misclassification under domain shifts exploitable as backdoors&lt;/li&gt;&lt;li&gt;Highlights entanglement of features leading to attack vectors&lt;/li&gt;&lt;li&gt;Demonstrates limitations of post-processing security measures&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Xinyu Cao', 'Bimal Adhikari', 'Shangqing Zhao', 'Jingxian Wu', 'Yanjun Pan']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial attacks', 'security', 'red teaming', 'deep learning', 'RF fingerprinting']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2507.14109</guid><pubDate>Tue, 22 Jul 2025 00:00:00 +0000</pubDate></item><item><title>Minimalist Concept Erasure in Generative Models</title><link>https://arxiv.org/abs/2507.13386</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a minimalist concept erasure method for generative models&lt;/li&gt;&lt;li&gt;Focuses on safety and copyright concerns by removing unwanted concepts&lt;/li&gt;&lt;li&gt;Uses distributional distance of outputs and neuron masking for robustness&lt;/li&gt;&lt;li&gt;Demonstrates effectiveness without degrading model performance&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yang Zhang', 'Er Jin', 'Yanfei Dong', 'Yixuan Wu', 'Philip Torr', 'Ashkan Khakzar', 'Johannes Stegmaier', 'Kenji Kawaguchi']&lt;/li&gt;&lt;li&gt;Tags: ['safety', 'robustness', 'concept erasure', 'generative models', 'neuron masking']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2507.13386</guid><pubDate>Tue, 22 Jul 2025 00:00:00 +0000</pubDate></item><item><title>Fake or Real: The Impostor Hunt in Texts for Space Operations</title><link>https://arxiv.org/abs/2507.13508</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces a competition to detect malicious modifications in LLM outputs&lt;/li&gt;&lt;li&gt;Focuses on data poisoning and overreliance in LLMs&lt;/li&gt;&lt;li&gt;Aims to develop new techniques for distinguishing real vs fake outputs&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Agata Kaczmarek (Warsaw University of Technology)', 'Dawid P{\\l}udowski (Warsaw University of Technology)', "Piotr Wilczy\\'nski (Warsaw University of Technology)", 'Przemys{\\l}aw Biecek (Warsaw University of Technology)', 'Krzysztof Kotowski (KP Labs)', 'Ramez Shendy (KP Labs)', 'Jakub Nalepa (KP Labs', 'Silesian University of Technology)', 'Artur Janicki (Warsaw University of Technology)', 'Evridiki Ntagiou (European Space Agency', 'European Space Operations Center)']&lt;/li&gt;&lt;li&gt;Tags: ['data poisoning', 'adversarial prompting', 'safety evaluation', 'LLM red teaming']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2507.13508</guid><pubDate>Tue, 22 Jul 2025 00:00:00 +0000</pubDate></item><item><title>BLAST: A Stealthy Backdoor Leverage Attack against Cooperative Multi-Agent Deep Reinforcement Learning based Systems</title><link>https://arxiv.org/abs/2501.01593</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces BLAST, a stealthy backdoor attack on cooperative multi-agent DRL systems&lt;/li&gt;&lt;li&gt;Uses behavioral patterns as triggers and modifies reward function to inject backdoor into a single agent&lt;/li&gt;&lt;li&gt;Evaluated against major c-MADRL algorithms (VDN, QMIX, MAPPO) and environments (SMAC, Pursuit)&lt;/li&gt;&lt;li&gt;Demonstrates high attack success rate and low clean performance variance&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jing Fang', 'Saihao Yan', 'Xueyu Yin', 'Yinbo Yu', 'Chunwei Tian', 'Jiajia Liu']&lt;/li&gt;&lt;li&gt;Tags: ['backdoor attack', 'multi-agent systems', 'deep reinforcement learning', 'red teaming', 'security']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2501.01593</guid><pubDate>Tue, 22 Jul 2025 00:00:00 +0000</pubDate></item><item><title>GIFT: Gradient-aware Immunization of diffusion models against malicious Fine-Tuning with safe concepts retention</title><link>https://arxiv.org/abs/2507.13598</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces GIFT: a gradient-aware immunization technique for diffusion models&lt;/li&gt;&lt;li&gt;Defends against malicious fine-tuning while preserving safe content generation&lt;/li&gt;&lt;li&gt;Uses bi-level optimization: upper-level degrades harmful concepts, lower-level preserves safe performance&lt;/li&gt;&lt;li&gt;Experimental results show robust resistance to adversarial attacks while maintaining safe quality&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Amro Abdalla', 'Ismail Shaheen', 'Dan DeGenaro', 'Rupayan Mallick', 'Bogdan Raita', 'Sarah Adel Bargal']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial training', 'model security', 'diffusion models', 'robustness', 'fine-tuning defense']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2507.13598</guid><pubDate>Tue, 22 Jul 2025 00:00:00 +0000</pubDate></item></channel></rss>