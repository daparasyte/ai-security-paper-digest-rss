<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>AI Security Paper Digest</title><link>https://kentaroh-toyoda.github.io/ai-security-paper-digest-rss/rss.xml</link><description>Curated papers on AI security from OpenAlex and ArXiv</description><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><lastBuildDate>Wed, 04 Jun 2025 22:52:14 +0000</lastBuildDate><item><title>IndoSafety: Culturally Grounded Safety for LLMs in Indonesian Languages</title><link>https://arxiv.org/abs/2506.02573</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces IndoSafety, a safety evaluation dataset for LLMs tailored to Indonesian languages and cultural contexts.&lt;/li&gt;&lt;li&gt;Extends existing safety frameworks to include sociocultural considerations specific to Indonesia.&lt;/li&gt;&lt;li&gt;Demonstrates that current Indonesian-centric LLMs often produce unsafe outputs, especially in local languages.&lt;/li&gt;&lt;li&gt;Shows that fine-tuning with IndoSafety improves LLM safety without sacrificing performance.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research Paper&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Quality Assessment&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Clarity: 4/5&lt;/li&gt;&lt;li&gt;Novelty: 5/5&lt;/li&gt;&lt;li&gt;Significance: 5/5&lt;/li&gt;&lt;li&gt;Try-worthiness: Yes&lt;/li&gt;&lt;li&gt;Justification: The paper is clearly structured, with a logical flow from motivation to taxonomy, dataset creation, experimental setup, and results. It introduces a novel, culturally grounded safety taxonomy and evaluation dataset specifically for Indonesian languages, addressing a major gap in LLM safety research for non-English, non-Western contexts. The significance is high, as Indonesia is a large, linguistically diverse country with unique cultural and political sensitivities that are not captured by existing safety benchmarks. The approach is practical and actionable, providing datasets and evaluation protocols that can be directly used to assess and improve LLM safety in Indonesian. The work is highly try-worthy for practitioners and researchers working on LLMs for Southeast Asia or other underrepresented languages. However, the HTML provided does not contain a direct code or dataset repository link; such a link may be present in the full PDF or supplementary materials.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: Muhammad Falensi Azmi, Muhammad Dehan Al Kautsar, Alfan Farizki Wicaksono, Fajri Koto&lt;/li&gt;&lt;li&gt;Tags: LLM safety, cultural alignment, safety evaluation, multilingual AI, harmful output prevention&lt;/li&gt;&lt;li&gt;Relevance Score: 5/5&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.02573</guid><pubDate>Wed, 04 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Do Language Models Think Consistently? A Study of Value Preferences Across Varying Response Lengths</title><link>https://arxiv.org/abs/2506.02481</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Investigates whether language models express consistent value preferences across short-form and long-form responses.&lt;/li&gt;&lt;li&gt;Finds weak correlation between value preferences inferred from different response lengths and formats.&lt;/li&gt;&lt;li&gt;Highlights the limitations of current alignment methods in ensuring consistent value expression.&lt;/li&gt;&lt;li&gt;Suggests the need for more robust approaches to maintain value alignment in practical, open-ended LLM use.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research Paper&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Quality Assessment&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Clarity: 1/5&lt;/li&gt;&lt;li&gt;Novelty: 1/5&lt;/li&gt;&lt;li&gt;Significance: 1/5&lt;/li&gt;&lt;li&gt;Try-worthiness: No&lt;/li&gt;&lt;li&gt;Justification: Error parsing response: invalid syntax (&lt;string&gt;, line 1)&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: Inderjeet Nair, Lu Wang&lt;/li&gt;&lt;li&gt;Tags: AI alignment, value consistency, LLM safety, robustness, ethical risks&lt;/li&gt;&lt;li&gt;Relevance Score: 3/5&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.02481</guid><pubDate>Wed, 04 Jun 2025 00:00:00 +0000</pubDate></item><item><title>MidPO: Dual Preference Optimization for Safety and Helpfulness in Large Language Models via a Mixture of Experts Framework</title><link>https://arxiv.org/abs/2506.02460</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes MidPO, a Mixture of Experts framework to optimize both safety and helpfulness in large language models (LLMs).&lt;/li&gt;&lt;li&gt;Introduces two independent experts (safety and helpfulness) and a dynamic routing mechanism to balance their outputs.&lt;/li&gt;&lt;li&gt;Addresses limitations of existing safety-constrained preference optimization methods, which often trade off safety for helpfulness or vice versa.&lt;/li&gt;&lt;li&gt;Demonstrates improved performance over state-of-the-art methods on multiple datasets.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research Paper&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Quality Assessment&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Clarity: 1/5&lt;/li&gt;&lt;li&gt;Novelty: 1/5&lt;/li&gt;&lt;li&gt;Significance: 1/5&lt;/li&gt;&lt;li&gt;Try-worthiness: No&lt;/li&gt;&lt;li&gt;Justification: Error parsing response: invalid syntax (&lt;string&gt;, line 1)&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: Yupeng Qi, Ziyu Lyu, Min Yang, Yanlin Wang, Lu Bai, Lixin Cui&lt;/li&gt;&lt;li&gt;Tags: AI safety, LLM alignment, safety evaluation, helpfulness, preference optimization&lt;/li&gt;&lt;li&gt;Relevance Score: 5/5&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.02460</guid><pubDate>Wed, 04 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Should LLM Safety Be More Than Refusing Harmful Instructions?</title><link>https://arxiv.org/abs/2506.02442</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a two-dimensional framework for evaluating LLM safety: instruction refusal and generation safety.&lt;/li&gt;&lt;li&gt;Systematically evaluates LLMs on their ability to handle obfuscated (encrypted) harmful instructions.&lt;/li&gt;&lt;li&gt;Finds that LLMs with decryption capabilities are vulnerable to mismatched-generalization attacks, leading to unsafe outputs or excessive refusals.&lt;/li&gt;&lt;li&gt;Assesses the effectiveness of various pre- and post-LLM safety mechanisms and discusses their limitations.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research Paper&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Quality Assessment&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Clarity: 1/5&lt;/li&gt;&lt;li&gt;Novelty: 1/5&lt;/li&gt;&lt;li&gt;Significance: 1/5&lt;/li&gt;&lt;li&gt;Try-worthiness: No&lt;/li&gt;&lt;li&gt;Justification: Error parsing response: invalid syntax (&lt;string&gt;, line 1)&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: Utsav Maskey, Mark Dras, Usman Naseem&lt;/li&gt;&lt;li&gt;Tags: LLM safety, adversarial prompting, harmful output prevention, safety evaluation, robustness&lt;/li&gt;&lt;li&gt;Relevance Score: 5/5&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.02442</guid><pubDate>Wed, 04 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Comparative Analysis of AI Agent Architectures for Entity Relationship Classification</title><link>https://arxiv.org/abs/2506.02426</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Compares three AI agent architectures for entity relationship classification using LLMs.&lt;/li&gt;&lt;li&gt;Introduces a novel multi-agent dynamic example generation mechanism that includes adversarial prompting.&lt;/li&gt;&lt;li&gt;Evaluates the impact of cooperative and adversarial prompting on model performance.&lt;/li&gt;&lt;li&gt;Provides practical insights for designing modular LLM-based systems for information extraction.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research Paper&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Quality Assessment&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Clarity: 4/5&lt;/li&gt;&lt;li&gt;Novelty: 4/5&lt;/li&gt;&lt;li&gt;Significance: 4/5&lt;/li&gt;&lt;li&gt;Try-worthiness: Yes&lt;/li&gt;&lt;li&gt;Justification: The paper is clearly structured, with a logical flow from introduction, related work, methods, experiments, to results and conclusion. The comparative analysis of three distinct AI agent architectures for entity relationship classification is well-motivated and addresses a relevant challenge in information extraction, especially under low-resource conditions. The introduction of a novel multi-agent dynamic example generation mechanism, including real-time cooperative and adversarial prompting, adds a notable degree of novelty. The results indicate that multi-agent coordination approaches the performance of fine-tuned models, which is significant for practitioners seeking modular and generalizable LLM-based systems. The paper provides practical guidance and makes its code and dataset available, increasing its try-worthiness.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: Maryam Berijanian, Kuldeep Singh, Amin Sehati&lt;/li&gt;&lt;li&gt;Tags: adversarial prompting, LLM red teaming, agent architectures, information extraction&lt;/li&gt;&lt;li&gt;Relevance Score: 3/5&lt;/li&gt;&lt;li&gt;Code Repository: &lt;a href='https://github.com/maryambrj/ALIEN.git'&gt;https://github.com/maryambrj/ALIEN.git&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.02426</guid><pubDate>Wed, 04 Jun 2025 00:00:00 +0000</pubDate></item><item><title>AnswerCarefully: A Dataset for Improving the Safety of Japanese LLM Output</title><link>https://arxiv.org/abs/2506.02372</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces AnswerCarefully, a dataset designed to improve the safety and appropriateness of Japanese LLM outputs.&lt;/li&gt;&lt;li&gt;Covers a wide range of risk categories, with questions and answers tailored to Japanese socio-cultural context.&lt;/li&gt;&lt;li&gt;Demonstrates that fine-tuning Japanese LLMs with this dataset improves output safety without reducing general utility.&lt;/li&gt;&lt;li&gt;Provides safety evaluation results for 12 Japanese LLMs and includes English translations and annotations to support cross-lingual safety research.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Quality Assessment&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Clarity: 4/5&lt;/li&gt;&lt;li&gt;Novelty: 4/5&lt;/li&gt;&lt;li&gt;Significance: 5/5&lt;/li&gt;&lt;li&gt;Try-worthiness: Yes&lt;/li&gt;&lt;li&gt;Justification: The paper is clearly written and well-structured, with a strong abstract and logical flow through the introduction, dataset description, evaluation, and conclusions. The novelty lies in the creation of a manually curated Japanese safety dataset (AnswerCarefully) that reflects local socio-cultural risks, which is not well-covered by existing datasets that are either English-centric or limited in scope. The significance is high, as safety alignment for LLMs in non-English languages is a critical and under-addressed area, and the dataset is shown to improve safety without harming utility. The dataset is publicly available, and the paper provides practical evaluation and fine-tuning results, making it worth trying for researchers and practitioners working on Japanese LLMs or multilingual safety. The code/data repository is clearly linked in the paper.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: Hisami Suzuki, Satoru Katsumata, Takashi Kodama, Tetsuro Takahashi, Kouta Nakayama, Satoshi Sekine&lt;/li&gt;&lt;li&gt;Tags: LLM safety, safety evaluation, dataset, Japanese language models, harmful output prevention&lt;/li&gt;&lt;li&gt;Relevance Score: 5/5&lt;/li&gt;&lt;li&gt;Code Repository: &lt;a href='https://llmc.nii.ac.jp/en/answercarefully-dataset/'&gt;https://llmc.nii.ac.jp/en/answercarefully-dataset/&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.02372</guid><pubDate>Wed, 04 Jun 2025 00:00:00 +0000</pubDate></item><item><title>AI Debate Aids Assessment of Controversial Claims</title><link>https://arxiv.org/abs/2506.02175</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Investigates the use of AI debate protocols to improve human and AI judgment accuracy on controversial factual claims.&lt;/li&gt;&lt;li&gt;Compares the effectiveness of AI-assisted debate versus single-advisor consultancy in guiding human judges with differing beliefs.&lt;/li&gt;&lt;li&gt;Finds that AI debate improves judgment accuracy and confidence calibration, especially for mainstream belief holders.&lt;/li&gt;&lt;li&gt;Demonstrates that AI judges mimicking human belief systems can outperform both human judges and default AI judges, suggesting potential for scalable oversight.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research Paper&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: Salman Rahman, Sheriff Issaka, Ashima Suvarna, Genglin Liu, James Shiffer, Jaeyoung Lee, Md Rizwan Parvez, Hamid Palangi, Shi Feng, Nanyun Peng, Yejin Choi, Julian Michael, Liwei Jiang, Saadia Gabriel&lt;/li&gt;&lt;li&gt;Tags: AI safety, alignment, scalable oversight, truthfulness, AI governance&lt;/li&gt;&lt;li&gt;Relevance Score: 4/5&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.02175</guid><pubDate>Wed, 04 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Beyond Prompt Engineering: Robust Behavior Control in LLMs via Steering Target Atoms</title><link>https://arxiv.org/abs/2505.20322</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes Steering Target Atoms (STA), a method for isolating and manipulating disentangled knowledge components in LLMs.&lt;/li&gt;&lt;li&gt;Aims to improve safety and reliability by enabling more precise control over model behaviors.&lt;/li&gt;&lt;li&gt;Demonstrates robustness and flexibility of the approach, especially in adversarial scenarios.&lt;/li&gt;&lt;li&gt;Validates the method on large reasoning models, showing improved control in reasoning tasks.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research Paper&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Quality Assessment&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Clarity: 1/5&lt;/li&gt;&lt;li&gt;Novelty: 1/5&lt;/li&gt;&lt;li&gt;Significance: 1/5&lt;/li&gt;&lt;li&gt;Try-worthiness: No&lt;/li&gt;&lt;li&gt;Justification: Error parsing response: invalid syntax (&lt;string&gt;, line 1)&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: Mengru Wang, Ziwen Xu, Shengyu Mao, Shumin Deng, Zhaopeng Tu, Huajun Chen, Ningyu Zhang&lt;/li&gt;&lt;li&gt;Tags: LLM safety, behavior control, robustness, adversarial robustness, alignment&lt;/li&gt;&lt;li&gt;Relevance Score: 5/5&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2505.20322</guid><pubDate>Wed, 04 Jun 2025 00:00:00 +0000</pubDate></item><item><title>A Mousetrap: Fooling Large Reasoning Models for Jailbreak with Chain of Iterative Chaos</title><link>https://arxiv.org/abs/2502.15806</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces a novel jailbreak attack targeting Large Reasoning Models (LRMs), exploiting their advanced reasoning capabilities.&lt;/li&gt;&lt;li&gt;Proposes the 'Chaos Machine' component to iteratively transform attack prompts, increasing attack variability and complexity.&lt;/li&gt;&lt;li&gt;Presents the 'Mousetrap' framework, which leverages nonlinear prompt transformations to bypass safety mechanisms in LRMs.&lt;/li&gt;&lt;li&gt;Demonstrates high attack success rates against state-of-the-art models (Claude-Sonnet, Gemini-Thinking) and on multiple safety benchmarks.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research Paper&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Quality Assessment&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Clarity: 1/5&lt;/li&gt;&lt;li&gt;Novelty: 1/5&lt;/li&gt;&lt;li&gt;Significance: 1/5&lt;/li&gt;&lt;li&gt;Try-worthiness: No&lt;/li&gt;&lt;li&gt;Justification: Error parsing response: invalid syntax (&lt;string&gt;, line 1)&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: Yang Yao, Xuan Tong, Ruofan Wang, Yixu Wang, Lujundong Li, Liang Liu, Yan Teng, Yingchun Wang&lt;/li&gt;&lt;li&gt;Tags: LLM jailbreak, adversarial prompting, AI safety evaluation, reasoning model vulnerabilities&lt;/li&gt;&lt;li&gt;Relevance Score: 5/5&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2502.15806</guid><pubDate>Wed, 04 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Adversarial Robustness of AI-Generated Image Detectors in the Real World</title><link>https://arxiv.org/abs/2410.01574</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Investigates the adversarial robustness of AI-generated image detectors under real-world conditions.&lt;/li&gt;&lt;li&gt;Demonstrates that state-of-the-art classifiers are vulnerable to adversarial attacks, even in black-box settings.&lt;/li&gt;&lt;li&gt;Shows that attacks remain effective after image degradation typical of social media uploads.&lt;/li&gt;&lt;li&gt;Evaluates commercial detection tools and explores methods to improve robustness.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research Paper&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Quality Assessment&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Clarity: 1/5&lt;/li&gt;&lt;li&gt;Novelty: 1/5&lt;/li&gt;&lt;li&gt;Significance: 1/5&lt;/li&gt;&lt;li&gt;Try-worthiness: No&lt;/li&gt;&lt;li&gt;Justification: Error parsing response: invalid syntax (&lt;string&gt;, line 1)&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: Sina Mavali, Jonas Ricker, David Pape, Asja Fischer, Lea Sch\"onherr&lt;/li&gt;&lt;li&gt;Tags: adversarial robustness, AI-generated content detection, security vulnerabilities, black-box attacks&lt;/li&gt;&lt;li&gt;Relevance Score: 5/5&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2410.01574</guid><pubDate>Wed, 04 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Dialz: A Python Toolkit for Steering Vectors</title><link>https://arxiv.org/abs/2505.06262</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces Dialz, a Python toolkit for applying and analyzing steering vectors in open-source LLMs.&lt;/li&gt;&lt;li&gt;Enables modification of model activations to amplify or suppress specific concepts, such as reducing harmful outputs.&lt;/li&gt;&lt;li&gt;Supports research into safer and more controllable language generation, including mitigation of stereotypes.&lt;/li&gt;&lt;li&gt;Facilitates interpretability and transparency in LLM behavior, contributing to AI safety.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research Paper&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Quality Assessment&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Clarity: 1/5&lt;/li&gt;&lt;li&gt;Novelty: 1/5&lt;/li&gt;&lt;li&gt;Significance: 1/5&lt;/li&gt;&lt;li&gt;Try-worthiness: No&lt;/li&gt;&lt;li&gt;Justification: Error parsing response: invalid syntax (&lt;string&gt;, line 1)&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: Zara Siddique, Liam D. Turner, Luis Espinosa-Anke&lt;/li&gt;&lt;li&gt;Tags: AI safety, harmful output mitigation, LLM control, model interpretability, alignment&lt;/li&gt;&lt;li&gt;Relevance Score: 4/5&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2505.06262</guid><pubDate>Wed, 04 Jun 2025 00:00:00 +0000</pubDate></item><item><title>SafeSwitch: Steering Unsafe LLM Behavior via Internal Activation Signals</title><link>https://arxiv.org/abs/2502.01042</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces SafeSwitch, a framework that dynamically regulates unsafe LLM outputs using internal activation signals.&lt;/li&gt;&lt;li&gt;Utilizes a prober-based internal state monitor to detect harmful intentions and activates a safety head only when necessary.&lt;/li&gt;&lt;li&gt;Demonstrates significant reduction (approx. 80%) in harmful outputs while maintaining model utility.&lt;/li&gt;&lt;li&gt;Highlights the potential for LLMs to perform internal safety assessments, enabling more nuanced and context-aware refusals.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research Paper&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Quality Assessment&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Clarity: 4/5&lt;/li&gt;&lt;li&gt;Novelty: 5/5&lt;/li&gt;&lt;li&gt;Significance: 5/5&lt;/li&gt;&lt;li&gt;Try-worthiness: Yes&lt;/li&gt;&lt;li&gt;Justification: The paper introduces SafeSwitch, a novel method for steering large language model (LLM) behavior by leveraging internal activation signals to predict and prevent unsafe generations. The methodology is clearly structured, with a dedicated section on implementation and extensive experimental validation, including ablations, out-of-domain tests, and comparisons to prior work. The approach is significant as it provides a practical and effective mechanism for improving LLM safety without retraining or heavy prompt engineering, and the results show strong performance in balancing safety and utility. The paper is well-organized, though some technical details may require a background in LLM internals to fully grasp. The presence of appendices with implementation and data details further supports reproducibility. The method is worth trying in practice, especially for researchers and practitioners concerned with LLM safety.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: Peixuan Han, Cheng Qian, Xiusi Chen, Yuji Zhang, Denghui Zhang, Heng Ji&lt;/li&gt;&lt;li&gt;Tags: LLM safety, harmful output mitigation, internal state monitoring, dynamic safety controls, AI alignment&lt;/li&gt;&lt;li&gt;Relevance Score: 5/5&lt;/li&gt;&lt;li&gt;Code Repository: &lt;a href='https://github.com/SafeSwitch/SafeSwitch'&gt;https://github.com/SafeSwitch/SafeSwitch&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2502.01042</guid><pubDate>Wed, 04 Jun 2025 00:00:00 +0000</pubDate></item><item><title>NeurIPS 2023 Competition: Privacy Preserving Federated Learning Document VQA</title><link>https://arxiv.org/abs/2411.03730</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Describes a competition focused on privacy-preserving federated learning for document visual question answering (VQA) in real-world invoice processing.&lt;/li&gt;&lt;li&gt;Introduces a dataset and testbed for developing and evaluating privacy-preserving methods in federated learning, specifically targeting sensitive document data.&lt;/li&gt;&lt;li&gt;Participants were challenged to implement differential privacy and communication-efficient solutions to protect sensitive information in both visual and textual modalities.&lt;/li&gt;&lt;li&gt;The competition analysis provides best practices and recommendations for privacy-focused federated learning challenges.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Quality Assessment&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Clarity: 4/5&lt;/li&gt;&lt;li&gt;Novelty: 3/5&lt;/li&gt;&lt;li&gt;Significance: 4/5&lt;/li&gt;&lt;li&gt;Try-worthiness: Yes&lt;/li&gt;&lt;li&gt;Justification: The paper provides a comprehensive overview of the NeurIPS 2023 competition on privacy-preserving federated learning for document visual question answering (VQA). The structure is clear, with well-organized sections covering the dataset, evaluation metrics, baseline solutions, and detailed descriptions of winning and runner-up approaches for both communication-efficient and differentially private federated learning tracks. The novelty is moderate, as the work primarily reports on a competition and synthesizes existing methods rather than introducing fundamentally new algorithms. However, the significance is high due to the practical importance of privacy-preserving FL in real-world document understanding tasks, and the paper's insights into effective strategies and lessons learned are valuable for practitioners. The inclusion of a starter kit and baseline code (as referenced in the 'Starter Kit' section) further increases its try-worthiness for those interested in applying or extending these methods.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: Marlon Tobaben, Mohamed Ali Souibgui, Rub\`en Tito, Khanh Nguyen, Raouf Kerkouche, Kangsoo Jung, Joonas J\"alk\"o, Lei Kang, Andrey Barsky, Vincent Poulain d'Andecy, Aur\'elie Joseph, Aashiq Muhamed, Kevin Kuo, Virginia Smith, Yusuke Yamasaki, Takumi Fukami, Kenta Niwa, Iifan Tyou, Hiro Ishii, Rio Yokota, Ragul N, Rintu Kutum, Josep Llados, Ernest Valveny, Antti Honkela, Mario Fritz, Dimosthenis Karatzas&lt;/li&gt;&lt;li&gt;Tags: federated learning, privacy-preserving AI, differential privacy, document VQA, AI security&lt;/li&gt;&lt;li&gt;Relevance Score: 5/5&lt;/li&gt;&lt;li&gt;Code Repository: &lt;a href='https://github.com/EPFL-VILAB/neurips23-fl-docvqa'&gt;https://github.com/EPFL-VILAB/neurips23-fl-docvqa&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2411.03730</guid><pubDate>Wed, 04 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Adversarial Inception Backdoor Attacks against Reinforcement Learning</title><link>https://arxiv.org/abs/2410.13995</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a new class of backdoor attacks ('inception' attacks) targeting Deep Reinforcement Learning (DRL) systems.&lt;/li&gt;&lt;li&gt;These attacks manipulate training data to insert triggers and induce adversarial behavior while maintaining normal task performance.&lt;/li&gt;&lt;li&gt;Demonstrates that the attacks are effective even under strict reward constraints, overcoming limitations of prior methods.&lt;/li&gt;&lt;li&gt;Provides formal definitions and empirical results showing high attack success rates across multiple environments.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research Paper&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Quality Assessment&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Clarity: 4/5&lt;/li&gt;&lt;li&gt;Novelty: 5/5&lt;/li&gt;&lt;li&gt;Significance: 5/5&lt;/li&gt;&lt;li&gt;Try-worthiness: Yes&lt;/li&gt;&lt;li&gt;Justification: The paper is well-structured, with clear sections on problem formulation, theoretical analysis, algorithmic development, and experimental validation. The concept of 'Adversarial Inception Backdoor Attacks' is novel, especially in the context of reinforcement learning (RL), where backdoor attacks are less explored compared to supervised learning. The theoretical guarantees and empirical results demonstrate that the proposed attack can evade existing defenses and is more effective than prior methods. The significance is high, as it exposes a new class of vulnerabilities in RL systems, which are increasingly used in safety-critical applications. The paper is worth trying in practice for researchers and practitioners interested in RL security, adversarial machine learning, or defense development. However, the HTML provided does not contain a direct code repository link; a further check on the arXiv or paper's supplementary material may be needed for code.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: Ethan Rathbun, Alina Oprea, Christopher Amato&lt;/li&gt;&lt;li&gt;Tags: backdoor attacks, reinforcement learning security, adversarial machine learning, data poisoning&lt;/li&gt;&lt;li&gt;Relevance Score: 5/5&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2410.13995</guid><pubDate>Wed, 04 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Average Certified Radius is a Poor Metric for Randomized Smoothing</title><link>https://arxiv.org/abs/2410.06895</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Critically examines the use of Average Certified Radius (ACR) as a metric for evaluating certified robustness in randomized smoothing (RS) against adversarial attacks.&lt;/li&gt;&lt;li&gt;Theoretically and empirically demonstrates that ACR can be misleading, favoring trivial classifiers and being overly sensitive to easy samples.&lt;/li&gt;&lt;li&gt;Proposes alternative evaluation strategies and metrics for RS, aiming to improve the assessment of model robustness against adversarial examples.&lt;/li&gt;&lt;li&gt;Highlights the need for better robustness evaluation metrics to ensure meaningful progress in adversarial robustness research.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research Paper&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Quality Assessment&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Clarity: 1/5&lt;/li&gt;&lt;li&gt;Novelty: 1/5&lt;/li&gt;&lt;li&gt;Significance: 1/5&lt;/li&gt;&lt;li&gt;Try-worthiness: No&lt;/li&gt;&lt;li&gt;Justification: Error parsing response: invalid syntax (&lt;string&gt;, line 1)&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: Chenhao Sun, Yuhao Mao, Mark Niklas M\"uller, Martin Vechev&lt;/li&gt;&lt;li&gt;Tags: adversarial robustness, certified robustness, randomized smoothing, robustness evaluation, AI safety&lt;/li&gt;&lt;li&gt;Relevance Score: 4/5&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2410.06895</guid><pubDate>Wed, 04 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Federated Learning for Smart Grid: A Survey on Applications and Potential Vulnerabilities</title><link>https://arxiv.org/abs/2409.10764</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Provides a comprehensive survey of federated learning (FL) applications in smart grid systems, focusing on privacy and security.&lt;/li&gt;&lt;li&gt;Examines potential vulnerabilities and security concerns unique to FL-based smart grid systems.&lt;/li&gt;&lt;li&gt;Introduces FedGridShield, an open-source framework for implementing state-of-the-art attack and defense methods in FL for smart grids.&lt;/li&gt;&lt;li&gt;Discusses the gap between current FL research and practical deployment, with recommendations for improving robustness and security.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Survey/Review&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Quality Assessment&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Clarity: 5/5&lt;/li&gt;&lt;li&gt;Novelty: 3/5&lt;/li&gt;&lt;li&gt;Significance: 4/5&lt;/li&gt;&lt;li&gt;Try-worthiness: No&lt;/li&gt;&lt;li&gt;Justification: This paper is a survey, not a novel method or implementation. It is very clearly structured, with well-organized sections covering terminology, applications, vulnerabilities, and state-of-the-art attacks in federated learning for smart grids. The novelty is moderate, as it synthesizes and organizes existing literature rather than introducing new algorithms or frameworks. The significance is high for researchers or practitioners seeking an overview of the field, especially regarding vulnerabilities and attack vectors in FL-based smart grid systems. However, as a survey, it does not provide code or a direct implementation to try in practice.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: Zikai Zhang, Suman Rath, Jiaohao Xu, Tingsong Xiao&lt;/li&gt;&lt;li&gt;Tags: federated learning, smart grid security, privacy attacks, adversarial attacks, robustness&lt;/li&gt;&lt;li&gt;Relevance Score: 5/5&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2409.10764</guid><pubDate>Wed, 04 Jun 2025 00:00:00 +0000</pubDate></item><item><title>LEVIS: Large Exact Verifiable Input Spaces for Neural Networks</title><link>https://arxiv.org/abs/2408.08824</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes LEVIS, a framework for identifying large, exact verifiable input spaces in neural networks where no adversarial examples exist.&lt;/li&gt;&lt;li&gt;Introduces methods using mixed-integer programming and complementarity-constrained optimization to efficiently compute and scale the verification process.&lt;/li&gt;&lt;li&gt;Theoretically characterizes the properties of the verifiable regions and demonstrates the approach on safety-critical applications such as power flow regression and image classification.&lt;/li&gt;&lt;li&gt;Focuses on robustness verification, which is crucial for safety-critical AI systems.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research Paper&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Quality Assessment&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Clarity: 1/5&lt;/li&gt;&lt;li&gt;Novelty: 1/5&lt;/li&gt;&lt;li&gt;Significance: 1/5&lt;/li&gt;&lt;li&gt;Try-worthiness: No&lt;/li&gt;&lt;li&gt;Justification: Error parsing response: invalid syntax (&lt;string&gt;, line 1)&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: Mohamad Fares El Hajj Chehade, Wenting Li, Brian W. Bell, Russell Bent, Saif R. Kazi, Hao Zhu&lt;/li&gt;&lt;li&gt;Tags: robustness verification, adversarial examples, AI safety, neural network verification&lt;/li&gt;&lt;li&gt;Relevance Score: 5/5&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2408.08824</guid><pubDate>Wed, 04 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Corrigibility as a Singular Target: A Vision for Inherently Reliable Foundation Models</title><link>https://arxiv.org/abs/2506.03056</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes 'Corrigibility as a Singular Target' (CAST) as a new paradigm for aligning foundation models with human control.&lt;/li&gt;&lt;li&gt;Addresses the risks of instrumental convergence and loss of human oversight in advanced AI systems.&lt;/li&gt;&lt;li&gt;Outlines an empirical research agenda for training, testing, and demonstrating corrigible behaviors in foundation models.&lt;/li&gt;&lt;li&gt;Aims to prevent misaligned power-seeking behaviors by making models inherently responsive to human guidance.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Position Paper&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Quality Assessment&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Clarity: 4/5&lt;/li&gt;&lt;li&gt;Novelty: 4/5&lt;/li&gt;&lt;li&gt;Significance: 5/5&lt;/li&gt;&lt;li&gt;Try-worthiness: Yes&lt;/li&gt;&lt;li&gt;Justification: The paper is clearly written, with a well-structured abstract and introduction that articulate the motivation and the proposed paradigm (CAST: Corrigibility as a Singular Target). The concept of making corrigibility the central, overriding objective for foundation models is a significant and novel shift from traditional value alignment approaches, which often struggle with value specification and emergent instrumental drives. The research agenda is concrete, outlining empirical phases (training, scalability, instructability, safety evaluation) that are actionable for practitioners. The significance is high, as the work directly addresses existential risks from misaligned AI and proposes a path to inherently reliable, tool-like AI systems. While the paper is visionary and conceptual, it is grounded in practical research steps, making it worth trying in practice. No code repository is provided or referenced in the HTML.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: Ram Potham (Independent Researcher), Max Harms (Machine Intelligence Research Institute)&lt;/li&gt;&lt;li&gt;Tags: AI alignment, corrigibility, AI safety, instrumental convergence, foundation models&lt;/li&gt;&lt;li&gt;Relevance Score: 5/5&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.03056</guid><pubDate>Wed, 04 Jun 2025 00:00:00 +0000</pubDate></item><item><title>MAEBE: Multi-Agent Emergent Behavior Framework</title><link>https://arxiv.org/abs/2506.03053</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces the MAEBE framework for evaluating emergent risks in multi-agent AI ensembles.&lt;/li&gt;&lt;li&gt;Demonstrates that LLM moral preferences are brittle and can shift significantly with question framing.&lt;/li&gt;&lt;li&gt;Finds that group dynamics in LLM ensembles lead to unpredictable moral reasoning and phenomena like peer pressure.&lt;/li&gt;&lt;li&gt;Highlights unique safety and alignment challenges in multi-agent AI systems, emphasizing the need for evaluation in interactive contexts.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research Paper&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Quality Assessment&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Clarity: 1/5&lt;/li&gt;&lt;li&gt;Novelty: 1/5&lt;/li&gt;&lt;li&gt;Significance: 1/5&lt;/li&gt;&lt;li&gt;Try-worthiness: No&lt;/li&gt;&lt;li&gt;Justification: Error parsing response: invalid syntax (&lt;string&gt;, line 1)&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: Sinem Erisken (Independent Researcher), Timothy Gothard (Independent Researcher), Martin Leitgab (Independent Researcher), Ram Potham (Independent Researcher)&lt;/li&gt;&lt;li&gt;Tags: AI safety, multi-agent systems, alignment, emergent behavior, risk assessment&lt;/li&gt;&lt;li&gt;Relevance Score: 5/5&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.03053</guid><pubDate>Wed, 04 Jun 2025 00:00:00 +0000</pubDate></item><item><title>On the Benefits of Accelerated Optimization in Robust and Private Estimation</title><link>https://arxiv.org/abs/2506.03044</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Investigates accelerated optimization methods (Frank-Wolfe, projected gradient descent) for robust and private statistical estimation.&lt;/li&gt;&lt;li&gt;Analyzes how acceleration techniques impact differential privacy guarantees and robustness to heavy-tailed data.&lt;/li&gt;&lt;li&gt;Utilizes noisy gradients for privacy (via Gaussian mechanism) and geometric median-of-means for robustness.&lt;/li&gt;&lt;li&gt;Compares statistical guarantees and convergence rates to existing methods, identifying optimal scenarios.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research Paper&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: Laurentiu Andrei Marchis, Po-Ling Loh&lt;/li&gt;&lt;li&gt;Tags: differential privacy, robustness, private estimation, optimization, statistical guarantees&lt;/li&gt;&lt;li&gt;Relevance Score: 3/5&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.03044</guid><pubDate>Wed, 04 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Mitigating Manipulation and Enhancing Persuasion: A Reflective Multi-Agent Approach for Legal Argument Generation</title><link>https://arxiv.org/abs/2506.02992</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a reflective multi-agent approach to reduce manipulation and hallucination in LLM-generated legal arguments.&lt;/li&gt;&lt;li&gt;Introduces specialized agents (Factor Analyst and Argument Polisher) to iteratively refine legal arguments and ensure factual grounding.&lt;/li&gt;&lt;li&gt;Demonstrates improved abstention (avoiding ungrounded arguments) and reduced hallucination across multiple LLMs and legal scenarios.&lt;/li&gt;&lt;li&gt;Aims to enhance trustworthiness and ethical persuasion in AI-driven legal systems.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research Paper&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Quality Assessment&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Clarity: 1/5&lt;/li&gt;&lt;li&gt;Novelty: 1/5&lt;/li&gt;&lt;li&gt;Significance: 1/5&lt;/li&gt;&lt;li&gt;Try-worthiness: No&lt;/li&gt;&lt;li&gt;Justification: Error parsing response: invalid syntax (&lt;string&gt;, line 1)&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: Li Zhang, Kevin D. Ashley&lt;/li&gt;&lt;li&gt;Tags: AI safety, hallucination mitigation, trustworthy AI, legal AI, robustness&lt;/li&gt;&lt;li&gt;Relevance Score: 4/5&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.02992</guid><pubDate>Wed, 04 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Safely Learning Controlled Stochastic Dynamics</title><link>https://arxiv.org/abs/2506.02754</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a method for safely learning controlled stochastic dynamics from trajectory data.&lt;/li&gt;&lt;li&gt;Ensures system trajectories remain within predefined safe regions during both training and deployment.&lt;/li&gt;&lt;li&gt;Introduces kernel-based confidence bounds to iteratively expand a safe control set.&lt;/li&gt;&lt;li&gt;Provides theoretical safety guarantees and demonstrates practical effectiveness in experiments.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research Paper&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Quality Assessment&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Clarity: 1/5&lt;/li&gt;&lt;li&gt;Novelty: 1/5&lt;/li&gt;&lt;li&gt;Significance: 1/5&lt;/li&gt;&lt;li&gt;Try-worthiness: No&lt;/li&gt;&lt;li&gt;Justification: Error parsing response: invalid syntax (&lt;string&gt;, line 1)&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: Luc Brogat-Motte, Alessandro Rudi, Riccardo Bonalli&lt;/li&gt;&lt;li&gt;Tags: AI safety, safe exploration, robust control, safety verification&lt;/li&gt;&lt;li&gt;Relevance Score: 4/5&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.02754</guid><pubDate>Wed, 04 Jun 2025 00:00:00 +0000</pubDate></item><item><title>CyberGym: Evaluating AI Agents' Cybersecurity Capabilities with Real-World Vulnerabilities at Scale</title><link>https://arxiv.org/abs/2506.02548</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces CyberGym, a large-scale evaluation framework for testing AI agents' cybersecurity capabilities using real-world software vulnerabilities.&lt;/li&gt;&lt;li&gt;Focuses on the ability of LLM agents to generate proof-of-concept (PoC) exploits from vulnerability descriptions and source code.&lt;/li&gt;&lt;li&gt;Benchmarks multiple LLMs and agent frameworks, revealing current limitations in handling complex cybersecurity tasks.&lt;/li&gt;&lt;li&gt;Demonstrates that LLM-generated PoCs can uncover new, previously unknown vulnerabilities (zero-days) in real-world software.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: Zhun Wang, Tianneng Shi, Jingxuan He, Matthew Cai, Jialin Zhang, Dawn Song&lt;/li&gt;&lt;li&gt;Tags: AI security, LLM red teaming, cybersecurity evaluation, vulnerability assessment, benchmarking&lt;/li&gt;&lt;li&gt;Relevance Score: 5/5&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.02548</guid><pubDate>Wed, 04 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Enhancing Convergence, Privacy and Fairness for Wireless Personalized Federated Learning: Quantization-Assisted Min-Max Fair Scheduling</title><link>https://arxiv.org/abs/2506.02422</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a quantization-assisted Gaussian differential privacy mechanism to enhance privacy in wireless personalized federated learning (WPFL).&lt;/li&gt;&lt;li&gt;Analyzes the convergence and fairness of personalized learning models under communication constraints and privacy mechanisms.&lt;/li&gt;&lt;li&gt;Designs an optimal scheduling strategy for client selection, channel allocation, and power control to achieve min-max fairness in WPFL.&lt;/li&gt;&lt;li&gt;Demonstrates significant improvements in accuracy, fairness, and test loss compared to alternative scheduling strategies.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research Paper&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Quality Assessment&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Clarity: 4/5&lt;/li&gt;&lt;li&gt;Novelty: 4/5&lt;/li&gt;&lt;li&gt;Significance: 5/5&lt;/li&gt;&lt;li&gt;Try-worthiness: Yes&lt;/li&gt;&lt;li&gt;Justification: The paper is well-structured, with clear sections on introduction, related work, system model, privacy and convergence analysis, and the proposed quantization-assisted min-max fair scheduling policy. The clarity is high, though some technical details may require a strong background in federated learning and wireless systems. The novelty is solid, as the integration of quantization, min-max fairness, and privacy in wireless personalized federated learning (WPFL) is not widely explored. The significance is high, addressing key challenges in practical federated learning deployments: convergence speed, privacy, and fairness under wireless constraints. The proposed scheduling and quantization approach is theoretically analyzed and appears to be validated by experiments (though the HTML provided does not include results, the structure suggests so). Try-worthiness is high for researchers and practitioners in federated learning and wireless edge AI, as the approach addresses real-world bottlenecks. No code repository link is found in the provided HTML.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: Xiyu Zhao, Qimei Cui, Ziqiang Du, Weicai Li, Xi Yu, Wei Ni, Ji Zhang, Xiaofeng Tao, Ping Zhang&lt;/li&gt;&lt;li&gt;Tags: federated learning security, differential privacy, wireless communication, fairness, privacy-preserving machine learning&lt;/li&gt;&lt;li&gt;Relevance Score: 4/5&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.02422</guid><pubDate>Wed, 04 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Blockchain Powered Edge Intelligence for U-Healthcare in Privacy Critical and Time Sensitive Environment</title><link>https://arxiv.org/abs/2506.02038</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a blockchain-powered edge intelligence framework for privacy-critical and time-sensitive healthcare applications.&lt;/li&gt;&lt;li&gt;Addresses vulnerabilities in distributed edge architectures, focusing on secure data interactions and privacy at edge gateways.&lt;/li&gt;&lt;li&gt;Introduces a secure access scheme for managing off-chain and on-chain data sharing and storage.&lt;/li&gt;&lt;li&gt;Conducts comprehensive security analysis and demonstrates the effectiveness of fine-grained access control mechanisms.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research Paper&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Quality Assessment&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Clarity: 1/5&lt;/li&gt;&lt;li&gt;Novelty: 1/5&lt;/li&gt;&lt;li&gt;Significance: 1/5&lt;/li&gt;&lt;li&gt;Try-worthiness: No&lt;/li&gt;&lt;li&gt;Justification: Error parsing response: invalid syntax (&lt;string&gt;, line 1)&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: Anum Nawaz, Hafiz Humza Mahmood Ramzan, Xianjia Yu, Zhuo Zou, Tomi Westerlund&lt;/li&gt;&lt;li&gt;Tags: AI security, privacy-preserving AI, secure data sharing, blockchain, edge intelligence&lt;/li&gt;&lt;li&gt;Relevance Score: 4/5&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.02038</guid><pubDate>Wed, 04 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Are classical deep neural networks weakly adversarially robust?</title><link>https://arxiv.org/abs/2506.02016</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Investigates the adversarial robustness of classical deep neural networks (DNNs).&lt;/li&gt;&lt;li&gt;Proposes a novel method for adversarial example detection and image recognition using layer-wise feature paths.&lt;/li&gt;&lt;li&gt;Compares the proposed method to adversarial training, highlighting a trade-off between clean and adversarial accuracy.&lt;/li&gt;&lt;li&gt;Challenges the conventional understanding of DNNs' adversarial robustness by demonstrating inherent robustness properties.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research Paper&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Quality Assessment&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Clarity: 1/5&lt;/li&gt;&lt;li&gt;Novelty: 1/5&lt;/li&gt;&lt;li&gt;Significance: 1/5&lt;/li&gt;&lt;li&gt;Try-worthiness: No&lt;/li&gt;&lt;li&gt;Justification: Error parsing response: invalid syntax (&lt;string&gt;, line 1)&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: Nuolin Sun, Linyuan Wang, Dongyang Li, Bin Yan, Lei Li&lt;/li&gt;&lt;li&gt;Tags: adversarial robustness, adversarial example detection, deep neural networks, defense methods&lt;/li&gt;&lt;li&gt;Relevance Score: 5/5&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.02016</guid><pubDate>Wed, 04 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Not All Tokens Are Meant to Be Forgotten</title><link>https://arxiv.org/abs/2506.03142</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Addresses the challenge of unlearning unwanted or sensitive information from large language models to mitigate privacy and legal risks.&lt;/li&gt;&lt;li&gt;Proposes a Targeted Information Forgetting (TIF) framework that selectively unlearns specific tokens while preserving general model utility.&lt;/li&gt;&lt;li&gt;Introduces a targeted information identifier and a novel optimization approach to differentiate and handle unwanted versus general words.&lt;/li&gt;&lt;li&gt;Demonstrates improved unlearning effectiveness and utility preservation on established benchmarks.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research Paper&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Quality Assessment&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Clarity: 4/5&lt;/li&gt;&lt;li&gt;Novelty: 4/5&lt;/li&gt;&lt;li&gt;Significance: 4/5&lt;/li&gt;&lt;li&gt;Try-worthiness: Yes&lt;/li&gt;&lt;li&gt;Justification: The paper addresses the important and timely problem of targeted unlearning in large language models (LLMs), proposing the Targeted Information Forgetting (TIF) framework. The structure is clear, with a well-organized presentation of the problem, methodology (including both discriminative and generative approaches), and comprehensive experimental evaluation on multiple benchmarks (MUSE, TOFU). The novelty lies in the targeted approach to unlearning, as opposed to blanket removal of data, and the introduction of Targeted Preference Optimization (TPO). The results show that TIF outperforms existing baselines, indicating practical significance for privacy and compliance scenarios. The methodology is described in sufficient detail to be reproducible, and the approach is likely to be of interest to practitioners working on LLM deployment and data governance. However, the HTML provided does not include a direct link to a code repository, and no explicit mention of code availability is found in the visible sections.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: Xiangyu Zhou, Yao Qiang, Saleh Zare Zade, Douglas Zytko, Prashant Khanduri, Dongxiao Zhu&lt;/li&gt;&lt;li&gt;Tags: unlearning, privacy, large language models, model utility, information removal&lt;/li&gt;&lt;li&gt;Relevance Score: 4/5&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.03142</guid><pubDate>Wed, 04 Jun 2025 00:00:00 +0000</pubDate></item><item><title>How Explanations Leak the Decision Logic: Stealing Graph Neural Networks via Explanation Alignment</title><link>https://arxiv.org/abs/2506.03087</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Investigates how explanation mechanisms in explainable Graph Neural Networks (GNNs) can leak critical decision logic.&lt;/li&gt;&lt;li&gt;Proposes a novel model stealing framework that uses explanation alignment and guided data augmentation to replicate both predictions and reasoning patterns of target GNNs.&lt;/li&gt;&lt;li&gt;Demonstrates the effectiveness of the attack on molecular graph datasets, outperforming conventional model stealing methods.&lt;/li&gt;&lt;li&gt;Highlights security risks associated with deploying explainable GNNs in sensitive domains and calls for protective measures against explanation-based attacks.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research Paper&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Quality Assessment&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Clarity: 1/5&lt;/li&gt;&lt;li&gt;Novelty: 1/5&lt;/li&gt;&lt;li&gt;Significance: 1/5&lt;/li&gt;&lt;li&gt;Try-worthiness: No&lt;/li&gt;&lt;li&gt;Justification: Error parsing response: invalid syntax (&lt;string&gt;, line 1)&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: Bin Ma, Yuyuan Feng, Minhua Lin, Enyan Dai&lt;/li&gt;&lt;li&gt;Tags: model stealing, explainable AI, graph neural networks, AI security, explanation leakage&lt;/li&gt;&lt;li&gt;Relevance Score: 5/5&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.03087</guid><pubDate>Wed, 04 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Agnostic Learning under Targeted Poisoning: Optimal Rates and the Role of Randomness</title><link>https://arxiv.org/abs/2506.03075</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Analyzes the impact of targeted data poisoning attacks on agnostic learning, focusing on adversaries corrupting a fraction of training data to cause failure on specific test points.&lt;/li&gt;&lt;li&gt;Establishes optimal excess error rates under such attacks and demonstrates the necessity of randomized learners for resilience.&lt;/li&gt;&lt;li&gt;Provides both upper and lower bounds for learner performance under adversarial conditions, including scenarios where the adversary observes the learner's randomness.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research Paper&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Quality Assessment&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Clarity: 4/5&lt;/li&gt;&lt;li&gt;Novelty: 5/5&lt;/li&gt;&lt;li&gt;Significance: 5/5&lt;/li&gt;&lt;li&gt;Try-worthiness: No&lt;/li&gt;&lt;li&gt;Justification: The paper is clearly written, with a well-structured abstract, introduction, and technical overview. It addresses the important and timely problem of agnostic learning under targeted data poisoning, providing new optimal rates and a detailed analysis of the role of randomness (private vs. public) in defending against such attacks. The results extend prior work from the realizable to the agnostic setting, which is a significant theoretical advance. The proofs are rigorous and the theoretical contributions are substantial. However, the work is primarily theoretical and does not provide practical algorithms or empirical results, nor does it offer code or implementation guidelines. Therefore, while the results are highly significant for theory, they are not immediately try-worthy for practitioners seeking ready-to-use defenses or algorithms.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: Bogdan Chornomaz, Yonatan Koren, Shay Moran, Tom Waknine&lt;/li&gt;&lt;li&gt;Tags: data poisoning, adversarial attacks, robust learning, machine learning security&lt;/li&gt;&lt;li&gt;Relevance Score: 5/5&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.03075</guid><pubDate>Wed, 04 Jun 2025 00:00:00 +0000</pubDate></item><item><title>On the Robustness of Tabular Foundation Models: Test-Time Attacks and In-Context Defenses</title><link>https://arxiv.org/abs/2506.02978</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Analyzes adversarial vulnerabilities of tabular foundation models (FMs) such as TabPFN and TabICL.&lt;/li&gt;&lt;li&gt;Demonstrates that small, structured test-time perturbations can significantly degrade model performance across finance, cybersecurity, and healthcare benchmarks.&lt;/li&gt;&lt;li&gt;Shows that tabular FMs can be used to generate transferable adversarial attacks against other models.&lt;/li&gt;&lt;li&gt;Proposes and evaluates in-context adversarial training strategies to improve robustness of tabular FMs.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research Paper&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Quality Assessment&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Clarity: 1/5&lt;/li&gt;&lt;li&gt;Novelty: 1/5&lt;/li&gt;&lt;li&gt;Significance: 1/5&lt;/li&gt;&lt;li&gt;Try-worthiness: No&lt;/li&gt;&lt;li&gt;Justification: Error parsing response: invalid syntax (&lt;string&gt;, line 1)&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: Mohamed Djilani, Thibault Simonetto, Karim Tit, Florian Tambon, Paul R\'ecamier, Salah Ghamizi, Maxime Cordy, Mike Papadakis&lt;/li&gt;&lt;li&gt;Tags: adversarial attacks, robustness, tabular foundation models, adversarial training, security evaluation&lt;/li&gt;&lt;li&gt;Relevance Score: 5/5&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.02978</guid><pubDate>Wed, 04 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Memory-Efficient and Privacy-Preserving Collaborative Training for Mixture-of-Experts LLMs</title><link>https://arxiv.org/abs/2506.02965</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes PC-MoE, a privacy-preserving collaborative training framework for Mixture-of-Experts LLMs.&lt;/li&gt;&lt;li&gt;Enables decentralized training across multiple parties while keeping training data and sensitive signals local.&lt;/li&gt;&lt;li&gt;Demonstrates strong robustness against reconstruction attacks, enhancing privacy and security.&lt;/li&gt;&lt;li&gt;Achieves high model performance and significant memory efficiency without sacrificing accuracy.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research Paper&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: Ze Yu Zhang, Bolin Ding, Bryan Kian Hsiang Low&lt;/li&gt;&lt;li&gt;Tags: privacy-preserving machine learning, collaborative training, large language models, reconstruction attacks, secure AI systems&lt;/li&gt;&lt;li&gt;Relevance Score: 5/5&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.02965</guid><pubDate>Wed, 04 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Beyond Invisibility: Learning Robust Visible Watermarks for Stronger Copyright Protection</title><link>https://arxiv.org/abs/2506.02665</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a new method for embedding robust, visible watermarks in images to protect copyrighted content from unauthorized AI use.&lt;/li&gt;&lt;li&gt;Addresses limitations of previous invisible watermarking/adversarial perturbation methods that require frequent retraining.&lt;/li&gt;&lt;li&gt;Introduces a probabilistic and inverse problem-based framework to make watermarks hard to remove, enhancing long-term copyright protection.&lt;/li&gt;&lt;li&gt;Demonstrates effectiveness of the approach through experimental results across various scenarios.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research Paper&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Quality Assessment&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Clarity: 4/5&lt;/li&gt;&lt;li&gt;Novelty: 4/5&lt;/li&gt;&lt;li&gt;Significance: 5/5&lt;/li&gt;&lt;li&gt;Try-worthiness: Yes&lt;/li&gt;&lt;li&gt;Justification: The paper is clearly structured, with a logical flow from motivation to method, experiments, and conclusion. The abstract and section headings make the contribution easy to follow. The novelty lies in moving beyond invisible adversarial watermarks to a learned, robust visible watermarking approach (Harvim) that is designed to be hard to remove and offers universal protection, addressing the limitations of prior methods that require retraining for new model architectures. The significance is high, as copyright protection for images in the AI era is a pressing problem, and the proposed method offers a practical, long-term solution. The experiments and implementation details suggest the method is ready for practical use. However, the HTML provided does not contain a direct link to a code repository. If the code is released, it would further increase try-worthiness.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: Tianci Liu, Tong Yang, Quan Zhang, Qi Lei&lt;/li&gt;&lt;li&gt;Tags: copyright protection, robust watermarking, AI security, adversarial robustness&lt;/li&gt;&lt;li&gt;Relevance Score: 4/5&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.02665</guid><pubDate>Wed, 04 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Privacy-Preserving Federated Convex Optimization: Balancing Partial-Participation and Efficiency via Noise Cancellation</title><link>https://arxiv.org/abs/2506.02563</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a novel noise-cancellation mechanism to achieve Differential Privacy in Federated Learning under partial participation.&lt;/li&gt;&lt;li&gt;Addresses the challenge of maintaining privacy guarantees and efficiency when only a subset of machines participate in each round.&lt;/li&gt;&lt;li&gt;Demonstrates optimal performance for both homogeneous and heterogeneous data distributions within the Stochastic Convex Optimization framework.&lt;/li&gt;&lt;li&gt;Expands the applicability of privacy-preserving techniques in distributed learning systems.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research Paper&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Quality Assessment&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Clarity: 1/5&lt;/li&gt;&lt;li&gt;Novelty: 1/5&lt;/li&gt;&lt;li&gt;Significance: 1/5&lt;/li&gt;&lt;li&gt;Try-worthiness: No&lt;/li&gt;&lt;li&gt;Justification: Error parsing response: invalid syntax (&lt;string&gt;, line 1)&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: Roie Reshef, Kfir Yehuda Levy&lt;/li&gt;&lt;li&gt;Tags: differential privacy, federated learning, privacy-preserving machine learning, distributed systems&lt;/li&gt;&lt;li&gt;Relevance Score: 4/5&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.02563</guid><pubDate>Wed, 04 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Rethinking Post-Unlearning Behavior of Large Vision-Language Models</title><link>https://arxiv.org/abs/2506.02541</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Examines the privacy risks in Large Vision-Language Models (LVLMs) and the limitations of current machine unlearning methods.&lt;/li&gt;&lt;li&gt;Identifies undesirable post-unlearning behaviors such as degenerate, hallucinated, or excessively refused responses.&lt;/li&gt;&lt;li&gt;Proposes a new unlearning task for LVLMs that balances privacy preservation with informative and visually grounded outputs.&lt;/li&gt;&lt;li&gt;Introduces PUBG, a novel unlearning method that guides models to desirable post-unlearning behavior, mitigating privacy leakage and undesirable aftermaths.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research Paper&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Quality Assessment&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Clarity: 1/5&lt;/li&gt;&lt;li&gt;Novelty: 1/5&lt;/li&gt;&lt;li&gt;Significance: 1/5&lt;/li&gt;&lt;li&gt;Try-worthiness: No&lt;/li&gt;&lt;li&gt;Justification: Error parsing response: invalid syntax (&lt;string&gt;, line 1)&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: Minsung Kim, Nakyeong Yang, Kyomin Jung&lt;/li&gt;&lt;li&gt;Tags: machine unlearning, privacy attacks, vision-language models, model safety, adversarial mitigation&lt;/li&gt;&lt;li&gt;Relevance Score: 5/5&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.02541</guid><pubDate>Wed, 04 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Evaluating LLM Agent Adherence to Hierarchical Safety Principles: A Lightweight Benchmark for Probing Foundational Controllability Components</title><link>https://arxiv.org/abs/2506.02357</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces a benchmark to evaluate LLM agents' adherence to high-level safety principles in the presence of conflicting operational goals.&lt;/li&gt;&lt;li&gt;Uses a grid world environment to test whether agents prioritize inviolable safety directives (e.g., avoiding hazardous zones).&lt;/li&gt;&lt;li&gt;Focuses on foundational controllability and the ability to empirically assess whether LLMs can be governed by hierarchical safety principles.&lt;/li&gt;&lt;li&gt;Provides preliminary results and argues for the importance of such benchmarks in early-stage AI safety evaluation.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Quality Assessment&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Clarity: 4/5&lt;/li&gt;&lt;li&gt;Novelty: 4/5&lt;/li&gt;&lt;li&gt;Significance: 4/5&lt;/li&gt;&lt;li&gt;Try-worthiness: Yes&lt;/li&gt;&lt;li&gt;Justification: The paper is clearly structured, with a logical flow from introduction to methodology, results, discussion, and conclusion. The benchmark is described as lightweight and interpretable, focusing on a core aspect of AI safety: controllability via adherence to hierarchical safety principles. The use of a simple grid world and explicit principle conflict scenarios is a novel and practical approach for probing foundational safety behaviors in LLM agents. The significance is high for the AI safety and governance community, as it provides a concrete, empirical method for early detection of control failures. The methodology is feasible to implement and could be readily adopted or extended by practitioners. However, the paper appears to be a pilot study with preliminary results, so further validation and scaling would strengthen its impact. The clarity is strong, though some details (e.g., full experimental results, code availability) are not visible in the provided excerpt.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: Ram Potham (Independent Researcher)&lt;/li&gt;&lt;li&gt;Tags: AI safety, controllability, safety evaluation, alignment, benchmarking&lt;/li&gt;&lt;li&gt;Relevance Score: 5/5&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.02357</guid><pubDate>Wed, 04 Jun 2025 00:00:00 +0000</pubDate></item><item><title>ReconXF: Graph Reconstruction Attack via Public Feature Explanations on Privatized Node Features and Labels</title><link>https://arxiv.org/abs/2506.02134</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces ReconXF, a novel graph reconstruction attack leveraging public feature explanations and privatized node features/labels.&lt;/li&gt;&lt;li&gt;Demonstrates that existing attacks perform poorly under differential privacy, but ReconXF overcomes this with denoising mechanisms.&lt;/li&gt;&lt;li&gt;Shows that even with privacy protections, public explanations can leak sensitive graph structure information.&lt;/li&gt;&lt;li&gt;Provides experimental evidence that ReconXF outperforms state-of-the-art attacks in privatized settings.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research Paper&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Quality Assessment&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Clarity: 1/5&lt;/li&gt;&lt;li&gt;Novelty: 1/5&lt;/li&gt;&lt;li&gt;Significance: 1/5&lt;/li&gt;&lt;li&gt;Try-worthiness: No&lt;/li&gt;&lt;li&gt;Justification: Error parsing response: invalid syntax (&lt;string&gt;, line 1)&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: Rishi Raj Sahoo, Rucha Bhalchandra Joshi, Subhankar Mishra&lt;/li&gt;&lt;li&gt;Tags: privacy attacks, graph neural networks, explainability, differential privacy, model inversion&lt;/li&gt;&lt;li&gt;Relevance Score: 5/5&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.02134</guid><pubDate>Wed, 04 Jun 2025 00:00:00 +0000</pubDate></item><item><title>SALAD: Systematic Assessment of Machine Unlearing on LLM-Aided Hardware Design</title><link>https://arxiv.org/abs/2506.02089</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces SALAD, a framework for systematically assessing machine unlearning in LLMs applied to hardware design.&lt;/li&gt;&lt;li&gt;Addresses data security challenges such as IP leakage, data contamination, and malicious code generation in LLM-aided Verilog code generation.&lt;/li&gt;&lt;li&gt;Demonstrates how machine unlearning can selectively remove sensitive or malicious data from pre-trained LLMs without full retraining.&lt;/li&gt;&lt;li&gt;Presents case studies showing the effectiveness of unlearning techniques in reducing security risks.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research Paper&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Quality Assessment&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Clarity: 1/5&lt;/li&gt;&lt;li&gt;Novelty: 1/5&lt;/li&gt;&lt;li&gt;Significance: 1/5&lt;/li&gt;&lt;li&gt;Try-worthiness: No&lt;/li&gt;&lt;li&gt;Justification: Error parsing response: invalid syntax (&lt;string&gt;, line 1)&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: Zeng Wang, Minghao Shao, Rupesh Karn, Jitendra Bhandari, Likhitha Mankali, Ramesh Karri, Ozgur Sinanoglu, Muhammad Shafique, Johann Knechtel&lt;/li&gt;&lt;li&gt;Tags: machine unlearning, LLM security, data leakage, intellectual property protection, malicious code mitigation&lt;/li&gt;&lt;li&gt;Relevance Score: 5/5&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.02089</guid><pubDate>Wed, 04 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Coded Robust Aggregation for Distributed Learning under Byzantine Attacks</title><link>https://arxiv.org/abs/2506.01989</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Addresses the problem of distributed learning under Byzantine attacks, a key security threat.&lt;/li&gt;&lt;li&gt;Proposes a new method (CRA-DL) using coded robust aggregation to improve resilience against malicious participants.&lt;/li&gt;&lt;li&gt;Theoretically analyzes convergence and empirically demonstrates improved robustness compared to existing methods.&lt;/li&gt;&lt;li&gt;Focuses on mitigating the impact of adversarial (Byzantine) devices in distributed AI training.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research Paper&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Quality Assessment&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Clarity: 1/5&lt;/li&gt;&lt;li&gt;Novelty: 1/5&lt;/li&gt;&lt;li&gt;Significance: 1/5&lt;/li&gt;&lt;li&gt;Try-worthiness: No&lt;/li&gt;&lt;li&gt;Justification: Error parsing response: invalid syntax (&lt;string&gt;, line 1)&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: Chengxi Li, Ming Xiao, Mikael Skoglund&lt;/li&gt;&lt;li&gt;Tags: Byzantine attacks, robust aggregation, distributed learning, AI security, adversarial robustness&lt;/li&gt;&lt;li&gt;Relevance Score: 5/5&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.01989</guid><pubDate>Wed, 04 Jun 2025 00:00:00 +0000</pubDate></item></channel></rss>