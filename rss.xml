<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>AI Security Digest</title><link>https://yourdomain.com/rss.xml</link><description>Summarized AI security papers from arXiv</description><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><lastBuildDate>Tue, 20 May 2025 04:25:55 +0000</lastBuildDate><item><title>Near-Optimal Sample Complexities of Divergence-based S-rectangular Distributionally Robust Reinforcement Learning</title><link>https://arxiv.org/abs/2505.12202</link><description>• Presents new sample complexity bounds for divergence-based S-rectangular distributionally robust reinforcement learning (DR-RL), which models adversarial discrepancies between training and testing environments.&lt;br/&gt;• Demonstrates that S-rectangular models more accurately capture real-world distributional shifts and can yield more robust policies compared to SA-rectangular models.&lt;br/&gt;• Validates theoretical results with experiments, showing improved learning performance in adversarially perturbed environments.&lt;br/&gt;&lt;br/&gt;Tags: reinforcement learning, distributional robustness, adversarial robustness, sample complexity, theory&lt;br/&gt;&lt;br/&gt;&lt;a href='https://arxiv.org/abs/2505.12202'&gt;Read on arXiv&lt;/a&gt;</description><pubDate>Tue, 20 May 2025 04:25:55 +0000</pubDate></item><item><title>Internal Causal Mechanisms Robustly Predict Language Model Out-of-Distribution Behaviors</title><link>https://arxiv.org/abs/2505.11770</link><description>• Proposes methods using internal causal mechanisms to predict language model behavior on out-of-distribution (OOD) inputs.&lt;br/&gt;• Demonstrates that causal-based techniques outperform causal-agnostic ones for predicting correctness, especially in OOD scenarios.&lt;br/&gt;• Highlights the importance of internal causal analysis for understanding and potentially securing language models against unexpected behaviors.&lt;br/&gt;&lt;br/&gt;Tags: LLM, OOD detection, interpretability, robustness, language models&lt;br/&gt;&lt;br/&gt;&lt;a href='https://arxiv.org/abs/2505.11770'&gt;Read on arXiv&lt;/a&gt;</description><pubDate>Tue, 20 May 2025 04:25:55 +0000</pubDate></item><item><title>Cybersecurity threat detection based on a UEBA framework using Deep Autoencoders</title><link>https://arxiv.org/abs/2505.11542</link><description>• Proposes an explainable anomaly detection framework for cybersecurity threats using Deep Autoencoders within a User and Entity Behaviour Analytics (UEBA) system.&lt;br/&gt;• Combines Deep Autoencoders and Doc2Vec to process both numerical and textual features for detecting and explaining security incidents.&lt;br/&gt;• Demonstrates effectiveness in identifying both real and synthetic anomalies, with potential for integration into enterprise security systems.&lt;br/&gt;&lt;br/&gt;Tags: cybersecurity, anomaly detection, deep learning, explainability, UEBA, text, enterprise security&lt;br/&gt;&lt;br/&gt;&lt;a href='https://arxiv.org/abs/2505.11542'&gt;Read on arXiv&lt;/a&gt;</description><pubDate>Tue, 20 May 2025 04:25:55 +0000</pubDate></item><item><title>Causality-Inspired Robustness for Nonlinear Models via Representation Learning</title><link>https://arxiv.org/abs/2505.12868</link><description>• Proposes a causality-inspired method for achieving distributional robustness in nonlinear prediction models via identifiable representation learning.&lt;br/&gt;• Establishes finite-radius robustness guarantees in nonlinear settings, extending previous work limited to linear models.&lt;br/&gt;• Empirically validates the method on synthetic and real-world data, emphasizing the importance of robust models under distribution shifts.&lt;br/&gt;&lt;br/&gt;Tags: robustness, causality, representation learning, distribution shift, theory&lt;br/&gt;&lt;br/&gt;&lt;a href='https://arxiv.org/abs/2505.12868'&gt;Read on arXiv&lt;/a&gt;</description><pubDate>Tue, 20 May 2025 04:25:55 +0000</pubDate></item><item><title>Codec-Based Deepfake Source Tracing via Neural Audio Codec Taxonomy</title><link>https://arxiv.org/abs/2505.12994</link><description>• Introduces a method for tracing the source of codec-based deepfake audio (CodecFake) by analyzing neural audio codec taxonomies.&lt;br/&gt;• Demonstrates the feasibility of identifying which neural audio codec system generated a given deepfake using experiments on the CodecFake+ dataset.&lt;br/&gt;• Highlights challenges and open problems in reliably tracing the origin of neural audio codec-based speech deepfakes.&lt;br/&gt;&lt;br/&gt;Tags: audio, deepfakes, source tracing, neural codecs, anti-spoofing, forensics&lt;br/&gt;&lt;br/&gt;&lt;a href='https://arxiv.org/abs/2505.12994'&gt;Read on arXiv&lt;/a&gt;</description><pubDate>Tue, 20 May 2025 04:25:55 +0000</pubDate></item><item><title>RoVo: Robust Voice Protection Against Unauthorized Speech Synthesis with Embedding-Level Perturbations</title><link>https://arxiv.org/abs/2505.12686</link><description>• Proposes RoVo, a defense method that injects adversarial perturbations at the embedding level of audio signals to protect against unauthorized speech synthesis and voice spoofing attacks.&lt;br/&gt;• RoVo demonstrates strong resistance to both speech synthesis attacks and secondary attacks from speech enhancement models, achieving high defense success rates.&lt;br/&gt;• User studies confirm that RoVo maintains the naturalness and usability of speech while providing robust protection.&lt;br/&gt;&lt;br/&gt;Tags: audio, adversarial defense, voice spoofing, deepfake prevention, embedding perturbation, speech synthesis, model robustness&lt;br/&gt;&lt;br/&gt;&lt;a href='https://arxiv.org/abs/2505.12686'&gt;Read on arXiv&lt;/a&gt;</description><pubDate>Tue, 20 May 2025 04:25:55 +0000</pubDate></item><item><title>VoiceCloak: A Multi-Dimensional Defense Framework against Unauthorized Diffusion-based Voice Cloning</title><link>https://arxiv.org/abs/2505.12332</link><description>• Presents VoiceCloak, a proactive defense framework against unauthorized voice cloning using diffusion models.&lt;br/&gt;• VoiceCloak introduces adversarial perturbations to obfuscate speaker identity and degrade the perceptual quality of cloned voices.&lt;br/&gt;• The framework disrupts representation learning and conditional guidance in diffusion models, demonstrating high defense success rates in experiments.&lt;br/&gt;&lt;br/&gt;Tags: audio, voice cloning, adversarial defense, diffusion models, identity obfuscation, deepfake prevention&lt;br/&gt;&lt;br/&gt;&lt;a href='https://arxiv.org/abs/2505.12332'&gt;Read on arXiv&lt;/a&gt;</description><pubDate>Tue, 20 May 2025 04:25:55 +0000</pubDate></item><item><title>Automatic Speech Recognition for African Low-Resource Languages: Challenges and Future Directions</title><link>https://arxiv.org/abs/2505.11690</link><description>• Analyzes challenges in developing ASR systems for African low-resource languages, including data scarcity, linguistic complexity, and privacy concerns.&lt;br/&gt;• Discusses strategies such as community-driven data collection, self-supervised learning, and privacy-preserving techniques to address these challenges.&lt;br/&gt;• Highlights the importance of ethical considerations, bias mitigation, and privacy in deploying ASR technologies for underrepresented languages.&lt;br/&gt;&lt;br/&gt;Tags: speech, ASR, low-resource languages, privacy, ethics, bias, africa&lt;br/&gt;&lt;br/&gt;&lt;a href='https://arxiv.org/abs/2505.11690'&gt;Read on arXiv&lt;/a&gt;</description><pubDate>Tue, 20 May 2025 04:25:55 +0000</pubDate></item><item><title>Universal Semantic Disentangled Privacy-preserving Speech Representation Learning</title><link>https://arxiv.org/abs/2505.13085</link><description>• Proposes a Universal Speech Codec (USC) that disentangles speech into privacy-preserving semantic representations and residual speaker/acoustic features.&lt;br/&gt;• USC enables high-fidelity speech reconstruction while removing identifiable speaker attributes, addressing privacy concerns in training LLMs with speech data.&lt;br/&gt;• Introduces an evaluation methodology for privacy-preserving properties and demonstrates trade-offs between speaker anonymization and content retention.&lt;br/&gt;&lt;br/&gt;Tags: audio, privacy, representation learning, speech, LLM, anonymization&lt;br/&gt;&lt;br/&gt;&lt;a href='https://arxiv.org/abs/2505.13085'&gt;Read on arXiv&lt;/a&gt;</description><pubDate>Tue, 20 May 2025 04:25:55 +0000</pubDate></item><item><title>AnalyticKWS: Towards Exemplar-Free Analytic Class Incremental Learning for Small-footprint Keyword Spotting</title><link>https://arxiv.org/abs/2505.11817</link><description>• Proposes AnalyticKWS, an exemplar-free continual learning method for keyword spotting that avoids storing user data, addressing privacy risks.&lt;br/&gt;• The method updates model parameters analytically without revisiting earlier data, making it suitable for small-footprint, resource-limited devices.&lt;br/&gt;• AnalyticKWS mitigates catastrophic forgetting and outperforms existing continual learning methods in experiments.&lt;br/&gt;&lt;br/&gt;Tags: audio, privacy, continual learning, edge devices, keyword spotting&lt;br/&gt;&lt;br/&gt;&lt;a href='https://arxiv.org/abs/2505.11817'&gt;Read on arXiv&lt;/a&gt;</description><pubDate>Tue, 20 May 2025 04:25:55 +0000</pubDate></item></channel></rss>