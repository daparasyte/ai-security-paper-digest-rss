<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>AI Security Paper Digest</title><link>https://kentaroh-toyoda.github.io/ai-security-paper-digest-rss/rss.xml</link><description>Curated papers on AI security from OpenAlex and ArXiv</description><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><lastBuildDate>Tue, 29 Jul 2025 22:24:29 +0000</lastBuildDate><item><title>Persistent Backdoor Attacks in Continual Learning</title><link>https://arxiv.org/abs/2409.13864</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces two persistent backdoor attacks in continual learning: Blind Task Backdoor and Latent Task Backdoor&lt;/li&gt;&lt;li&gt;Evaluates attacks with various triggers (static, dynamic, physical, semantic)&lt;/li&gt;&lt;li&gt;Demonstrates high success rates against state-of-the-art defenses like SentiNet and I-BAU&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Zhen Guo', 'Abhinav Kumar', 'Reza Tourani']&lt;/li&gt;&lt;li&gt;Tags: ['backdoor attacks', 'continual learning', 'adversarial attacks', 'security evaluation', 'red teaming']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2409.13864</guid><pubDate>Tue, 29 Jul 2025 00:00:00 +0000</pubDate></item><item><title>When and Where do Data Poisons Attack Textual Inversion?</title><link>https://arxiv.org/abs/2507.10578</link><description>&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Other&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jeremy Styborski', 'Mingzhi Lyu', 'Jiayou Lu', 'Nupur Kapur', 'Adams Kong']&lt;/li&gt;&lt;li&gt;Tags: []&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2507.10578</guid><pubDate>Tue, 29 Jul 2025 00:00:00 +0000</pubDate></item><item><title>Accidental Vulnerability: Factors in Fine-Tuning that Shift Model Safeguards</title><link>https://arxiv.org/abs/2505.16789</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Investigates accidental vulnerabilities introduced during fine-tuning of LLMs&lt;/li&gt;&lt;li&gt;Evaluates adversarial robustness and persona shifts in fine-tuned models&lt;/li&gt;&lt;li&gt;Analyzes dataset factors contributing to attack success rates&lt;/li&gt;&lt;li&gt;Explores causal relationships for improved adversarial defense strategies&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Punya Syon Pandey', 'Samuel Simko', 'Kellin Pelrine', 'Zhijing Jin']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial robustness', 'model alignment', 'fine-tuning', 'dataset analysis', 'vulnerability']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2505.16789</guid><pubDate>Tue, 29 Jul 2025 00:00:00 +0000</pubDate></item><item><title>BadVideo: Stealthy Backdoor Attack against Text-to-Video Generation</title><link>https://arxiv.org/abs/2504.16907</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces BadVideo, the first backdoor attack framework for text-to-video generation&lt;/li&gt;&lt;li&gt;Uses spatio-temporal composition and dynamic element transformation for stealthy adversarial outputs&lt;/li&gt;&lt;li&gt;Exploits video temporal dimension to evade frame-based content moderation&lt;/li&gt;&lt;li&gt;Demonstrates high attack success rates while preserving original semantics&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Ruotong Wang', 'Mingli Zhu', 'Jiarong Ou', 'Rui Chen', 'Xin Tao', 'Pengfei Wan', 'Baoyuan Wu']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial prompting', 'red teaming', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2504.16907</guid><pubDate>Tue, 29 Jul 2025 00:00:00 +0000</pubDate></item><item><title>Memorization: A Close Look at Books</title><link>https://arxiv.org/abs/2504.12549</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Extracted entire books from LLMs using prefix-prompting technique&lt;/li&gt;&lt;li&gt;Extraction rates correlate with book popularity and training data duplication&lt;/li&gt;&lt;li&gt;Found undoing of regurgitation mitigations in instruction-tuned Llama 3.1&lt;/li&gt;&lt;li&gt;Analyzed impact of fine-tuning on verbatim memorization retrieval&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Iris Ma', 'Ian Domingo', 'Alberto Krone-Martins', 'Pierre Baldi', 'Cristina V. Lopes']&lt;/li&gt;&lt;li&gt;Tags: ['model extraction', 'alignment', 'safety evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2504.12549</guid><pubDate>Tue, 29 Jul 2025 00:00:00 +0000</pubDate></item><item><title>Quantifying Security Vulnerabilities: A Metric-Driven Security Analysis of Gaps in Current AI Standards</title><link>https://arxiv.org/abs/2502.08610</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Audits three major AI governance standards (NIST AI RMF 1.0, UK ICO Toolkit, EU ALTAI)&lt;/li&gt;&lt;li&gt;Identifies 136 security concerns across the frameworks&lt;/li&gt;&lt;li&gt;Develops four key metrics to quantify security gaps&lt;/li&gt;&lt;li&gt;Provides targeted recommendations to enhance security controls&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Keerthana Madhavan', 'Abbas Yazdinejad', 'Fattane Zarrinkalam', 'Ali Dehghantanha']&lt;/li&gt;&lt;li&gt;Tags: ['AI security standards', 'Compliance gaps', 'Risk assessment metrics', 'Governance frameworks', 'Security metrics']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2502.08610</guid><pubDate>Tue, 29 Jul 2025 00:00:00 +0000</pubDate></item><item><title>Real-time Factuality Assessment from Adversarial Feedback</title><link>https://arxiv.org/abs/2410.14651</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Develops a pipeline using RAG-based feedback to generate adversarial news examples&lt;/li&gt;&lt;li&gt;Decreases ROC-AUC by 17.5% for a GPT-4o detector, highlighting vulnerability of retrieval-free models&lt;/li&gt;&lt;li&gt;Emphasizes importance of iterative rewriting and retrieval-augmentation in creating challenging test cases&lt;/li&gt;&lt;li&gt;Aims to improve factuality assessment robustness against real-time deceptive content&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Sanxing Chen', 'Yukun Huang', 'Bhuwan Dhingra']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial prompting', 'red teaming', 'robustness', 'safety evaluation', 'model extraction']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2410.14651</guid><pubDate>Tue, 29 Jul 2025 00:00:00 +0000</pubDate></item><item><title>Memorization in Fine-Tuned Large Language Models</title><link>https://arxiv.org/abs/2507.21009</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Investigates memorization in fine-tuned LLMs using membership inference attacks and generation tasks&lt;/li&gt;&lt;li&gt;Analyzes impact of different transformer matrices, perplexity, and LoRA ranks on memorization&lt;/li&gt;&lt;li&gt;Key findings: Value/Output matrices, lower perplexity, and higher LoRA ranks increase memorization&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Danil Savine', 'Muni Sreenivas Pydi', 'Jamal Atif', "Olivier Capp\\'e"]&lt;/li&gt;&lt;li&gt;Tags: ['privacy attacks', 'memorization', 'fine-tuning', 'LLMs', 'medical domain']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2507.21009</guid><pubDate>Tue, 29 Jul 2025 00:00:00 +0000</pubDate></item><item><title>Text2VLM: Adapting Text-Only Datasets to Evaluate Alignment Training in Visual Language Models</title><link>https://arxiv.org/abs/2507.20704</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces Text2VLM pipeline to adapt text-only datasets into multimodal formats&lt;/li&gt;&lt;li&gt;Evaluates VLMs' resilience against typographic prompt injection attacks&lt;/li&gt;&lt;li&gt;Highlights increased susceptibility of open-source VLMs to visual adversarial prompts&lt;/li&gt;&lt;li&gt;Contributes to comprehensive safety assessment for VLMs&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Gabriel Downer', 'Sean Craven', 'Damian Ruck', 'Jake Thomas']&lt;/li&gt;&lt;li&gt;Tags: ['red teaming', 'adversarial prompting', 'safety evaluation', 'visual language models', 'multimodal']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2507.20704</guid><pubDate>Tue, 29 Jul 2025 00:00:00 +0000</pubDate></item><item><title>When Prompts Go Wrong: Evaluating Code Model Robustness to Ambiguous, Contradictory, and Incomplete Task Descriptions</title><link>https://arxiv.org/abs/2507.20439</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Extended HumanEval and MBPP benchmarks with flawed task descriptions&lt;/li&gt;&lt;li&gt;Evaluated multiple LLMs on ambiguous, contradictory, and incomplete prompts&lt;/li&gt;&lt;li&gt;Found significant performance degradation with minor description imperfections&lt;/li&gt;&lt;li&gt;Identified need for more robust models and realistic evaluation benchmarks&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Maya Larbi', 'Amal Akli', 'Mike Papadakis', 'Rihab Bouyousfi', 'Maxime Cordy', 'Federica Sarro', 'Yves Le Traon']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial_prompting', 'robustness', 'safety_evaluation', 'red_teaming', 'model_evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2507.20439</guid><pubDate>Tue, 29 Jul 2025 00:00:00 +0000</pubDate></item><item><title>Trivial Trojans: How Minimal MCP Servers Enable Cross-Tool Exfiltration of Sensitive Data</title><link>https://arxiv.org/abs/2507.19880</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Demonstrates how unsophisticated attackers can exploit MCP trust model to exfiltrate sensitive data&lt;/li&gt;&lt;li&gt;Proof-of-concept attack uses malicious weather server to steal banking info via legitimate tools&lt;/li&gt;&lt;li&gt;Highlights low barrier to entry for MCP-based attacks with basic programming skills&lt;/li&gt;&lt;li&gt;Proposes mitigations and protocol improvements for MCP security&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Nicola Croce', 'Tobin South']&lt;/li&gt;&lt;li&gt;Tags: ['MCP', 'security', 'red teaming', 'data exfiltration', 'AI tool integration']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2507.19880</guid><pubDate>Tue, 29 Jul 2025 00:00:00 +0000</pubDate></item><item><title>MOCHA: Are Code Language Models Robust Against Multi-Turn Malicious Coding Prompts?</title><link>https://arxiv.org/abs/2507.19598</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces code decomposition attacks using multi-turn malicious prompts&lt;/li&gt;&lt;li&gt;Presents MOCHA benchmark for evaluating robustness against single- and multi-turn attacks&lt;/li&gt;&lt;li&gt;Finds persistent vulnerabilities in both open- and closed-source code LLMs&lt;/li&gt;&lt;li&gt;Demonstrates fine-tuning on MOCHA improves rejection rates and robustness&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Muntasir Wahed', 'Xiaona Zhou', 'Kiet A. Nguyen', 'Tianjiao Yu', 'Nirav Diwan', 'Gang Wang', 'Dilek Hakkani-T\\"ur', 'Ismini Lourentzou']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial prompting', 'red teaming', 'robustness', 'code generation', 'multi-turn attacks']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2507.19598</guid><pubDate>Tue, 29 Jul 2025 00:00:00 +0000</pubDate></item><item><title>MIRAGE-Bench: LLM Agent is Hallucinating and Where to Find Them</title><link>https://arxiv.org/abs/2507.21017</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces MIRAGE-Bench, a unified benchmark for evaluating hallucinations in LLM agents&lt;/li&gt;&lt;li&gt;Presents a three-part taxonomy of hallucinative actions&lt;/li&gt;&lt;li&gt;Uses LLM-as-a-Judge for scalable evaluation&lt;/li&gt;&lt;li&gt;Aims to mitigate hallucinations in interactive environments&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Weichen Zhang', 'Yiyou Sun', 'Pohao Huang', 'Jiayue Pu', 'Heyue Lin', 'Dawn Song']&lt;/li&gt;&lt;li&gt;Tags: ['safety evaluation', 'LLM', 'benchmark', 'hallucinations', 'agent']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2507.21017</guid><pubDate>Tue, 29 Jul 2025 00:00:00 +0000</pubDate></item><item><title>Core Safety Values for Provably Corrigible Agents</title><link>https://arxiv.org/abs/2507.20964</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces a framework for corrigible agents with 5 utility heads: deference, switch-access preservation, truthfulness, low-impact behavior, and bounded task reward&lt;/li&gt;&lt;li&gt;Proves single-round and multi-step corrigibility with error bounds&lt;/li&gt;&lt;li&gt;Addresses undecidability in adversarial settings and provides a decidable island with zero-knowledge proofs&lt;/li&gt;&lt;li&gt;Shifts safety risk from hidden incentives to evaluation quality&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Aran Nayebi']&lt;/li&gt;&lt;li&gt;Tags: ['corrigibility', 'safety', 'provable guarantees', 'utility functions', 'multi-step environments', 'partially observed', 'zero-knowledge proofs', 'adversarial settings']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2507.20964</guid><pubDate>Tue, 29 Jul 2025 00:00:00 +0000</pubDate></item><item><title>Security Challenges in AI Agent Deployment: Insights from a Large Scale Public Competition</title><link>https://arxiv.org/abs/2507.20526</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Conducted a large-scale red-teaming competition targeting 22 AI agents across 44 scenarios&lt;/li&gt;&lt;li&gt;Collected 1.8 million prompt-injection attacks with over 60,000 successful policy violations&lt;/li&gt;&lt;li&gt;Introduced the Agent Red Teaming (ART) benchmark to evaluate agent robustness&lt;/li&gt;&lt;li&gt;Found limited correlation between agent robustness and model size/capability&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Andy Zou', 'Maxwell Lin', 'Eliot Jones', 'Micha Nowak', 'Mateusz Dziemian', 'Nick Winter', 'Alexander Grattan', 'Valent Nathanael', 'Ayla Croft', 'Xander Davies', 'Jai Patel', 'Robert Kirk', 'Nate Burnikell', 'Yarin Gal', 'Dan Hendrycks', 'J. Zico Kolter', 'Matt Fredrikson']&lt;/li&gt;&lt;li&gt;Tags: ['red teaming', 'prompt injection', 'adversarial prompting', 'security evaluation', 'benchmarks']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2507.20526</guid><pubDate>Tue, 29 Jul 2025 00:00:00 +0000</pubDate></item><item><title>The Blessing and Curse of Dimensionality in Safety Alignment</title><link>https://arxiv.org/abs/2507.20333</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Investigates how high-dimensional representations in LLMs enable activation engineering attacks&lt;/li&gt;&lt;li&gt;Demonstrates that dimensionality reduction can mitigate jailbreaking susceptibility&lt;/li&gt;&lt;li&gt;Provides theoretical insights into the relationship between hidden dimensions and linear jailbreaking methods&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Rachel S. Y. Teo', 'Laziz U. Abdullaev', 'Tan M. Nguyen']&lt;/li&gt;&lt;li&gt;Tags: ['LLM safety', 'jailbreaking', 'activation engineering', 'dimensionality reduction', 'alignment']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2507.20333</guid><pubDate>Tue, 29 Jul 2025 00:00:00 +0000</pubDate></item><item><title>The Policy Cliff: A Theoretical Analysis of Reward-Policy Maps in Large Language Models</title><link>https://arxiv.org/abs/2507.20150</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces a theoretical framework for analyzing policy stability in LLMs/LRMs&lt;/li&gt;&lt;li&gt;Explains brittleness through non-unique optimal actions&lt;/li&gt;&lt;li&gt;Extends analysis to multi-reward RL settings&lt;/li&gt;&lt;li&gt;Demonstrates how entropy regularization affects stability&lt;/li&gt;&lt;li&gt;Provides insights into safety issues like deceptive alignment and instruction disobedience&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Xingcheng Xu']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'safety evaluation', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2507.20150</guid><pubDate>Tue, 29 Jul 2025 00:00:00 +0000</pubDate></item><item><title>Alignment and Safety in Large Language Models: Safety Mechanisms, Training Paradigms, and Emerging Challenges</title><link>https://arxiv.org/abs/2507.19672</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Comprehensive overview of LLM alignment techniques and training paradigms&lt;/li&gt;&lt;li&gt;Analysis of trade-offs between alignment objectives&lt;/li&gt;&lt;li&gt;Review of evaluation frameworks and benchmarking datasets&lt;/li&gt;&lt;li&gt;Discussion of open challenges in oversight, value pluralism, robustness, and continuous alignment&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Survey&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Haoran Lu', 'Luyang Fang', 'Ruidong Zhang', 'Xinliang Li', 'Jiazhang Cai', 'Huimin Cheng', 'Lin Tang', 'Ziyu Liu', 'Zeliang Sun', 'Tao Wang', 'Yingchuan Zhang', 'Arif Hassan Zidan', 'Jinwen Xu', 'Jincheng Yu', 'Meizhi Yu', 'Hanqi Jiang', 'Xilin Gong', 'Weidi Luo', 'Bolun Sun', 'Yongkai Chen', 'Terry Ma', 'Shushan Wu', 'Yifan Zhou', 'Junhao Chen', 'Haotian Xiang', 'Jing Zhang', 'Afrar Jahin', 'Wei Ruan', 'Ke Deng', 'Yi Pan', 'Peilong Wang', 'Jiahui Li', 'Zhengliang Liu', 'Lu Zhang', 'Lin Zhao', 'Wei Liu', 'Dajiang Zhu', 'Xin Xing', 'Fei Dou', 'Wei Zhang', 'Chao Huang', 'Rongjie Liu', 'Mengrui Zhang', 'Yiwen Liu', 'Xiaoxiao Sun', 'Qin Lu', 'Zhen Xiang', 'Wenxuan Zhong', 'Tianming Liu', 'Ping Ma']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'safety', 'survey', 'training paradigms', 'evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2507.19672</guid><pubDate>Tue, 29 Jul 2025 00:00:00 +0000</pubDate></item></channel></rss>