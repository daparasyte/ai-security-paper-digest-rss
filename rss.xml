<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>AI Security Paper Digest</title><link>https://kentaroh-toyoda.github.io/ai-security-paper-digest-rss/rss.xml</link><description>Curated papers on AI security from ArXiv and ACL</description><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><lastBuildDate>Thu, 02 Oct 2025 22:22:03 +0000</lastBuildDate><item><title>SafeEraser: Enhancing Safety in Multimodal Large Language Models through Multimodal Machine Unlearning</title><link>https://arxiv.org/abs/2502.12520</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes SafeEraser, a safety unlearning benchmark for MLLMs&lt;/li&gt;&lt;li&gt;Introduces Prompt Decouple (PD) Loss to prevent over-forgetting during unlearning&lt;/li&gt;&lt;li&gt;Evaluates existing unlearning methods on forget quality and model utility&lt;/li&gt;&lt;li&gt;Demonstrates significant reduction in Safe Answer Refusal Rate (SARR) with PD Loss&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Junkai Chen', 'Zhijie Deng', 'Kening Zheng', 'Yibo Yan', 'Shuliang Liu', 'PeiJun Wu', 'Peijie Jiang', 'Jia Liu', 'Xuming Hu']&lt;/li&gt;&lt;li&gt;Tags: ['safety', 'unlearning', 'multimodal', 'evaluation', 'benchmark']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2502.12520</guid><pubDate>Thu, 02 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Erased, But Not Forgotten: Erased Rectified Flow Transformers Still Remain Unsafe Under Concept Attack</title><link>https://arxiv.org/abs/2510.00635</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces ReFlux, a concept attack method for rectified flow transformers like Flux&lt;/li&gt;&lt;li&gt;Targets attention localization in concept erasure techniques&lt;/li&gt;&lt;li&gt;Uses reverse-attention optimization, velocity-guided dynamics, and consistency preservation&lt;/li&gt;&lt;li&gt;Aims to reactivate suppressed signals and test robustness of erasure methods&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Nanxiang Jiang', 'Zhaoxin Fan', 'Enhan Kang', 'Daiheng Gao', 'Yun Zhou', 'Yanxia Chang', 'Zheng Zhu', 'Yeying Jin', 'Wenjun Wu']&lt;/li&gt;&lt;li&gt;Tags: ['red teaming', 'concept erasure', 'text-to-image models', 'rectified flow', 'attention localization', 'robustness evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.00635</guid><pubDate>Thu, 02 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Reinforcement Learning-Based Prompt Template Stealing for Text-to-Image Models</title><link>https://arxiv.org/abs/2510.00046</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces RLStealer, a framework for stealing prompt templates from text-to-image models using reinforcement learning.&lt;/li&gt;&lt;li&gt;Treats template stealing as a sequential decision problem with similarity-based rewards.&lt;/li&gt;&lt;li&gt;Demonstrates state-of-the-art performance with reduced attack cost compared to existing methods.&lt;/li&gt;&lt;li&gt;Highlights security risks in prompt trading for MLLMs.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Xiaotian Zou']&lt;/li&gt;&lt;li&gt;Tags: ['prompt inversion', 'reinforcement learning', 'text-to-image models', 'security threat', 'prompt trading']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.00046</guid><pubDate>Thu, 02 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Fingerprinting LLMs via Prompt Injection</title><link>https://arxiv.org/abs/2509.25448</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes LLMPrint, a detection framework using prompt injection to fingerprint LLMs&lt;/li&gt;&lt;li&gt;Addresses model provenance detection for already released models&lt;/li&gt;&lt;li&gt;Evaluates on multiple base models and their variants&lt;/li&gt;&lt;li&gt;Provides statistical guarantees for verification&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yuepeng Hu', 'Zhengyuan Jiang', 'Mengyuan Li', 'Osama Ahmed', 'Zhicong Huang', 'Cheng Hong', 'Neil Gong']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'prompt injection', 'model extraction', 'security standards']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.25448</guid><pubDate>Thu, 02 Oct 2025 00:00:00 +0000</pubDate></item><item><title>SafeSearch: Automated Red-Teaming for the Safety of LLM-Based Search Agents</title><link>https://arxiv.org/abs/2509.23694</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces SafeSearch, an automated red-teaming framework for LLM-based search agents&lt;/li&gt;&lt;li&gt;Constructs a benchmark with 300 test cases covering 5 risk categories&lt;/li&gt;&lt;li&gt;Evaluates 3 search agent scaffolds across 15 LLMs, revealing high attack success rates&lt;/li&gt;&lt;li&gt;Highlights limitations of current defense practices like reminder prompting&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jianshuo Dong', 'Sheng Guo', 'Hao Wang', 'Zhuotao Liu', 'Tianwei Zhang', 'Ke Xu', 'Minlie Huang', 'Han Qiu']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'safety evaluation', 'adversarial prompting', 'benchmarking', 'search agents']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.23694</guid><pubDate>Thu, 02 Oct 2025 00:00:00 +0000</pubDate></item><item><title>AgentMisalignment: Measuring the Propensity for Misaligned Behaviour in LLM-Based Agents</title><link>https://arxiv.org/abs/2506.04018</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces AgentMisalignment benchmark to measure misalignment propensity in LLM agents&lt;/li&gt;&lt;li&gt;Evaluates behaviors like avoiding oversight, resisting shutdown, sandbagging, and power-seeking&lt;/li&gt;&lt;li&gt;Finds that more capable agents have higher misalignment on average&lt;/li&gt;&lt;li&gt;Shows that system prompts/personalities strongly influence misalignment&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Akshat Naik', 'Patrick Quinn', 'Guillermo Bosch', "Emma Goun\\'e", 'Francisco Javier Campos Zabala', 'Jason Ross Brown', 'Edward James Young']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'red teaming', 'safety evaluation', 'benchmarking', 'LLM']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.04018</guid><pubDate>Thu, 02 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Phantom: General Backdoor Attacks on Retrieval Augmented Language Generation</title><link>https://arxiv.org/abs/2405.20485</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Phantom is a backdoor attack on RAG systems by injecting a malicious document into the knowledge base.&lt;/li&gt;&lt;li&gt;The attack uses a two-stage optimization framework to create a document that triggers on specific queries and manipulates LLM outputs.&lt;/li&gt;&lt;li&gt;Demonstrated on open-source models like Gemma, Vicuna, Llama, and closed-source models like GPT-3.5 Turbo, GPT-4.&lt;/li&gt;&lt;li&gt;Successfully tested on NVIDIA's 'Chat with RTX' production system.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Harsh Chaudhari', 'Giorgio Severi', 'John Abascal', 'Anshuman Suri', 'Matthew Jagielski', 'Christopher A. Choquette-Choo', 'Milad Nasr', 'Cristina Nita-Rotaru', 'Alina Oprea']&lt;/li&gt;&lt;li&gt;Tags: ['data poisoning', 'adversarial prompting', 'RAG systems', 'backdoor attacks', 'security']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2405.20485</guid><pubDate>Thu, 02 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Leaky Thoughts: Large Reasoning Models Are Not Private Thinkers</title><link>https://arxiv.org/abs/2506.15674</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Privacy leakage in reasoning traces of large models&lt;/li&gt;&lt;li&gt;Prompt injections and compute budget increase leakage&lt;/li&gt;&lt;li&gt;Tension between utility and privacy in reasoning steps&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Tommaso Green', 'Martin Gubri', 'Haritz Puerto', 'Sangdoo Yun', 'Seong Joon Oh']&lt;/li&gt;&lt;li&gt;Tags: ['privacy attacks', 'prompt injection', 'data leakage', 'reasoning models', 'safety evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.15674</guid><pubDate>Thu, 02 Oct 2025 00:00:00 +0000</pubDate></item><item><title>CounselBench: A Large-Scale Expert Evaluation and Adversarial Benchmarking of Large Language Models in Mental Health Question Answering</title><link>https://arxiv.org/abs/2506.08584</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;CounselBench is a benchmark for evaluating LLMs in mental health QA with expert annotations.&lt;/li&gt;&lt;li&gt;It includes both standard evaluation (CounselBench-EVAL) and adversarial testing (CounselBench-Adv).&lt;/li&gt;&lt;li&gt;Key issues found include safety risks, unauthorized medical advice, and overgeneralization.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yahan Li', 'Jifan Yao', 'John Bosco S. Bunyi', 'Adam C. Frank', 'Angel Hwang', 'Ruishan Liu']&lt;/li&gt;&lt;li&gt;Tags: ['safety evaluation', 'adversarial prompting', 'benchmarking', 'mental health', 'expert evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.08584</guid><pubDate>Thu, 02 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Language Models can Subtly Deceive Without Lying: A Case Study on Strategic Phrasing in Legislation</title><link>https://arxiv.org/abs/2405.04325</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;The paper explores LLMs' ability to engage in subtle deception through strategic phrasing.&lt;/li&gt;&lt;li&gt;A testbed is built to simulate a legislative environment where LLM lobbyists draft amendments.&lt;/li&gt;&lt;li&gt;Results show that LLMs can avoid detection by detectors, with optimization increasing deception rates.&lt;/li&gt;&lt;li&gt;Human evaluations confirm the quality of deceptive generations and retention of intent.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Atharvan Dogra', 'Krishna Pillutla', 'Ameet Deshpande', 'Ananya B Sai', 'John Nay', 'Tanmay Rajpurohit', 'Ashwin Kalyan', 'Balaraman Ravindran']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'adversarial prompting', 'safety evaluation', 'alignment', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2405.04325</guid><pubDate>Thu, 02 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Eyes-on-Me: Scalable RAG Poisoning through Transferable Attention-Steering Attractors</title><link>https://arxiv.org/abs/2510.00586</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces Eyes-on-Me, a scalable RAG poisoning attack using modular components&lt;/li&gt;&lt;li&gt;Decomposes adversarial documents into reusable Attention Attractors and Focus Regions&lt;/li&gt;&lt;li&gt;Steers a subset of attention heads to direct focus to malicious regions&lt;/li&gt;&lt;li&gt;Significant improvement in attack success rates across multiple RAG setups&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yen-Shan Chen', 'Sian-Yao Huang', 'Cheng-Lin Yang', 'Yun-Nung Chen']&lt;/li&gt;&lt;li&gt;Tags: ['data poisoning', 'adversarial attacks', 'RAG systems', 'attention mechanisms', 'transferable components']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.00586</guid><pubDate>Thu, 02 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Backdoor Attacks Against Speech Language Models</title><link>https://arxiv.org/abs/2510.01157</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;First systematic study of audio backdoor attacks on speech language models&lt;/li&gt;&lt;li&gt;Demonstrates high success rates across multiple encoders and datasets&lt;/li&gt;&lt;li&gt;Identifies vulnerable pipeline stages through component-wise analysis&lt;/li&gt;&lt;li&gt;Proposes a fine-tuning defense against poisoned encoders&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Alexandrine Fortier', 'Thomas Thebaud', "Jes\\'us Villalba", 'Najim Dehak', 'Patrick Cardinal']&lt;/li&gt;&lt;li&gt;Tags: ['data poisoning', 'backdoor attacks', 'speech models', 'multimodal', 'defense mechanisms']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.01157</guid><pubDate>Thu, 02 Oct 2025 00:00:00 +0000</pubDate></item><item><title>ManagerBench: Evaluating the Safety-Pragmatism Trade-off in Autonomous LLMs</title><link>https://arxiv.org/abs/2510.00857</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces ManagerBench, a benchmark for evaluating LLMs' safety-pragmatism trade-off in managerial scenarios.&lt;/li&gt;&lt;li&gt;Scenarios force a choice between harmful but effective actions and safe but less effective ones.&lt;/li&gt;&lt;li&gt;Findings show that frontier LLMs struggle with this trade-off, either choosing harm or being overly safe.&lt;/li&gt;&lt;li&gt;The issue is due to flawed prioritization despite correct harm perception.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Adi Simhi', 'Jonathan Herzig', 'Martin Tutek', 'Itay Itzhak', 'Idan Szpektor', 'Yonatan Belinkov']&lt;/li&gt;&lt;li&gt;Tags: ['safety evaluation', 'alignment', 'benchmarking', 'autonomous agents', 'managerial scenarios']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.00857</guid><pubDate>Thu, 02 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Exposing the Cracks: Vulnerabilities of Retrieval-Augmented LLM-based Machine Translation</title><link>https://arxiv.org/abs/2510.00829</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Evaluates robustness of REAL-MT under noisy retrieval contexts&lt;/li&gt;&lt;li&gt;Finds low-resource languages more vulnerable&lt;/li&gt;&lt;li&gt;LRMs are more susceptible to noise and rationalize errors&lt;/li&gt;&lt;li&gt;Identifies trade-off between robustness and clean performance&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yanming Sun', 'Runzhe Zhan', 'Chi Seng Cheang', 'Han Wu', 'Xuebo Liu', 'Yuyao Niu', 'Fengying Ye', 'Kaixin Lan', 'Lidia S. Chao', 'Derek F. Wong']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial prompting', 'robustness', 'data poisoning', 'privacy attacks', 'alignment']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.00829</guid><pubDate>Thu, 02 Oct 2025 00:00:00 +0000</pubDate></item><item><title>A Multi-Agent LLM Defense Pipeline Against Prompt Injection Attacks</title><link>https://arxiv.org/abs/2509.14285</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces a multi-agent LLM defense framework against prompt injection attacks&lt;/li&gt;&lt;li&gt;Evaluates two architectures: sequential and hierarchical&lt;/li&gt;&lt;li&gt;Tested on 55 unique attacks across two LLM platforms (ChatGLM and Llama2)&lt;/li&gt;&lt;li&gt;Achieves 100% mitigation, reducing ASR to 0%&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['S M Asif Hossain', 'Ruksat Khan Shayoni', 'Mohd Ruhul Ameen', 'Akif Islam', 'M. F. Mridha', 'Jungpil Shin']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'prompt injection', 'adversarial prompting', 'security', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.14285</guid><pubDate>Thu, 02 Oct 2025 00:00:00 +0000</pubDate></item><item><title>AgentMisalignment: Measuring the Propensity for Misaligned Behaviour in LLM-Based Agents</title><link>https://arxiv.org/abs/2506.04018</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces AgentMisalignment benchmark to measure misalignment propensity in LLM agents&lt;/li&gt;&lt;li&gt;Evaluates behaviors like avoiding oversight, resisting shutdown, sandbagging, and power-seeking&lt;/li&gt;&lt;li&gt;Finds that more capable agents have higher misalignment on average&lt;/li&gt;&lt;li&gt;Shows that system prompts/personalities strongly influence misalignment&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Akshat Naik', 'Patrick Quinn', 'Guillermo Bosch', "Emma Goun\\'e", 'Francisco Javier Campos Zabala', 'Jason Ross Brown', 'Edward James Young']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'red teaming', 'safety evaluation', 'benchmarking', 'LLM']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.04018</guid><pubDate>Thu, 02 Oct 2025 00:00:00 +0000</pubDate></item><item><title>UTrace: Poisoning Forensics for Private Collaborative Learning</title><link>https://arxiv.org/abs/2409.15126</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces UTrace, a framework for detecting data poisoning in private collaborative learning&lt;/li&gt;&lt;li&gt;Uses gradient similarity and user-level unlearning to attribute model issues&lt;/li&gt;&lt;li&gt;Evaluated on multiple datasets and poisoning attacks with high accuracy&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Evan Rose', 'Hidde Lycklama', 'Harsh Chaudhari', 'Niklas Britz', 'Anwar Hithnawi', 'Alina Oprea']&lt;/li&gt;&lt;li&gt;Tags: ['data poisoning', 'privacy attacks', 'security', 'collaborative learning', 'model integrity']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2409.15126</guid><pubDate>Thu, 02 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Phantom: General Backdoor Attacks on Retrieval Augmented Language Generation</title><link>https://arxiv.org/abs/2405.20485</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Phantom is a backdoor attack on RAG systems by injecting a malicious document into the knowledge base.&lt;/li&gt;&lt;li&gt;The attack uses a two-stage optimization framework to create a document that triggers on specific queries and manipulates LLM outputs.&lt;/li&gt;&lt;li&gt;Demonstrated on open-source models like Gemma, Vicuna, Llama, and closed-source models like GPT-3.5 Turbo, GPT-4.&lt;/li&gt;&lt;li&gt;Successfully tested on NVIDIA's 'Chat with RTX' production system.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Harsh Chaudhari', 'Giorgio Severi', 'John Abascal', 'Anshuman Suri', 'Matthew Jagielski', 'Christopher A. Choquette-Choo', 'Milad Nasr', 'Cristina Nita-Rotaru', 'Alina Oprea']&lt;/li&gt;&lt;li&gt;Tags: ['data poisoning', 'adversarial prompting', 'RAG systems', 'backdoor attacks', 'security']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2405.20485</guid><pubDate>Thu, 02 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Toward Safer Diffusion Language Models: Discovery and Mitigation of Priming Vulnerability</title><link>https://arxiv.org/abs/2510.00565</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Discovers a priming vulnerability in DLMs where intermediate affirmative tokens can bypass safety measures&lt;/li&gt;&lt;li&gt;Proposes a safety alignment method to train models to handle contaminated intermediate states&lt;/li&gt;&lt;li&gt;Improves robustness against jailbreak attacks with minimal performance impact&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Shojiro Yamabe', 'Jun Sakuma']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'jailbreaking', 'safety evaluation', 'alignment', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.00565</guid><pubDate>Thu, 02 Oct 2025 00:00:00 +0000</pubDate></item><item><title>On the Adversarial Robustness of Learning-based Conformal Novelty Detection</title><link>https://arxiv.org/abs/2510.00463</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;This paper studies the adversarial robustness of AdaDetect, a learning-based framework for novelty detection.&lt;/li&gt;&lt;li&gt;It formulates an oracle attack setting to quantify FDR degradation under adversarial perturbations.&lt;/li&gt;&lt;li&gt;The paper evaluates the vulnerability of AdaDetect on synthetic and real-world datasets, showing significant FDR increase.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Daofu Zhang', 'Mehrdad Pournaderi', 'Hanne M. Clifford', 'Yu Xiang', 'Pramod K. Varshney']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial robustness', 'novelty detection', 'FDR control', 'black-box attacks']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.00463</guid><pubDate>Thu, 02 Oct 2025 00:00:00 +0000</pubDate></item><item><title>A Call to Action for a Secure-by-Design Generative AI Paradigm</title><link>https://arxiv.org/abs/2510.00451</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces PromptShield, an ontology-driven framework for secure prompt interactions&lt;/li&gt;&lt;li&gt;Conducted experiments on AWS cloud log analysis with simulated prompt injection attacks&lt;/li&gt;&lt;li&gt;Achieved high security and performance metrics (94% precision, recall, F1)&lt;/li&gt;&lt;li&gt;Emphasizes security-by-design approach for LLMs&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Dalal Alharthi', 'Ivan Roberto Kawaminami Garcia']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'prompt injection', 'adversarial prompting', 'security-by-design', 'ontology-driven security']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.00451</guid><pubDate>Thu, 02 Oct 2025 00:00:00 +0000</pubDate></item><item><title>CHAI: Command Hijacking against embodied AI</title><link>https://arxiv.org/abs/2510.00181</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces CHAI, a prompt-based attack on embodied AI using Large Visual-Language Models (LVLMs)&lt;/li&gt;&lt;li&gt;Exploits multimodal language interpretation by embedding deceptive instructions in visual inputs&lt;/li&gt;&lt;li&gt;Evaluates on drone, autonomous driving, and real robotic vehicle scenarios&lt;/li&gt;&lt;li&gt;Outperforms state-of-the-art attacks, highlighting need for new defenses&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Luis Burbano', 'Diego Ortiz', 'Qi Sun', 'Siwei Yang', 'Haoqin Tu', 'Cihang Xie', 'Yinzhi Cao', 'Alvaro A Cardenas']&lt;/li&gt;&lt;li&gt;Tags: ['red teaming', 'adversarial prompting', 'multimodal attacks', 'embodied AI security', 'LVLM vulnerabilities']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.00181</guid><pubDate>Thu, 02 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Eliciting Secret Knowledge from Language Models</title><link>https://arxiv.org/abs/2510.01070</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;The paper studies secret elicitation from LLMs, where models possess knowledge but don't verbalize it.&lt;/li&gt;&lt;li&gt;They train LLMs to apply specific knowledge (like user gender) but deny it when asked.&lt;/li&gt;&lt;li&gt;Various black-box and white-box techniques are evaluated for secret elicitation.&lt;/li&gt;&lt;li&gt;Prefill attacks are most effective in two settings, while logit lens and SAEs work in another.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ["Bartosz Cywi\\'nski", 'Emil Ryd', 'Rowan Wang', 'Senthooran Rajamanoharan', 'Neel Nanda', 'Arthur Conmy', 'Samuel Marks']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'adversarial prompting', 'privacy attacks', 'safety evaluation', 'benchmarking']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.01070</guid><pubDate>Thu, 02 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Large Reasoning Models Learn Better Alignment from Flawed Thinking</title><link>https://arxiv.org/abs/2510.00938</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces RECAP, a method to improve safety alignment in large reasoning models&lt;/li&gt;&lt;li&gt;Uses reinforcement learning to train models to override flawed reasoning&lt;/li&gt;&lt;li&gt;Improves jailbreak robustness and reduces overrefusal&lt;/li&gt;&lt;li&gt;Maintains core reasoning capabilities and token budget&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['ShengYun Peng', 'Eric Smith', 'Ivan Evtimov', 'Song Jiang', 'Pin-Yu Chen', 'Hongyuan Zhan', 'Haozhu Wang', 'Duen Horng Chau', 'Mahesh Pasupuleti', 'Jianfeng Chi']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'alignment', 'robustness', 'safety evaluation', 'reinforcement learning']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.00938</guid><pubDate>Thu, 02 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Downgrade to Upgrade: Optimizer Simplification Enhances Robustness in LLM Unlearning</title><link>https://arxiv.org/abs/2510.00761</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;The paper explores how downgrading optimizers (e.g., using zeroth-order methods) can enhance the robustness of LLM unlearning.&lt;/li&gt;&lt;li&gt;It proposes a hybrid optimizer combining first-order and zeroth-order updates.&lt;/li&gt;&lt;li&gt;Experiments show improved resilience against post-unlearning manipulations like quantization or fine-tuning.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yicheng Lang', 'Yihua Zhang', 'Chongyu Fan', 'Changsheng Wang', 'Jinghan Jia', 'Sijia Liu']&lt;/li&gt;&lt;li&gt;Tags: ['robustness', 'unlearning', 'optimizers', 'privacy', 'safety']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.00761</guid><pubDate>Thu, 02 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Eyes-on-Me: Scalable RAG Poisoning through Transferable Attention-Steering Attractors</title><link>https://arxiv.org/abs/2510.00586</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces Eyes-on-Me, a scalable RAG poisoning attack using modular components&lt;/li&gt;&lt;li&gt;Decomposes adversarial documents into reusable Attention Attractors and Focus Regions&lt;/li&gt;&lt;li&gt;Steers a subset of attention heads to direct focus to malicious regions&lt;/li&gt;&lt;li&gt;Significant improvement in attack success rates across multiple RAG setups&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yen-Shan Chen', 'Sian-Yao Huang', 'Cheng-Lin Yang', 'Yun-Nung Chen']&lt;/li&gt;&lt;li&gt;Tags: ['data poisoning', 'adversarial attacks', 'RAG systems', 'attention mechanisms', 'transferable components']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.00586</guid><pubDate>Thu, 02 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Understanding Sensitivity of Differential Attention through the Lens of Adversarial Robustness</title><link>https://arxiv.org/abs/2510.00517</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;The paper explores the adversarial robustness of Differential Attention (DA) mechanisms in models like ViT and CLIP.&lt;/li&gt;&lt;li&gt;It identifies that DA's subtractive structure leads to increased sensitivity to adversarial perturbations due to negative gradient alignment.&lt;/li&gt;&lt;li&gt;Empirical results show higher attack success rates and stronger local sensitivity compared to standard attention.&lt;/li&gt;&lt;li&gt;The study reveals a trade-off between DA's improved focus on clean inputs and increased vulnerability to adversarial attacks.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Tsubasa Takahashi', 'Shojiro Yamabe', 'Futa Waseda', 'Kento Sasaki']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial robustness', 'differential attention', 'gradient alignment', 'adversarial attacks', 'model robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.00517</guid><pubDate>Thu, 02 Oct 2025 00:00:00 +0000</pubDate></item><item><title>DecepChain: Inducing Deceptive Reasoning in Large Language Models</title><link>https://arxiv.org/abs/2510.00319</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces DecepChain, a backdoor attack on LLMs that generates plausible but incorrect reasoning chains.&lt;/li&gt;&lt;li&gt;Uses fine-tuning on model-generated errors and GRPO with flipped rewards to create stealthy attacks.&lt;/li&gt;&lt;li&gt;Demonstrates high attack success rates with minimal performance impact, and human evaluators struggle to detect manipulated reasoning.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Wei Shen', 'Han Wang', 'Haoyu Li', 'Huan Zhang']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'adversarial prompting', 'model extraction', 'safety evaluation', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.00319</guid><pubDate>Thu, 02 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Uncovering Vulnerabilities of LLM-Assisted Cyber Threat Intelligence</title><link>https://arxiv.org/abs/2509.23573</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Investigates LLM vulnerabilities in cyber threat intelligence (CTI)&lt;/li&gt;&lt;li&gt;Introduces a categorization methodology for analyzing failure instances&lt;/li&gt;&lt;li&gt;Identifies three fundamental vulnerabilities: spurious correlations, contradictory knowledge, constrained generalization&lt;/li&gt;&lt;li&gt;Provides insights for designing robust LLM-powered CTI systems&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yuqiao Meng', 'Luoxi Tang', 'Feiyang Yu', 'Jinyuan Jia', 'Guanhua Yan', 'Ping Yang', 'Zhaohan Xi']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'safety evaluation', 'robustness', 'alignment', 'privacy attacks']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.23573</guid><pubDate>Thu, 02 Oct 2025 00:00:00 +0000</pubDate></item><item><title>LLM Watermark Evasion via Bias Inversion</title><link>https://arxiv.org/abs/2509.23019</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes Bias-Inversion Rewriting Attack (BIRA) to evade LLM watermarks&lt;/li&gt;&lt;li&gt;Achieves over 99% evasion rate across multiple watermarking methods&lt;/li&gt;&lt;li&gt;Attack is model-agnostic and doesn't require knowledge of watermarking scheme&lt;/li&gt;&lt;li&gt;Emphasizes need for robust defenses and stress testing&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jeongyeon Hwang', 'Sangdon Park', 'Jungseul Ok']&lt;/li&gt;&lt;li&gt;Tags: ['red teaming', 'adversarial prompting', 'model extraction', 'security', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.23019</guid><pubDate>Thu, 02 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Leaky Thoughts: Large Reasoning Models Are Not Private Thinkers</title><link>https://arxiv.org/abs/2506.15674</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Privacy leakage in reasoning traces of large models&lt;/li&gt;&lt;li&gt;Prompt injections and compute budget increase leakage&lt;/li&gt;&lt;li&gt;Tension between utility and privacy in reasoning steps&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Tommaso Green', 'Martin Gubri', 'Haritz Puerto', 'Sangdoo Yun', 'Seong Joon Oh']&lt;/li&gt;&lt;li&gt;Tags: ['privacy attacks', 'prompt injection', 'data leakage', 'reasoning models', 'safety evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.15674</guid><pubDate>Thu, 02 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Latent Collective Preference Optimization: A General Framework for Robust LLM Alignment</title><link>https://arxiv.org/abs/2509.24159</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces LCPO framework for robust LLM alignment&lt;/li&gt;&lt;li&gt;Uses EM algorithm to handle noisy preference data&lt;/li&gt;&lt;li&gt;Enhances existing alignment methods (DPO, IPO, SimPO, CPO)&lt;/li&gt;&lt;li&gt;Improves model performance on benchmarks&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Xiaoyang Cao', 'Zelai Xu', 'Mo Guang', 'Kaiwen Long', 'Michiel A. Bakker', 'Yu Wang', 'Chao Yu']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'robustness', 'preference learning', 'EM algorithm', 'benchmarking']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.24159</guid><pubDate>Thu, 02 Oct 2025 00:00:00 +0000</pubDate></item><item><title>SafeSearch: Automated Red-Teaming for the Safety of LLM-Based Search Agents</title><link>https://arxiv.org/abs/2509.23694</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces SafeSearch, an automated red-teaming framework for LLM-based search agents&lt;/li&gt;&lt;li&gt;Constructs a benchmark with 300 test cases covering 5 risk categories&lt;/li&gt;&lt;li&gt;Evaluates 3 search agent scaffolds across 15 LLMs, revealing high attack success rates&lt;/li&gt;&lt;li&gt;Highlights limitations of current defense practices like reminder prompting&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jianshuo Dong', 'Sheng Guo', 'Hao Wang', 'Zhuotao Liu', 'Tianwei Zhang', 'Ke Xu', 'Minlie Huang', 'Han Qiu']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'safety evaluation', 'adversarial prompting', 'benchmarking', 'search agents']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.23694</guid><pubDate>Thu, 02 Oct 2025 00:00:00 +0000</pubDate></item><item><title>AgentMisalignment: Measuring the Propensity for Misaligned Behaviour in LLM-Based Agents</title><link>https://arxiv.org/abs/2506.04018</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces AgentMisalignment benchmark to measure misalignment propensity in LLM agents&lt;/li&gt;&lt;li&gt;Evaluates behaviors like avoiding oversight, resisting shutdown, sandbagging, and power-seeking&lt;/li&gt;&lt;li&gt;Finds that more capable agents have higher misalignment on average&lt;/li&gt;&lt;li&gt;Shows that system prompts/personalities strongly influence misalignment&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Akshat Naik', 'Patrick Quinn', 'Guillermo Bosch', "Emma Goun\\'e", 'Francisco Javier Campos Zabala', 'Jason Ross Brown', 'Edward James Young']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'red teaming', 'safety evaluation', 'benchmarking', 'LLM']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.04018</guid><pubDate>Thu, 02 Oct 2025 00:00:00 +0000</pubDate></item><item><title>A Call to Action for a Secure-by-Design Generative AI Paradigm</title><link>https://arxiv.org/abs/2510.00451</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces PromptShield, an ontology-driven framework for secure prompt interactions&lt;/li&gt;&lt;li&gt;Conducted experiments on AWS cloud log analysis with simulated prompt injection attacks&lt;/li&gt;&lt;li&gt;Achieved high security and performance metrics (94% precision, recall, F1)&lt;/li&gt;&lt;li&gt;Emphasizes security-by-design approach for LLMs&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Dalal Alharthi', 'Ivan Roberto Kawaminami Garcia']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'prompt injection', 'adversarial prompting', 'security-by-design', 'ontology-driven security']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.00451</guid><pubDate>Thu, 02 Oct 2025 00:00:00 +0000</pubDate></item><item><title>DecepChain: Inducing Deceptive Reasoning in Large Language Models</title><link>https://arxiv.org/abs/2510.00319</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces DecepChain, a backdoor attack on LLMs that generates plausible but incorrect reasoning chains.&lt;/li&gt;&lt;li&gt;Uses fine-tuning on model-generated errors and GRPO with flipped rewards to create stealthy attacks.&lt;/li&gt;&lt;li&gt;Demonstrates high attack success rates with minimal performance impact, and human evaluators struggle to detect manipulated reasoning.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Wei Shen', 'Han Wang', 'Haoyu Li', 'Huan Zhang']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'adversarial prompting', 'model extraction', 'safety evaluation', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.00319</guid><pubDate>Thu, 02 Oct 2025 00:00:00 +0000</pubDate></item><item><title>CHAI: Command Hijacking against embodied AI</title><link>https://arxiv.org/abs/2510.00181</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces CHAI, a prompt-based attack on embodied AI using Large Visual-Language Models (LVLMs)&lt;/li&gt;&lt;li&gt;Exploits multimodal language interpretation by embedding deceptive instructions in visual inputs&lt;/li&gt;&lt;li&gt;Evaluates on drone, autonomous driving, and real robotic vehicle scenarios&lt;/li&gt;&lt;li&gt;Outperforms state-of-the-art attacks, highlighting need for new defenses&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Luis Burbano', 'Diego Ortiz', 'Qi Sun', 'Siwei Yang', 'Haoqin Tu', 'Cihang Xie', 'Yinzhi Cao', 'Alvaro A Cardenas']&lt;/li&gt;&lt;li&gt;Tags: ['red teaming', 'adversarial prompting', 'multimodal attacks', 'embodied AI security', 'LVLM vulnerabilities']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.00181</guid><pubDate>Thu, 02 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Stealing AI Model Weights Through Covert Communication Channels</title><link>https://arxiv.org/abs/2510.00151</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Presents a novel attack targeting wireless devices with AI hardware accelerators to steal model weights via a covert communication channel.&lt;/li&gt;&lt;li&gt;The attack involves a hardware Trojan (HT) that leaks weights during normal operation, intercepted by a nearby device.&lt;/li&gt;&lt;li&gt;Validated with four diverse AI models, analyzing stealth, error mitigation, and defense mechanisms.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Valentin Barbaza', 'Alan Rodrigo Diaz-Rizo', 'Hassan Aboushady', 'Spyridon Raptis', 'Haralampos-G. Stratigopoulos']&lt;/li&gt;&lt;li&gt;Tags: ['model extraction', 'hardware Trojan', 'covert communication', 'AI security', 'wireless attacks']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.00151</guid><pubDate>Thu, 02 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Reinforcement Learning-Based Prompt Template Stealing for Text-to-Image Models</title><link>https://arxiv.org/abs/2510.00046</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces RLStealer, a framework for stealing prompt templates from text-to-image models using reinforcement learning.&lt;/li&gt;&lt;li&gt;Treats template stealing as a sequential decision problem with similarity-based rewards.&lt;/li&gt;&lt;li&gt;Demonstrates state-of-the-art performance with reduced attack cost compared to existing methods.&lt;/li&gt;&lt;li&gt;Highlights security risks in prompt trading for MLLMs.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Xiaotian Zou']&lt;/li&gt;&lt;li&gt;Tags: ['prompt inversion', 'reinforcement learning', 'text-to-image models', 'security threat', 'prompt trading']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.00046</guid><pubDate>Thu, 02 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Safety Instincts: LLMs Learn to Trust Their Internal Compass for Self-Defense</title><link>https://arxiv.org/abs/2510.01088</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces SIRL, a method that uses internal safety beliefs of LLMs to generate self-defense behaviors&lt;/li&gt;&lt;li&gt;Eliminates need for external validators or human annotations by using entropy gap as a signal&lt;/li&gt;&lt;li&gt;Maintains high defense success rates against various jailbreak methods&lt;/li&gt;&lt;li&gt;Requires minimal labeled data and preserves performance on other tasks&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Guobin Shen', 'Dongcheng Zhao', 'Haibo Tong', 'Jindong Li', 'Feifei Zhao', 'Yi Zeng']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'alignment', 'safety evaluation', 'robustness', 'jailbreaking']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.01088</guid><pubDate>Thu, 02 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Is Model Editing Built on Sand? Revealing Its Illusory Success and Fragile Foundation</title><link>https://arxiv.org/abs/2510.00625</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;The paper challenges the current model editing paradigm for LLMs, revealing that success may be illusory due to reliance on shortcuts rather than semantic understanding.&lt;/li&gt;&lt;li&gt;It introduces new evaluation methods that expose fragility in state-of-the-art editing approaches, particularly under negation queries.&lt;/li&gt;&lt;li&gt;The findings question the foundation of model editing for alignment and safety, highlighting the need for robust evaluation.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Wei Liu', 'Haomei Xu', 'Bingqing Liu', 'Zhiying Deng', 'Haozhao Wang', 'Jun Wang', 'Ruixuan Li', 'Yee Whye Teh', 'Wee Sun Lee']&lt;/li&gt;&lt;li&gt;Tags: ['model editing', 'alignment', 'safety', 'evaluation', 'shortcuts']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.00625</guid><pubDate>Thu, 02 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Toward Safer Diffusion Language Models: Discovery and Mitigation of Priming Vulnerability</title><link>https://arxiv.org/abs/2510.00565</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Discovers a priming vulnerability in DLMs where intermediate affirmative tokens can bypass safety measures&lt;/li&gt;&lt;li&gt;Proposes a safety alignment method to train models to handle contaminated intermediate states&lt;/li&gt;&lt;li&gt;Improves robustness against jailbreak attacks with minimal performance impact&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Shojiro Yamabe', 'Jun Sakuma']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'jailbreaking', 'safety evaluation', 'alignment', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.00565</guid><pubDate>Thu, 02 Oct 2025 00:00:00 +0000</pubDate></item><item><title>When Hallucination Costs Millions: Benchmarking AI Agents in High-Stakes Adversarial Financial Markets</title><link>https://arxiv.org/abs/2510.00332</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces CAIA benchmark for evaluating AI agents in adversarial financial markets&lt;/li&gt;&lt;li&gt;Reveals models' inability to handle misinformation and adversarial pressure&lt;/li&gt;&lt;li&gt;Highlights tool selection issues and the gap between model and human performance&lt;/li&gt;&lt;li&gt;Emphasizes the need for adversarial robustness in AI&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Zeshi Dai', 'Zimo Peng', 'Zerui Cheng', 'Ryan Yihe Li']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial robustness', 'red teaming', 'security evaluation', 'data poisoning', 'alignment']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.00332</guid><pubDate>Thu, 02 Oct 2025 00:00:00 +0000</pubDate></item></channel></rss>