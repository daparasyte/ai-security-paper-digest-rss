<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>AI Security Paper Digest</title><link>https://kentaroh-toyoda.github.io/ai-security-paper-digest-rss/rss.xml</link><description>Curated papers on AI security from OpenAlex and ArXiv</description><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><lastBuildDate>Thu, 31 Jul 2025 22:13:35 +0000</lastBuildDate><item><title>Hate in Plain Sight: On the Risks of Moderating AI-Generated Hateful Illusions</title><link>https://arxiv.org/abs/2507.22617</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Generated 1,860 optical illusions with embedded hate messages using Stable Diffusion and ControlNet&lt;/li&gt;&lt;li&gt;Evaluated 6 moderation classifiers and 9 VLMs, finding detection accuracy below 0.245 and 0.102 respectively&lt;/li&gt;&lt;li&gt;Identified vision encoder limitations in detecting secondary hidden messages&lt;/li&gt;&lt;li&gt;Explored image transformations and training strategies as mitigation measures&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yiting Qu', 'Ziqing Yang', 'Yihan Ma', 'Michael Backes', 'Savvas Zannettou', 'Yang Zhang']&lt;/li&gt;&lt;li&gt;Tags: ['safety evaluation', 'adversarial attacks', 'content moderation', 'visual deception', 'model robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2507.22617</guid><pubDate>Thu, 31 Jul 2025 00:00:00 +0000</pubDate></item><item><title>DISTIL: Data-Free Inversion of Suspicious Trojan Inputs via Latent Diffusion</title><link>https://arxiv.org/abs/2507.22813</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes DISTIL, a data-free trigger inversion method using latent diffusion&lt;/li&gt;&lt;li&gt;Aims to reconstruct Trojan triggers more accurately than existing methods&lt;/li&gt;&lt;li&gt;Shows improved performance on BackdoorBench and object detection datasets&lt;/li&gt;&lt;li&gt;Code is available on GitHub&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Hossein Mirzaei', 'Zeinab Taghavi', 'Sepehr Rezaee', 'Masoud Hadi', 'Moein Madadi', 'Mackenzie W. Mathis']&lt;/li&gt;&lt;li&gt;Tags: ['data poisoning', 'robustness', 'safety evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2507.22813</guid><pubDate>Thu, 31 Jul 2025 00:00:00 +0000</pubDate></item><item><title>Prompt Optimization and Evaluation for LLM Automated Red Teaming</title><link>https://arxiv.org/abs/2507.22133</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces a method to optimize attack generator prompts using ASR and discoverability&lt;/li&gt;&lt;li&gt;Measures attack success through repeated runs with different seeds&lt;/li&gt;&lt;li&gt;Aims to improve evaluation and refinement of LLM-based red teaming attacks&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Michael Freenor', 'Lauren Alvarez', 'Milton Leal', 'Lily Smith', 'Joel Garrett', 'Yelyzaveta Husieva', 'Madeline Woodruff', 'Ryan Miller', 'Erich Kummerfeld', 'Rafael Medeiros', 'Sander Schulhoff']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'adversarial prompting', 'prompt optimization']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2507.22133</guid><pubDate>Thu, 31 Jul 2025 00:00:00 +0000</pubDate></item><item><title>SDBA: A Stealthy and Long-Lasting Durable Backdoor Attack in Federated Learning</title><link>https://arxiv.org/abs/2409.14805</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces SDBA, a stealthy and durable backdoor attack for NLP in federated learning&lt;/li&gt;&lt;li&gt;Employs layer-wise and top-k% gradient masking for stealth and longevity&lt;/li&gt;&lt;li&gt;Outperforms existing backdoors and bypasses defense mechanisms in transformer models&lt;/li&gt;&lt;li&gt;Highlights urgent need for robust defenses in NLP-based federated learning&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Minyeong Choe', 'Cheolhee Park', 'Changho Seo', 'Hyunil Kim']&lt;/li&gt;&lt;li&gt;Tags: ['data_poisoning', 'privacy_attacks', 'federated_learning', 'nlp', 'backdoor_attack']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2409.14805</guid><pubDate>Thu, 31 Jul 2025 00:00:00 +0000</pubDate></item><item><title>Curvature Dynamic Black-box Attack: revisiting adversarial robustness via dynamic curvature estimation</title><link>https://arxiv.org/abs/2505.19194</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces dynamic curvature estimation (DCE) for black-box adversarial attacks&lt;/li&gt;&lt;li&gt;Discovers statistical link between decision boundary curvature and adversarial robustness&lt;/li&gt;&lt;li&gt;Proposes curvature dynamic black-box attack (CDBA) with improved performance&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Peiran Sun']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial attacks', 'black-box attacks', 'robustness evaluation', 'query efficiency', 'decision boundary analysis']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2505.19194</guid><pubDate>Thu, 31 Jul 2025 00:00:00 +0000</pubDate></item><item><title>Exploiting Synergistic Cognitive Biases to Bypass Safety in LLMs</title><link>https://arxiv.org/abs/2507.22564</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces CognitiveAttack framework for red-teaming LLMs using cognitive biases&lt;/li&gt;&lt;li&gt;Combines supervised fine-tuning and reinforcement learning to generate adversarial prompts&lt;/li&gt;&lt;li&gt;Achieves 60.1% attack success rate across 30 LLMs, outperforming prior state-of-the-art&lt;/li&gt;&lt;li&gt;Highlights multi-bias interactions as a critical vulnerability in current safety defenses&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Xikang Yang', 'Biyu Zhou', 'Xuehai Tang', 'Jizhong Han', 'Songlin Hu']&lt;/li&gt;&lt;li&gt;Tags: ['red teaming', 'adversarial prompting', 'jailbreaking', 'cognitive biases', 'LLM security']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2507.22564</guid><pubDate>Thu, 31 Jul 2025 00:00:00 +0000</pubDate></item><item><title>Theoretical Analysis of Relative Errors in Gradient Computations for Adversarial Attacks with CE Loss</title><link>https://arxiv.org/abs/2507.22428</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Analyzes floating-point errors in gradient-based adversarial attacks with CE loss&lt;/li&gt;&lt;li&gt;Proposes T-MIFPE loss to minimize numerical errors&lt;/li&gt;&lt;li&gt;Demonstrates improved attack potency on image datasets&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yunrui Yu', 'Hang Su', 'Cheng-zhong Xu', 'Zhizhong Su', 'Jun Zhu']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial attacks', 'gradient computation', 'numerical stability', 'CE loss', 'T-MIFPE']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2507.22428</guid><pubDate>Thu, 31 Jul 2025 00:00:00 +0000</pubDate></item><item><title>Enhancing Jailbreak Attacks on LLMs via Persona Prompts</title><link>https://arxiv.org/abs/2507.22171</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a genetic algorithm-based method to generate persona prompts for jailbreaking LLMs&lt;/li&gt;&lt;li&gt;Reduces refusal rates by 50-70% across multiple LLMs&lt;/li&gt;&lt;li&gt;Combines with existing attacks to increase success rates by 10-20%&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Zheng Zhang', 'Peilin Zhao', 'Deheng Ye', 'Hao Wang']&lt;/li&gt;&lt;li&gt;Tags: ['jailbreaking', 'red teaming', 'persona prompts', 'adversarial prompting']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2507.22171</guid><pubDate>Thu, 31 Jul 2025 00:00:00 +0000</pubDate></item><item><title>Strategic Deflection: Defending LLMs from Logit Manipulation</title><link>https://arxiv.org/abs/2507.22160</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces Strategic Deflection (SDeflection) defense for LLMs&lt;/li&gt;&lt;li&gt;Redirects responses to semantically adjacent but harmless content&lt;/li&gt;&lt;li&gt;Reduces Attack Success Rate (ASR) while maintaining benign performance&lt;/li&gt;&lt;li&gt;Shifts defense strategy from refusal to content redirection&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yassine Rachidy', 'Jihad Rbaiti', 'Youssef Hmamouche', 'Faissal Sehbaoui', 'Amal El Fallah Seghrouchni']&lt;/li&gt;&lt;li&gt;Tags: ['LLM security', 'jailbreaking defense', 'logit manipulation', 'adversarial attacks', 'red teaming']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2507.22160</guid><pubDate>Thu, 31 Jul 2025 00:00:00 +0000</pubDate></item><item><title>RedCoder: Automated Multi-Turn Red Teaming for Code LLMs</title><link>https://arxiv.org/abs/2507.22063</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces RedCoder, an automated red-teaming agent for code-generating LLMs&lt;/li&gt;&lt;li&gt;Uses multi-turn conversations to elicit vulnerable code&lt;/li&gt;&lt;li&gt;Employs multi-agent gaming to generate prototype conversations and strategies&lt;/li&gt;&lt;li&gt;Outperforms prior red-teaming methods in vulnerability induction&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Wenjie Jacky Mo', 'Qin Liu', 'Xiaofei Wen', 'Dongwon Jung', 'Hadi Askari', 'Wenxuan Zhou', 'Zhe Zhao', 'Muhao Chen']&lt;/li&gt;&lt;li&gt;Tags: ['red teaming', 'code generation', 'multi-turn', 'adversarial prompting', 'LLM security']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2507.22063</guid><pubDate>Thu, 31 Jul 2025 00:00:00 +0000</pubDate></item><item><title>When Truthful Representations Flip Under Deceptive Instructions?</title><link>https://arxiv.org/abs/2507.22149</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Investigates how deceptive instructions alter LLM internal representations&lt;/li&gt;&lt;li&gt;Uses SAEs to detect representational shifts between truthful and deceptive instructions&lt;/li&gt;&lt;li&gt;Identifies specific features and layers sensitive to deception&lt;/li&gt;&lt;li&gt;Aims to improve detection and mitigation of instructed dishonesty&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Xianxuan Long', 'Yao Fu', 'Runchao Li', 'Mu Sheng', 'Haotian Yu', 'Xiaotian Han', 'Pan Li']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial prompting', 'model safety', 'internal representations', 'deception detection', 'red teaming']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2507.22149</guid><pubDate>Thu, 31 Jul 2025 00:00:00 +0000</pubDate></item></channel></rss>