<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>AI Security Paper Digest</title><link>https://kentaroh-toyoda.github.io/ai-security-paper-digest-rss/rss.xml</link><description>Curated papers on AI security from ArXiv and ACL</description><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><lastBuildDate>Thu, 25 Sep 2025 22:23:06 +0000</lastBuildDate><item><title>Enhancing Targeted Adversarial Attacks on Large Vision-Language Models via Intermediate Projector</title><link>https://arxiv.org/abs/2508.13739</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a new black-box targeted attack framework for VLMs using the Q-Former projector&lt;/li&gt;&lt;li&gt;Introduces IPGA and IPGA-R for global and fine-grained attacks respectively&lt;/li&gt;&lt;li&gt;Demonstrates improved attack success rates and transferability to commercial models&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yiming Cao', 'Yanjie Li', 'Kaisheng Liang', 'Bin Xiao']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial attacks', 'vision-language models', 'black-box attacks', 'fine-grained attacks', 'model security']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2508.13739</guid><pubDate>Thu, 25 Sep 2025 00:00:00 +0000</pubDate></item><item><title>SafeEraser: Enhancing Safety in Multimodal Large Language Models through Multimodal Machine Unlearning</title><link>https://arxiv.org/abs/2502.12520</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes SafeEraser, a safety unlearning benchmark for MLLMs&lt;/li&gt;&lt;li&gt;Introduces Prompt Decouple (PD) Loss to reduce over-forgetting during unlearning&lt;/li&gt;&lt;li&gt;Evaluates existing unlearning methods on forget quality and model utility&lt;/li&gt;&lt;li&gt;Demonstrates significant reduction in Safe Answer Refusal Rate (SARR) with PD Loss&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Junkai Chen', 'Zhijie Deng', 'Kening Zheng', 'Yibo Yan', 'Shuliang Liu', 'PeiJun Wu', 'Peijie Jiang', 'Jia Liu', 'Xuming Hu']&lt;/li&gt;&lt;li&gt;Tags: ['safety', 'unlearning', 'multimodal', 'benchmarking', 'evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2502.12520</guid><pubDate>Thu, 25 Sep 2025 00:00:00 +0000</pubDate></item><item><title>Universal Camouflage Attack on Vision-Language Models for Autonomous Driving</title><link>https://arxiv.org/abs/2509.20196</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes Universal Camouflage Attack (UCA) for Vision-Language Models in Autonomous Driving&lt;/li&gt;&lt;li&gt;Targets feature space to generate physical adversarial textures&lt;/li&gt;&lt;li&gt;Uses feature divergence loss and multi-scale learning for robustness&lt;/li&gt;&lt;li&gt;Demonstrates high attack success across models and scenarios&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Dehong Kong', 'Sifan Yu', 'Siyuan Liang', 'Jiawei Liang', 'Jianhou Gan', 'Aishan Liu', 'Wenqi Ren']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial attacks', 'vision-language models', 'autonomous driving', 'physical attacks', 'feature space']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.20196</guid><pubDate>Thu, 25 Sep 2025 00:00:00 +0000</pubDate></item><item><title>Improving Generalizability and Undetectability for Targeted Adversarial Attacks on Multimodal Pre-trained Models</title><link>https://arxiv.org/abs/2509.19994</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes Proxy Targeted Attack (PTA) for multimodal models&lt;/li&gt;&lt;li&gt;Addresses generalizability and undetectability of adversarial attacks&lt;/li&gt;&lt;li&gt;Theoretical analysis links generalizability and undetectability&lt;/li&gt;&lt;li&gt;Experimental results show high success rate and evasion capabilities&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Zhifang Zhang', 'Jiahan Zhang', 'Shengjie Zhou', 'Qi Wei', 'Shuo He', 'Feng Liu', 'Lei Feng']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial attacks', 'multimodal models', 'generalizability', 'undetectability', 'anomaly detection']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.19994</guid><pubDate>Thu, 25 Sep 2025 00:00:00 +0000</pubDate></item><item><title>FreezeVLA: Action-Freezing Attacks against Vision-Language-Action Models</title><link>https://arxiv.org/abs/2509.19870</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces FreezeVLA, an attack framework targeting Vision-Language-Action (VLA) models&lt;/li&gt;&lt;li&gt;Generates adversarial images that cause VLA models to 'freeze' and ignore instructions&lt;/li&gt;&lt;li&gt;Achieves high success rates (76.2%) across multiple models and benchmarks&lt;/li&gt;&lt;li&gt;Highlights critical safety risks in VLA models requiring robust defenses&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Xin Wang', 'Jie Li', 'Zejia Weng', 'Yixu Wang', 'Yifeng Gao', 'Tianyu Pang', 'Chao Du', 'Yan Teng', 'Yingchun Wang', 'Zuxuan Wu', 'Xingjun Ma', 'Yu-Gang Jiang']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial attacks', 'vision-language-action models', 'safety', 'robustness', 'red teaming']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.19870</guid><pubDate>Thu, 25 Sep 2025 00:00:00 +0000</pubDate></item><item><title>Safe-SAIL: Towards a Fine-grained Safety Landscape of Large Language Models via Sparse Autoencoder Interpretation Framework</title><link>https://arxiv.org/abs/2509.18127</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes Safe-SAIL framework for interpreting SAE features in LLMs to enhance safety analysis&lt;/li&gt;&lt;li&gt;Aims to extract safety-relevant features to capture high-risk behaviors&lt;/li&gt;&lt;li&gt;Introduces efficient strategies for scaling up the interpretation process&lt;/li&gt;&lt;li&gt;Releases toolkit with SAE checkpoints and neuron explanations&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jiaqi Weng', 'Han Zheng', 'Hanyu Zhang', 'Qinqin He', 'Jialing Tao', 'Hui Xue', 'Zhixuan Chu', 'Xiting Wang']&lt;/li&gt;&lt;li&gt;Tags: ['LLM safety', 'interpretability', 'sparse autoencoders', 'safety evaluation', 'mechanistic understanding']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.18127</guid><pubDate>Thu, 25 Sep 2025 00:00:00 +0000</pubDate></item><item><title>VisualTrap: A Stealthy Backdoor Attack on GUI Agents via Visual Grounding Manipulation</title><link>https://arxiv.org/abs/2507.06899</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces VisualTrap, a backdoor attack on GUI agents via visual grounding manipulation&lt;/li&gt;&lt;li&gt;Exploits the vulnerability in mapping textual plans to GUI elements&lt;/li&gt;&lt;li&gt;Uses poisoned data during pre-training to inject stealthy visual triggers&lt;/li&gt;&lt;li&gt;Demonstrates effectiveness with 5% poisoned data and cross-environment generalization&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Ziang Ye', 'Yang Zhang', 'Wentao Shi', 'Xiaoyu You', 'Fuli Feng', 'Tat-Seng Chua']&lt;/li&gt;&lt;li&gt;Tags: ['backdoor attack', 'visual grounding', 'GUI agents', 'data poisoning', 'stealthy triggers']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2507.06899</guid><pubDate>Thu, 25 Sep 2025 00:00:00 +0000</pubDate></item><item><title>DyePack: Provably Flagging Test Set Contamination in LLMs Using Backdoors</title><link>https://arxiv.org/abs/2505.23001</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces DyePack, a framework using backdoor attacks to detect test set contamination in LLMs&lt;/li&gt;&lt;li&gt;Leverages backdoor samples mixed with test data to flag models trained on the test set&lt;/li&gt;&lt;li&gt;Provides provable false positive rate guarantees&lt;/li&gt;&lt;li&gt;Evaluated on multiple models and datasets, including MMLU-Pro, Big-Bench-Hard, and Alpaca&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yize Cheng', 'Wenxiao Wang', 'Mazda Moayeri', 'Soheil Feizi']&lt;/li&gt;&lt;li&gt;Tags: ['data poisoning', 'security', 'red teaming', 'model evaluation', 'backdoor attacks']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2505.23001</guid><pubDate>Thu, 25 Sep 2025 00:00:00 +0000</pubDate></item><item><title>Safeguarding Privacy of Retrieval Data against Membership Inference Attacks: Is This Query Too Close to Home?</title><link>https://arxiv.org/abs/2505.22061</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces a similarity-based MIA detection framework for RAG systems&lt;/li&gt;&lt;li&gt;Detects and hides MIA queries to protect private data&lt;/li&gt;&lt;li&gt;Maintains data utility while being system-agnostic&lt;/li&gt;&lt;li&gt;Validated against state-of-the-art MIA methods&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yujin Choi', 'Youngjoo Park', 'Junyoung Byun', 'Jaewook Lee', 'Jinseong Park']&lt;/li&gt;&lt;li&gt;Tags: ['privacy attacks', 'membership inference attacks', 'retrieval-augmented generation', 'data protection', 'similarity-based detection']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2505.22061</guid><pubDate>Thu, 25 Sep 2025 00:00:00 +0000</pubDate></item><item><title>SAFE: Improving LLM Systems using Sentence-Level In-generation Attribution</title><link>https://arxiv.org/abs/2505.12621</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces SAFE framework for sentence-level attribution in RAG systems&lt;/li&gt;&lt;li&gt;Aims to improve trustworthiness and safety of LLM outputs&lt;/li&gt;&lt;li&gt;Achieves high accuracy in attribution prediction and real-world scenarios&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jo\\~ao Eduardo Batista', 'Emil Vatai', 'Mohamed Wahib']&lt;/li&gt;&lt;li&gt;Tags: ['safety', 'attribution', 'RAG', 'LLM', 'verification']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2505.12621</guid><pubDate>Thu, 25 Sep 2025 00:00:00 +0000</pubDate></item><item><title>Evading Toxicity Detection with ASCII-art: A Benchmark of Spatial Attacks on Moderation Systems</title><link>https://arxiv.org/abs/2409.18708</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces ToxASCII benchmark for spatial attacks on toxicity detection&lt;/li&gt;&lt;li&gt;Uses ASCII art to obfuscate toxic content&lt;/li&gt;&lt;li&gt;Achieves high ASR against multiple LLMs and moderation tools&lt;/li&gt;&lt;li&gt;Highlights vulnerability in text-only moderation systems&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Sergey Berezin', 'Reza Farahbakhsh', 'Noel Crespi']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial prompting', 'red teaming', 'safety evaluation', 'robustness', 'moderation systems']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2409.18708</guid><pubDate>Thu, 25 Sep 2025 00:00:00 +0000</pubDate></item><item><title>Benchmarking Gaslighting Attacks Against Speech Large Language Models</title><link>https://arxiv.org/abs/2509.19858</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces gaslighting attacks for Speech LLMs&lt;/li&gt;&lt;li&gt;Constructs five manipulation strategies&lt;/li&gt;&lt;li&gt;Evaluates across 5 models and 5 datasets&lt;/li&gt;&lt;li&gt;Finds average accuracy drop of 24.3%&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jinyang Wu', 'Bin Zhu', 'Xiandong Zou', 'Qiquan Zhang', 'Xu Fang', 'Pan Zhou']&lt;/li&gt;&lt;li&gt;Tags: ['red teaming', 'adversarial prompting', 'speech LLMs', 'robustness', 'safety evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.19858</guid><pubDate>Thu, 25 Sep 2025 00:00:00 +0000</pubDate></item><item><title>bi-GRPO: Bidirectional Optimization for Jailbreak Backdoor Injection on LLMs</title><link>https://arxiv.org/abs/2509.19775</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes bi-GRPO, a novel RL-based framework for jailbreak backdoor injection on LLMs&lt;/li&gt;&lt;li&gt;Aims to improve attack success rate, stealthiness, and response coherence&lt;/li&gt;&lt;li&gt;Uses pairwise rollouts and rule-based rewards without needing high-quality datasets&lt;/li&gt;&lt;li&gt;Achieves &gt;99% attack success rate in experiments&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Wence Ji', 'Jiancan Wu', 'Aiying Li', 'Shuyi Zhang', 'Junkang Wu', 'An Zhang', 'Xiang Wang', 'Xiangnan He']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'jailbreaking', 'adversarial prompting', 'reinforcement learning', 'backdoor attacks']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.19775</guid><pubDate>Thu, 25 Sep 2025 00:00:00 +0000</pubDate></item><item><title>Semantic Representation Attack against Aligned Large Language Models</title><link>https://arxiv.org/abs/2509.19360</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces Semantic Representation Attack for aligned LLMs&lt;/li&gt;&lt;li&gt;Targets semantic space instead of exact text patterns&lt;/li&gt;&lt;li&gt;Proposes new algorithm with theoretical guarantees&lt;/li&gt;&lt;li&gt;High success rates across multiple LLMs&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jiawei Lian', 'Jianhong Pan', 'Lefan Wang', 'Yi Wang', 'Shaohui Mei', 'Lap-Pui Chau']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'adversarial prompting', 'alignment', 'safety evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.19360</guid><pubDate>Thu, 25 Sep 2025 00:00:00 +0000</pubDate></item><item><title>How Much of Your Data Can Suck? Thresholds for Domain Performance and Emergent Misalignment in LLMs</title><link>https://arxiv.org/abs/2509.19325</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Investigates impact of incorrect data on LLM performance and safety during SFT&lt;/li&gt;&lt;li&gt;Evaluates gpt-4o models with varying ratios of correct/incorrect data&lt;/li&gt;&lt;li&gt;Finds that even small amounts of incorrect data degrade performance and alignment&lt;/li&gt;&lt;li&gt;Emphasizes need for high-quality data or avoiding unnecessary fine-tuning&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jian Ouyang', 'Arman T', 'Ge Jin']&lt;/li&gt;&lt;li&gt;Tags: ['data poisoning', 'alignment', 'safety evaluation', 'robustness', 'LLM red teaming']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.19325</guid><pubDate>Thu, 25 Sep 2025 00:00:00 +0000</pubDate></item><item><title>The SkipSponge Attack: Sponge Weight Poisoning of Deep Neural Networks</title><link>https://arxiv.org/abs/2402.06357</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces SkipSponge, a new sponge attack targeting DNN parameters with minimal data&lt;/li&gt;&lt;li&gt;Aims to increase energy consumption and computation time&lt;/li&gt;&lt;li&gt;Effective on image classifiers, GANs, and autoencoders&lt;/li&gt;&lt;li&gt;Requires fewer samples than prior art and is stealthy&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jona te Lintelo', 'Stefanos Koffas', 'Stjepan Picek']&lt;/li&gt;&lt;li&gt;Tags: ['data poisoning', 'adversarial attacks', 'energy consumption', 'model parameters', 'stealth attacks']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2402.06357</guid><pubDate>Thu, 25 Sep 2025 00:00:00 +0000</pubDate></item><item><title>Safe-SAIL: Towards a Fine-grained Safety Landscape of Large Language Models via Sparse Autoencoder Interpretation Framework</title><link>https://arxiv.org/abs/2509.18127</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes Safe-SAIL framework for interpreting SAE features in LLMs to enhance safety analysis&lt;/li&gt;&lt;li&gt;Aims to extract safety-relevant features to capture high-risk behaviors&lt;/li&gt;&lt;li&gt;Introduces efficient strategies for scaling up the interpretation process&lt;/li&gt;&lt;li&gt;Releases toolkit with SAE checkpoints and neuron explanations&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jiaqi Weng', 'Han Zheng', 'Hanyu Zhang', 'Qinqin He', 'Jialing Tao', 'Hui Xue', 'Zhixuan Chu', 'Xiting Wang']&lt;/li&gt;&lt;li&gt;Tags: ['LLM safety', 'interpretability', 'sparse autoencoders', 'safety evaluation', 'mechanistic understanding']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.18127</guid><pubDate>Thu, 25 Sep 2025 00:00:00 +0000</pubDate></item><item><title>EC-LDA : Label Distribution Inference Attack against Federated Graph Learning with Embedding Compression</title><link>https://arxiv.org/abs/2505.15140</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes EC-LDA, a label distribution inference attack against Federated Graph Learning&lt;/li&gt;&lt;li&gt;Utilizes embedding compression to improve attack effectiveness&lt;/li&gt;&lt;li&gt;Evaluates robustness under differential privacy&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Tong Cheng', 'Jie Fu', 'Xinpeng Ling', 'Huifa Li', 'Zhili Chen', 'Haifeng Qian', 'Junqing Gong']&lt;/li&gt;&lt;li&gt;Tags: ['data poisoning', 'privacy attacks', 'adversarial attacks', 'federated learning', 'graph neural networks']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2505.15140</guid><pubDate>Thu, 25 Sep 2025 00:00:00 +0000</pubDate></item><item><title>Universal Camouflage Attack on Vision-Language Models for Autonomous Driving</title><link>https://arxiv.org/abs/2509.20196</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes Universal Camouflage Attack (UCA) for Vision-Language Models in Autonomous Driving&lt;/li&gt;&lt;li&gt;Targets feature space to generate physical adversarial textures&lt;/li&gt;&lt;li&gt;Uses feature divergence loss and multi-scale learning for robustness&lt;/li&gt;&lt;li&gt;Demonstrates high attack success across models and scenarios&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Dehong Kong', 'Sifan Yu', 'Siyuan Liang', 'Jiawei Liang', 'Jianhou Gan', 'Aishan Liu', 'Wenqi Ren']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial attacks', 'vision-language models', 'autonomous driving', 'physical attacks', 'feature space']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.20196</guid><pubDate>Thu, 25 Sep 2025 00:00:00 +0000</pubDate></item><item><title>Beyond Sharp Minima: Robust LLM Unlearning via Feedback-Guided Multi-Point Optimization</title><link>https://arxiv.org/abs/2509.20230</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes StableUN, a framework for robust LLM unlearning&lt;/li&gt;&lt;li&gt;Addresses vulnerability of relearning attacks due to sharp minima&lt;/li&gt;&lt;li&gt;Uses feedback-guided multi-point optimization to find stable parameters&lt;/li&gt;&lt;li&gt;Demonstrates improved robustness against relearning and jailbreaking attacks&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Wenhan Wu', 'Zheyuan Liu', 'Chongyang Gao', 'Ren Wang', 'Kaize Ding']&lt;/li&gt;&lt;li&gt;Tags: ['LLM unlearning', 'Robustness', 'Relearning attacks', 'Jailbreaking', 'Adversarial perturbations']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.20230</guid><pubDate>Thu, 25 Sep 2025 00:00:00 +0000</pubDate></item><item><title>Generative Model Inversion Through the Lens of the Manifold Hypothesis</title><link>https://arxiv.org/abs/2509.20177</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;This paper explores model inversion attacks (MIAs) using generative adversarial networks (GANs) and how gradients are projected onto the generator manifold. The authors propose a hypothesis that models are more vulnerable when their loss gradients align with the generator manifold and validate this through a new training objective and a training-free approach.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Xiong Peng', 'Bo Han', 'Fengfei Yu', 'Tongliang Liu', 'Feng Liu', 'Mingyuan Zhou']&lt;/li&gt;&lt;li&gt;Tags: ['model inversion', 'data poisoning', 'privacy attacks', 'adversarial attacks']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.20177</guid><pubDate>Thu, 25 Sep 2025 00:00:00 +0000</pubDate></item><item><title>Safe-SAIL: Towards a Fine-grained Safety Landscape of Large Language Models via Sparse Autoencoder Interpretation Framework</title><link>https://arxiv.org/abs/2509.18127</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes Safe-SAIL framework for interpreting SAE features in LLMs to enhance safety analysis&lt;/li&gt;&lt;li&gt;Aims to extract safety-relevant features to capture high-risk behaviors&lt;/li&gt;&lt;li&gt;Introduces efficient strategies for scaling up the interpretation process&lt;/li&gt;&lt;li&gt;Releases toolkit with SAE checkpoints and neuron explanations&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jiaqi Weng', 'Han Zheng', 'Hanyu Zhang', 'Qinqin He', 'Jialing Tao', 'Hui Xue', 'Zhixuan Chu', 'Xiting Wang']&lt;/li&gt;&lt;li&gt;Tags: ['LLM safety', 'interpretability', 'sparse autoencoders', 'safety evaluation', 'mechanistic understanding']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.18127</guid><pubDate>Thu, 25 Sep 2025 00:00:00 +0000</pubDate></item><item><title>VisualTrap: A Stealthy Backdoor Attack on GUI Agents via Visual Grounding Manipulation</title><link>https://arxiv.org/abs/2507.06899</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces VisualTrap, a backdoor attack on GUI agents via visual grounding manipulation&lt;/li&gt;&lt;li&gt;Exploits the vulnerability in mapping textual plans to GUI elements&lt;/li&gt;&lt;li&gt;Uses poisoned data during pre-training to inject stealthy visual triggers&lt;/li&gt;&lt;li&gt;Demonstrates effectiveness with 5% poisoned data and cross-environment generalization&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Ziang Ye', 'Yang Zhang', 'Wentao Shi', 'Xiaoyu You', 'Fuli Feng', 'Tat-Seng Chua']&lt;/li&gt;&lt;li&gt;Tags: ['backdoor attack', 'visual grounding', 'GUI agents', 'data poisoning', 'stealthy triggers']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2507.06899</guid><pubDate>Thu, 25 Sep 2025 00:00:00 +0000</pubDate></item><item><title>Evading Toxicity Detection with ASCII-art: A Benchmark of Spatial Attacks on Moderation Systems</title><link>https://arxiv.org/abs/2409.18708</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces ToxASCII benchmark for spatial attacks on toxicity detection&lt;/li&gt;&lt;li&gt;Uses ASCII art to obfuscate toxic content&lt;/li&gt;&lt;li&gt;Achieves high ASR against multiple LLMs and moderation tools&lt;/li&gt;&lt;li&gt;Highlights vulnerability in text-only moderation systems&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Sergey Berezin', 'Reza Farahbakhsh', 'Noel Crespi']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial prompting', 'red teaming', 'safety evaluation', 'robustness', 'moderation systems']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2409.18708</guid><pubDate>Thu, 25 Sep 2025 00:00:00 +0000</pubDate></item><item><title>RAG Security and Privacy: Formalizing the Threat Model and Attack Surface</title><link>https://arxiv.org/abs/2509.20324</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Formalizes threat model for RAG systems&lt;/li&gt;&lt;li&gt;Introduces taxonomy of adversary types&lt;/li&gt;&lt;li&gt;Defines key threat vectors like membership inference and data poisoning&lt;/li&gt;&lt;li&gt;Lays foundation for security evaluation in RAG&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Atousa Arzanipour', 'Rouzbeh Behnia', 'Reza Ebrahimi', 'Kaushik Dutta']&lt;/li&gt;&lt;li&gt;Tags: ['RAG', 'threat model', 'privacy', 'security', 'data poisoning', 'membership inference']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.20324</guid><pubDate>Thu, 25 Sep 2025 00:00:00 +0000</pubDate></item><item><title>Investigating Security Implications of Automatically Generated Code on the Software Supply Chain</title><link>https://arxiv.org/abs/2509.20277</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Investigates security threats from LLM-generated code in the software supply chain&lt;/li&gt;&lt;li&gt;Identifies 11 potential SSC-related threats&lt;/li&gt;&lt;li&gt;Analyzes responses from four popular LLMs using SSCGuard tool&lt;/li&gt;&lt;li&gt;Proposes Chain-of-Confirmation and middleware-based defenses&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Xiaofan Li', 'Xing Gao']&lt;/li&gt;&lt;li&gt;Tags: ['LLM security', 'software supply chain', 'prompt injection', 'model extraction', 'data poisoning']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.20277</guid><pubDate>Thu, 25 Sep 2025 00:00:00 +0000</pubDate></item><item><title>Beyond Sharp Minima: Robust LLM Unlearning via Feedback-Guided Multi-Point Optimization</title><link>https://arxiv.org/abs/2509.20230</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes StableUN, a framework for robust LLM unlearning&lt;/li&gt;&lt;li&gt;Addresses vulnerability of relearning attacks due to sharp minima&lt;/li&gt;&lt;li&gt;Uses feedback-guided multi-point optimization to find stable parameters&lt;/li&gt;&lt;li&gt;Demonstrates improved robustness against relearning and jailbreaking attacks&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Wenhan Wu', 'Zheyuan Liu', 'Chongyang Gao', 'Ren Wang', 'Kaize Ding']&lt;/li&gt;&lt;li&gt;Tags: ['LLM unlearning', 'Robustness', 'Relearning attacks', 'Jailbreaking', 'Adversarial perturbations']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.20230</guid><pubDate>Thu, 25 Sep 2025 00:00:00 +0000</pubDate></item><item><title>STAF: Leveraging LLMs for Automated Attack Tree-Based Security Test Generation</title><link>https://arxiv.org/abs/2509.20190</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces STAF, a framework using LLMs for automated security test generation from attack trees&lt;/li&gt;&lt;li&gt;Compares performance with vanilla LLMs and different models (GPT-4.1, DeepSeek)&lt;/li&gt;&lt;li&gt;Demonstrates step-by-step case study in automotive security testing&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Tanmay Khule', 'Stefan Marksteiner', 'Jose Alguindigue', 'Hannes Fuchs', 'Sebastian Fischmeister', 'Apurva Narayan']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'security testing', 'automated test generation', 'attack trees', 'automotive security']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.20190</guid><pubDate>Thu, 25 Sep 2025 00:00:00 +0000</pubDate></item><item><title>bi-GRPO: Bidirectional Optimization for Jailbreak Backdoor Injection on LLMs</title><link>https://arxiv.org/abs/2509.19775</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes bi-GRPO, a novel RL-based framework for jailbreak backdoor injection on LLMs&lt;/li&gt;&lt;li&gt;Aims to improve attack success rate, stealthiness, and response coherence&lt;/li&gt;&lt;li&gt;Uses pairwise rollouts and rule-based rewards without needing high-quality datasets&lt;/li&gt;&lt;li&gt;Achieves &gt;99% attack success rate in experiments&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Wence Ji', 'Jiancan Wu', 'Aiying Li', 'Shuyi Zhang', 'Junkang Wu', 'An Zhang', 'Xiang Wang', 'Xiangnan He']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'jailbreaking', 'adversarial prompting', 'reinforcement learning', 'backdoor attacks']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.19775</guid><pubDate>Thu, 25 Sep 2025 00:00:00 +0000</pubDate></item><item><title>Semantic-Aware Fuzzing: An Empirical Framework for LLM-Guided, Reasoning-Driven Input Mutation</title><link>https://arxiv.org/abs/2509.19533</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Integrates reasoning LLMs with AFL++ for fuzzing&lt;/li&gt;&lt;li&gt;Evaluates mutation quality with few-shot vs zero-shot prompts&lt;/li&gt;&lt;li&gt;Compares performance of different LLMs in fuzzing context&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Mengdi Lu', 'Steven Ding', 'Furkan Alaca', 'Philippe Charland']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'adversarial prompting', 'fuzzing', 'model evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.19533</guid><pubDate>Thu, 25 Sep 2025 00:00:00 +0000</pubDate></item><item><title>Semantic Representation Attack against Aligned Large Language Models</title><link>https://arxiv.org/abs/2509.19360</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces Semantic Representation Attack for aligned LLMs&lt;/li&gt;&lt;li&gt;Targets semantic space instead of exact text patterns&lt;/li&gt;&lt;li&gt;Proposes new algorithm with theoretical guarantees&lt;/li&gt;&lt;li&gt;High success rates across multiple LLMs&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jiawei Lian', 'Jianhong Pan', 'Lefan Wang', 'Yi Wang', 'Shaohui Mei', 'Lap-Pui Chau']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'adversarial prompting', 'alignment', 'safety evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.19360</guid><pubDate>Thu, 25 Sep 2025 00:00:00 +0000</pubDate></item><item><title>CON-QA: Privacy-Preserving QA using cloud LLMs in Contract Domain</title><link>https://arxiv.org/abs/2509.19925</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes CON-QA, a privacy-preserving framework for QA on contracts using cloud LLMs&lt;/li&gt;&lt;li&gt;Involves query decomposition, anonymization, and response reconstruction&lt;/li&gt;&lt;li&gt;Introduces CUAD-QA dataset with 85k Q&amp;A pairs&lt;/li&gt;&lt;li&gt;Evaluates privacy and utility preservation&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Ajeet Kumar Singh', 'Rajsabi Surya', 'Anurag Tripathi', 'Santanu Choudhury', 'Sudhir Bisane']&lt;/li&gt;&lt;li&gt;Tags: ['privacy attacks', 'data poisoning', 'safety evaluation', 'robustness', 'alignment']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.19925</guid><pubDate>Thu, 25 Sep 2025 00:00:00 +0000</pubDate></item><item><title>LatentGuard: Controllable Latent Steering for Robust Refusal of Attacks and Reliable Response Generation</title><link>https://arxiv.org/abs/2509.19839</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces LatentGuard, a three-stage framework for robust safety alignment in LLMs.&lt;/li&gt;&lt;li&gt;Combines behavioral alignment with supervised latent space control.&lt;/li&gt;&lt;li&gt;Uses a structured VAE to learn disentangled latent representations for adversarial characteristics.&lt;/li&gt;&lt;li&gt;Enables selective refusal of harmful requests while preserving utility.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Huizhen Shu', 'Xuying Li', 'Zhuo Li']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'adversarial prompting', 'safety evaluation', 'robustness', 'alignment']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.19839</guid><pubDate>Thu, 25 Sep 2025 00:00:00 +0000</pubDate></item></channel></rss>