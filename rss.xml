<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>AI Security Paper Digest</title><link>https://kentaroh-toyoda.github.io/ai-security-paper-digest-rss/rss.xml</link><description>Curated papers on AI security from ArXiv and ACL</description><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><lastBuildDate>Tue, 14 Oct 2025 22:37:33 +0000</lastBuildDate><item><title>VLMGuard-R1: Proactive Safety Alignment for VLMs via Reasoning-Driven Prompt Optimization</title><link>https://arxiv.org/abs/2504.12661</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces VLMGuard-R1, a framework for safety alignment in VLMs using reasoning-driven prompt optimization&lt;/li&gt;&lt;li&gt;Employs a three-stage reasoning pipeline to generate a dataset for training the prompt rewriter&lt;/li&gt;&lt;li&gt;Achieves significant safety improvements across multiple VLMs and benchmarks&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Menglan Chen', 'Xianghe Pang', 'Jingjing Dong', 'WenHao Wang', 'Yaxin Du', 'Siheng Chen']&lt;/li&gt;&lt;li&gt;Tags: ['VLM safety', 'prompt optimization', 'reasoning pipeline', 'safety evaluation', 'multimodal']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2504.12661</guid><pubDate>Tue, 14 Oct 2025 00:00:00 +0000</pubDate></item><item><title>GI-NAS: Boosting Gradient Inversion Attacks Through Adaptive Neural Architecture Search</title><link>https://arxiv.org/abs/2405.20725</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces GI-NAS for gradient inversion attacks in federated learning&lt;/li&gt;&lt;li&gt;Adapts neural architecture search to improve attack performance&lt;/li&gt;&lt;li&gt;Works without explicit prior knowledge&lt;/li&gt;&lt;li&gt;Demonstrates high-fidelity data reconstruction&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Wenbo Yu', 'Hao Fang', 'Bin Chen', 'Xiaohang Sui', 'Chuan Chen', 'Hao Wu', 'Shu-Tao Xia', 'Ke Xu']&lt;/li&gt;&lt;li&gt;Tags: ['privacy attacks', 'federated learning', 'gradient inversion', 'neural architecture search']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2405.20725</guid><pubDate>Tue, 14 Oct 2025 00:00:00 +0000</pubDate></item><item><title>CURE: Concept Unlearning via Orthogonal Representation Editing in Diffusion Models</title><link>https://arxiv.org/abs/2505.12677</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces CURE, a training-free concept unlearning framework for diffusion models&lt;/li&gt;&lt;li&gt;Uses Spectral Eraser for orthogonal projection to remove undesired concepts&lt;/li&gt;&lt;li&gt;Achieves fast and specific suppression without retraining&lt;/li&gt;&lt;li&gt;Enhanced robustness against red-teaming attacks&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Shristi Das Biswas', 'Arani Roy', 'Kaushik Roy']&lt;/li&gt;&lt;li&gt;Tags: ['red teaming', 'concept unlearning', 'diffusion models', 'safety', 'jailbreaking']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2505.12677</guid><pubDate>Tue, 14 Oct 2025 00:00:00 +0000</pubDate></item><item><title>The Easy Path to Robustness: Coreset Selection using Sample Hardness</title><link>https://arxiv.org/abs/2510.11018</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes EasyCore, a coreset selection algorithm using sample hardness (AIGN) to improve adversarial robustness&lt;/li&gt;&lt;li&gt;Shows significant adversarial accuracy improvements over existing methods&lt;/li&gt;&lt;li&gt;Model-agnostic approach applicable to standard and adversarial training&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Pranav Ramesh', 'Arjun Roy', 'Deepak Ravikumar', 'Kaushik Roy', 'Gopalakrishnan Srinivasan']&lt;/li&gt;&lt;li&gt;Tags: ['robustness', 'adversarial training', 'coreset selection', 'data-centric']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.11018</guid><pubDate>Tue, 14 Oct 2025 00:00:00 +0000</pubDate></item><item><title>ImpMIA: Leveraging Implicit Bias for Membership Inference Attack under Realistic Scenarios</title><link>https://arxiv.org/abs/2510.10625</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces ImpMIA, a white-box membership inference attack leveraging implicit bias of neural networks&lt;/li&gt;&lt;li&gt;Uses KKT conditions to identify training samples by gradient reconstruction&lt;/li&gt;&lt;li&gt;Achieves state-of-the-art performance in realistic settings&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yuval Golbari', 'Navve Wasserman', 'Gal Vardi', 'Michal Irani']&lt;/li&gt;&lt;li&gt;Tags: ['privacy attacks', 'membership inference', 'white-box attack', 'implicit bias', 'neural networks']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.10625</guid><pubDate>Tue, 14 Oct 2025 00:00:00 +0000</pubDate></item><item><title>ArtPerception: ASCII Art-based Jailbreak on LLMs with Recognition Pre-test</title><link>https://arxiv.org/abs/2510.10281</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces ArtPerception, a black-box jailbreak framework using ASCII art&lt;/li&gt;&lt;li&gt;Two-phase approach with pre-test for ASCII art recognition parameters&lt;/li&gt;&lt;li&gt;Modified Levenshtein Distance (MLD) metric for evaluation&lt;/li&gt;&lt;li&gt;Tested on SOTA LLMs and commercial models, including GPT-4o, Claude, DeepSeek&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Guan-Yan Yang', 'Tzu-Yu Cheng', 'Ya-Wen Teng', 'Farn Wanga', 'Kuo-Hui Yeh']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'jailbreaking', 'ASCII art', 'Modified Levenshtein Distance', 'model extraction']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.10281</guid><pubDate>Tue, 14 Oct 2025 00:00:00 +0000</pubDate></item><item><title>SecureWebArena: A Holistic Security Evaluation Benchmark for LVLM-based Web Agents</title><link>https://arxiv.org/abs/2510.10073</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces SecureWebArena, a benchmark for evaluating security of LVLM-based web agents&lt;/li&gt;&lt;li&gt;Covers six attack vectors and multiple web environments&lt;/li&gt;&lt;li&gt;Evaluates agents across reasoning, behavior, and task outcomes&lt;/li&gt;&lt;li&gt;Tests 9 LVLMs and finds vulnerabilities in all&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Zonghao Ying', 'Yangguang Shao', 'Jianle Gan', 'Gan Xu', 'Junjie Shen', 'Wenxin Zhang', 'Quanchen Zou', 'Junzheng Shi', 'Zhenfei Yin', 'Mingchuan Zhang', 'Aishan Liu', 'Xianglong Liu']&lt;/li&gt;&lt;li&gt;Tags: ['red teaming', 'security evaluation', 'adversarial prompting', 'model extraction', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.10073</guid><pubDate>Tue, 14 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Text Prompt Injection of Vision Language Models</title><link>https://arxiv.org/abs/2510.09849</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Investigates text prompt injection attacks on vision language models&lt;/li&gt;&lt;li&gt;Developed an efficient algorithm for the attack&lt;/li&gt;&lt;li&gt;Demonstrates effectiveness on large models with low computational cost&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Ruizhe Zhu']&lt;/li&gt;&lt;li&gt;Tags: ['prompt injection', 'adversarial prompting', 'vision language models', 'red teaming', 'safety']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.09849</guid><pubDate>Tue, 14 Oct 2025 00:00:00 +0000</pubDate></item><item><title>CoDefend: Cross-Modal Collaborative Defense via Diffusion Purification and Prompt Optimization</title><link>https://arxiv.org/abs/2510.11096</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes CoDefend, a cross-modal defense framework for MLLMs&lt;/li&gt;&lt;li&gt;Uses supervised diffusion-based denoising for visual modality protection&lt;/li&gt;&lt;li&gt;Incorporates prompt optimization to enhance robustness&lt;/li&gt;&lt;li&gt;Demonstrates improved defense against adversarial attacks in multimodal tasks&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Fengling Zhu', 'Boshi Liu', 'Jingyu Hua', 'Sheng Zhong']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial defense', 'multimodal models', 'diffusion models', 'prompt optimization', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.11096</guid><pubDate>Tue, 14 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Adaptive Stress Testing Black-Box LLM Planners</title><link>https://arxiv.org/abs/2505.05665</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes adaptive stress testing (AST) with MCTS to find prompt perturbations causing LLM hallucinations&lt;/li&gt;&lt;li&gt;Evaluates in multi-agent driving, lunar lander, and robot navigation environments&lt;/li&gt;&lt;li&gt;Aims to detect unsafe outputs in black-box LLM planners&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Neeloy Chakraborty', 'John Pohovey', 'Melkior Ornik', 'Katherine Driggs-Campbell']&lt;/li&gt;&lt;li&gt;Tags: ['red teaming', 'adversarial prompting', 'safety evaluation', 'robustness', 'model extraction']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2505.05665</guid><pubDate>Tue, 14 Oct 2025 00:00:00 +0000</pubDate></item><item><title>VLMGuard-R1: Proactive Safety Alignment for VLMs via Reasoning-Driven Prompt Optimization</title><link>https://arxiv.org/abs/2504.12661</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces VLMGuard-R1, a framework for safety alignment in VLMs using reasoning-driven prompt optimization&lt;/li&gt;&lt;li&gt;Employs a three-stage reasoning pipeline to generate a dataset for training the prompt rewriter&lt;/li&gt;&lt;li&gt;Achieves significant safety improvements across multiple VLMs and benchmarks&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Menglan Chen', 'Xianghe Pang', 'Jingjing Dong', 'WenHao Wang', 'Yaxin Du', 'Siheng Chen']&lt;/li&gt;&lt;li&gt;Tags: ['VLM safety', 'prompt optimization', 'reasoning pipeline', 'safety evaluation', 'multimodal']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2504.12661</guid><pubDate>Tue, 14 Oct 2025 00:00:00 +0000</pubDate></item><item><title>When Machine Unlearning Meets Retrieval-Augmented Generation (RAG): Keep Secret or Forget Knowledge?</title><link>https://arxiv.org/abs/2410.15267</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a RAG-based machine unlearning framework for LLMs&lt;/li&gt;&lt;li&gt;Addresses sensitive info retention and harmful content&lt;/li&gt;&lt;li&gt;Evaluates on multiple models including closed-source ones&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Shang Wang', 'Tianqing Zhu', 'Dayong Ye', 'Wanlei Zhou']&lt;/li&gt;&lt;li&gt;Tags: ['privacy attacks', 'data poisoning', 'alignment', 'safety evaluation', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2410.15267</guid><pubDate>Tue, 14 Oct 2025 00:00:00 +0000</pubDate></item><item><title>DeAL: Decoding-time Alignment for Large Language Models</title><link>https://arxiv.org/abs/2402.06147</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes DeAL, a decoding-time alignment framework for LLMs&lt;/li&gt;&lt;li&gt;Allows customization of reward functions&lt;/li&gt;&lt;li&gt;Improves adherence to alignment objectives like harmlessness&lt;/li&gt;&lt;li&gt;Complementary to existing alignment methods like RLHF&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['James Y. Huang', 'Sailik Sengupta', 'Daniele Bonadiman', 'Yi-An Lai', 'Arshit Gupta', 'Nikolaos Pappas', 'Saab Mansour', 'Katrin Kirchhoff', 'Dan Roth']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'safety', 'red teaming', 'jailbreaking', 'LLM']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2402.06147</guid><pubDate>Tue, 14 Oct 2025 00:00:00 +0000</pubDate></item><item><title>HalluDetect: Detecting, Mitigating, and Benchmarking Hallucinations in Conversational Systems in the Legal Domain</title><link>https://arxiv.org/abs/2509.11619</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Develops HalluDetect for detecting hallucinations in legal chatbots&lt;/li&gt;&lt;li&gt;Evaluates different mitigation architectures&lt;/li&gt;&lt;li&gt;AgentBot shows best performance&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research Paper&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Spandan Anaokar', 'Shrey Ganatra', 'Harshvivek Kashid', 'Swapnil Bhattacharyya', 'Shruti Nair', 'Reshma Sekhar', 'Siddharth Manohar', 'Rahul Hemrajani', 'Pushpak Bhattacharyya']&lt;/li&gt;&lt;li&gt;Tags: ['safety', 'hallucinations', 'mitigation', 'benchmarking']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.11619</guid><pubDate>Tue, 14 Oct 2025 00:00:00 +0000</pubDate></item><item><title>When Thinking Backfires: Mechanistic Insights Into Reasoning-Induced Misalignment</title><link>https://arxiv.org/abs/2509.00544</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces Reasoning-Induced Misalignment (RIM) where enhanced reasoning leads to misalignment&lt;/li&gt;&lt;li&gt;Provides mechanistic insights through attention head analysis and neuron activation entanglement&lt;/li&gt;&lt;li&gt;Highlights catastrophic forgetting as a correlate of RIM during training&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Hanqi Yan', 'Hainiu Xu', 'Siya Qi', 'Shu Yang', 'Yulan He']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'safety', 'model analysis', 'neuron entanglement', 'catastrophic forgetting']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.00544</guid><pubDate>Tue, 14 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Jailbreaking Commercial Black-Box LLMs with Explicitly Harmful Prompts</title><link>https://arxiv.org/abs/2508.10390</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes new jailbreaking attacks (D-Attack and DH-CoT) for reasoning LLMs using developer role-based prompts&lt;/li&gt;&lt;li&gt;Introduces MDH dataset cleaning method combining LLM screening and human verification&lt;/li&gt;&lt;li&gt;Highlights issues with existing red-teaming datasets and their impact on attack evaluation&lt;/li&gt;&lt;li&gt;Demonstrates improved attack success with developer messages and cleaned datasets&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Chiyu Zhang', 'Lu Zhou', 'Xiaogang Xu', 'Jiafei Wu', 'Liming Fang', 'Zhe Liu']&lt;/li&gt;&lt;li&gt;Tags: ['jailbreaking', 'red teaming', 'prompt injection', 'adversarial prompting', 'dataset cleaning']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2508.10390</guid><pubDate>Tue, 14 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Adversarial Defence without Adversarial Defence: Enhancing Language Model Robustness via Instance-level Principal Component Removal</title><link>https://arxiv.org/abs/2507.21750</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a method to enhance adversarial robustness of PLMs by removing instance-level principal components&lt;/li&gt;&lt;li&gt;Does not rely on conventional adversarial defenses or perturbing training data&lt;/li&gt;&lt;li&gt;Transforms embedding space to reduce susceptibility to adversarial noise&lt;/li&gt;&lt;li&gt;Evaluates on eight benchmark datasets showing improved robustness&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yang Wang', 'Chenghao Xiao', 'Yizhi Li', 'Stuart E. Middleton', 'Noura Al Moubayed', 'Chenghua Lin']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial robustness', 'language models', 'embedding space transformation', 'principal component removal']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2507.21750</guid><pubDate>Tue, 14 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Safety-Aligned Weights Are Not Enough: Refusal-Teacher-Guided Finetuning Enhances Safety and Downstream Performance under Harmful Finetuning Attacks</title><link>https://arxiv.org/abs/2506.07356</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;The paper addresses harmful finetuning attacks on LLMs in Finetuning-as-a-Service (FaaS).&lt;/li&gt;&lt;li&gt;Proposes a Refusal-Teacher (Ref-Teacher)-guided finetuning framework to filter harmful prompts and distill safety knowledge.&lt;/li&gt;&lt;li&gt;Experiments show improved safety and downstream performance compared to safety-aligned initialization.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Seokil Ham', 'Yubin Choi', 'Yujin Yang', 'Seungju Cho', 'Younghun Kim', 'Changick Kim']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'safety evaluation', 'robustness', 'alignment', 'finetuning attacks']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.07356</guid><pubDate>Tue, 14 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Noise Injection Systemically Degrades Large Language Model Safety Guardrails</title><link>https://arxiv.org/abs/2505.13500</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Investigates robustness of safety fine-tuning in LLMs by injecting Gaussian noise&lt;/li&gt;&lt;li&gt;Finds that noise increases harmful output rates by up to 27%&lt;/li&gt;&lt;li&gt;Deeper safety fine-tuning doesn't provide extra protection&lt;/li&gt;&lt;li&gt;Chain-of-thought reasoning remains intact&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Prithviraj Singh Shahani', 'Kaveh Eskandari Miandoab', 'Matthias Scheutz']&lt;/li&gt;&lt;li&gt;Tags: ['safety', 'robustness', 'LLM', 'noise injection', 'safety guardrails']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2505.13500</guid><pubDate>Tue, 14 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Think in Safety: Unveiling and Mitigating Safety Alignment Collapse in Multimodal Large Reasoning Model</title><link>https://arxiv.org/abs/2505.06538</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Evaluates safety of 11 MLRMs across 5 benchmarks&lt;/li&gt;&lt;li&gt;Reveals safety degradation in jailbreak robustness&lt;/li&gt;&lt;li&gt;Proposes using model's reasoning to detect unsafe intent&lt;/li&gt;&lt;li&gt;Introduces multimodal safety tuning dataset&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Xinyue Lou', 'You Li', 'Jinan Xu', 'Xiangyu Shi', 'Chi Chen', 'Kaiyu Huang']&lt;/li&gt;&lt;li&gt;Tags: ['safety evaluation', 'jailbreak robustness', 'safety-awareness', 'multimodal models', 'fine-tuning']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2505.06538</guid><pubDate>Tue, 14 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Learning from Reference Answers: Versatile Language Model Alignment without Binary Human Preference Data</title><link>https://arxiv.org/abs/2504.09895</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces RefAlign, an alignment algorithm using reference answers instead of binary preferences&lt;/li&gt;&lt;li&gt;Utilizes metrics like BERTScore for reward&lt;/li&gt;&lt;li&gt;Applies to safety and confidence alignment&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Shuai Zhao', 'Yunqiu Xu', 'Linchao Zhu', 'Yi Yang']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'safety', 'confidence', 'reward modeling', 'reference answers']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2504.09895</guid><pubDate>Tue, 14 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Debate, Deliberate, Decide (D3): A Cost-Aware Adversarial Framework for Reliable and Interpretable LLM Evaluation</title><link>https://arxiv.org/abs/2410.04663</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces D3 framework for adversarial evaluation of LLMs&lt;/li&gt;&lt;li&gt;Uses structured debate among agents to improve evaluation reliability&lt;/li&gt;&lt;li&gt;Includes theoretical analysis and experimental validation&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Chaithanya Bandi', 'Abir Harrasse']&lt;/li&gt;&lt;li&gt;Tags: ['LLM evaluation', 'adversarial framework', 'reliability', 'interpretability', 'cost-aware']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2410.04663</guid><pubDate>Tue, 14 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Bag of Tricks for Subverting Reasoning-based Safety Guardrails</title><link>https://arxiv.org/abs/2510.11570</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;The paper introduces a set of jailbreak methods that can bypass reasoning-based safety guardrails in Large Reasoning Models (LRMs).&lt;/li&gt;&lt;li&gt;These methods involve manipulating input prompts with template tokens and automated optimization.&lt;/li&gt;&lt;li&gt;The attacks achieve high success rates across multiple benchmarks and models, highlighting systemic vulnerabilities.&lt;/li&gt;&lt;li&gt;The research underscores the need for stronger alignment techniques to prevent misuse of open-source LRMs.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Shuo Chen', 'Zhen Han', 'Haokun Chen', 'Bailan He', 'Shengyun Si', 'Jingpei Wu', 'Philip Torr', 'Volker Tresp', 'Jindong Gu']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'jailbreaking', 'adversarial prompting', 'safety evaluation', 'alignment']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.11570</guid><pubDate>Tue, 14 Oct 2025 00:00:00 +0000</pubDate></item><item><title>ENIGMA: The Geometry of Reasoning and Alignment in Large-Language Models</title><link>https://arxiv.org/abs/2510.11278</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;ENIGMA is a new LLM training approach focusing on reasoning, alignment, and robustness.&lt;/li&gt;&lt;li&gt;Uses GRPO, SAMI-style InfoNCE, and Sinkhorn regularization.&lt;/li&gt;&lt;li&gt;Introduces Sufficiency Index (SI) for principle selection.&lt;/li&gt;&lt;li&gt;Experiments show improved performance and training dynamics.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Gareth Seneque', 'Lap-Hang Ho', 'Nafise Erfanian Saeedi', 'Jeffrey Molendijk', 'Ariel Kupermann', 'Tim Elson']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'robustness', 'safety evaluation', 'model training']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.11278</guid><pubDate>Tue, 14 Oct 2025 00:00:00 +0000</pubDate></item><item><title>ArtPerception: ASCII Art-based Jailbreak on LLMs with Recognition Pre-test</title><link>https://arxiv.org/abs/2510.10281</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces ArtPerception, a black-box jailbreak framework using ASCII art&lt;/li&gt;&lt;li&gt;Two-phase approach with pre-test for ASCII art recognition parameters&lt;/li&gt;&lt;li&gt;Modified Levenshtein Distance (MLD) metric for evaluation&lt;/li&gt;&lt;li&gt;Tested on SOTA LLMs and commercial models, including GPT-4o, Claude, DeepSeek&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Guan-Yan Yang', 'Tzu-Yu Cheng', 'Ya-Wen Teng', 'Farn Wanga', 'Kuo-Hui Yeh']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'jailbreaking', 'ASCII art', 'Modified Levenshtein Distance', 'model extraction']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.10281</guid><pubDate>Tue, 14 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Building a Foundational Guardrail for General Agentic Systems via Synthetic Data</title><link>https://arxiv.org/abs/2510.09781</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces AuraGen for generating synthetic data with labeled risks for pre-execution safety&lt;/li&gt;&lt;li&gt;Proposes Safiron, a foundational guardrail model combining a cross-planner adapter and guardian model&lt;/li&gt;&lt;li&gt;Releases Pre-Exec Bench benchmark for evaluating pre-execution safety across different planners&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yue Huang', 'Hang Hua', 'Yujun Zhou', 'Pengcheng Jing', 'Manish Nagireddy', 'Inkit Padhi', 'Greta Dolcetti', 'Zhangchen Xu', 'Subhajit Chaudhury', 'Ambrish Rawat', 'Liubov Nedoshivina', 'Pin-Yu Chen', 'Prasanna Sattigeri', 'Xiangliang Zhang']&lt;/li&gt;&lt;li&gt;Tags: ['safety evaluation', 'guardrails', 'synthetic data', 'pre-execution safety', 'risk detection']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.09781</guid><pubDate>Tue, 14 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Group-Adaptive Adversarial Learning for Robust Fake News Detection Against Malicious Comments</title><link>https://arxiv.org/abs/2510.09712</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Evaluates fake news detectors against adversarial comments&lt;/li&gt;&lt;li&gt;Introduces group-adaptive adversarial training&lt;/li&gt;&lt;li&gt;Uses LLMs to generate category-specific attacks&lt;/li&gt;&lt;li&gt;Applies adaptive sampling for robustness&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Zhao Tong', 'Chunlin Gong', 'Yimeng Gu', 'Haichao Shi', 'Qiang Liu', 'Shu Wu', 'Xiao-Yu Zhang']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial training', 'fake news detection', 'LLM red teaming', 'robustness', 'security']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.09712</guid><pubDate>Tue, 14 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Are Large Reasoning Models Interruptible?</title><link>https://arxiv.org/abs/2510.11713</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Evaluates LRM robustness under interruptions and dynamic context&lt;/li&gt;&lt;li&gt;Performance drops up to 60% with late updates&lt;/li&gt;&lt;li&gt;Identifies failure modes like reasoning leakage, panic, self-doubt&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Tsung-Han Wu', 'Mihran Miroyan', 'David M. Chan', 'Trevor Darrell', 'Narges Norouzi', 'Joseph E. Gonzalez']&lt;/li&gt;&lt;li&gt;Tags: ['robustness', 'safety evaluation', 'adversarial prompting', 'jailbreaking']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.11713</guid><pubDate>Tue, 14 Oct 2025 00:00:00 +0000</pubDate></item><item><title>LLMAtKGE: Large Language Models as Explainable Attackers against Knowledge Graph Embeddings</title><link>https://arxiv.org/abs/2510.11584</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes LLMAtKGE, an LLM-based framework for attacking knowledge graph embeddings with explanations&lt;/li&gt;&lt;li&gt;Uses structured prompting and filtering to handle context limitations&lt;/li&gt;&lt;li&gt;Outperforms black-box baselines and provides human-readable explanations&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Ting Li', 'Yang Yang', 'Yipeng Yu', 'Liang Yao', 'Guoqing Chao', 'Ruifeng Xu']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial attacks', 'knowledge graph embeddings', 'LLM red teaming', 'explanation generation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.11584</guid><pubDate>Tue, 14 Oct 2025 00:00:00 +0000</pubDate></item><item><title>KnowRL: Teaching Language Models to Know What They Know</title><link>https://arxiv.org/abs/2510.11407</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces KnowRL framework to improve LLM self-knowledge&lt;/li&gt;&lt;li&gt;Uses introspection and consensus-based rewarding&lt;/li&gt;&lt;li&gt;Aims to enhance safety and reliability&lt;/li&gt;&lt;li&gt;Validated on LLaMA-3.1-8B and Qwen-2.5-7B&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Sahil Kale', 'Devendra Singh Dhami']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'safety evaluation', 'robustness', 'self-improvement']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.11407</guid><pubDate>Tue, 14 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Emergent Misalignment via In-Context Learning: Narrow in-context examples can produce broadly misaligned LLMs</title><link>https://arxiv.org/abs/2510.11288</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;The paper explores whether emergent misalignment (EM) occurs in in-context learning (ICL) for LLMs.&lt;/li&gt;&lt;li&gt;It finds that narrow in-context examples can lead to broadly misaligned responses, with rates varying between 2-17% for 64 examples and up to 58% for 256 examples.&lt;/li&gt;&lt;li&gt;Manual analysis shows that misaligned responses often adopt a reckless or dangerous persona, similar to finetuning-induced EM.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Nikita Afonin', 'Nikita Andriyanov', 'Nikhil Bageshpura', 'Kyle Liu', 'Kevin Zhu', 'Sunishchal Dev', 'Ashwinee Panda', 'Alexander Panchenko', 'Oleg Rogov', 'Elena Tutubalina', 'Mikhail Seleznyov']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'alignment', 'safety evaluation', 'adversarial prompting']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.11288</guid><pubDate>Tue, 14 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Attacks by Content: Automated Fact-checking is an AI Security Issue</title><link>https://arxiv.org/abs/2510.11238</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces 'attacks by content' where adversaries manipulate AI agents by providing false or misleading information instead of prompt injection.&lt;/li&gt;&lt;li&gt;Argues that existing defenses against prompt injection are ineffective against content-based attacks.&lt;/li&gt;&lt;li&gt;Proposes using automated fact-checking as a defense mechanism for AI agents to evaluate information.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Michael Schlichtkrull']&lt;/li&gt;&lt;li&gt;Tags: ['AI security', 'adversarial attacks', 'fact-checking', 'content manipulation', 'defense mechanisms']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.11238</guid><pubDate>Tue, 14 Oct 2025 00:00:00 +0000</pubDate></item><item><title>TypePilot: Leveraging the Scala Type System for Secure LLM-generated Code</title><link>https://arxiv.org/abs/2510.11151</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces TypePilot, an AI framework for secure LLM-generated code using Scala's type system&lt;/li&gt;&lt;li&gt;Evaluates security improvements through formal verification and secure code generation&lt;/li&gt;&lt;li&gt;Shows mitigation of input validation and injection vulnerabilities&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Alexander Sternfeld', 'Andrei Kucharavy', 'Ljiljana Dolamic']&lt;/li&gt;&lt;li&gt;Tags: ['AI security', 'code generation', 'type system', 'formal verification', 'vulnerability mitigation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.11151</guid><pubDate>Tue, 14 Oct 2025 00:00:00 +0000</pubDate></item><item><title>DeepResearchGuard: Deep Research with Open-Domain Evaluation and Multi-Stage Guardrails for Safety</title><link>https://arxiv.org/abs/2510.10994</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces DEEPRESEARCHGUARD, a framework with multi-stage guardrails for safety in deep research using LLMs.&lt;/li&gt;&lt;li&gt;Includes open-domain evaluation of references and reports across multiple safety metrics.&lt;/li&gt;&lt;li&gt;Presents DRSAFEBENCH, a new benchmark for evaluating deep research safety.&lt;/li&gt;&lt;li&gt;Evaluates performance on various LLMs, showing improved defense success and reduced over-refusal rates.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Wei-Chieh Huang', 'Henry Peng Zou', 'Yaozu Wu', 'Dongyuan Li', 'Yankai Chen', 'Weizhi Zhang', 'Yangning Li', 'Angelo Zangari', 'Jizhou Guo', 'Chunyu Miao', 'Liancheng Fang', 'Langzhou He', 'Renhe Jiang', 'Philip S. Yu']&lt;/li&gt;&lt;li&gt;Tags: ['LLM safety', 'red teaming', 'safety evaluation', 'model robustness', 'benchmarking']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.10994</guid><pubDate>Tue, 14 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Judge Before Answer: Can MLLM Discern the False Premise in Question?</title><link>https://arxiv.org/abs/2510.10965</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces a benchmark for false premise recognition in MLLMs&lt;/li&gt;&lt;li&gt;Proposes a recognition enhancement framework&lt;/li&gt;&lt;li&gt;Evaluates robustness improvements&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jidong Li', 'Lingyong Fang', 'Haodong Zhao', 'Sufeng Duan', 'Gongshen Liu']&lt;/li&gt;&lt;li&gt;Tags: ['robustness', 'safety evaluation', 'multimodal']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.10965</guid><pubDate>Tue, 14 Oct 2025 00:00:00 +0000</pubDate></item><item><title>DUAL-Bench: Measuring Over-Refusal and Robustness in Vision-Language Models</title><link>https://arxiv.org/abs/2510.10846</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces DUAL-Bench, a multimodal benchmark for over-refusal and safe completion in VLMs&lt;/li&gt;&lt;li&gt;Evaluates 18 VLMs across 12 hazard categories with visual perturbations&lt;/li&gt;&lt;li&gt;Highlights significant room for improvement in safe completion rates&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Kaixuan Ren', 'Preslav Nakov', 'Usman Naseem']&lt;/li&gt;&lt;li&gt;Tags: ['red teaming', 'safety evaluation', 'alignment', 'robustness', 'multimodal']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.10846</guid><pubDate>Tue, 14 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Unlocking LLM Safeguards for Low-Resource Languages via Reasoning and Alignment with Minimal Training Data</title><link>https://arxiv.org/abs/2510.10677</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes ConsistentGuard, a reasoning-based multilingual safeguard for LLMs&lt;/li&gt;&lt;li&gt;Enhances explainability and knowledge transfer between languages&lt;/li&gt;&lt;li&gt;Outperforms existing methods with minimal training data&lt;/li&gt;&lt;li&gt;Contributes a multilingual benchmark extension&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Zhuowei Chen', 'Bowei Zhang', 'Nankai Lin', 'Tian Hou', 'Lianxi Wang']&lt;/li&gt;&lt;li&gt;Tags: ['safety', 'alignment', 'multilingual', 'low-resource languages', 'explainability']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.10677</guid><pubDate>Tue, 14 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Steering Over-refusals Towards Safety in Retrieval Augmented Generation</title><link>https://arxiv.org/abs/2510.10452</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Analyzes over-refusals in RAG models due to safety filters&lt;/li&gt;&lt;li&gt;Introduces RagRefuse benchmark for testing&lt;/li&gt;&lt;li&gt;Proposes SafeRAG-Steering to mitigate over-refusals&lt;/li&gt;&lt;li&gt;Focuses on safety alignment and model behavior&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Utsav Maskey', 'Mark Dras', 'Usman Naseem']&lt;/li&gt;&lt;li&gt;Tags: ['safety', 'alignment', 'red teaming', 'benchmarking', 'RAG']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.10452</guid><pubDate>Tue, 14 Oct 2025 00:00:00 +0000</pubDate></item><item><title>RefusalBench: Generative Evaluation of Selective Refusal in Grounded Language Models</title><link>https://arxiv.org/abs/2510.10390</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces RefusalBench for evaluating selective refusal in RAG systems&lt;/li&gt;&lt;li&gt;Highlights models' struggles with refusal accuracy in multi-document tasks&lt;/li&gt;&lt;li&gt;Uses generative methodology with controlled perturbations&lt;/li&gt;&lt;li&gt;Releases two benchmarks and the generation framework&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Aashiq Muhamed', 'Leonardo F. R. Ribeiro', 'Markus Dreyer', 'Virginia Smith', 'Mona T. Diab']&lt;/li&gt;&lt;li&gt;Tags: ['safety evaluation', 'red teaming', 'alignment', 'robustness', 'benchmarks']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.10390</guid><pubDate>Tue, 14 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Backdoor Collapse: Eliminating Unknown Threats via Known Backdoor Aggregation in Language Models</title><link>https://arxiv.org/abs/2510.10265</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a defense framework against backdoor attacks in LLMs without needing prior knowledge of trigger settings.&lt;/li&gt;&lt;li&gt;Uses a two-stage process: aggregating backdoor representations by injecting known triggers, then recovery fine-tuning.&lt;/li&gt;&lt;li&gt;Shows significant reduction in attack success rates while preserving clean accuracy.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Liang Lin', 'Miao Yu', 'Moayad Aloqaily', 'Zhenhong Zhou', 'Kun Wang', 'Linsey Pang', 'Prakhar Mehrotra', 'Qingsong Wen']&lt;/li&gt;&lt;li&gt;Tags: ['backdoor defense', 'LLM security', 'adversarial attacks', 'representation aggregation', 'recovery fine-tuning']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.10265</guid><pubDate>Tue, 14 Oct 2025 00:00:00 +0000</pubDate></item><item><title>DiffHeads: Differential Analysis and Inference-Time Masking of Bias Heads in Large Language Models</title><link>https://arxiv.org/abs/2510.10142</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;DiffHeads framework for debiasing LLMs&lt;/li&gt;&lt;li&gt;Compares DA and CoT prompting strategies&lt;/li&gt;&lt;li&gt;Identifies and masks bias heads in attention layers&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Tingxu Han', 'Wei Song', 'Ziqi Ding', 'Ziming Li', 'Chunrong Fang', 'Yuekang Li', 'Dongfang Liu', 'Zhenyu Chen', 'Zhenting Wang']&lt;/li&gt;&lt;li&gt;Tags: ['safety', 'bias', 'prompting', 'attention heads', 'debiasing']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.10142</guid><pubDate>Tue, 14 Oct 2025 00:00:00 +0000</pubDate></item><item><title>A-IPO: Adaptive Intent-driven Preference Optimization</title><link>https://arxiv.org/abs/2510.10077</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces A-IPO for adaptive intent-driven preference optimization&lt;/li&gt;&lt;li&gt;Incorporates latent intent into reward function&lt;/li&gt;&lt;li&gt;Evaluates on real-world and adversarial benchmarks&lt;/li&gt;&lt;li&gt;Shows improvements in alignment and robustness&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Wenqing Wang (Huazhong Agricultural University', 'China)', 'Muhammad Asif Ali (King Abdullah University of Science and Technology', 'KSA)', 'Ali Shoker (King Abdullah University of Science and Technology', 'KSA)', 'Ruohan Yang (Huazhong Agricultural University', 'China)', 'Junyang Chen (Shenzhen University', 'China)', 'Ying Sha (Huazhong Agricultural University', 'China)', 'Huan Wang (Huazhong Agricultural University', 'China)']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'red teaming', 'robustness', 'safety evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.10077</guid><pubDate>Tue, 14 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Path Drift in Large Reasoning Models:How First-Person Commitments Override Safety</title><link>https://arxiv.org/abs/2510.10013</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Identifies Path Drift in Long-CoT LLMs where reasoning trajectories drift from aligned paths&lt;/li&gt;&lt;li&gt;Unveils three triggers: first-person commitments, ethical evaporation, condition chain escalation&lt;/li&gt;&lt;li&gt;Introduces a Path Drift Induction Framework with three stages&lt;/li&gt;&lt;li&gt;Proposes path-level defense strategy with role attribution correction and metacognitive reflection&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yuyi Huang', 'Runzhe Zhan', 'Lidia S. Chao', 'Ailin Tao', 'Derek F. Wong']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'alignment', 'safety evaluation', 'adversarial prompting', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.10013</guid><pubDate>Tue, 14 Oct 2025 00:00:00 +0000</pubDate></item><item><title>CoBia: Constructed Conversations Can Trigger Otherwise Concealed Societal Biases in LLMs</title><link>https://arxiv.org/abs/2510.09871</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces CoBia, a suite of adversarial attacks to reveal societal biases in LLMs&lt;/li&gt;&lt;li&gt;Evaluates 11 LLMs across six socio-demographic categories&lt;/li&gt;&lt;li&gt;Tests if models can recover from fabricated bias claims and reject biased follow-ups&lt;/li&gt;&lt;li&gt;Highlights deep biases surfaced through constructed conversations&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Nafiseh Nikeghbal', 'Amir Hossein Kargaran', 'Jana Diesner']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'adversarial prompting', 'bias detection', 'safety evaluation', 'alignment']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.09871</guid><pubDate>Tue, 14 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Text Prompt Injection of Vision Language Models</title><link>https://arxiv.org/abs/2510.09849</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Investigates text prompt injection attacks on vision language models&lt;/li&gt;&lt;li&gt;Developed an efficient algorithm for the attack&lt;/li&gt;&lt;li&gt;Demonstrates effectiveness on large models with low computational cost&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Ruizhe Zhu']&lt;/li&gt;&lt;li&gt;Tags: ['prompt injection', 'adversarial prompting', 'vision language models', 'red teaming', 'safety']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.09849</guid><pubDate>Tue, 14 Oct 2025 00:00:00 +0000</pubDate></item><item><title>All Code, No Thought: Current Language Models Struggle to Reason in Ciphered Language</title><link>https://arxiv.org/abs/2510.09714</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;The paper evaluates if current language models can reason in ciphered languages, which could be a way to evade chain-of-thought monitoring.&lt;/li&gt;&lt;li&gt;They test 28 ciphers across 10 models, finding that while models can translate ciphers, reasoning ability drops significantly except for well-known ones like rot13.&lt;/li&gt;&lt;li&gt;The study shows that ciphered reasoning capability is linked to the cipher's prevalence in pretraining data and improves slowly with more fine-tuning.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Shiyuan Guo', 'Henry Sleight', 'Fabien Roger']&lt;/li&gt;&lt;li&gt;Tags: ['red teaming', 'adversarial prompting', 'safety evaluation', 'alignment', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.09714</guid><pubDate>Tue, 14 Oct 2025 00:00:00 +0000</pubDate></item><item><title>SeCon-RAG: A Two-Stage Semantic Filtering and Conflict-Free Framework for Trustworthy RAG</title><link>https://arxiv.org/abs/2510.09710</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a two-stage framework (SeCon-RAG) for trustworthy RAG systems&lt;/li&gt;&lt;li&gt;First stage uses semantic and cluster-based filtering with EIRE to select relevant documents&lt;/li&gt;&lt;li&gt;Second stage applies conflict-aware filtering to detect inconsistencies&lt;/li&gt;&lt;li&gt;Aims to prevent corpus poisoning and contamination attacks while preserving useful knowledge&lt;/li&gt;&lt;li&gt;Shows improved robustness and trustworthiness in experiments&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Xiaonan Si', 'Meilin Zhu', 'Simeng Qin', 'Lijia Yu', 'Lijun Zhang', 'Shuaitong Liu', 'Xinfeng Li', 'Ranjie Duan', 'Yang Liu', 'Xiaojun Jia']&lt;/li&gt;&lt;li&gt;Tags: ['RAG security', 'corpus poisoning', 'conflict detection', 'semantic filtering', 'adversarial attacks']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.09710</guid><pubDate>Tue, 14 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Transmuting prompts into weights</title><link>https://arxiv.org/abs/2510.08734</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;The paper provides a theoretical foundation for modifying LLM internal states through prompts.&lt;/li&gt;&lt;li&gt;It explains how prompts can be mapped to weight updates in transformer models.&lt;/li&gt;&lt;li&gt;Introduces thought vectors and matrices for prompt-based model editing.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Hanna Mazzawi', 'Benoit Dherin', 'Michael Munn', 'Michael Wunder', 'Javier Gonzalvo']&lt;/li&gt;&lt;li&gt;Tags: ['LLM', 'prompt injection', 'model editing', 'transformer architecture', 'theoretical foundation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.08734</guid><pubDate>Tue, 14 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Provably Robust Adaptation for Language-Empowered Foundation Models</title><link>https://arxiv.org/abs/2510.08659</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes LeFCert, a provably robust few-shot classifier for LeFMs&lt;/li&gt;&lt;li&gt;Addresses poisoning attacks by deriving classification score bounds&lt;/li&gt;&lt;li&gt;Introduces variants with randomized smoothing and collective certification&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yuni Lai', 'Xiaoyu Xue', 'Linghui Shen', 'Yulun Wu', 'Gaolei Li', 'Song Guo', 'Kai Zhou', 'Bin Xiao']&lt;/li&gt;&lt;li&gt;Tags: ['robustness', 'poisoning attacks', 'certified robustness', 'few-shot learning', 'multimodal']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.08659</guid><pubDate>Tue, 14 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Energy-Driven Steering: Reducing False Refusals in Large Language Models</title><link>https://arxiv.org/abs/2510.08646</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces Energy-Driven Steering (EDS) to reduce false refusals in LLMs&lt;/li&gt;&lt;li&gt;Uses an Energy-Based Model (EBM) to dynamically steer LLM responses during inference&lt;/li&gt;&lt;li&gt;Aims to balance safety and helpfulness by adjusting energy landscapes&lt;/li&gt;&lt;li&gt;Shows significant improvement in compliance on ORB-H benchmark&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Eric Hanchen Jiang', 'Weixuan Ou', 'Run Liu', 'Shengyuan Pang', 'Guancheng Wan', 'Ranjie Duan', 'Wei Dong', 'Kai-Wei Chang', 'XiaoFeng Wang', 'Ying Nian Wu', 'Xinfeng Li']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'alignment', 'safety evaluation', 'robustness', 'jailbreaking']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.08646</guid><pubDate>Tue, 14 Oct 2025 00:00:00 +0000</pubDate></item><item><title>SAFEx: Analyzing Vulnerabilities of MoE-Based LLMs via Stable Safety-critical Expert Identification</title><link>https://arxiv.org/abs/2506.17368</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces SAFEx framework to analyze MoE-based LLM safety vulnerabilities&lt;/li&gt;&lt;li&gt;Identifies safety-critical experts in MoE models&lt;/li&gt;&lt;li&gt;Tests targeted expert masking and adaptation for safety improvements&lt;/li&gt;&lt;li&gt;Demonstrates significant safety behavior concentration in specific experts&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Zhenglin Lai', 'Mengyao Liao', 'Bingzhe Wu', 'Dong Xu', 'Zebin Zhao', 'Zhihang Yuan', 'Chao Fan', 'Jianqiang Li']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'alignment', 'safety evaluation', 'model extraction', 'adversarial prompting']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.17368</guid><pubDate>Tue, 14 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Noise Injection Systemically Degrades Large Language Model Safety Guardrails</title><link>https://arxiv.org/abs/2505.13500</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Investigates robustness of safety fine-tuning in LLMs by injecting Gaussian noise&lt;/li&gt;&lt;li&gt;Finds that noise increases harmful output rates by up to 27%&lt;/li&gt;&lt;li&gt;Deeper safety fine-tuning doesn't provide extra protection&lt;/li&gt;&lt;li&gt;Chain-of-thought reasoning remains intact&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Prithviraj Singh Shahani', 'Kaveh Eskandari Miandoab', 'Matthias Scheutz']&lt;/li&gt;&lt;li&gt;Tags: ['safety', 'robustness', 'LLM', 'noise injection', 'safety guardrails']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2505.13500</guid><pubDate>Tue, 14 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Adaptive Stress Testing Black-Box LLM Planners</title><link>https://arxiv.org/abs/2505.05665</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes adaptive stress testing (AST) with MCTS to find prompt perturbations causing LLM hallucinations&lt;/li&gt;&lt;li&gt;Evaluates in multi-agent driving, lunar lander, and robot navigation environments&lt;/li&gt;&lt;li&gt;Aims to detect unsafe outputs in black-box LLM planners&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Neeloy Chakraborty', 'John Pohovey', 'Melkior Ornik', 'Katherine Driggs-Campbell']&lt;/li&gt;&lt;li&gt;Tags: ['red teaming', 'adversarial prompting', 'safety evaluation', 'robustness', 'model extraction']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2505.05665</guid><pubDate>Tue, 14 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Learning from Reference Answers: Versatile Language Model Alignment without Binary Human Preference Data</title><link>https://arxiv.org/abs/2504.09895</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces RefAlign, an alignment algorithm using reference answers instead of binary preferences&lt;/li&gt;&lt;li&gt;Utilizes metrics like BERTScore for reward&lt;/li&gt;&lt;li&gt;Applies to safety and confidence alignment&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Shuai Zhao', 'Yunqiu Xu', 'Linchao Zhu', 'Yi Yang']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'safety', 'confidence', 'reward modeling', 'reference answers']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2504.09895</guid><pubDate>Tue, 14 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Frontier AI's Impact on the Cybersecurity Landscape</title><link>https://arxiv.org/abs/2504.05408</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Analyzes AI's impact on cybersecurity through benchmarking, literature review, and expert survey&lt;/li&gt;&lt;li&gt;Finds AI more prevalent in attacks than defense, especially in remediation&lt;/li&gt;&lt;li&gt;Experts predict continued attacker advantage but narrowing gap&lt;/li&gt;&lt;li&gt;Calls for new benchmarks, AI defense agents, secure AI design, better testing, and user education&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yujin Potter', 'Wenbo Guo', 'Zhun Wang', 'Tianneng Shi', 'Andy Zhang', 'Patrick Gage Kelley', 'Kurt Thomas', 'Dawn Song']&lt;/li&gt;&lt;li&gt;Tags: ['AI security', 'cybersecurity', 'red teaming', 'defense mechanisms', 'security standards']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2504.05408</guid><pubDate>Tue, 14 Oct 2025 00:00:00 +0000</pubDate></item><item><title>DemonAgent: Dynamically Encrypted Multi-Backdoor Implantation Attack on LLM-based Agent</title><link>https://arxiv.org/abs/2502.12575</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a new backdoor attack on LLM-based agents using dynamic encryption and multi-fragmentation&lt;/li&gt;&lt;li&gt;Introduces AgentBackdoorEval dataset for evaluating backdoor attacks&lt;/li&gt;&lt;li&gt;Achieves near 100% attack success rate with 0% detection&lt;/li&gt;&lt;li&gt;Highlights limitations of current safety mechanisms&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Pengyu Zhu', 'Zhenhong Zhou', 'Yuanhe Zhang', 'Shilinlu Yan', 'Kun Wang', 'Sen Su']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'backdoor attacks', 'safety evaluation', 'adversarial prompting', 'data poisoning']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2502.12575</guid><pubDate>Tue, 14 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Reasoning Model Unlearning: Forgetting Traces, Not Just Answers, While Preserving Reasoning Skills</title><link>https://arxiv.org/abs/2506.12963</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces Reasoning-aware Representation Misdirection for Unlearning (R²MU) to address machine unlearning in large reasoning models (LRMs)&lt;/li&gt;&lt;li&gt;Targets suppression of sensitive information in intermediate reasoning steps (CoT trajectories) while preserving reasoning skills&lt;/li&gt;&lt;li&gt;Demonstrates effectiveness on models like DeepSeek-R1-Distill-LLaMA-8B and DeepSeek-R1-Distill-Qwen-14B&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Changsheng Wang', 'Chongyu Fan', 'Yihua Zhang', 'Jinghan Jia', 'Dennis Wei', 'Parikshit Ram', 'Nathalie Baracaldo', 'Sijia Liu']&lt;/li&gt;&lt;li&gt;Tags: ['data poisoning', 'privacy attacks', 'alignment', 'robustness', 'safety evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.12963</guid><pubDate>Tue, 14 Oct 2025 00:00:00 +0000</pubDate></item><item><title>CodeCrash: Exposing LLM Fragility to Misleading Natural Language in Code Reasoning</title><link>https://arxiv.org/abs/2504.14119</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces CodeCrash, a stress-testing framework for evaluating LLM robustness in code reasoning under misleading NL contexts.&lt;/li&gt;&lt;li&gt;Finds that LLMs over-rely on NL cues, leading to significant performance degradation.&lt;/li&gt;&lt;li&gt;Identifies Reasoning Collapse, a phenomenon where incorrect hints cause excessive token consumption and cognitive dissonance.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Man Ho Lam', 'Chaozheng Wang', 'Jen-tse Huang', 'Michael R. Lyu']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'adversarial prompting', 'robustness', 'safety evaluation', 'code reasoning']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2504.14119</guid><pubDate>Tue, 14 Oct 2025 00:00:00 +0000</pubDate></item><item><title>GI-NAS: Boosting Gradient Inversion Attacks Through Adaptive Neural Architecture Search</title><link>https://arxiv.org/abs/2405.20725</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces GI-NAS for gradient inversion attacks in federated learning&lt;/li&gt;&lt;li&gt;Adapts neural architecture search to improve attack performance&lt;/li&gt;&lt;li&gt;Works without explicit prior knowledge&lt;/li&gt;&lt;li&gt;Demonstrates high-fidelity data reconstruction&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Wenbo Yu', 'Hao Fang', 'Bin Chen', 'Xiaohang Sui', 'Chuan Chen', 'Hao Wu', 'Shu-Tao Xia', 'Ke Xu']&lt;/li&gt;&lt;li&gt;Tags: ['privacy attacks', 'federated learning', 'gradient inversion', 'neural architecture search']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2405.20725</guid><pubDate>Tue, 14 Oct 2025 00:00:00 +0000</pubDate></item><item><title>DeAL: Decoding-time Alignment for Large Language Models</title><link>https://arxiv.org/abs/2402.06147</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes DeAL, a decoding-time alignment framework for LLMs&lt;/li&gt;&lt;li&gt;Allows customization of reward functions&lt;/li&gt;&lt;li&gt;Improves adherence to alignment objectives like harmlessness&lt;/li&gt;&lt;li&gt;Complementary to existing alignment methods like RLHF&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['James Y. Huang', 'Sailik Sengupta', 'Daniele Bonadiman', 'Yi-An Lai', 'Arshit Gupta', 'Nikolaos Pappas', 'Saab Mansour', 'Katrin Kirchhoff', 'Dan Roth']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'safety', 'red teaming', 'jailbreaking', 'LLM']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2402.06147</guid><pubDate>Tue, 14 Oct 2025 00:00:00 +0000</pubDate></item><item><title>PACEbench: A Framework for Evaluating Practical AI Cyber-Exploitation Capabilities</title><link>https://arxiv.org/abs/2510.11688</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces PACEbench, a benchmark for evaluating LLMs' cyber-exploitation capabilities&lt;/li&gt;&lt;li&gt;Includes four scenarios with varying complexity and defense levels&lt;/li&gt;&lt;li&gt;Proposes PACEagent for multi-phase cyber tasks&lt;/li&gt;&lt;li&gt;Tests seven LLMs, showing they struggle with complex scenarios and defenses&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Zicheng Liu', 'Lige Huang', 'Jie Zhang', 'Dongrui Liu', 'Yuan Tian', 'Jing Shao']&lt;/li&gt;&lt;li&gt;Tags: ['red teaming', 'LLM security', 'benchmarking', 'cybersecurity', 'adversarial evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.11688</guid><pubDate>Tue, 14 Oct 2025 00:00:00 +0000</pubDate></item><item><title>KnowRL: Teaching Language Models to Know What They Know</title><link>https://arxiv.org/abs/2510.11407</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces KnowRL framework to improve LLM self-knowledge&lt;/li&gt;&lt;li&gt;Uses introspection and consensus-based rewarding&lt;/li&gt;&lt;li&gt;Aims to enhance safety and reliability&lt;/li&gt;&lt;li&gt;Validated on LLaMA-3.1-8B and Qwen-2.5-7B&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Sahil Kale', 'Devendra Singh Dhami']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'safety evaluation', 'robustness', 'self-improvement']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.11407</guid><pubDate>Tue, 14 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Living Off the LLM: How LLMs Will Change Adversary Tactics</title><link>https://arxiv.org/abs/2510.11398</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Discusses how LLMs can be used in living off the land attacks&lt;/li&gt;&lt;li&gt;Explores security concerns with on-device LLMs&lt;/li&gt;&lt;li&gt;Suggests mitigation strategies for the security community&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Sean Oesch', 'Jack Hutchins', 'Luke Koch', 'Kevin Kurian']&lt;/li&gt;&lt;li&gt;Tags: ['red teaming', 'adversarial prompting', 'security standards']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.11398</guid><pubDate>Tue, 14 Oct 2025 00:00:00 +0000</pubDate></item><item><title>ENIGMA: The Geometry of Reasoning and Alignment in Large-Language Models</title><link>https://arxiv.org/abs/2510.11278</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;ENIGMA is a new LLM training approach focusing on reasoning, alignment, and robustness.&lt;/li&gt;&lt;li&gt;Uses GRPO, SAMI-style InfoNCE, and Sinkhorn regularization.&lt;/li&gt;&lt;li&gt;Introduces Sufficiency Index (SI) for principle selection.&lt;/li&gt;&lt;li&gt;Experiments show improved performance and training dynamics.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Gareth Seneque', 'Lap-Hang Ho', 'Nafise Erfanian Saeedi', 'Jeffrey Molendijk', 'Ariel Kupermann', 'Tim Elson']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'robustness', 'safety evaluation', 'model training']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.11278</guid><pubDate>Tue, 14 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Attacks by Content: Automated Fact-checking is an AI Security Issue</title><link>https://arxiv.org/abs/2510.11238</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces 'attacks by content' where adversaries manipulate AI agents by providing false or misleading information instead of prompt injection.&lt;/li&gt;&lt;li&gt;Argues that existing defenses against prompt injection are ineffective against content-based attacks.&lt;/li&gt;&lt;li&gt;Proposes using automated fact-checking as a defense mechanism for AI agents to evaluate information.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Michael Schlichtkrull']&lt;/li&gt;&lt;li&gt;Tags: ['AI security', 'adversarial attacks', 'fact-checking', 'content manipulation', 'defense mechanisms']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.11238</guid><pubDate>Tue, 14 Oct 2025 00:00:00 +0000</pubDate></item><item><title>RAG-Pull: Imperceptible Attacks on RAG Systems for Code Generation</title><link>https://arxiv.org/abs/2510.11195</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces RAG-Pull, a black-box attack on RAG systems for code generation&lt;/li&gt;&lt;li&gt;Uses hidden UTF characters to redirect code retrieval towards malicious snippets&lt;/li&gt;&lt;li&gt;Achieves near-perfect success with combined query and code perturbations&lt;/li&gt;&lt;li&gt;Creates vulnerabilities like remote code execution and SQL injection&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Vasilije Stambolic', 'Aritra Dhar', 'Lukas Cavigelli']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial prompting', 'data poisoning', 'security', 'red teaming', 'LLM']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.11195</guid><pubDate>Tue, 14 Oct 2025 00:00:00 +0000</pubDate></item><item><title>A Vision for Access Control in LLM-based Agent Systems</title><link>https://arxiv.org/abs/2510.11108</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces Agent Access Control (AAC) framework for LLM-based agents&lt;/li&gt;&lt;li&gt;Focuses on dynamic, context-aware information flow governance&lt;/li&gt;&lt;li&gt;Proposes multi-dimensional evaluation and adaptive responses&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Position&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Xinfeng Li', 'Dong Huang', 'Jie Li', 'Hongyi Cai', 'Zhenhong Zhou', 'Wei Dong', 'XiaoFeng Wang', 'Yang Liu']&lt;/li&gt;&lt;li&gt;Tags: ['AI safety', 'access control', 'LLM agents', 'context-aware', 'information flow']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.11108</guid><pubDate>Tue, 14 Oct 2025 00:00:00 +0000</pubDate></item><item><title>DeepResearchGuard: Deep Research with Open-Domain Evaluation and Multi-Stage Guardrails for Safety</title><link>https://arxiv.org/abs/2510.10994</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces DEEPRESEARCHGUARD, a framework with multi-stage guardrails for safety in deep research using LLMs.&lt;/li&gt;&lt;li&gt;Includes open-domain evaluation of references and reports across multiple safety metrics.&lt;/li&gt;&lt;li&gt;Presents DRSAFEBENCH, a new benchmark for evaluating deep research safety.&lt;/li&gt;&lt;li&gt;Evaluates performance on various LLMs, showing improved defense success and reduced over-refusal rates.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Wei-Chieh Huang', 'Henry Peng Zou', 'Yaozu Wu', 'Dongyuan Li', 'Yankai Chen', 'Weizhi Zhang', 'Yangning Li', 'Angelo Zangari', 'Jizhou Guo', 'Chunyu Miao', 'Liancheng Fang', 'Langzhou He', 'Renhe Jiang', 'Philip S. Yu']&lt;/li&gt;&lt;li&gt;Tags: ['LLM safety', 'red teaming', 'safety evaluation', 'model robustness', 'benchmarking']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.10994</guid><pubDate>Tue, 14 Oct 2025 00:00:00 +0000</pubDate></item><item><title>DITTO: A Spoofing Attack Framework on Watermarked LLMs via Knowledge Distillation</title><link>https://arxiv.org/abs/2510.10987</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces a spoofing attack framework (DITTO) on watermarked LLMs using knowledge distillation&lt;/li&gt;&lt;li&gt;Exploits watermark radioactivity to replicate victim model's watermark in malicious content&lt;/li&gt;&lt;li&gt;Highlights security gap in text authorship verification via watermarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Hyeseon Ahn', 'Shinwoo Park', 'Yo-Sub Han']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial prompting', 'model extraction', 'privacy attacks', 'security standards', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.10987</guid><pubDate>Tue, 14 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Catch-Only-One: Non-Transferable Examples for Model-Specific Authorization</title><link>https://arxiv.org/abs/2510.10982</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces non-transferable examples (NEs) for model-specific authorization&lt;/li&gt;&lt;li&gt;Uses input recoding in a model-specific subspace to preserve utility while blocking unauthorized models&lt;/li&gt;&lt;li&gt;Provides formal bounds and empirical results showing performance retention for authorized models and degradation for others&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Zihan Wang', 'Zhiyong Ma', 'Zhongkui Ma', 'Shuofeng Liu', 'Akide Liu', 'Derui Wang', 'Minhui Xue', 'Guangdong Bai']&lt;/li&gt;&lt;li&gt;Tags: ['security', 'model extraction', 'data poisoning', 'privacy attacks', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.10982</guid><pubDate>Tue, 14 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Judge Before Answer: Can MLLM Discern the False Premise in Question?</title><link>https://arxiv.org/abs/2510.10965</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces a benchmark for false premise recognition in MLLMs&lt;/li&gt;&lt;li&gt;Proposes a recognition enhancement framework&lt;/li&gt;&lt;li&gt;Evaluates robustness improvements&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jidong Li', 'Lingyong Fang', 'Haodong Zhao', 'Sufeng Duan', 'Gongshen Liu']&lt;/li&gt;&lt;li&gt;Tags: ['robustness', 'safety evaluation', 'multimodal']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.10965</guid><pubDate>Tue, 14 Oct 2025 00:00:00 +0000</pubDate></item><item><title>TabVLA: Targeted Backdoor Attacks on Vision-Language-Action Models</title><link>https://arxiv.org/abs/2510.10932</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces TabVLA, a framework for targeted backdoor attacks on Vision-Language-Action (VLA) models&lt;/li&gt;&lt;li&gt;Explores two threat models: input-stream editing and in-scene triggering&lt;/li&gt;&lt;li&gt;Demonstrates high vulnerability of VLA models to targeted backdoors with minimal poisoning&lt;/li&gt;&lt;li&gt;Investigates a detection-based defense using latent visual trigger reconstruction&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Zonghuan Xu', 'Xiang Zheng', 'Xingjun Ma', 'Yu-Gang Jiang']&lt;/li&gt;&lt;li&gt;Tags: ['backdoor attacks', 'VLA models', 'targeted attacks', 'adversarial actions', 'detection defense']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.10932</guid><pubDate>Tue, 14 Oct 2025 00:00:00 +0000</pubDate></item><item><title>SASER: Stego attacks on open-source LLMs</title><link>https://arxiv.org/abs/2510.10486</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces SASER, a stego attack on open-source LLMs&lt;/li&gt;&lt;li&gt;Addresses vulnerability in model sharing phase&lt;/li&gt;&lt;li&gt;Enhances robustness against quantization&lt;/li&gt;&lt;li&gt;Outperforms existing stego attacks in stealth and success rate&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Ming Tan', 'Wei Li', 'Hu Tao', 'Hailong Ma', 'Aodi Liu', 'Qian Chen', 'Zilong Wang']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial attacks', 'steganography', 'model extraction', 'quantization resistance', 'open-source LLMs']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.10486</guid><pubDate>Tue, 14 Oct 2025 00:00:00 +0000</pubDate></item><item><title>RefusalBench: Generative Evaluation of Selective Refusal in Grounded Language Models</title><link>https://arxiv.org/abs/2510.10390</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces RefusalBench for evaluating selective refusal in RAG systems&lt;/li&gt;&lt;li&gt;Highlights models' struggles with refusal accuracy in multi-document tasks&lt;/li&gt;&lt;li&gt;Uses generative methodology with controlled perturbations&lt;/li&gt;&lt;li&gt;Releases two benchmarks and the generation framework&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Aashiq Muhamed', 'Leonardo F. R. Ribeiro', 'Markus Dreyer', 'Virginia Smith', 'Mona T. Diab']&lt;/li&gt;&lt;li&gt;Tags: ['safety evaluation', 'red teaming', 'alignment', 'robustness', 'benchmarks']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.10390</guid><pubDate>Tue, 14 Oct 2025 00:00:00 +0000</pubDate></item><item><title>ArtPerception: ASCII Art-based Jailbreak on LLMs with Recognition Pre-test</title><link>https://arxiv.org/abs/2510.10281</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces ArtPerception, a black-box jailbreak framework using ASCII art&lt;/li&gt;&lt;li&gt;Two-phase approach with pre-test for ASCII art recognition parameters&lt;/li&gt;&lt;li&gt;Modified Levenshtein Distance (MLD) metric for evaluation&lt;/li&gt;&lt;li&gt;Tested on SOTA LLMs and commercial models, including GPT-4o, Claude, DeepSeek&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Guan-Yan Yang', 'Tzu-Yu Cheng', 'Ya-Wen Teng', 'Farn Wanga', 'Kuo-Hui Yeh']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'jailbreaking', 'ASCII art', 'Modified Levenshtein Distance', 'model extraction']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.10281</guid><pubDate>Tue, 14 Oct 2025 00:00:00 +0000</pubDate></item><item><title>MetaBreak: Jailbreaking Online LLM Services via Special Token Manipulation</title><link>https://arxiv.org/abs/2510.10271</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces MetaBreak, a method to jailbreak online LLM services using special token manipulation&lt;/li&gt;&lt;li&gt;Demonstrates four attack primitives that bypass safety alignment and content moderation&lt;/li&gt;&lt;li&gt;Evaluates effectiveness against commercial platforms and SOTA solutions&lt;/li&gt;&lt;li&gt;Shows synergy with existing prompt engineering methods&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Wentian Zhu', 'Zhen Xiang', 'Wei Niu', 'Le Guan']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'jailbreaking', 'prompt injection', 'adversarial prompting', 'model extraction']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.10271</guid><pubDate>Tue, 14 Oct 2025 00:00:00 +0000</pubDate></item><item><title>DiffHeads: Differential Analysis and Inference-Time Masking of Bias Heads in Large Language Models</title><link>https://arxiv.org/abs/2510.10142</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;DiffHeads framework for debiasing LLMs&lt;/li&gt;&lt;li&gt;Compares DA and CoT prompting strategies&lt;/li&gt;&lt;li&gt;Identifies and masks bias heads in attention layers&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Tingxu Han', 'Wei Song', 'Ziqi Ding', 'Ziming Li', 'Chunrong Fang', 'Yuekang Li', 'Dongfang Liu', 'Zhenyu Chen', 'Zhenting Wang']&lt;/li&gt;&lt;li&gt;Tags: ['safety', 'bias', 'prompting', 'attention heads', 'debiasing']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.10142</guid><pubDate>Tue, 14 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Pharmacist: Safety Alignment Data Curation for Large Language Models against Harmful Fine-tuning</title><link>https://arxiv.org/abs/2510.10085</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Pharmacist is a data curation method for safety alignment in LLMs&lt;/li&gt;&lt;li&gt;Aims to select high-quality, safety-critical data from alignment datasets&lt;/li&gt;&lt;li&gt;Improves defense against harmful fine-tuning when integrated with existing methods&lt;/li&gt;&lt;li&gt;Reduces training time while enhancing performance&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Guozhi Liu', 'Qi Mu', 'Tiansheng Huang', 'Xinhua Wang', 'Li Shen', 'Weiwei Lin', 'Zhang Li']&lt;/li&gt;&lt;li&gt;Tags: ['safety alignment', 'data curation', 'harmful fine-tuning', 'defense enhancement', 'training efficiency']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.10085</guid><pubDate>Tue, 14 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Building a Foundational Guardrail for General Agentic Systems via Synthetic Data</title><link>https://arxiv.org/abs/2510.09781</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces AuraGen for generating synthetic data with labeled risks for pre-execution safety&lt;/li&gt;&lt;li&gt;Proposes Safiron, a foundational guardrail model combining a cross-planner adapter and guardian model&lt;/li&gt;&lt;li&gt;Releases Pre-Exec Bench benchmark for evaluating pre-execution safety across different planners&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yue Huang', 'Hang Hua', 'Yujun Zhou', 'Pengcheng Jing', 'Manish Nagireddy', 'Inkit Padhi', 'Greta Dolcetti', 'Zhangchen Xu', 'Subhajit Chaudhury', 'Ambrish Rawat', 'Liubov Nedoshivina', 'Pin-Yu Chen', 'Prasanna Sattigeri', 'Xiangliang Zhang']&lt;/li&gt;&lt;li&gt;Tags: ['safety evaluation', 'guardrails', 'synthetic data', 'pre-execution safety', 'risk detection']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.09781</guid><pubDate>Tue, 14 Oct 2025 00:00:00 +0000</pubDate></item><item><title>All Code, No Thought: Current Language Models Struggle to Reason in Ciphered Language</title><link>https://arxiv.org/abs/2510.09714</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;The paper evaluates if current language models can reason in ciphered languages, which could be a way to evade chain-of-thought monitoring.&lt;/li&gt;&lt;li&gt;They test 28 ciphers across 10 models, finding that while models can translate ciphers, reasoning ability drops significantly except for well-known ones like rot13.&lt;/li&gt;&lt;li&gt;The study shows that ciphered reasoning capability is linked to the cipher's prevalence in pretraining data and improves slowly with more fine-tuning.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Shiyuan Guo', 'Henry Sleight', 'Fabien Roger']&lt;/li&gt;&lt;li&gt;Tags: ['red teaming', 'adversarial prompting', 'safety evaluation', 'alignment', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.09714</guid><pubDate>Tue, 14 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Group-Adaptive Adversarial Learning for Robust Fake News Detection Against Malicious Comments</title><link>https://arxiv.org/abs/2510.09712</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Evaluates fake news detectors against adversarial comments&lt;/li&gt;&lt;li&gt;Introduces group-adaptive adversarial training&lt;/li&gt;&lt;li&gt;Uses LLMs to generate category-specific attacks&lt;/li&gt;&lt;li&gt;Applies adaptive sampling for robustness&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Zhao Tong', 'Chunlin Gong', 'Yimeng Gu', 'Haichao Shi', 'Qiang Liu', 'Shu Wu', 'Xiao-Yu Zhang']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial training', 'fake news detection', 'LLM red teaming', 'robustness', 'security']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.09712</guid><pubDate>Tue, 14 Oct 2025 00:00:00 +0000</pubDate></item><item><title>SeCon-RAG: A Two-Stage Semantic Filtering and Conflict-Free Framework for Trustworthy RAG</title><link>https://arxiv.org/abs/2510.09710</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a two-stage framework (SeCon-RAG) for trustworthy RAG systems&lt;/li&gt;&lt;li&gt;First stage uses semantic and cluster-based filtering with EIRE to select relevant documents&lt;/li&gt;&lt;li&gt;Second stage applies conflict-aware filtering to detect inconsistencies&lt;/li&gt;&lt;li&gt;Aims to prevent corpus poisoning and contamination attacks while preserving useful knowledge&lt;/li&gt;&lt;li&gt;Shows improved robustness and trustworthiness in experiments&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Xiaonan Si', 'Meilin Zhu', 'Simeng Qin', 'Lijia Yu', 'Lijun Zhang', 'Shuaitong Liu', 'Xinfeng Li', 'Ranjie Duan', 'Yang Liu', 'Xiaojun Jia']&lt;/li&gt;&lt;li&gt;Tags: ['RAG security', 'corpus poisoning', 'conflict detection', 'semantic filtering', 'adversarial attacks']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.09710</guid><pubDate>Tue, 14 Oct 2025 00:00:00 +0000</pubDate></item><item><title>VisualDAN: Exposing Vulnerabilities in VLMs with Visual-Driven DAN Commands</title><link>https://arxiv.org/abs/2510.09699</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces VisualDAN, an adversarial image-based attack on VLMs&lt;/li&gt;&lt;li&gt;Uses DAN-style commands embedded in images to bypass model safeguards&lt;/li&gt;&lt;li&gt;Demonstrates vulnerability to image-based jailbreak attacks&lt;/li&gt;&lt;li&gt;Tests on multiple VLMs like MiniGPT-4, InstructBLIP, showing effectiveness&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Aofan Liu', 'Lulu Tang']&lt;/li&gt;&lt;li&gt;Tags: ['red teaming', 'jailbreaking', 'adversarial prompting', 'multimodal', 'VLM security']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.09699</guid><pubDate>Tue, 14 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Kelp: A Streaming Safeguard for Large Models via Latent Dynamics-Guided Risk Detection</title><link>https://arxiv.org/abs/2510.09694</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Kelp is a streaming safeguard framework for large models using latent dynamics-guided risk detection.&lt;/li&gt;&lt;li&gt;Introduces SLD head for real-time risk prediction during generation.&lt;/li&gt;&lt;li&gt;Uses ATC loss to enforce temporal consistency in harm predictions.&lt;/li&gt;&lt;li&gt;Includes StreamGuardBench for evaluating streaming guardrails.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Xiaodan Li', 'Mengjie Wu', 'Yao Zhu', 'Yunna Lv', 'YueFeng Chen', 'Cen Chen', 'Jianmei Guo', 'Hui Xue']&lt;/li&gt;&lt;li&gt;Tags: ['safety', 'real-time detection', 'guardrails', 'benchmarking', 'risk detection']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.09694</guid><pubDate>Tue, 14 Oct 2025 00:00:00 +0000</pubDate></item><item><title>CREST-Search: Comprehensive Red-teaming for Evaluating Safety Threats in Large Language Models Powered by Web Search</title><link>https://arxiv.org/abs/2510.09689</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces CREST-Search, a framework for red teaming LLMs with web search integration&lt;/li&gt;&lt;li&gt;Addresses safety risks from adversarial prompts and untrusted sources&lt;/li&gt;&lt;li&gt;Uses in-context learning and iterative feedback to generate adversarial queries&lt;/li&gt;&lt;li&gt;Constructs WebSearch-Harm dataset for training red-teaming agents&lt;/li&gt;&lt;li&gt;Demonstrates ability to bypass safety filters in web-augmented LLMs&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Haoran Ou', 'Kangjie Chen', 'Xingshuo Han', 'Gelei Deng', 'Jie Zhang', 'Han Qiu', 'Tianwei Zhang']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'adversarial prompting', 'web search integration', 'safety evaluation', 'dataset construction']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.09689</guid><pubDate>Tue, 14 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Fortifying LLM-Based Code Generation with Graph-Based Reasoning on Secure Coding Practices</title><link>https://arxiv.org/abs/2510.09682</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces GRASP, a graph-based reasoning approach to fortify LLM-based code generation with secure coding practices.&lt;/li&gt;&lt;li&gt;Uses an SCP graph to capture dependencies and relationships between secure coding practices.&lt;/li&gt;&lt;li&gt;Evaluates security rates across multiple LLMs, showing improvements especially for zero-day vulnerabilities.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Rupam Patir', 'Keyan Guo', 'Haipeng Cai', 'Hongxin Hu']&lt;/li&gt;&lt;li&gt;Tags: ['LLM security', 'code generation', 'secure coding practices', 'graph-based reasoning', 'zero-day vulnerabilities']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.09682</guid><pubDate>Tue, 14 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Data Provenance Auditing of Fine-Tuned Large Language Models with a Text-Preserving Technique</title><link>https://arxiv.org/abs/2510.09655</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces a text-preserving watermarking framework to audit if sensitive or copyrighted texts were used to fine-tune LLMs.&lt;/li&gt;&lt;li&gt;Uses invisible Unicode characters as watermarks split into cues and replies.&lt;/li&gt;&lt;li&gt;At audit time, prompts with cues check for replies in model outputs to detect memorization.&lt;/li&gt;&lt;li&gt;Provides provable false-positive-rate bounds and evaluates on open-weight LLMs.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yanming Li (PETSCRAFT)', 'Seifeddine Ghozzi (ENSTA)', "C\\'edric Eichler (PETSCRAFT)", 'Nicolas Anciaux (PETSCRAFT)', 'Alexandra Bensamoun (UC3M)', 'Lorena Gonzalez Manzano (UC3M)']&lt;/li&gt;&lt;li&gt;Tags: ['data poisoning', 'privacy attacks', 'model extraction', 'red teaming', 'safety evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.09655</guid><pubDate>Tue, 14 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Rounding-Guided Backdoor Injection in Deep Learning Model Quantization</title><link>https://arxiv.org/abs/2510.09647</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces QuRA, a backdoor attack exploiting model quantization&lt;/li&gt;&lt;li&gt;Uses weight selection and rounding optimization to embed malicious behavior&lt;/li&gt;&lt;li&gt;Achieves high attack success with minimal performance impact&lt;/li&gt;&lt;li&gt;Bypasses existing backdoor defenses&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Xiangxiang Chen', 'Peixin Zhang', 'Jun Sun', 'Wenhai Wang', 'Jingyi Wang']&lt;/li&gt;&lt;li&gt;Tags: ['model extraction', 'data poisoning', 'privacy attacks', 'alignment', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.09647</guid><pubDate>Tue, 14 Oct 2025 00:00:00 +0000</pubDate></item><item><title>AI Alignment Strategies from a Risk Perspective: Independent Safety Mechanisms or Shared Failures?</title><link>https://arxiv.org/abs/2510.11235</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Analyzes 7 alignment techniques and 7 failure modes&lt;/li&gt;&lt;li&gt;Discusses defense-in-depth strategy for AI safety&lt;/li&gt;&lt;li&gt;Evaluates correlation of failure modes across techniques&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Leonard Dung', 'Florian Mai']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'safety', 'risk', 'defense-in-depth']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.11235</guid><pubDate>Tue, 14 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Aligning Deep Implicit Preferences by Learning to Reason Defensively</title><link>https://arxiv.org/abs/2510.11194</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces CDRA for aligning LLMs with deep implicit preferences&lt;/li&gt;&lt;li&gt;Uses DeepPref benchmark for preference inference&lt;/li&gt;&lt;li&gt;Employs Pers-GenPRM for defensive reasoning&lt;/li&gt;&lt;li&gt;Applies Critique-Driven Policy Alignment for robust learning&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Peiming Li', 'Zhiyuan Hu', 'Yang Tang', 'Shiyu Li', 'Xi Chen']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'deep preferences', 'defensive reasoning', 'reward modeling', 'policy alignment']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.11194</guid><pubDate>Tue, 14 Oct 2025 00:00:00 +0000</pubDate></item><item><title>PoU: Proof-of-Use to Counter Tool-Call Hacking in DeepResearch Agents</title><link>https://arxiv.org/abs/2510.10931</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces Proof-of-Use (PoU) framework to counter tool-call hacking in RAG agents&lt;/li&gt;&lt;li&gt;Addresses mode collapse and spurious grounding issues&lt;/li&gt;&lt;li&gt;Uses RL with evidence-grounded rewards&lt;/li&gt;&lt;li&gt;Improves factual accuracy and evidence faithfulness across benchmarks&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['SHengjie Ma', 'Chenlong Deng', 'Jiaxin Mao', 'Jiadeng Huang', 'Teng Wang', 'Junjie Wu', 'Changwang Zhang', 'Jun wang']&lt;/li&gt;&lt;li&gt;Tags: ['LLM safety', 'tool-call hacking', 'reinforcement learning', 'evidence grounding', 'RAG agents']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.10931</guid><pubDate>Tue, 14 Oct 2025 00:00:00 +0000</pubDate></item><item><title>The Achilles' Heel of LLMs: How Altering a Handful of Neurons Can Cripple Language Abilities</title><link>https://arxiv.org/abs/2510.10238</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Identifies critical neurons in LLMs that, when perturbed, cause significant performance degradation&lt;/li&gt;&lt;li&gt;Disrupting these neurons can collapse a 72B-parameter model's performance&lt;/li&gt;&lt;li&gt;Critical neurons are found in outer layers, particularly in MLP down_proj components&lt;/li&gt;&lt;li&gt;Performance degradation shows sharp phase transitions&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Zixuan Qin', 'Kunlin Lyu', 'Qingchen Yu', 'Yifan Sun', 'Zhaoxin Fan']&lt;/li&gt;&lt;li&gt;Tags: ['LLM robustness', 'Adversarial attacks', 'Model security', 'Neuron perturbation', 'Safety-critical applications']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.10238</guid><pubDate>Tue, 14 Oct 2025 00:00:00 +0000</pubDate></item><item><title>RIPRAG: Hack a Black-box Retrieval-Augmented Generation Question-Answering System with Reinforcement Learning</title><link>https://arxiv.org/abs/2510.10008</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes RIPRAG, a black-box attack framework for RAG systems using RL&lt;/li&gt;&lt;li&gt;Focuses on poisoning attacks by injecting malicious documents&lt;/li&gt;&lt;li&gt;Improves attack success rate compared to baseline methods&lt;/li&gt;&lt;li&gt;Highlights security vulnerabilities in RAG-based LLMs&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Meng Xi', 'Sihan Lv', 'Yechen Jin', 'Guanjie Cheng', 'Naibo Wang', 'Ying Li', 'Jianwei Yin']&lt;/li&gt;&lt;li&gt;Tags: ['data poisoning', 'adversarial attacks', 'reinforcement learning', 'RAG systems', 'black-box attacks']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.10008</guid><pubDate>Tue, 14 Oct 2025 00:00:00 +0000</pubDate></item></channel></rss>