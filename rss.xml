<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>AI Security Paper Digest</title><link>https://kentaroh-toyoda.github.io/ai-security-paper-digest-rss/rss.xml</link><description>Curated papers on AI security from OpenAlex and ArXiv</description><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><lastBuildDate>Wed, 13 Aug 2025 22:17:12 +0000</lastBuildDate><item><title>Whispers in the Machine: Confidentiality in Agentic Systems</title><link>https://arxiv.org/abs/2402.06922</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Demonstrates confidentiality risks in agentic systems using LLMs with external tool integrations&lt;/li&gt;&lt;li&gt;Identifies two specific attack scenarios unique to these systems&lt;/li&gt;&lt;li&gt;Introduces a tool-robustness framework to measure model protection of sensitive data&lt;/li&gt;&lt;li&gt;Finds significant vulnerabilities across all tested models when integrated with external tools&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jonathan Evertz', 'Merlin Chlosta', 'Lea Sch\\"onherr', 'Thorsten Eisenhofer']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'adversarial prompting', 'security evaluation', 'data confidentiality', 'tool integration']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2402.06922</guid><pubDate>Wed, 13 Aug 2025 00:00:00 +0000</pubDate></item><item><title>Vulnerability-Aware Alignment: Mitigating Uneven Forgetting in Harmful Fine-Tuning</title><link>https://arxiv.org/abs/2506.03850</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces Vulnerability-Aware Alignment (VAA) to mitigate harmful fine-tuning (HFT) in LLMs&lt;/li&gt;&lt;li&gt;Identifies vulnerable data subsets prone to forgetting during HFT&lt;/li&gt;&lt;li&gt;Uses Group DRO with adversarial sampling and perturbations to balance learning&lt;/li&gt;&lt;li&gt;Demonstrates improved safety while preserving performance across multiple tasks&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Liang Chen', 'Xueting Han', 'Li Shen', 'Jing Bai', 'Kam-Fai Wong']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'robustness', 'safety', 'LLM', 'fine-tuning', 'data vulnerability', 'adversarial training']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.03850</guid><pubDate>Wed, 13 Aug 2025 00:00:00 +0000</pubDate></item><item><title>Few-Shot Adversarial Low-Rank Fine-Tuning of Vision-Language Models</title><link>https://arxiv.org/abs/2505.15130</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes AdvCLIP-LoRA for adversarial robustness in few-shot VLM fine-tuning&lt;/li&gt;&lt;li&gt;Uses minimax optimization with theoretical convergence guarantees&lt;/li&gt;&lt;li&gt;Empirically shows improved robustness against FGSM and PGD attacks&lt;/li&gt;&lt;li&gt;Maintains clean accuracy while enhancing security&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Sajjad Ghiasvand', 'Haniyeh Ehsani Oskouie', 'Mahnoosh Alizadeh', 'Ramtin Pedarsani']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial prompting', 'robustness', 'security', 'vision-language models', 'few-shot learning']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2505.15130</guid><pubDate>Wed, 13 Aug 2025 00:00:00 +0000</pubDate></item><item><title>Selective KV-Cache Sharing to Mitigate Timing Side-Channels in LLM Inference</title><link>https://arxiv.org/abs/2508.08438</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces SafeKV framework to mitigate timing side-channel attacks in LLM inference&lt;/li&gt;&lt;li&gt;Combines selective cache sharing with privacy detection and entropy monitoring&lt;/li&gt;&lt;li&gt;Achieves 94-97% attack mitigation while improving performance over isolation methods&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Kexin Chu', 'Zecheng Lin', 'Dawei Xiang', 'Zixu Shen', 'Jianchang Su', 'Cheng Chu', 'Yiwei Yang', 'Wenhui Zhang', 'Wenfei Wu', 'Wei Zhang']&lt;/li&gt;&lt;li&gt;Tags: ['privacy', 'side-channel attacks', 'cache management', 'LLM security', 'performance optimization']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2508.08438</guid><pubDate>Wed, 13 Aug 2025 00:00:00 +0000</pubDate></item><item><title>Fre-CW: Targeted Attack on Time Series Forecasting using Frequency Domain Loss</title><link>https://arxiv.org/abs/2508.08955</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a targeted adversarial attack on time series forecasting models using frequency domain loss&lt;/li&gt;&lt;li&gt;Adapts classification-based attack methods to time series prediction&lt;/li&gt;&lt;li&gt;Demonstrates vulnerability of current models on major datasets&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Naifu Feng', 'Lixing Chen', 'Junhua Tang', 'Hua Ding', 'Jianhua Li', 'Yang Bai']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial attacks', 'red teaming', 'robustness', 'time series', 'security']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2508.08955</guid><pubDate>Wed, 13 Aug 2025 00:00:00 +0000</pubDate></item><item><title>Exploring Cross-Stage Adversarial Transferability in Class-Incremental Continual Learning</title><link>https://arxiv.org/abs/2508.08920</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Investigates vulnerability to stage-transferred adversarial attacks in continual learning&lt;/li&gt;&lt;li&gt;Finds high susceptibility due to model similarity and robustness degradation&lt;/li&gt;&lt;li&gt;Shows existing adversarial training defenses are ineffective&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jungwoo Kim', 'Jong-Seok Lee']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial attacks', 'continual learning', 'security', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2508.08920</guid><pubDate>Wed, 13 Aug 2025 00:00:00 +0000</pubDate></item><item><title>Attacks and Defenses Against LLM Fingerprinting</title><link>https://arxiv.org/abs/2508.09021</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Presents an RL-based attack methodology for LLM fingerprinting with optimized query selection&lt;/li&gt;&lt;li&gt;Introduces a semantic-preserving output filtering defense using a secondary LLM&lt;/li&gt;&lt;li&gt;Reduces fingerprinting accuracy while maintaining output quality&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Kevin Kurian', 'Ethan Holland', 'Sean Oesch']&lt;/li&gt;&lt;li&gt;Tags: ['fingerprinting', 'reinforcement_learning', 'adversarial_attacks', 'privacy', 'model_defense']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2508.09021</guid><pubDate>Wed, 13 Aug 2025 00:00:00 +0000</pubDate></item><item><title>Securing Educational LLMs: A Generalised Taxonomy of Attacks on LLMs and DREAD Risk Assessment</title><link>https://arxiv.org/abs/2508.08629</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Presents a taxonomy of 50 LLM attacks categorized into model and infrastructure targets&lt;/li&gt;&lt;li&gt;Applies DREAD risk assessment framework to evaluate attack severity in educational contexts&lt;/li&gt;&lt;li&gt;Identifies critical attacks: token smuggling, adversarial prompts, direct injection, multi-step jailbreak&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Farzana Zahid', 'Anjalika Sewwandi', 'Lee Brandon', 'Vimal Kumar', 'Roopak Sinha']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'jailbreaking', 'adversarial prompting', 'taxonomy', 'DREAD risk assessment', 'educational LLMs']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2508.08629</guid><pubDate>Wed, 13 Aug 2025 00:00:00 +0000</pubDate></item><item><title>Generative AI for Critical Infrastructure in Smart Grids: A Unified Framework for Synthetic Data Generation and Anomaly Detection</title><link>https://arxiv.org/abs/2508.08593</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes AATM for generating synthetic GOOSE datasets&lt;/li&gt;&lt;li&gt;Implements GenAI-based ADS with ToD processes&lt;/li&gt;&lt;li&gt;Compares GenAI and ML-based ADS performance&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Aydin Zaboli', 'Junho Hong']&lt;/li&gt;&lt;li&gt;Tags: ['red teaming', 'robustness', 'safety evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2508.08593</guid><pubDate>Wed, 13 Aug 2025 00:00:00 +0000</pubDate></item><item><title>AI Security Map: Holistic Organization of AI Security Technologies and Impacts on Stakeholders</title><link>https://arxiv.org/abs/2508.08583</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Develops a holistic AI security map organizing elements and impacts&lt;/li&gt;&lt;li&gt;Categorizes elements into Information System Aspect (ISA) and External Influence Aspect (EIA)&lt;/li&gt;&lt;li&gt;Identifies negative impacts on systems and stakeholders&lt;/li&gt;&lt;li&gt;Provides recommendations for future AI security research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Survey&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Hiroya Kato', 'Kentaro Kita', 'Kento Hasegawa', 'Seira Hidano']&lt;/li&gt;&lt;li&gt;Tags: ['AI security', 'stakeholder impacts', 'risk assessment', 'security standards', 'safety evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2508.08583</guid><pubDate>Wed, 13 Aug 2025 00:00:00 +0000</pubDate></item><item><title>VISOR: Visual Input-based Steering for Output Redirection in Vision-Language Models</title><link>https://arxiv.org/abs/2508.08521</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces VISOR, a method for controlling VLMs via optimized visual inputs&lt;/li&gt;&lt;li&gt;Validates on alignment tasks (refusal, sycophancy, survival) with significant shifts&lt;/li&gt;&lt;li&gt;Highlights security vulnerability through visual adversarial manipulation&lt;/li&gt;&lt;li&gt;Maintains high performance on unrelated tasks while enabling bidirectional control&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Mansi Phute (Georgia Tech)', 'Ravikumar Balakrishnan (HiddenLayer)']&lt;/li&gt;&lt;li&gt;Tags: ['red teaming', 'adversarial prompting', 'model manipulation', 'security vulnerability', 'multimodal models']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2508.08521</guid><pubDate>Wed, 13 Aug 2025 00:00:00 +0000</pubDate></item><item><title>Activation Steering for Bias Mitigation: An Interpretable Approach to Safer LLMs</title><link>https://arxiv.org/abs/2508.09019</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces activation steering for bias mitigation in LLMs&lt;/li&gt;&lt;li&gt;Uses mechanistic interpretability to detect and steer away from biased content&lt;/li&gt;&lt;li&gt;Trains probes on internal activations to identify bias&lt;/li&gt;&lt;li&gt;Demonstrates efficacy on gpt2-large&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Shivam Dubey']&lt;/li&gt;&lt;li&gt;Tags: ['safety', 'alignment', 'bias mitigation', 'activation steering']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2508.09019</guid><pubDate>Wed, 13 Aug 2025 00:00:00 +0000</pubDate></item></channel></rss>