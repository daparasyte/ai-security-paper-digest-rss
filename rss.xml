<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>AI Security Paper Digest</title><link>https://kentaroh-toyoda.github.io/ai-security-paper-digest-rss/rss.xml</link><description>Curated papers on AI security from ArXiv and ACL</description><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><lastBuildDate>Thu, 23 Oct 2025 22:25:06 +0000</lastBuildDate><item><title>RADAR: A Risk-Aware Dynamic Multi-Agent Framework for LLM Safety Evaluation via Role-Specialized Collaboration</title><link>https://arxiv.org/abs/2509.25271</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces RADAR, a multi-agent framework for LLM safety evaluation&lt;/li&gt;&lt;li&gt;Decomposes risk into explicit, implicit, and non-risk subspaces&lt;/li&gt;&lt;li&gt;Uses role-specialized agents and dynamic updates for risk concept evolution&lt;/li&gt;&lt;li&gt;Validated with 800 challenging cases and public benchmarks&lt;/li&gt;&lt;li&gt;Shows significant improvement in risk identification accuracy&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Xiuyuan Chen', 'Jian Zhao', 'Yuchen Yuan', 'Tianle Zhang', 'Huilin Zhou', 'Zheng Zhu', 'Ping Hu', 'Linghe Kong', 'Chi Zhang', 'Weiran Huang', 'Xuelong Li']&lt;/li&gt;&lt;li&gt;Tags: ['LLM safety evaluation', 'Multi-agent systems', 'Risk concept decomposition', 'Dynamic risk concept updates', 'Role-specialized collaboration']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.25271</guid><pubDate>Thu, 23 Oct 2025 00:00:00 +0000</pubDate></item><item><title>FeatureFool: Zero-Query Fooling of Video Models via Feature Map</title><link>https://arxiv.org/abs/2510.18362</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes FeatureFool, a zero-query black-box attack on video models using feature maps&lt;/li&gt;&lt;li&gt;Achieves high success rates without queries&lt;/li&gt;&lt;li&gt;Targets both traditional classifiers and Video-LLMs&lt;/li&gt;&lt;li&gt;Generates imperceptible adversarial videos&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Duoxun Tang', 'Xi Xiao', 'Guangwu Hu', 'Kangkang Sun', 'Xiao Yang', 'Dongyang Chen', 'Qing Li', 'Yongjie Yin', 'Jiyao Wang']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial attacks', 'video models', 'zero-query', 'black-box', 'feature maps']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.18362</guid><pubDate>Thu, 23 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Can You Trust What You See? Alpha Channel No-Box Attacks on Video Object Detection</title><link>https://arxiv.org/abs/2510.19574</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces Î±-Cloak, a no-box adversarial attack on video object detection using the alpha channel&lt;/li&gt;&lt;li&gt;Exploits alpha channel to fuse malicious and benign videos, creating stealthy attacks&lt;/li&gt;&lt;li&gt;Evaluates against multiple models including LLMs, achieving 100% success rate&lt;/li&gt;&lt;li&gt;Highlights vulnerability in video perception systems and need for alpha channel defenses&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Ariana Yi', 'Ce Zhou', 'Liyang Xiao', 'Qiben Yan']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial attacks', 'video object detection', 'alpha channel', 'no-box attacks', 'security']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.19574</guid><pubDate>Thu, 23 Oct 2025 00:00:00 +0000</pubDate></item><item><title>PLAGUE: Plug-and-play framework for Lifelong Adaptive Generation of Multi-turn Exploits</title><link>https://arxiv.org/abs/2510.17947</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;PLAGUE is a framework for multi-turn jailbreaking attacks on LLMs&lt;/li&gt;&lt;li&gt;Improves attack success rates by 30% across leading models&lt;/li&gt;&lt;li&gt;Specifically effective against OpenAI's o3 and Claude's Opus 4.1&lt;/li&gt;&lt;li&gt;Focuses on plan initialization, context optimization, and lifelong learning&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Neeladri Bhuiya', 'Madhav Aggarwal', 'Diptanshu Purwar']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'jailbreaking', 'multi-turn attacks', 'adversarial prompting', 'safety evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.17947</guid><pubDate>Thu, 23 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Robustness Assessment and Enhancement of Text Watermarking for Google's SynthID</title><link>https://arxiv.org/abs/2508.20228</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Assesses robustness of SynthID-Text watermarking against meaning-preserving attacks&lt;/li&gt;&lt;li&gt;Proposes SynGuard framework combining lexical and semantic watermarking&lt;/li&gt;&lt;li&gt;Improves watermark recovery by 11.1% in F1 score&lt;/li&gt;&lt;li&gt;Code and datasets are publicly available&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Xia Han', 'Qi Li', 'Jianbing Ni', 'Mohammad Zulkernine']&lt;/li&gt;&lt;li&gt;Tags: ['watermarking', 'robustness', 'semantic alignment', 'provenance tracking', 'meaning-preserving attacks']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2508.20228</guid><pubDate>Thu, 23 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Unlearned but Not Forgotten: Data Extraction after Exact Unlearning in LLM</title><link>https://arxiv.org/abs/2505.24379</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces a data extraction attack on LLMs after exact unlearning&lt;/li&gt;&lt;li&gt;Leverages pre-unlearning model signals to guide post-unlearning extraction&lt;/li&gt;&lt;li&gt;Significant improvements in extraction rates across benchmarks&lt;/li&gt;&lt;li&gt;Highlights real-world privacy risks with exact unlearning&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Xiaoyu Wu', 'Yifei Pang', 'Terrance Liu', 'Zhiwei Steven Wu']&lt;/li&gt;&lt;li&gt;Tags: ['data poisoning', 'privacy attacks', 'model extraction', 'unlearning', 'LLM security']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2505.24379</guid><pubDate>Thu, 23 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Blackbox Model Provenance via Palimpsestic Membership Inference</title><link>https://arxiv.org/abs/2510.19796</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;This paper addresses model provenance and membership inference in blackbox models.&lt;/li&gt;&lt;li&gt;It uses palimpsestic memorization to detect if a model was trained on specific data.&lt;/li&gt;&lt;li&gt;The methods involve querying the model and analyzing text output.&lt;/li&gt;&lt;li&gt;Statistical tests are used to check correlations with training data order.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Rohith Kuditipudi', 'Jing Huang', 'Sally Zhu', 'Diyi Yang', 'Christopher Potts', 'Percy Liang']&lt;/li&gt;&lt;li&gt;Tags: ['model extraction', 'data poisoning', 'privacy attacks', 'adversarial prompting', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.19796</guid><pubDate>Thu, 23 Oct 2025 00:00:00 +0000</pubDate></item><item><title>OpenGuardrails: An Open-Source Context-Aware AI Guardrails Platform</title><link>https://arxiv.org/abs/2510.19169</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces OpenGuardrails, an open-source platform for AI guardrails&lt;/li&gt;&lt;li&gt;Protects against content safety, model manipulation, and data leakage&lt;/li&gt;&lt;li&gt;Uses a unified model for safety and manipulation detection&lt;/li&gt;&lt;li&gt;Supports deployment as a security gateway or API service&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Thomas Wang', 'Haowen Li']&lt;/li&gt;&lt;li&gt;Tags: ['red teaming', 'jailbreaking', 'prompt injection', 'model extraction', 'data poisoning', 'privacy attacks', 'alignment', 'robustness', 'safety evaluation', 'security standards']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.19169</guid><pubDate>Thu, 23 Oct 2025 00:00:00 +0000</pubDate></item><item><title>A Graph Signal Processing Framework for Hallucination Detection in Large Language Models</title><link>https://arxiv.org/abs/2510.19117</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a spectral analysis framework using graph signal processing to detect hallucinations in LLMs&lt;/li&gt;&lt;li&gt;Defines diagnostics like Dirichlet energy and spectral entropy&lt;/li&gt;&lt;li&gt;Shows different spectral signatures for various hallucination types&lt;/li&gt;&lt;li&gt;Achieves higher accuracy than perplexity-based methods&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Valentin No\\"el']&lt;/li&gt;&lt;li&gt;Tags: ['safety', 'evaluation', 'hallucination detection', 'graph signal processing']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.19117</guid><pubDate>Thu, 23 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Dynamic Evaluation for Oversensitivity in LLMs</title><link>https://arxiv.org/abs/2510.19005</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces a dynamic evaluation framework for oversensitivity in LLMs&lt;/li&gt;&lt;li&gt;Creates OVERBENCH with 450k samples from 25 models&lt;/li&gt;&lt;li&gt;Aims to continuously monitor defensive triggers&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Sophia Xiao Pu', 'Sitao Cheng', 'Xin Eric Wang', 'William Yang Wang']&lt;/li&gt;&lt;li&gt;Tags: ['safety evaluation', 'LLM red teaming', 'oversensitivity']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.19005</guid><pubDate>Thu, 23 Oct 2025 00:00:00 +0000</pubDate></item><item><title>PLAGUE: Plug-and-play framework for Lifelong Adaptive Generation of Multi-turn Exploits</title><link>https://arxiv.org/abs/2510.17947</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;PLAGUE is a framework for multi-turn jailbreaking attacks on LLMs&lt;/li&gt;&lt;li&gt;Improves attack success rates by 30% across leading models&lt;/li&gt;&lt;li&gt;Specifically effective against OpenAI's o3 and Claude's Opus 4.1&lt;/li&gt;&lt;li&gt;Focuses on plan initialization, context optimization, and lifelong learning&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Neeladri Bhuiya', 'Madhav Aggarwal', 'Diptanshu Purwar']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'jailbreaking', 'multi-turn attacks', 'adversarial prompting', 'safety evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.17947</guid><pubDate>Thu, 23 Oct 2025 00:00:00 +0000</pubDate></item><item><title>RADAR: A Risk-Aware Dynamic Multi-Agent Framework for LLM Safety Evaluation via Role-Specialized Collaboration</title><link>https://arxiv.org/abs/2509.25271</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces RADAR, a multi-agent framework for LLM safety evaluation&lt;/li&gt;&lt;li&gt;Decomposes risk into explicit, implicit, and non-risk subspaces&lt;/li&gt;&lt;li&gt;Uses role-specialized agents and dynamic updates for risk concept evolution&lt;/li&gt;&lt;li&gt;Validated with 800 challenging cases and public benchmarks&lt;/li&gt;&lt;li&gt;Shows significant improvement in risk identification accuracy&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Xiuyuan Chen', 'Jian Zhao', 'Yuchen Yuan', 'Tianle Zhang', 'Huilin Zhou', 'Zheng Zhu', 'Ping Hu', 'Linghe Kong', 'Chi Zhang', 'Weiran Huang', 'Xuelong Li']&lt;/li&gt;&lt;li&gt;Tags: ['LLM safety evaluation', 'Multi-agent systems', 'Risk concept decomposition', 'Dynamic risk concept updates', 'Role-specialized collaboration']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.25271</guid><pubDate>Thu, 23 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Are Modern Speech Enhancement Systems Vulnerable to Adversarial Attacks?</title><link>https://arxiv.org/abs/2509.21087</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Demonstrates vulnerability of speech enhancement models to adversarial attacks&lt;/li&gt;&lt;li&gt;Shows adversarial noise can change semantic meaning of enhanced speech&lt;/li&gt;&lt;li&gt;Highlights robustness of diffusion models with stochastic samplers&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Rostislav Makarov', 'Lea Sch\\"onherr', 'Timo Gerkmann']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial attacks', 'speech enhancement', 'model robustness', 'diffusion models']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.21087</guid><pubDate>Thu, 23 Oct 2025 00:00:00 +0000</pubDate></item><item><title>SEC-bench: Automated Benchmarking of LLM Agents on Real-World Software Security Tasks</title><link>https://arxiv.org/abs/2506.11791</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces SEC-bench, an automated benchmarking framework for LLM agents on real-world security tasks&lt;/li&gt;&lt;li&gt;Evaluates LLM agents on PoC generation and vulnerability patching&lt;/li&gt;&lt;li&gt;Reveals significant performance gaps in current LLM agents for security tasks&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Hwiwon Lee', 'Ziqi Zhang', 'Hanxiao Lu', 'Lingming Zhang']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'security evaluation', 'benchmarking', 'vulnerability patching', 'PoC generation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.11791</guid><pubDate>Thu, 23 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Unlearned but Not Forgotten: Data Extraction after Exact Unlearning in LLM</title><link>https://arxiv.org/abs/2505.24379</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces a data extraction attack on LLMs after exact unlearning&lt;/li&gt;&lt;li&gt;Leverages pre-unlearning model signals to guide post-unlearning extraction&lt;/li&gt;&lt;li&gt;Significant improvements in extraction rates across benchmarks&lt;/li&gt;&lt;li&gt;Highlights real-world privacy risks with exact unlearning&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Xiaoyu Wu', 'Yifei Pang', 'Terrance Liu', 'Zhiwei Steven Wu']&lt;/li&gt;&lt;li&gt;Tags: ['data poisoning', 'privacy attacks', 'model extraction', 'unlearning', 'LLM security']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2505.24379</guid><pubDate>Thu, 23 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Collaborative penetration testing suite for emerging generative AI algorithms</title><link>https://arxiv.org/abs/2510.19303</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a collaborative penetration testing suite for generative AI&lt;/li&gt;&lt;li&gt;Integrates DAST, SAST, IAST, blockchain logging, and quantum cryptography&lt;/li&gt;&lt;li&gt;Includes AI red team simulations with adversarial ML and quantum-assisted attacks&lt;/li&gt;&lt;li&gt;Reports significant reduction in vulnerabilities and high resolution efficiency&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Petar Radanliev']&lt;/li&gt;&lt;li&gt;Tags: ['red teaming', 'adversarial prompting', 'data poisoning', 'model inversion', 'quantum cryptography']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.19303</guid><pubDate>Thu, 23 Oct 2025 00:00:00 +0000</pubDate></item><item><title>HAMLOCK: HArdware-Model LOgically Combined attacK</title><link>https://arxiv.org/abs/2510.19145</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces HAMLOCK, a hardware-software combined attack on DNNs&lt;/li&gt;&lt;li&gt;Distributes attack logic across model and hardware&lt;/li&gt;&lt;li&gt;Uses minimal model changes and hardware Trojans&lt;/li&gt;&lt;li&gt;High stealth and attack success, evades current defenses&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Sanskar Amgain', 'Daniel Lobo', 'Atri Chatterjee', 'Swarup Bhunia', 'Fnu Suya']&lt;/li&gt;&lt;li&gt;Tags: ['security', 'hardware', 'backdoor', 'stealth', 'defense']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.19145</guid><pubDate>Thu, 23 Oct 2025 00:00:00 +0000</pubDate></item><item><title>A Graph Signal Processing Framework for Hallucination Detection in Large Language Models</title><link>https://arxiv.org/abs/2510.19117</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a spectral analysis framework using graph signal processing to detect hallucinations in LLMs&lt;/li&gt;&lt;li&gt;Defines diagnostics like Dirichlet energy and spectral entropy&lt;/li&gt;&lt;li&gt;Shows different spectral signatures for various hallucination types&lt;/li&gt;&lt;li&gt;Achieves higher accuracy than perplexity-based methods&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Valentin No\\"el']&lt;/li&gt;&lt;li&gt;Tags: ['safety', 'evaluation', 'hallucination detection', 'graph signal processing']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.19117</guid><pubDate>Thu, 23 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Blackbox Model Provenance via Palimpsestic Membership Inference</title><link>https://arxiv.org/abs/2510.19796</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;This paper addresses model provenance and membership inference in blackbox models.&lt;/li&gt;&lt;li&gt;It uses palimpsestic memorization to detect if a model was trained on specific data.&lt;/li&gt;&lt;li&gt;The methods involve querying the model and analyzing text output.&lt;/li&gt;&lt;li&gt;Statistical tests are used to check correlations with training data order.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Rohith Kuditipudi', 'Jing Huang', 'Sally Zhu', 'Diyi Yang', 'Christopher Potts', 'Percy Liang']&lt;/li&gt;&lt;li&gt;Tags: ['model extraction', 'data poisoning', 'privacy attacks', 'adversarial prompting', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.19796</guid><pubDate>Thu, 23 Oct 2025 00:00:00 +0000</pubDate></item><item><title>The Tail Tells All: Estimating Model-Level Membership Inference Vulnerability Without Reference Models</title><link>https://arxiv.org/abs/2510.19773</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a method to estimate model-level vulnerability to membership inference attacks without reference models&lt;/li&gt;&lt;li&gt;Uses loss distribution analysis focusing on tail behavior&lt;/li&gt;&lt;li&gt;Evaluates across multiple architectures and datasets&lt;/li&gt;&lt;li&gt;Shows potential for large-language models&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Euodia Dodd', 'Nata\\v{s}a Kr\\v{c}o', 'Igor Shilov', 'Yves-Alexandre de Montjoye']&lt;/li&gt;&lt;li&gt;Tags: ['privacy attacks', 'membership inference', 'model vulnerability', 'loss distribution']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.19773</guid><pubDate>Thu, 23 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Teaming LLMs to Detect and Mitigate Hallucinations</title><link>https://arxiv.org/abs/2510.19507</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;The paper explores using multiple LLMs with different training data and architectures to improve hallucination detection and mitigation.&lt;/li&gt;&lt;li&gt;It evaluates the consortium consistency approach across various model teams.&lt;/li&gt;&lt;li&gt;The method can lead to better performance with reduced inference costs compared to single-model methods.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Demian Till', 'John Smeaton', 'Peter Haubrick', 'Gouse Saheb', 'Florian Graef', 'David Berman']&lt;/li&gt;&lt;li&gt;Tags: ['safety', 'hallucinations', 'consistency', 'multi-model']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.19507</guid><pubDate>Thu, 23 Oct 2025 00:00:00 +0000</pubDate></item><item><title>A Concrete Roadmap towards Safety Cases based on Chain-of-Thought Monitoring</title><link>https://arxiv.org/abs/2510.19476</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a safety case based on chain-of-thought monitoring&lt;/li&gt;&lt;li&gt;Analyzes threats to monitorability like neuralese and encoded reasoning&lt;/li&gt;&lt;li&gt;Explores techniques for maintaining CoT faithfulness&lt;/li&gt;&lt;li&gt;Uses prediction markets to forecast feasibility&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Julian Schulz']&lt;/li&gt;&lt;li&gt;Tags: ['safety', 'alignment', 'monitoring', 'chain-of-thought', 'threat analysis']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.19476</guid><pubDate>Thu, 23 Oct 2025 00:00:00 +0000</pubDate></item><item><title>A New Type of Adversarial Examples</title><link>https://arxiv.org/abs/2510.19347</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces new adversarial example methods (NI-FGSM, NI-FGM, NMI-FGSM, NMI-FGM)&lt;/li&gt;&lt;li&gt;These methods create examples very different from originals but same model output&lt;/li&gt;&lt;li&gt;Shows adversarial examples spread across sample space&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Xingyang Nie', 'Guojie Xiao', 'Su Pan', 'Biao Wang', 'Huilin Ge', 'Tao Fang']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial examples', 'security', 'model robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.19347</guid><pubDate>Thu, 23 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Subliminal Corruption: Mechanisms, Thresholds, and Interpretability</title><link>https://arxiv.org/abs/2510.19152</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Investigates subliminal corruption in AI systems&lt;/li&gt;&lt;li&gt;Uses teacher-student setup with GPT-2&lt;/li&gt;&lt;li&gt;Finds sharp phase transition in alignment failure&lt;/li&gt;&lt;li&gt;Highlights need for new safety protocols&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Reya Vir', 'Sarvesh Bhatnagar']&lt;/li&gt;&lt;li&gt;Tags: ['data poisoning', 'alignment', 'safety evaluation', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.19152</guid><pubDate>Thu, 23 Oct 2025 00:00:00 +0000</pubDate></item><item><title>POLAR: Policy-based Layerwise Reinforcement Learning Method for Stealthy Backdoor Attacks in Federated Learning</title><link>https://arxiv.org/abs/2510.19056</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes POLAR, a policy-based RL method for stealthy backdoor attacks in federated learning&lt;/li&gt;&lt;li&gt;Focuses on selecting critical layers for poisoning to maintain stealth&lt;/li&gt;&lt;li&gt;Introduces regularization to limit modified layers&lt;/li&gt;&lt;li&gt;Outperforms existing attacks against SOTA defenses&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Kuai Yu', 'Xiaoyu Wu', 'Peishen Yan', 'Qingqian Yang', 'Linshan Jiang', 'Hao Wang', 'Yang Hua', 'Tao Song', 'Haibing Guan']&lt;/li&gt;&lt;li&gt;Tags: ['backdoor attacks', 'federated learning', 'reinforcement learning', 'stealthiness', 'layer selection']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.19056</guid><pubDate>Thu, 23 Oct 2025 00:00:00 +0000</pubDate></item><item><title>PLAGUE: Plug-and-play framework for Lifelong Adaptive Generation of Multi-turn Exploits</title><link>https://arxiv.org/abs/2510.17947</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;PLAGUE is a framework for multi-turn jailbreaking attacks on LLMs&lt;/li&gt;&lt;li&gt;Improves attack success rates by 30% across leading models&lt;/li&gt;&lt;li&gt;Specifically effective against OpenAI's o3 and Claude's Opus 4.1&lt;/li&gt;&lt;li&gt;Focuses on plan initialization, context optimization, and lifelong learning&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Neeladri Bhuiya', 'Madhav Aggarwal', 'Diptanshu Purwar']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'jailbreaking', 'multi-turn attacks', 'adversarial prompting', 'safety evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.17947</guid><pubDate>Thu, 23 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Unlearned but Not Forgotten: Data Extraction after Exact Unlearning in LLM</title><link>https://arxiv.org/abs/2505.24379</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces a data extraction attack on LLMs after exact unlearning&lt;/li&gt;&lt;li&gt;Leverages pre-unlearning model signals to guide post-unlearning extraction&lt;/li&gt;&lt;li&gt;Significant improvements in extraction rates across benchmarks&lt;/li&gt;&lt;li&gt;Highlights real-world privacy risks with exact unlearning&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Xiaoyu Wu', 'Yifei Pang', 'Terrance Liu', 'Zhiwei Steven Wu']&lt;/li&gt;&lt;li&gt;Tags: ['data poisoning', 'privacy attacks', 'model extraction', 'unlearning', 'LLM security']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2505.24379</guid><pubDate>Thu, 23 Oct 2025 00:00:00 +0000</pubDate></item><item><title>RADAR: A Risk-Aware Dynamic Multi-Agent Framework for LLM Safety Evaluation via Role-Specialized Collaboration</title><link>https://arxiv.org/abs/2509.25271</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces RADAR, a multi-agent framework for LLM safety evaluation&lt;/li&gt;&lt;li&gt;Decomposes risk into explicit, implicit, and non-risk subspaces&lt;/li&gt;&lt;li&gt;Uses role-specialized agents and dynamic updates for risk concept evolution&lt;/li&gt;&lt;li&gt;Validated with 800 challenging cases and public benchmarks&lt;/li&gt;&lt;li&gt;Shows significant improvement in risk identification accuracy&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Xiuyuan Chen', 'Jian Zhao', 'Yuchen Yuan', 'Tianle Zhang', 'Huilin Zhou', 'Zheng Zhu', 'Ping Hu', 'Linghe Kong', 'Chi Zhang', 'Weiran Huang', 'Xuelong Li']&lt;/li&gt;&lt;li&gt;Tags: ['LLM safety evaluation', 'Multi-agent systems', 'Risk concept decomposition', 'Dynamic risk concept updates', 'Role-specialized collaboration']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.25271</guid><pubDate>Thu, 23 Oct 2025 00:00:00 +0000</pubDate></item><item><title>A Concrete Roadmap towards Safety Cases based on Chain-of-Thought Monitoring</title><link>https://arxiv.org/abs/2510.19476</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a safety case based on chain-of-thought monitoring&lt;/li&gt;&lt;li&gt;Analyzes threats to monitorability like neuralese and encoded reasoning&lt;/li&gt;&lt;li&gt;Explores techniques for maintaining CoT faithfulness&lt;/li&gt;&lt;li&gt;Uses prediction markets to forecast feasibility&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Julian Schulz']&lt;/li&gt;&lt;li&gt;Tags: ['safety', 'alignment', 'monitoring', 'chain-of-thought', 'threat analysis']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.19476</guid><pubDate>Thu, 23 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Collaborative penetration testing suite for emerging generative AI algorithms</title><link>https://arxiv.org/abs/2510.19303</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a collaborative penetration testing suite for generative AI&lt;/li&gt;&lt;li&gt;Integrates DAST, SAST, IAST, blockchain logging, and quantum cryptography&lt;/li&gt;&lt;li&gt;Includes AI red team simulations with adversarial ML and quantum-assisted attacks&lt;/li&gt;&lt;li&gt;Reports significant reduction in vulnerabilities and high resolution efficiency&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Petar Radanliev']&lt;/li&gt;&lt;li&gt;Tags: ['red teaming', 'adversarial prompting', 'data poisoning', 'model inversion', 'quantum cryptography']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.19303</guid><pubDate>Thu, 23 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Misalignment Bounty: Crowdsourcing AI Agent Misbehavior</title><link>https://arxiv.org/abs/2510.19738</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Crowdsourced project to collect AI agent misbehavior examples&lt;/li&gt;&lt;li&gt;Received 295 submissions with 9 winners&lt;/li&gt;&lt;li&gt;Discusses motivation and evaluation criteria&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research Paper&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Rustem Turtayev', 'Natalia Fedorova', 'Oleg Serikov', 'Sergey Koldyba', 'Lev Avagyan', 'Dmitrii Volkov']&lt;/li&gt;&lt;li&gt;Tags: ['red teaming', 'safety evaluation', 'alignment']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.19738</guid><pubDate>Thu, 23 Oct 2025 00:00:00 +0000</pubDate></item></channel></rss>