<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>AI Security Paper Digest</title><link>https://kentaroh-toyoda.github.io/ai-security-paper-digest-rss/rss.xml</link><description>Curated papers on AI security from ArXiv and ACL</description><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><lastBuildDate>Thu, 09 Oct 2025 22:23:13 +0000</lastBuildDate><item><title>SafeGuider: Robust and Practical Content Safety Control for Text-to-Image Models</title><link>https://arxiv.org/abs/2510.05173</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;SafeGuider is a framework for robust content safety control in text-to-image models.&lt;/li&gt;&lt;li&gt;It uses the [EOS] token's embedding to detect adversarial prompts.&lt;/li&gt;&lt;li&gt;Combines recognition model with safety-aware beam search to maintain quality while blocking attacks.&lt;/li&gt;&lt;li&gt;Effective against various attacks with low success rates and generates safe images instead of blocking.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Peigui Qi', 'Kunsheng Tang', 'Wenbo Zhou', 'Weiming Zhang', 'Nenghai Yu', 'Tianwei Zhang', 'Qing Guo', 'Jie Zhang']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial prompting', 'safety evaluation', 'robustness', 'text-to-image models', 'content safety']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.05173</guid><pubDate>Thu, 09 Oct 2025 00:00:00 +0000</pubDate></item><item><title>MM-PoisonRAG: Disrupting Multimodal RAG with Local and Global Poisoning Attacks</title><link>https://arxiv.org/abs/2502.17832</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes MM-PoisonRAG framework for multimodal RAG poisoning attacks&lt;/li&gt;&lt;li&gt;Introduces Localized Poisoning Attack (LPA) and Globalized Poisoning Attack (GPA)&lt;/li&gt;&lt;li&gt;Demonstrates high attack success rates on multimodal RAG models&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Hyeonjeong Ha', 'Qiusi Zhan', 'Jeonghwan Kim', 'Dimitrios Bralios', 'Saikrishna Sanniboina', 'Nanyun Peng', 'Kai-Wei Chang', 'Daniel Kang', 'Heng Ji']&lt;/li&gt;&lt;li&gt;Tags: ['data poisoning', 'multimodal', 'RAG', 'adversarial attacks', 'safety evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2502.17832</guid><pubDate>Thu, 09 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Is My Data in Your AI? Membership Inference Test (MINT) applied to Face Biometrics</title><link>https://arxiv.org/abs/2402.09225</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces MINT for detecting if data was used in model training&lt;/li&gt;&lt;li&gt;Focuses on face recognition models with high accuracy&lt;/li&gt;&lt;li&gt;Potential applications in privacy and fairness enforcement&lt;/li&gt;&lt;li&gt;Relevant to data poisoning and privacy attacks&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Daniel DeAlcala', 'Aythami Morales', 'Julian Fierrez', 'Gonzalo Mancera', 'Ruben Tolosana', 'Javier Ortega-Garcia']&lt;/li&gt;&lt;li&gt;Tags: ['privacy attacks', 'data poisoning', 'security', 'safety evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2402.09225</guid><pubDate>Thu, 09 Oct 2025 00:00:00 +0000</pubDate></item><item><title>SaFeR-VLM: Toward Safety-aware Fine-grained Reasoning in Multimodal Models</title><link>https://arxiv.org/abs/2510.06871</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes SaFeR-VLM, a safety-aligned reinforcement learning framework for multimodal models&lt;/li&gt;&lt;li&gt;Introduces QI-Safe-10K dataset for safety-critical cases&lt;/li&gt;&lt;li&gt;Uses safety-aware rollout with reflection and correction&lt;/li&gt;&lt;li&gt;Employs structured reward modeling and GRPO optimization&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Huahui Yi', 'Kun Wang', 'Qiankun Li', 'Miao Yu', 'Liang Lin', 'Gongli Xi', 'Hao Wu', 'Xuming Hu', 'Kang Li', 'Yang Liu']&lt;/li&gt;&lt;li&gt;Tags: ['multimodal', 'safety', 'reinforcement learning', 'dataset', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.06871</guid><pubDate>Thu, 09 Oct 2025 00:00:00 +0000</pubDate></item><item><title>The Alignment Auditor: A Bayesian Framework for Verifying and Refining LLM Objectives</title><link>https://arxiv.org/abs/2510.06096</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces a Bayesian framework for auditing and refining LLM objectives&lt;/li&gt;&lt;li&gt;Uses Bayesian IRL to infer reward distributions and reduce non-identifiability&lt;/li&gt;&lt;li&gt;Provides diagnostics for spurious shortcuts and out-of-distribution prompts&lt;/li&gt;&lt;li&gt;Validates utility in RLHF for training dynamics and toxicity reduction&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Matthieu Bou', 'Nyal Patel', 'Arjun Jagota', 'Satyapriya Krishna', 'Sonali Parbhoo']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'safety evaluation', 'bayesian methods', 'inverse reinforcement learning', 'trustworthy AI']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.06096</guid><pubDate>Thu, 09 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Membership Inference Attacks on LLM-based Recommender Systems</title><link>https://arxiv.org/abs/2508.18665</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;The paper introduces four membership inference attacks (MIAs) targeting LLM-based recommender systems.&lt;/li&gt;&lt;li&gt;These attacks aim to determine if a user's historical interactions were used in the system's prompts.&lt;/li&gt;&lt;li&gt;The attacks include direct inquiry, hallucination, similarity, and poisoning.&lt;/li&gt;&lt;li&gt;Experiments on three LLMs and two datasets show that direct inquiry and poisoning attacks are effective.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jiajie He', 'Yuechun Gu', 'Min-Chun Chen', 'Keke Chen']&lt;/li&gt;&lt;li&gt;Tags: ['privacy attacks', 'membership inference', 'LLM-based recommender systems', 'prompt injection', 'data poisoning']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2508.18665</guid><pubDate>Thu, 09 Oct 2025 00:00:00 +0000</pubDate></item><item><title>From Injection to Defense: Constructing Edit-Based Fingerprints for Large Language Models</title><link>https://arxiv.org/abs/2509.03122</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes RFEdit, a framework for embedding robust fingerprints in LLMs via sparse weight modification&lt;/li&gt;&lt;li&gt;Introduces Fingerprint Subspace-aware Fine-Tuning (FSFT) to preserve fingerprints during legitimate fine-tuning&lt;/li&gt;&lt;li&gt;Aims to balance fingerprint detectability and unintended triggering&lt;/li&gt;&lt;li&gt;Demonstrates robustness against quantization, pruning, and adversarial attacks&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yue Li', 'Xin Yi', 'Dongsheng Shi', 'Yongyi Cui', 'Gerard de Melo', 'Linlin Wang']&lt;/li&gt;&lt;li&gt;Tags: ['LLM security', 'fingerprinting', 'model protection', 'adversarial attacks', 'fine-tuning defense']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.03122</guid><pubDate>Thu, 09 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Guiding Giants: Lightweight Controllers for Weighted Activation Steering in LLMs</title><link>https://arxiv.org/abs/2505.20309</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces a lightweight controller network for activation steering in LLMs&lt;/li&gt;&lt;li&gt;Uses layer-specific weights and global scaling for dynamic modulation&lt;/li&gt;&lt;li&gt;Trained on harmful and benign prompts to apply interventions selectively&lt;/li&gt;&lt;li&gt;Shows improved refusal rates on safety benchmarks without fine-tuning&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Amr Hegazy', 'Mostafa Elhoushi', 'Amr Alanwar']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'safety evaluation', 'alignment', 'robustness', 'inference-time control']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2505.20309</guid><pubDate>Thu, 09 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Benchmarking Gaslighting Negation Attacks Against Multimodal Large Language Models</title><link>https://arxiv.org/abs/2501.19017</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces GaslightingBench to evaluate MLLMs' vulnerability to negation attacks&lt;/li&gt;&lt;li&gt;Finds proprietary models more resilient than open-source ones&lt;/li&gt;&lt;li&gt;Identifies subjective domains as more fragile&lt;/li&gt;&lt;li&gt;Highlights robustness gap in multimodal AI systems&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Bin Zhu', 'Yinxuan Gui', 'Huiyan Qi', 'Jingjing Chen', 'Chong-Wah Ngo', 'Ee-Peng Lim']&lt;/li&gt;&lt;li&gt;Tags: ['red teaming', 'adversarial prompting', 'robustness', 'safety evaluation', 'multimodal']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2501.19017</guid><pubDate>Thu, 09 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Evil twins are not that evil: Qualitative insights into machine-generated prompts</title><link>https://arxiv.org/abs/2412.08127</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Analyzes machine-generated prompts (autoprompts) for 6 LMs&lt;/li&gt;&lt;li&gt;Finds last token is often intelligible and influential&lt;/li&gt;&lt;li&gt;Identifies filler tokens and keywords in autoprompts&lt;/li&gt;&lt;li&gt;Human experts can identify influential tokens&lt;/li&gt;&lt;li&gt;Ablations show similar effects in natural language inputs&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Nathana\\"el Carraz Rakotonirina', 'Corentin Kervadec', 'Francesca Franzon', 'Marco Baroni']&lt;/li&gt;&lt;li&gt;Tags: ['jailbreaking', 'prompt injection', 'adversarial prompting', 'LLM red teaming', 'safety evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2412.08127</guid><pubDate>Thu, 09 Oct 2025 00:00:00 +0000</pubDate></item><item><title>RedTWIZ: Diverse LLM Red Teaming via Adaptive Attack Planning</title><link>https://arxiv.org/abs/2510.06994</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces RedTWIZ, a multi-turn red teaming framework for LLMs&lt;/li&gt;&lt;li&gt;Focuses on conversational jailbreaks and adaptive attack planning&lt;/li&gt;&lt;li&gt;Evaluates robustness against adversarial prompts&lt;/li&gt;&lt;li&gt;Demonstrates effectiveness against state-of-the-art LLMs&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Artur Horal', 'Daniel Pina', 'Henrique Paz', 'Iago Paulo', 'Jo\\~ao Soares', 'Rafael Ferreira', 'Diogo Tavares', "Diogo Gl\\'oria-Silva", 'Jo\\~ao Magalh\\~aes', 'David Semedo']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'jailbreaking', 'adversarial prompting', 'robustness', 'security evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.06994</guid><pubDate>Thu, 09 Oct 2025 00:00:00 +0000</pubDate></item><item><title>VelLMes: A high-interaction AI-based deception framework</title><link>https://arxiv.org/abs/2510.06975</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;VelLMes is an AI-based deception framework using LLMs to simulate multiple protocols/services like SSH, MySQL, POP3, HTTP as honeypots.&lt;/li&gt;&lt;li&gt;Evaluated generative capabilities via unit tests showing high realism with proper prompting.&lt;/li&gt;&lt;li&gt;Human attacker evaluation for SSH honeypot showed 30% believed it was real.&lt;/li&gt;&lt;li&gt;Deployed 10 SSH honeypots online to capture real attacks, demonstrating effectiveness against unstructured attacks.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ["Muris Sladi\\'c (Czech Technical University in Prague)", 'Veronica Valeros (Czech Technical University in Prague)', 'Carlos Catania (CONICET', 'UNCuyo)', 'Sebastian Garcia (Czech Technical University in Prague)']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'jailbreaking', 'adversarial prompting', 'model extraction', 'privacy attacks', 'alignment', 'robustness', 'safety evaluation', 'security standards']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.06975</guid><pubDate>Thu, 09 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Exposing Citation Vulnerabilities in Generative Engines</title><link>https://arxiv.org/abs/2510.06823</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Analyzes citation vulnerabilities in generative engines (GEs) focusing on poisoning attacks through web content manipulation.&lt;/li&gt;&lt;li&gt;Introduces evaluation criteria to assess the content-injection barrier using citation publisher attributes.&lt;/li&gt;&lt;li&gt;Conducts experiments in political domains showing higher risk in US compared to Japan.&lt;/li&gt;&lt;li&gt;Discusses mitigation strategies for primary source publishers.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Riku Mochizuki', 'Shusuke Komatsu', 'Souta Noguchi', 'Kazuto Ataka']&lt;/li&gt;&lt;li&gt;Tags: ['data poisoning', 'safety evaluation', 'alignment', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.06823</guid><pubDate>Thu, 09 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Red-Bandit: Test-Time Adaptation for LLM Red-Teaming via Bandit-Guided LoRA Experts</title><link>https://arxiv.org/abs/2510.07239</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces Red-Bandit, a framework for LLM red-teaming that adapts online to model vulnerabilities&lt;/li&gt;&lt;li&gt;Uses LoRA experts specialized in different attack styles (manipulation, slang) trained with RL&lt;/li&gt;&lt;li&gt;Employs multi-armed bandit policy to select best attack style dynamically&lt;/li&gt;&lt;li&gt;Achieves SOTA on AdvBench with better human readability of prompts&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Christos Ziakas', 'Nicholas Loo', 'Nishita Jain', 'Alessandra Russo']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'jailbreaking', 'adversarial prompting', 'reinforcement learning', 'LoRA']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.07239</guid><pubDate>Thu, 09 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Are LLMs Reliable Rankers? Rank Manipulation via Two-Stage Token Optimization</title><link>https://arxiv.org/abs/2510.06732</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces Rank Anything First (RAF), a method to manipulate LLM-based rankings&lt;/li&gt;&lt;li&gt;Uses two-stage token optimization to create natural-sounding prompts that promote a target item&lt;/li&gt;&lt;li&gt;Demonstrates vulnerability in LLM reranking with security implications&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Tiancheng Xing', 'Jerry Li', 'Yixuan Du', 'Xiyang Hu']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'adversarial prompting', 'security', 'robustness', 'jailbreaking']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.06732</guid><pubDate>Thu, 09 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Do Internal Layers of LLMs Reveal Patterns for Jailbreak Detection?</title><link>https://arxiv.org/abs/2510.06594</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;The paper explores detecting jailbreak prompts in LLMs by analyzing internal layer patterns.&lt;/li&gt;&lt;li&gt;It focuses on GPT-J and Mamba2 models, showing distinct layer responses.&lt;/li&gt;&lt;li&gt;Aims to improve jailbreak detection using internal model dynamics.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Sri Durga Sai Sowmya Kadali', 'Evangelos E. Papalexakis']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'jailbreaking', 'prompt injection', 'adversarial prompting', 'safety evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.06594</guid><pubDate>Thu, 09 Oct 2025 00:00:00 +0000</pubDate></item><item><title>A Survey on Agentic Security: Applications, Threats and Defenses</title><link>https://arxiv.org/abs/2510.06445</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Survey on agentic security focusing on applications, threats, and defenses&lt;/li&gt;&lt;li&gt;Covers over 150 papers with taxonomy&lt;/li&gt;&lt;li&gt;Identifies research gaps in model and modality coverage&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Survey&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Asif Shahriar', 'Md Nafiu Rahman', 'Sadif Ahmed', 'Farig Sadeque', 'Md Rizwan Parvez']&lt;/li&gt;&lt;li&gt;Tags: ['red teaming', 'jailbreaking', 'adversarial prompting', 'model extraction', 'privacy attacks', 'alignment', 'robustness', 'safety evaluation', 'security standards']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.06445</guid><pubDate>Thu, 09 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Reward Model Perspectives: Whose Opinions Do Reward Models Reward?</title><link>https://arxiv.org/abs/2510.06391</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Formalizes framework for measuring RM alignment&lt;/li&gt;&lt;li&gt;Investigates sociodemographic biases in RMs&lt;/li&gt;&lt;li&gt;Explores prompting effects on RM preferences&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research Paper&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Elle']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'safety', 'bias', 'reward models']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.06391</guid><pubDate>Thu, 09 Oct 2025 00:00:00 +0000</pubDate></item><item><title>From Injection to Defense: Constructing Edit-Based Fingerprints for Large Language Models</title><link>https://arxiv.org/abs/2509.03122</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes RFEdit, a framework for embedding robust fingerprints in LLMs via sparse weight modification&lt;/li&gt;&lt;li&gt;Introduces Fingerprint Subspace-aware Fine-Tuning (FSFT) to preserve fingerprints during legitimate fine-tuning&lt;/li&gt;&lt;li&gt;Aims to balance fingerprint detectability and unintended triggering&lt;/li&gt;&lt;li&gt;Demonstrates robustness against quantization, pruning, and adversarial attacks&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yue Li', 'Xin Yi', 'Dongsheng Shi', 'Yongyi Cui', 'Gerard de Melo', 'Linlin Wang']&lt;/li&gt;&lt;li&gt;Tags: ['LLM security', 'fingerprinting', 'model protection', 'adversarial attacks', 'fine-tuning defense']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.03122</guid><pubDate>Thu, 09 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Membership Inference Attacks on LLM-based Recommender Systems</title><link>https://arxiv.org/abs/2508.18665</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;The paper introduces four membership inference attacks (MIAs) targeting LLM-based recommender systems.&lt;/li&gt;&lt;li&gt;These attacks aim to determine if a user's historical interactions were used in the system's prompts.&lt;/li&gt;&lt;li&gt;The attacks include direct inquiry, hallucination, similarity, and poisoning.&lt;/li&gt;&lt;li&gt;Experiments on three LLMs and two datasets show that direct inquiry and poisoning attacks are effective.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jiajie He', 'Yuechun Gu', 'Min-Chun Chen', 'Keke Chen']&lt;/li&gt;&lt;li&gt;Tags: ['privacy attacks', 'membership inference', 'LLM-based recommender systems', 'prompt injection', 'data poisoning']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2508.18665</guid><pubDate>Thu, 09 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Enhancing GraphQL Security by Detecting Malicious Queries Using Large Language Models, Sentence Transformers, and Convolutional Neural Networks</title><link>https://arxiv.org/abs/2508.11711</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Presents an AI-driven approach for detecting malicious GraphQL queries using LLMs, Sentence Transformers, and CNNs&lt;/li&gt;&lt;li&gt;Combines static analysis with machine learning for real-time threat detection&lt;/li&gt;&lt;li&gt;Evaluates performance against SQL injection, OS command injection, XSS, DoS, and SSRF attacks&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Irash Perera (Department of Computer Science and Engineering', 'University of Moratuwa', 'Colombo', 'Sri Lanka)', 'Hiranya Abeyrathne (WSO2', 'Colombo', 'Sri Lanka)', 'Sanjeewa Malalgoda (WSO2', 'Colombo', 'Sri Lanka)', 'Arshardh Ifthikar (WSO2', 'Colombo', 'Sri Lanka)']&lt;/li&gt;&lt;li&gt;Tags: ['AI security', 'red teaming', 'adversarial prompting', 'model extraction', 'data poisoning', 'privacy attacks', 'alignment', 'robustness', 'safety evaluation', 'security standards']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2508.11711</guid><pubDate>Thu, 09 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Guiding Giants: Lightweight Controllers for Weighted Activation Steering in LLMs</title><link>https://arxiv.org/abs/2505.20309</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces a lightweight controller network for activation steering in LLMs&lt;/li&gt;&lt;li&gt;Uses layer-specific weights and global scaling for dynamic modulation&lt;/li&gt;&lt;li&gt;Trained on harmful and benign prompts to apply interventions selectively&lt;/li&gt;&lt;li&gt;Shows improved refusal rates on safety benchmarks without fine-tuning&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Amr Hegazy', 'Mostafa Elhoushi', 'Amr Alanwar']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'safety evaluation', 'alignment', 'robustness', 'inference-time control']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2505.20309</guid><pubDate>Thu, 09 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Jailbreak Attack Initializations as Extractors of Compliance Directions</title><link>https://arxiv.org/abs/2502.09755</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;The paper explores how jailbreak attacks on safety-aligned LLMs converge to specific compliance directions in the model's activation space.&lt;/li&gt;&lt;li&gt;It introduces CRI, an initialization framework that projects prompts along these compliance directions to improve attack success rates.&lt;/li&gt;&lt;li&gt;The research demonstrates increased ASR and reduced computational overhead across multiple attacks, models, and datasets.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Amit Levi', 'Rom Himelstein', 'Yaniv Nemcovsky', 'Avi Mendelson', 'Chaim Baskin']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'jailbreaking', 'adversarial prompting', 'safety evaluation', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2502.09755</guid><pubDate>Thu, 09 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Evil twins are not that evil: Qualitative insights into machine-generated prompts</title><link>https://arxiv.org/abs/2412.08127</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Analyzes machine-generated prompts (autoprompts) for 6 LMs&lt;/li&gt;&lt;li&gt;Finds last token is often intelligible and influential&lt;/li&gt;&lt;li&gt;Identifies filler tokens and keywords in autoprompts&lt;/li&gt;&lt;li&gt;Human experts can identify influential tokens&lt;/li&gt;&lt;li&gt;Ablations show similar effects in natural language inputs&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Nathana\\"el Carraz Rakotonirina', 'Corentin Kervadec', 'Francesca Franzon', 'Marco Baroni']&lt;/li&gt;&lt;li&gt;Tags: ['jailbreaking', 'prompt injection', 'adversarial prompting', 'LLM red teaming', 'safety evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2412.08127</guid><pubDate>Thu, 09 Oct 2025 00:00:00 +0000</pubDate></item><item><title>The Alignment Auditor: A Bayesian Framework for Verifying and Refining LLM Objectives</title><link>https://arxiv.org/abs/2510.06096</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces a Bayesian framework for auditing and refining LLM objectives&lt;/li&gt;&lt;li&gt;Uses Bayesian IRL to infer reward distributions and reduce non-identifiability&lt;/li&gt;&lt;li&gt;Provides diagnostics for spurious shortcuts and out-of-distribution prompts&lt;/li&gt;&lt;li&gt;Validates utility in RLHF for training dynamics and toxicity reduction&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Matthieu Bou', 'Nyal Patel', 'Arjun Jagota', 'Satyapriya Krishna', 'Sonali Parbhoo']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'safety evaluation', 'bayesian methods', 'inverse reinforcement learning', 'trustworthy AI']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.06096</guid><pubDate>Thu, 09 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Inoculation Prompting: Instructing LLMs to misbehave at train-time improves test-time alignment</title><link>https://arxiv.org/abs/2510.05024</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces Inoculation Prompting (IP) to prevent LLMs from learning undesired behaviors during training.&lt;/li&gt;&lt;li&gt;Modifies training prompts to explicitly request the undesired behavior to build resistance.&lt;/li&gt;&lt;li&gt;Tested across four settings, showing reduced undesired behavior without losing desired capabilities.&lt;/li&gt;&lt;li&gt;More effective when using prompts that strongly elicit the undesired behavior before fine-tuning.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Nevan Wichers', 'Aram Ebtekar', 'Ariana Azarbal', 'Victor Gillioz', 'Christine Ye', 'Emil Ryd', 'Neil Rathi', 'Henry Sleight', 'Alex Mallen', 'Fabien Roger', 'Samuel Marks']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'safety', 'red teaming', 'prompt injection', 'jailbreaking']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.05024</guid><pubDate>Thu, 09 Oct 2025 00:00:00 +0000</pubDate></item><item><title>SafeProtein: Red-Teaming Framework and Benchmark for Protein Foundation Models</title><link>https://arxiv.org/abs/2509.03487</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces SafeProtein, a red-teaming framework for protein foundation models&lt;/li&gt;&lt;li&gt;Combines multimodal prompt engineering and heuristic beam search for systematic testing&lt;/li&gt;&lt;li&gt;Curates SafeProtein-Bench, a benchmark dataset for red-teaming&lt;/li&gt;&lt;li&gt;Demonstrates jailbreaks on state-of-the-art models like ESM3 with up to 70% success rate&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jigang Fan', 'Zhenghong Zhou', 'Ruofan Jin', 'Le Cong', 'Mengdi Wang', 'Zaixi Zhang']&lt;/li&gt;&lt;li&gt;Tags: ['red teaming', 'jailbreaking', 'protein foundation models', 'safety evaluation', 'benchmarking']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.03487</guid><pubDate>Thu, 09 Oct 2025 00:00:00 +0000</pubDate></item><item><title>MM-PoisonRAG: Disrupting Multimodal RAG with Local and Global Poisoning Attacks</title><link>https://arxiv.org/abs/2502.17832</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes MM-PoisonRAG framework for multimodal RAG poisoning attacks&lt;/li&gt;&lt;li&gt;Introduces Localized Poisoning Attack (LPA) and Globalized Poisoning Attack (GPA)&lt;/li&gt;&lt;li&gt;Demonstrates high attack success rates on multimodal RAG models&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Hyeonjeong Ha', 'Qiusi Zhan', 'Jeonghwan Kim', 'Dimitrios Bralios', 'Saikrishna Sanniboina', 'Nanyun Peng', 'Kai-Wei Chang', 'Daniel Kang', 'Heng Ji']&lt;/li&gt;&lt;li&gt;Tags: ['data poisoning', 'multimodal', 'RAG', 'adversarial attacks', 'safety evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2502.17832</guid><pubDate>Thu, 09 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Towards the Worst-case Robustness of Large Language Models</title><link>https://arxiv.org/abs/2501.19040</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Studies worst-case robustness of LLMs against adversarial attacks&lt;/li&gt;&lt;li&gt;Proposes upper and lower bounds for robustness using white-box attacks and randomized smoothing&lt;/li&gt;&lt;li&gt;Analyzes deterministic and stochastic defenses&lt;/li&gt;&lt;li&gt;Provides theoretical lower bounds for empirical defenses&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Huanran Chen', 'Yinpeng Dong', 'Zeming Wei', 'Hang Su', 'Jun Zhu']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial attacks', 'robustness', 'randomized smoothing', 'lower bounds', 'upper bounds']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2501.19040</guid><pubDate>Thu, 09 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Poisoning Attacks on LLMs Require a Near-constant Number of Poison Samples</title><link>https://arxiv.org/abs/2510.07192</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;The paper demonstrates that poisoning attacks on LLMs require a near-constant number of poisoned documents regardless of dataset size.&lt;/li&gt;&lt;li&gt;Conducted large-scale experiments on models from 600M to 13B parameters, showing 250 poisoned documents can compromise models across all sizes.&lt;/li&gt;&lt;li&gt;Results highlight the need for better defenses against data poisoning attacks.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Alexandra Souly', 'Javier Rando', 'Ed Chapman', 'Xander Davies', 'Burak Hasircioglu', 'Ezzeldin Shereen', 'Carlos Mougan', 'Vasilios Mavroudis', 'Erik Jones', 'Chris Hicks', 'Nicholas Carlini', 'Yarin Gal', 'Robert Kirk']&lt;/li&gt;&lt;li&gt;Tags: ['data poisoning', 'LLM security', 'adversarial attacks', 'model robustness', 'safety evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.07192</guid><pubDate>Thu, 09 Oct 2025 00:00:00 +0000</pubDate></item><item><title>SaFeR-VLM: Toward Safety-aware Fine-grained Reasoning in Multimodal Models</title><link>https://arxiv.org/abs/2510.06871</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes SaFeR-VLM, a safety-aligned reinforcement learning framework for multimodal models&lt;/li&gt;&lt;li&gt;Introduces QI-Safe-10K dataset for safety-critical cases&lt;/li&gt;&lt;li&gt;Uses safety-aware rollout with reflection and correction&lt;/li&gt;&lt;li&gt;Employs structured reward modeling and GRPO optimization&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Huahui Yi', 'Kun Wang', 'Qiankun Li', 'Miao Yu', 'Liang Lin', 'Gongli Xi', 'Hao Wu', 'Xuming Hu', 'Kang Li', 'Yang Liu']&lt;/li&gt;&lt;li&gt;Tags: ['multimodal', 'safety', 'reinforcement learning', 'dataset', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.06871</guid><pubDate>Thu, 09 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Get RICH or Die Scaling: Profitably Trading Inference Compute for Robustness</title><link>https://arxiv.org/abs/2510.06790</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;The paper explores the Robustness from Inference Compute Hypothesis (RICH), which suggests that inference compute can improve model robustness against adversarial attacks, especially when the model's training data includes components of the attacked data.&lt;/li&gt;&lt;li&gt;It empirically tests this hypothesis across vision language models and different attack types, finding that compositional generalization plays a key role in enabling robustness gains from increased inference compute.&lt;/li&gt;&lt;li&gt;The study shows that defensive specifications and prompting can lower attack success rates on robustified models but not on non-robustified ones, indicating a synergy between training and test-time defenses.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Tavish McDonald', 'Bo Lei', 'Stanislav Fort', 'Bhavya Kailkhura', 'Brian Bartoldson']&lt;/li&gt;&lt;li&gt;Tags: ['robustness', 'adversarial attacks', 'jailbreaking', 'compositional generalization', 'inference compute']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.06790</guid><pubDate>Thu, 09 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Text-to-Image Models Leave Identifiable Signatures: Implications for Leaderboard Security</title><link>https://arxiv.org/abs/2510.06525</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;The paper demonstrates that text-to-image models can be deanonymized using CLIP embeddings, making leaderboard manipulation easier.&lt;/li&gt;&lt;li&gt;It uses a large dataset of generated images to show high accuracy in model identification.&lt;/li&gt;&lt;li&gt;Introduces a separability metric to find prompts that enable near-perfect deanonymization.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Ali Naseh', 'Anshuman Suri', 'Yuefeng Peng', 'Harsh Chaudhari', 'Alina Oprea', 'Amir Houmansadr']&lt;/li&gt;&lt;li&gt;Tags: ['data poisoning', 'privacy attacks', 'model extraction', 'security standards', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.06525</guid><pubDate>Thu, 09 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Geometry-Aware Backdoor Attacks: Leveraging Curvature in Hyperbolic Embeddings</title><link>https://arxiv.org/abs/2510.06397</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces geometry-aware backdoor attacks in hyperbolic embeddings&lt;/li&gt;&lt;li&gt;Exploits boundary-driven asymmetry where small input changes cause large shifts&lt;/li&gt;&lt;li&gt;Analyzes limitations of existing defenses&lt;/li&gt;&lt;li&gt;Proposes a geometry-adaptive trigger&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Ali Baheri']&lt;/li&gt;&lt;li&gt;Tags: ['backdoor attacks', 'hyperbolic geometry', 'non-Euclidean models', 'adversarial attacks']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.06397</guid><pubDate>Thu, 09 Oct 2025 00:00:00 +0000</pubDate></item><item><title>AutoDAN-Reasoning: Enhancing Strategies Exploration based Jailbreak Attacks with Test-Time Scaling</title><link>https://arxiv.org/abs/2510.05379</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Enhances AutoDAN-Turbo's jailbreak attacks with test-time scaling methods&lt;/li&gt;&lt;li&gt;Introduces Best-of-N and Beam Search scaling methods&lt;/li&gt;&lt;li&gt;Significant performance improvements in attack success rates&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Xiaogeng Liu', 'Chaowei Xiao']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'jailbreaking', 'adversarial prompting', 'test-time scaling', 'strategy exploration']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.05379</guid><pubDate>Thu, 09 Oct 2025 00:00:00 +0000</pubDate></item><item><title>SafeGuider: Robust and Practical Content Safety Control for Text-to-Image Models</title><link>https://arxiv.org/abs/2510.05173</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;SafeGuider is a framework for robust content safety control in text-to-image models.&lt;/li&gt;&lt;li&gt;It uses the [EOS] token's embedding to detect adversarial prompts.&lt;/li&gt;&lt;li&gt;Combines recognition model with safety-aware beam search to maintain quality while blocking attacks.&lt;/li&gt;&lt;li&gt;Effective against various attacks with low success rates and generates safe images instead of blocking.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Peigui Qi', 'Kunsheng Tang', 'Wenbo Zhou', 'Weiming Zhang', 'Nenghai Yu', 'Tianwei Zhang', 'Qing Guo', 'Jie Zhang']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial prompting', 'safety evaluation', 'robustness', 'text-to-image models', 'content safety']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.05173</guid><pubDate>Thu, 09 Oct 2025 00:00:00 +0000</pubDate></item><item><title>SafeProtein: Red-Teaming Framework and Benchmark for Protein Foundation Models</title><link>https://arxiv.org/abs/2509.03487</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces SafeProtein, a red-teaming framework for protein foundation models&lt;/li&gt;&lt;li&gt;Combines multimodal prompt engineering and heuristic beam search for systematic testing&lt;/li&gt;&lt;li&gt;Curates SafeProtein-Bench, a benchmark dataset for red-teaming&lt;/li&gt;&lt;li&gt;Demonstrates jailbreaks on state-of-the-art models like ESM3 with up to 70% success rate&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jigang Fan', 'Zhenghong Zhou', 'Ruofan Jin', 'Le Cong', 'Mengdi Wang', 'Zaixi Zhang']&lt;/li&gt;&lt;li&gt;Tags: ['red teaming', 'jailbreaking', 'protein foundation models', 'safety evaluation', 'benchmarking']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.03487</guid><pubDate>Thu, 09 Oct 2025 00:00:00 +0000</pubDate></item><item><title>From Injection to Defense: Constructing Edit-Based Fingerprints for Large Language Models</title><link>https://arxiv.org/abs/2509.03122</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes RFEdit, a framework for embedding robust fingerprints in LLMs via sparse weight modification&lt;/li&gt;&lt;li&gt;Introduces Fingerprint Subspace-aware Fine-Tuning (FSFT) to preserve fingerprints during legitimate fine-tuning&lt;/li&gt;&lt;li&gt;Aims to balance fingerprint detectability and unintended triggering&lt;/li&gt;&lt;li&gt;Demonstrates robustness against quantization, pruning, and adversarial attacks&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yue Li', 'Xin Yi', 'Dongsheng Shi', 'Yongyi Cui', 'Gerard de Melo', 'Linlin Wang']&lt;/li&gt;&lt;li&gt;Tags: ['LLM security', 'fingerprinting', 'model protection', 'adversarial attacks', 'fine-tuning defense']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.03122</guid><pubDate>Thu, 09 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Membership Inference Attacks on LLM-based Recommender Systems</title><link>https://arxiv.org/abs/2508.18665</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;The paper introduces four membership inference attacks (MIAs) targeting LLM-based recommender systems.&lt;/li&gt;&lt;li&gt;These attacks aim to determine if a user's historical interactions were used in the system's prompts.&lt;/li&gt;&lt;li&gt;The attacks include direct inquiry, hallucination, similarity, and poisoning.&lt;/li&gt;&lt;li&gt;Experiments on three LLMs and two datasets show that direct inquiry and poisoning attacks are effective.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jiajie He', 'Yuechun Gu', 'Min-Chun Chen', 'Keke Chen']&lt;/li&gt;&lt;li&gt;Tags: ['privacy attacks', 'membership inference', 'LLM-based recommender systems', 'prompt injection', 'data poisoning']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2508.18665</guid><pubDate>Thu, 09 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Enhancing GraphQL Security by Detecting Malicious Queries Using Large Language Models, Sentence Transformers, and Convolutional Neural Networks</title><link>https://arxiv.org/abs/2508.11711</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Presents an AI-driven approach for detecting malicious GraphQL queries using LLMs, Sentence Transformers, and CNNs&lt;/li&gt;&lt;li&gt;Combines static analysis with machine learning for real-time threat detection&lt;/li&gt;&lt;li&gt;Evaluates performance against SQL injection, OS command injection, XSS, DoS, and SSRF attacks&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Irash Perera (Department of Computer Science and Engineering', 'University of Moratuwa', 'Colombo', 'Sri Lanka)', 'Hiranya Abeyrathne (WSO2', 'Colombo', 'Sri Lanka)', 'Sanjeewa Malalgoda (WSO2', 'Colombo', 'Sri Lanka)', 'Arshardh Ifthikar (WSO2', 'Colombo', 'Sri Lanka)']&lt;/li&gt;&lt;li&gt;Tags: ['AI security', 'red teaming', 'adversarial prompting', 'model extraction', 'data poisoning', 'privacy attacks', 'alignment', 'robustness', 'safety evaluation', 'security standards']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2508.11711</guid><pubDate>Thu, 09 Oct 2025 00:00:00 +0000</pubDate></item><item><title>MM-PoisonRAG: Disrupting Multimodal RAG with Local and Global Poisoning Attacks</title><link>https://arxiv.org/abs/2502.17832</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes MM-PoisonRAG framework for multimodal RAG poisoning attacks&lt;/li&gt;&lt;li&gt;Introduces Localized Poisoning Attack (LPA) and Globalized Poisoning Attack (GPA)&lt;/li&gt;&lt;li&gt;Demonstrates high attack success rates on multimodal RAG models&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Hyeonjeong Ha', 'Qiusi Zhan', 'Jeonghwan Kim', 'Dimitrios Bralios', 'Saikrishna Sanniboina', 'Nanyun Peng', 'Kai-Wei Chang', 'Daniel Kang', 'Heng Ji']&lt;/li&gt;&lt;li&gt;Tags: ['data poisoning', 'multimodal', 'RAG', 'adversarial attacks', 'safety evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2502.17832</guid><pubDate>Thu, 09 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Evil twins are not that evil: Qualitative insights into machine-generated prompts</title><link>https://arxiv.org/abs/2412.08127</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Analyzes machine-generated prompts (autoprompts) for 6 LMs&lt;/li&gt;&lt;li&gt;Finds last token is often intelligible and influential&lt;/li&gt;&lt;li&gt;Identifies filler tokens and keywords in autoprompts&lt;/li&gt;&lt;li&gt;Human experts can identify influential tokens&lt;/li&gt;&lt;li&gt;Ablations show similar effects in natural language inputs&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Nathana\\"el Carraz Rakotonirina', 'Corentin Kervadec', 'Francesca Franzon', 'Marco Baroni']&lt;/li&gt;&lt;li&gt;Tags: ['jailbreaking', 'prompt injection', 'adversarial prompting', 'LLM red teaming', 'safety evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2412.08127</guid><pubDate>Thu, 09 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Is My Data in Your AI? Membership Inference Test (MINT) applied to Face Biometrics</title><link>https://arxiv.org/abs/2402.09225</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces MINT for detecting if data was used in model training&lt;/li&gt;&lt;li&gt;Focuses on face recognition models with high accuracy&lt;/li&gt;&lt;li&gt;Potential applications in privacy and fairness enforcement&lt;/li&gt;&lt;li&gt;Relevant to data poisoning and privacy attacks&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Daniel DeAlcala', 'Aythami Morales', 'Julian Fierrez', 'Gonzalo Mancera', 'Ruben Tolosana', 'Javier Ortega-Garcia']&lt;/li&gt;&lt;li&gt;Tags: ['privacy attacks', 'data poisoning', 'security', 'safety evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2402.09225</guid><pubDate>Thu, 09 Oct 2025 00:00:00 +0000</pubDate></item><item><title>VelLMes: A high-interaction AI-based deception framework</title><link>https://arxiv.org/abs/2510.06975</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;VelLMes is an AI-based deception framework using LLMs to simulate multiple protocols/services like SSH, MySQL, POP3, HTTP as honeypots.&lt;/li&gt;&lt;li&gt;Evaluated generative capabilities via unit tests showing high realism with proper prompting.&lt;/li&gt;&lt;li&gt;Human attacker evaluation for SSH honeypot showed 30% believed it was real.&lt;/li&gt;&lt;li&gt;Deployed 10 SSH honeypots online to capture real attacks, demonstrating effectiveness against unstructured attacks.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ["Muris Sladi\\'c (Czech Technical University in Prague)", 'Veronica Valeros (Czech Technical University in Prague)', 'Carlos Catania (CONICET', 'UNCuyo)', 'Sebastian Garcia (Czech Technical University in Prague)']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'jailbreaking', 'adversarial prompting', 'model extraction', 'privacy attacks', 'alignment', 'robustness', 'safety evaluation', 'security standards']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.06975</guid><pubDate>Thu, 09 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Are LLMs Reliable Rankers? Rank Manipulation via Two-Stage Token Optimization</title><link>https://arxiv.org/abs/2510.06732</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces Rank Anything First (RAF), a method to manipulate LLM-based rankings&lt;/li&gt;&lt;li&gt;Uses two-stage token optimization to create natural-sounding prompts that promote a target item&lt;/li&gt;&lt;li&gt;Demonstrates vulnerability in LLM reranking with security implications&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Tiancheng Xing', 'Jerry Li', 'Yixuan Du', 'Xiyang Hu']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'adversarial prompting', 'security', 'robustness', 'jailbreaking']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.06732</guid><pubDate>Thu, 09 Oct 2025 00:00:00 +0000</pubDate></item><item><title>A Survey on Agentic Security: Applications, Threats and Defenses</title><link>https://arxiv.org/abs/2510.06445</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Survey on agentic security focusing on applications, threats, and defenses&lt;/li&gt;&lt;li&gt;Covers over 150 papers with taxonomy&lt;/li&gt;&lt;li&gt;Identifies research gaps in model and modality coverage&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Survey&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Asif Shahriar', 'Md Nafiu Rahman', 'Sadif Ahmed', 'Farig Sadeque', 'Md Rizwan Parvez']&lt;/li&gt;&lt;li&gt;Tags: ['red teaming', 'jailbreaking', 'adversarial prompting', 'model extraction', 'privacy attacks', 'alignment', 'robustness', 'safety evaluation', 'security standards']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.06445</guid><pubDate>Thu, 09 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Geometry-Aware Backdoor Attacks: Leveraging Curvature in Hyperbolic Embeddings</title><link>https://arxiv.org/abs/2510.06397</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces geometry-aware backdoor attacks in hyperbolic embeddings&lt;/li&gt;&lt;li&gt;Exploits boundary-driven asymmetry where small input changes cause large shifts&lt;/li&gt;&lt;li&gt;Analyzes limitations of existing defenses&lt;/li&gt;&lt;li&gt;Proposes a geometry-adaptive trigger&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Ali Baheri']&lt;/li&gt;&lt;li&gt;Tags: ['backdoor attacks', 'hyperbolic geometry', 'non-Euclidean models', 'adversarial attacks']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.06397</guid><pubDate>Thu, 09 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Reward Model Perspectives: Whose Opinions Do Reward Models Reward?</title><link>https://arxiv.org/abs/2510.06391</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Formalizes framework for measuring RM alignment&lt;/li&gt;&lt;li&gt;Investigates sociodemographic biases in RMs&lt;/li&gt;&lt;li&gt;Explores prompting effects on RM preferences&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research Paper&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Elle']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'safety', 'bias', 'reward models']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.06391</guid><pubDate>Thu, 09 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Verifying Memoryless Sequential Decision-making of Large Language Models</title><link>https://arxiv.org/abs/2510.06756</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces a tool for verifying LLM-based policies in memoryless sequential decision-making tasks&lt;/li&gt;&lt;li&gt;Uses MDPs and PCTL formulas to check safety properties&lt;/li&gt;&lt;li&gt;Experiments show open-source LLMs underperform DRL baselines but can be verified when deterministically seeded&lt;/li&gt;&lt;li&gt;Tool integrates with Ollama and PRISM-specified tasks&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Dennis Gross', 'Helge Spieker', 'Arnaud Gotlieb']&lt;/li&gt;&lt;li&gt;Tags: ['safety evaluation', 'formal verification', 'MDP', 'PCTL', 'LLM policies']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.06756</guid><pubDate>Thu, 09 Oct 2025 00:00:00 +0000</pubDate></item></channel></rss>