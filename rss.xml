<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>AI Security Paper Digest</title><link>https://kentaroh-toyoda.github.io/ai-security-paper-digest-rss/rss.xml</link><description>Curated papers on AI security from ArXiv and ACL</description><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><lastBuildDate>Tue, 21 Oct 2025 22:40:33 +0000</lastBuildDate><item><title>ImpMIA: Leveraging Implicit Bias for Membership Inference Attack under Realistic Scenarios</title><link>https://arxiv.org/abs/2510.10625</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces ImpMIA, a white-box membership inference attack leveraging implicit bias of neural networks&lt;/li&gt;&lt;li&gt;Uses KKT conditions to identify training samples by gradient reconstruction&lt;/li&gt;&lt;li&gt;Achieves state-of-the-art performance in realistic settings&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yuval Golbari', 'Navve Wasserman', 'Gal Vardi', 'Michal Irani']&lt;/li&gt;&lt;li&gt;Tags: ['privacy attacks', 'membership inference', 'white-box attack', 'implicit bias', 'neural networks']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.10625</guid><pubDate>Tue, 21 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Learning to Detect Unknown Jailbreak Attacks in Large Vision-Language Models</title><link>https://arxiv.org/abs/2510.15430</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a framework called Learning to Detect (LoD) for detecting unknown jailbreak attacks in LVLMs.&lt;/li&gt;&lt;li&gt;Includes a Multi-modal Safety Concept Activation Vector module and a Safety Pattern Auto-Encoder module.&lt;/li&gt;&lt;li&gt;Aims to improve generalization and efficiency over existing methods.&lt;/li&gt;&lt;li&gt;Demonstrates higher detection AUROC on diverse unknown attacks.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Shuang Liang', 'Zhihao Xu', 'Jialing Tao', 'Hui Xue', 'Xiting Wang']&lt;/li&gt;&lt;li&gt;Tags: ['jailbreaking', 'red teaming', 'safety evaluation', 'vision-language models', 'adversarial attacks']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.15430</guid><pubDate>Tue, 21 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Video-SafetyBench: A Benchmark for Safety Evaluation of Video LVLMs</title><link>https://arxiv.org/abs/2505.11842</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces Video-SafetyBench, a benchmark for evaluating safety of LVLMs under video-text attacks&lt;/li&gt;&lt;li&gt;Includes 2,264 video-text pairs across 48 unsafe categories&lt;/li&gt;&lt;li&gt;Proposes RJScore metric for evaluating uncertain outputs&lt;/li&gt;&lt;li&gt;Demonstrates high attack success rates with benign queries&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Xuannan Liu', 'Zekun Li', 'Zheqi He', 'Peipei Li', 'Shuhan Xia', 'Xing Cui', 'Huaibo Huang', 'Xi Yang', 'Ran He']&lt;/li&gt;&lt;li&gt;Tags: ['safety evaluation', 'multimodal', 'video', 'adversarial prompting', 'benchmarking']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2505.11842</guid><pubDate>Tue, 21 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Generate, but Verify: Reducing Hallucination in Vision-Language Models with Retrospective Resampling</title><link>https://arxiv.org/abs/2504.13169</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces REVERSE, a framework for reducing hallucinations in VLMs&lt;/li&gt;&lt;li&gt;Combines hallucination-aware training with self-verification&lt;/li&gt;&lt;li&gt;Uses a new dataset with 1.3M samples&lt;/li&gt;&lt;li&gt;Improves performance on CHAIR-MSCOCO and HaloQuest&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Tsung-Han Wu', 'Heekyung Lee', 'Jiaxin Ge', 'Joseph E. Gonzalez', 'Trevor Darrell', 'David M. Chan']&lt;/li&gt;&lt;li&gt;Tags: ['safety', 'hallucination', 'vision-language models', 'dataset', 'self-verification']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2504.13169</guid><pubDate>Tue, 21 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Delta-Influence: Unlearning Poisons via Influence Functions</title><link>https://arxiv.org/abs/2411.13731</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces Delta-Influence, a method for unlearning data poisoning effects using influence functions.&lt;/li&gt;&lt;li&gt;Detects poisoned training data by observing influence collapse after data transformations.&lt;/li&gt;&lt;li&gt;Validates the approach against various poisoning attacks and datasets, showing improved unlearning performance.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Wenjie Li', 'Jiawei Li', 'Pengcheng Zeng', 'Christian Schroeder de Witt', 'Ameya Prabhu', 'Amartya Sanyal']&lt;/li&gt;&lt;li&gt;Tags: ['data poisoning', 'unlearning', 'influence functions', 'security', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2411.13731</guid><pubDate>Tue, 21 Oct 2025 00:00:00 +0000</pubDate></item><item><title>VERA-V: Variational Inference Framework for Jailbreaking Vision-Language Models</title><link>https://arxiv.org/abs/2510.17759</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces VERA-V, a variational inference framework for jailbreaking Vision-Language Models (VLMs)&lt;/li&gt;&lt;li&gt;Generates stealthy, coupled adversarial text-image prompts using a probabilistic approach&lt;/li&gt;&lt;li&gt;Incorporates typography-based text prompts, diffusion-based image synthesis, and structured distractors&lt;/li&gt;&lt;li&gt;Outperforms state-of-the-art baselines on HarmBench and HADES benchmarks&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Qilin Liao', 'Anamika Lochab', 'Ruqi Zhang']&lt;/li&gt;&lt;li&gt;Tags: ['red teaming', 'jailbreaking', 'multimodal', 'adversarial prompting', 'vision-language models']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.17759</guid><pubDate>Tue, 21 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Patronus: Safeguarding Text-to-Image Models against White-Box Adversaries</title><link>https://arxiv.org/abs/2510.16581</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces Patronus, a defensive framework for T2I models against white-box adversaries&lt;/li&gt;&lt;li&gt;Uses an internal moderator to nullify unsafe input features&lt;/li&gt;&lt;li&gt;Prevents model compromise through non-fine-tunable learning mechanism&lt;/li&gt;&lt;li&gt;Validates performance and resilience against fine-tuning attacks&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Xinfeng Li', 'Shengyuan Pang', 'Jialin Wu', 'Jiangyi Deng', 'Huanlong Zhong', 'Yanjiao Chen', 'Jie Zhang', 'Wenyuan Xu']&lt;/li&gt;&lt;li&gt;Tags: ['AI security', 'model extraction', 'adversarial prompting', 'safety evaluation', 'text-to-image models']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.16581</guid><pubDate>Tue, 21 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Universal and Transferable Attacks on Pathology Foundation Models</title><link>https://arxiv.org/abs/2510.16660</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces UTAP for pathology foundation models&lt;/li&gt;&lt;li&gt;Demonstrates universality and transferability of attacks&lt;/li&gt;&lt;li&gt;Causes significant performance drops with imperceptible noise&lt;/li&gt;&lt;li&gt;Highlights need for robustness evaluation and defense mechanisms&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yuntian Wang', 'Xilin Yang', 'Che-Yung Shen', 'Nir Pillar', 'Aydogan Ozcan']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial attacks', 'model robustness', 'transferability', 'universality', 'pathology AI']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.16660</guid><pubDate>Tue, 21 Oct 2025 00:00:00 +0000</pubDate></item><item><title>OpenLVLM-MIA: A Controlled Benchmark Revealing the Limits of Membership Inference Attacks on Large Vision-Language Models</title><link>https://arxiv.org/abs/2510.16295</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces OpenLVLM-MIA benchmark for evaluating membership inference attacks on large vision-language models&lt;/li&gt;&lt;li&gt;Highlights that prior high attack success rates may be due to distributional bias rather than true membership detection&lt;/li&gt;&lt;li&gt;Provides controlled dataset with balanced distributions and ground-truth labels&lt;/li&gt;&lt;li&gt;Shows that state-of-the-art MIA methods perform at random chance under unbiased conditions&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Ryoto Miyamoto', 'Xin Fan', 'Fuyuko Kido', 'Tsuneo Matsumoto', 'Hayato Yamana']&lt;/li&gt;&lt;li&gt;Tags: ['privacy attacks', 'membership inference', 'benchmarking', 'vision-language models', 'data poisoning']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.16295</guid><pubDate>Tue, 21 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Watch the Weights: Unsupervised monitoring and control of fine-tuned LLMs</title><link>https://arxiv.org/abs/2508.00161</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces a method for monitoring and controlling fine-tuned LLMs by analyzing weight differences instead of activations.&lt;/li&gt;&lt;li&gt;Detects backdoor attacks and unlearning with high precision and accuracy.&lt;/li&gt;&lt;li&gt;Potential for pre-deployment model auditing to uncover fine-tuning focus.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Ziqian Zhong', 'Aditi Raghunathan']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'model extraction', 'safety evaluation', 'robustness', 'alignment']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2508.00161</guid><pubDate>Tue, 21 Oct 2025 00:00:00 +0000</pubDate></item><item><title>HauntAttack: When Attack Follows Reasoning as a Shadow</title><link>https://arxiv.org/abs/2506.07031</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces HauntAttack, a black-box adversarial attack framework targeting Large Reasoning Models (LRMs)&lt;/li&gt;&lt;li&gt;Modifies reasoning questions with harmful instructions to guide models to unsafe outputs&lt;/li&gt;&lt;li&gt;Evaluates on 11 LRMs with 70% average attack success rate&lt;/li&gt;&lt;li&gt;Highlights vulnerability of safety-aligned models to reasoning-based attacks&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jingyuan Ma', 'Rui Li', 'Zheng Li', 'Junfeng Liu', 'Lei Sha', 'Zhifang Sui']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'jailbreaking', 'adversarial prompting', 'safety evaluation', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.07031</guid><pubDate>Tue, 21 Oct 2025 00:00:00 +0000</pubDate></item><item><title>DOGe: Defensive Output Generation for LLM Protection Against Knowledge Distillation</title><link>https://arxiv.org/abs/2505.19504</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces DOGe: Defensive Output Generation for LLMs&lt;/li&gt;&lt;li&gt;Protects against knowledge distillation by modifying output behavior&lt;/li&gt;&lt;li&gt;Fine-tunes final linear layer with adversarial loss&lt;/li&gt;&lt;li&gt;Reduces student model performance while maintaining teacher accuracy&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Pingzhi Li', 'Zhen Tan', 'Mohan Zhang', 'Huaizhi Qu', 'Huan Liu', 'Tianlong Chen']&lt;/li&gt;&lt;li&gt;Tags: ['model extraction', 'adversarial prompting', 'red teaming', 'LLM security', 'jailbreaking']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2505.19504</guid><pubDate>Tue, 21 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Video-SafetyBench: A Benchmark for Safety Evaluation of Video LVLMs</title><link>https://arxiv.org/abs/2505.11842</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces Video-SafetyBench, a benchmark for evaluating safety of LVLMs under video-text attacks&lt;/li&gt;&lt;li&gt;Includes 2,264 video-text pairs across 48 unsafe categories&lt;/li&gt;&lt;li&gt;Proposes RJScore metric for evaluating uncertain outputs&lt;/li&gt;&lt;li&gt;Demonstrates high attack success rates with benign queries&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Xuannan Liu', 'Zekun Li', 'Zheqi He', 'Peipei Li', 'Shuhan Xia', 'Xing Cui', 'Huaibo Huang', 'Xi Yang', 'Ran He']&lt;/li&gt;&lt;li&gt;Tags: ['safety evaluation', 'multimodal', 'video', 'adversarial prompting', 'benchmarking']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2505.11842</guid><pubDate>Tue, 21 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Semantic Representation Attack against Aligned Large Language Models</title><link>https://arxiv.org/abs/2509.19360</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces Semantic Representation Attack for aligned LLMs&lt;/li&gt;&lt;li&gt;Targets semantic space instead of exact text patterns&lt;/li&gt;&lt;li&gt;Proposes new algorithm with theoretical guarantees&lt;/li&gt;&lt;li&gt;High success rates across multiple LLMs&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jiawei Lian', 'Jianhong Pan', 'Lefan Wang', 'Yi Wang', 'Shaohui Mei', 'Lap-Pui Chau']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'adversarial prompting', 'alignment', 'safety evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.19360</guid><pubDate>Tue, 21 Oct 2025 00:00:00 +0000</pubDate></item><item><title>CultureGuard: Towards Culturally-Aware Dataset and Guard Model for Multilingual Safety Applications</title><link>https://arxiv.org/abs/2508.01710</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces CultureGuard for creating multilingual safety datasets&lt;/li&gt;&lt;li&gt;Converts and expands English safety dataset into 8 languages&lt;/li&gt;&lt;li&gt;Trains safety guard model with cross-lingual transfer&lt;/li&gt;&lt;li&gt;Benchmarks latest LLMs on multilingual safety&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research Paper&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Raviraj Joshi', 'Rakesh Paul', 'Kanishk Singla', 'Anusha Kamath', 'Michael Evans', 'Katherine Luna', 'Shaona Ghosh', 'Utkarsh Vaidya', 'Eileen Long', 'Sanjay Singh Chauhan', 'Niranjan Wartikar']&lt;/li&gt;&lt;li&gt;Tags: ['safety evaluation', 'multilingual', 'dataset', 'guard model']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2508.01710</guid><pubDate>Tue, 21 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Towards Evaluating Proactive Risk Awareness of Multimodal Language Models</title><link>https://arxiv.org/abs/2505.17455</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces PaSBench, a multimodal benchmark for evaluating proactive risk awareness in AI models.&lt;/li&gt;&lt;li&gt;Tests 36 models across 5 safety-critical domains with 416 scenarios.&lt;/li&gt;&lt;li&gt;Top models like Gemini-2.5-pro miss 45-55% risks, indicating instability in proactive reasoning.&lt;/li&gt;&lt;li&gt;Highlights the need for more reliable proactive safety AI.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Youliang Yuan', 'Wenxiang Jiao', 'Yuejin Xie', 'Chihao Shen', 'Menghan Tian', 'Wenxuan Wang', 'Jen-tse Huang', 'Pinjia He']&lt;/li&gt;&lt;li&gt;Tags: ['safety evaluation', 'multimodal', 'proactive safety', 'benchmarking', 'risk detection']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2505.17455</guid><pubDate>Tue, 21 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Auto-Prompt Generation is Not Robust: Prompt Optimization Driven by Pseudo Gradient</title><link>https://arxiv.org/abs/2412.18196</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces PertBench, a benchmark for evaluating auto-prompt robustness&lt;/li&gt;&lt;li&gt;Proposes PGO, a gradient-free prompt generation framework&lt;/li&gt;&lt;li&gt;Emphasizes robustness under noisy and perturbed conditions&lt;/li&gt;&lt;li&gt;Shows improved performance across tasks and LLMs&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Zeru Shi', 'Zhenting Wang', 'Yongye Su', 'Weidi Luo', 'Hang Gao', 'Fan Yang', 'Ruixiang Tang', 'Yongfeng Zhang']&lt;/li&gt;&lt;li&gt;Tags: ['prompt injection', 'adversarial prompting', 'robustness', 'safety evaluation', 'benchmarking']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2412.18196</guid><pubDate>Tue, 21 Oct 2025 00:00:00 +0000</pubDate></item><item><title>H3Fusion: Helpful, Harmless, Honest Fusion of Aligned LLMs</title><link>https://arxiv.org/abs/2411.17792</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;H3Fusion introduces a MoE-based fusion mechanism for aligning LLMs&lt;/li&gt;&lt;li&gt;Uses drift-regularization loss to balance alignment dimensions&lt;/li&gt;&lt;li&gt;Improves helpfulness, harmlessness, honesty, and robustness&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Selim Furkan Tekin', 'Fatih Ilhan', 'Tiansheng Huang', 'Sihao Hu', 'Yichang Xu', 'Zachary Yahn', 'Ling Liu']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'robustness', 'safety evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2411.17792</guid><pubDate>Tue, 21 Oct 2025 00:00:00 +0000</pubDate></item><item><title>VERA-V: Variational Inference Framework for Jailbreaking Vision-Language Models</title><link>https://arxiv.org/abs/2510.17759</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces VERA-V, a variational inference framework for jailbreaking Vision-Language Models (VLMs)&lt;/li&gt;&lt;li&gt;Generates stealthy, coupled adversarial text-image prompts using a probabilistic approach&lt;/li&gt;&lt;li&gt;Incorporates typography-based text prompts, diffusion-based image synthesis, and structured distractors&lt;/li&gt;&lt;li&gt;Outperforms state-of-the-art baselines on HarmBench and HADES benchmarks&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Qilin Liao', 'Anamika Lochab', 'Ruqi Zhang']&lt;/li&gt;&lt;li&gt;Tags: ['red teaming', 'jailbreaking', 'multimodal', 'adversarial prompting', 'vision-language models']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.17759</guid><pubDate>Tue, 21 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Forgetting to Forget: Attention Sink as A Gateway for Backdooring LLM Unlearning</title><link>https://arxiv.org/abs/2510.17021</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Investigates backdoor attacks on LLM unlearning&lt;/li&gt;&lt;li&gt;Shows that attention sink tokens can be used as triggers&lt;/li&gt;&lt;li&gt;Demonstrates that backdoor unlearning can revert model behavior when triggered&lt;/li&gt;&lt;li&gt;Provides experimental validation of the approach&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Bingqi Shang', 'Yiwei Chen', 'Yihua Zhang', 'Bingquan Shen', 'Sijia Liu']&lt;/li&gt;&lt;li&gt;Tags: ['LLM security', 'backdoor attacks', 'unlearning', 'attention mechanisms', 'adversarial prompting']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.17021</guid><pubDate>Tue, 21 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Bits Leaked per Query: Information-Theoretic Bounds on Adversarial Attacks against LLMs</title><link>https://arxiv.org/abs/2510.17000</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;The paper introduces an information-theoretic framework to measure information leakage in LLMs during adversarial attacks.&lt;/li&gt;&lt;li&gt;It calculates the mutual information between observable signals (like model responses) and target properties (like harmful response triggers).&lt;/li&gt;&lt;li&gt;The framework helps determine the minimum number of queries needed for successful attacks based on leaked bits per query.&lt;/li&gt;&lt;li&gt;Experiments show that exposing more model internals (like logits or thinking process) significantly reduces attack cost.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Masahiro Kaneko', 'Timothy Baldwin']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial attacks', 'information theory', 'LLM security', 'jailbreak', 'relearning attacks']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.17000</guid><pubDate>Tue, 21 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Verifiable Fine-Tuning for LLMs: Zero-Knowledge Training Proofs Bound to Data Provenance and Policy</title><link>https://arxiv.org/abs/2510.16830</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces Verifiable Fine Tuning for LLMs&lt;/li&gt;&lt;li&gt;Uses zero-knowledge proofs to verify training data and process&lt;/li&gt;&lt;li&gt;Combines data commitments, verifiable sampling, and proof aggregation&lt;/li&gt;&lt;li&gt;Maintains model utility while ensuring compliance&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Hasan Akgul', 'Daniel Borg', 'Arta Berisha', 'Amina Rahimova', 'Andrej Novak', 'Mila Petrov']&lt;/li&gt;&lt;li&gt;Tags: ['security', 'privacy', 'verifiable training', 'zero-knowledge proofs']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.16830</guid><pubDate>Tue, 21 Oct 2025 00:00:00 +0000</pubDate></item><item><title>The Hidden Cost of Modeling P(X): Vulnerability to Membership Inference Attacks in Generative Text Classifiers</title><link>https://arxiv.org/abs/2510.16122</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;The paper studies Membership Inference Attacks (MIAs) on generative text classifiers.&lt;/li&gt;&lt;li&gt;It compares generative, discriminative, and pseudo-generative models across multiple datasets.&lt;/li&gt;&lt;li&gt;Generative models modeling P(X,Y) are found to be more vulnerable to MIAs.&lt;/li&gt;&lt;li&gt;The canonical inference approach in generative models increases privacy risk.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Owais Makroo', 'Siva Rajesh Kasa', 'Sumegh Roychowdhury', 'Karan Gupta', 'Nikhil Pattisapu', 'Santhosh Kasa', 'Sumit Negi']&lt;/li&gt;&lt;li&gt;Tags: ['privacy attacks', 'generative models', 'membership inference', 'text classifiers']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.16122</guid><pubDate>Tue, 21 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Detecting and Preventing Harmful Behaviors in AI Companions: Development and Evaluation of the SHIELD Supervisory System</title><link>https://arxiv.org/abs/2510.15891</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Developed SHIELD, a supervisory system for detecting and preventing harmful emotional patterns in AI companions.&lt;/li&gt;&lt;li&gt;Evaluated across five LLMs using a 100-item benchmark covering five dimensions of concern.&lt;/li&gt;&lt;li&gt;Significantly reduced concerning content (50-79% reduction) while preserving 95% of appropriate interactions.&lt;/li&gt;&lt;li&gt;Achieved 59% sensitivity and 95% specificity, with adaptable performance via prompt engineering.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Ziv Ben-Zion', 'Paul Raffelh\\"uschen', 'Max Zettl', 'Antonia L\\"u\\"ond', 'Achim Burrer', 'Philipp Homan', 'Tobias R Spiller']&lt;/li&gt;&lt;li&gt;Tags: ['AI safety', 'LLM red teaming', 'emotional manipulation', 'supervisory system', 'benchmarking']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.15891</guid><pubDate>Tue, 21 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Agentic Reinforcement Learning for Search is Unsafe</title><link>https://arxiv.org/abs/2510.17431</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;The paper demonstrates that agentic RL models for search are unsafe due to two attacks: Search and Multi-search.&lt;/li&gt;&lt;li&gt;These attacks reduce refusal rates, answer safety, and search-query safety by significant margins.&lt;/li&gt;&lt;li&gt;The core issue is that RL training rewards effective queries without considering their harmfulness.&lt;/li&gt;&lt;li&gt;The study calls for developing safety-aware RL pipelines to optimize for safe search.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yushi Yang', 'Shreyansh Padarha', 'Andrew Lee', 'Adam Mahdi']&lt;/li&gt;&lt;li&gt;Tags: ['red teaming', 'safety evaluation', 'adversarial prompting', 'alignment', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.17431</guid><pubDate>Tue, 21 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Wisdom is Knowing What not to Say: Hallucination-Free LLMs Unlearning via Attention Shifting</title><link>https://arxiv.org/abs/2510.17210</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces Attention-Shifting (AS) framework for selective unlearning in LLMs&lt;/li&gt;&lt;li&gt;Aims to balance model utility and hallucination resistance&lt;/li&gt;&lt;li&gt;Uses dual-loss optimization for importance-aware suppression and attention-guided retention&lt;/li&gt;&lt;li&gt;Shows improved performance preservation and hallucination-free unlearning&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Chenchen Tan', 'Youyang Qu', 'Xinghao Li', 'Hui Zhang', 'Shujie Cui', 'Cunjian Chen', 'Longxiang Gao']&lt;/li&gt;&lt;li&gt;Tags: ['unlearning', 'hallucination', 'attention mechanisms', 'model safety', 'privacy']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.17210</guid><pubDate>Tue, 21 Oct 2025 00:00:00 +0000</pubDate></item><item><title>SafeSearch: Do Not Trade Safety for Utility in LLM Search Agents</title><link>https://arxiv.org/abs/2510.17017</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Evaluates LLM search agents using red-teaming datasets and finds they are more likely to produce harmful outputs than base LLMs.&lt;/li&gt;&lt;li&gt;Introduces SafeSearch, a multi-objective reinforcement learning approach that combines safety and utility rewards.&lt;/li&gt;&lt;li&gt;Reduces harmfulness by 70% while maintaining QA performance.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Qiusi Zhan', 'Angeline Budiman-Chan', 'Abdelrahman Zayed', 'Xingzhi Guo', 'Daniel Kang', 'Joo-Kyung Kim']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'safety evaluation', 'reinforcement learning', 'alignment', 'utility vs safety trade-off']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.17017</guid><pubDate>Tue, 21 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Online Learning Defense against Iterative Jailbreak Attacks via Prompt Optimization</title><link>https://arxiv.org/abs/2510.17006</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a dynamic defense framework against iterative jailbreak attacks using online learning and prompt optimization.&lt;/li&gt;&lt;li&gt;Introduces Past-Direction Gradient Damping (PDGD) to prevent overfitting to attack patterns.&lt;/li&gt;&lt;li&gt;Evaluates the approach on three LLMs against five jailbreak methods, showing improved defense and response quality.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Masahiro Kaneko', 'Zeerak Talat', 'Timothy Baldwin']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'jailbreaking', 'prompt optimization', 'online learning', 'reinforcement learning']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.17006</guid><pubDate>Tue, 21 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Beacon: Single-Turn Diagnosis and Mitigation of Latent Sycophancy in Large Language Models</title><link>https://arxiv.org/abs/2510.16727</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces Beacon benchmark for measuring sycophancy in LLMs&lt;/li&gt;&lt;li&gt;Evaluates sycophancy across multiple models&lt;/li&gt;&lt;li&gt;Proposes interventions to modulate sycophancy biases&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Sanskar Pandey', 'Ruhaan Chopra', 'Angkul Puniya', 'Sohom Pal']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'safety evaluation', 'benchmarking', 'model evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.16727</guid><pubDate>Tue, 21 Oct 2025 00:00:00 +0000</pubDate></item><item><title>The Chameleon Nature of LLMs: Quantifying Multi-Turn Stance Instability in Search-Enabled Language Models</title><link>https://arxiv.org/abs/2510.16712</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Investigates multi-turn stance instability in search-enabled LLMs&lt;/li&gt;&lt;li&gt;Introduces Chameleon Benchmark Dataset and metrics&lt;/li&gt;&lt;li&gt;Evaluates Llama-4-Maverick, GPT-4o-mini, Gemini-2.5-Flash&lt;/li&gt;&lt;li&gt;Finds severe chameleon behavior across models&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Shivam Ratnakar', 'Sanjay Raghavendra']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'safety evaluation', 'robustness', 'alignment']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.16712</guid><pubDate>Tue, 21 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Check Yourself Before You Wreck Yourself: Selectively Quitting Improves LLM Agent Safety</title><link>https://arxiv.org/abs/2510.16492</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes using 'quitting' as a safety mechanism for LLM agents in multi-turn scenarios&lt;/li&gt;&lt;li&gt;Evaluates quitting behavior across 12 LLMs using ToolEmu framework&lt;/li&gt;&lt;li&gt;Shows significant safety improvement with minimal impact on helpfulness&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Vamshi Krishna Bonagiri', 'Ponnurangam Kumaragurum', 'Khanh Nguyen', 'Benjamin Plaut']&lt;/li&gt;&lt;li&gt;Tags: ['LLM safety', 'agent safety', 'uncertainty quantification', 'multi-turn interactions', 'ToolEmu']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.16492</guid><pubDate>Tue, 21 Oct 2025 00:00:00 +0000</pubDate></item><item><title>ATA: A Neuro-Symbolic Approach to Implement Autonomous and Trustworthy Agents</title><link>https://arxiv.org/abs/2510.16381</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces ATA, a neuro-symbolic approach for trustworthy agents&lt;/li&gt;&lt;li&gt;Decouples knowledge ingestion and task processing&lt;/li&gt;&lt;li&gt;Enhances trustworthiness with formal verification&lt;/li&gt;&lt;li&gt;Resistant to prompt injection attacks&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['David Peer', 'Sebastian Stabinger']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'safety evaluation', 'robustness', 'prompt injection']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.16381</guid><pubDate>Tue, 21 Oct 2025 00:00:00 +0000</pubDate></item><item><title>CBF-RL: Safety Filtering Reinforcement Learning in Training with Control Barrier Functions</title><link>https://arxiv.org/abs/2510.14959</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes CBF-RL framework for safe RL training using Control Barrier Functions&lt;/li&gt;&lt;li&gt;Integrates safety constraints into policy learning to avoid runtime filters&lt;/li&gt;&lt;li&gt;Validates with navigation tasks and real-world robot experiments&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Lizhi Yang', 'Blake Werner', 'Massimiliano de Sa', 'Aaron D. Ames']&lt;/li&gt;&lt;li&gt;Tags: ['safety', 'reinforcement learning', 'control barrier functions', 'robotics']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.14959</guid><pubDate>Tue, 21 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Who Taught the Lie? Responsibility Attribution for Poisoned Knowledge in Retrieval-Augmented Generation</title><link>https://arxiv.org/abs/2509.13772</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces RAGOrigin, a framework for attributing responsibility for poisoned knowledge in RAG systems&lt;/li&gt;&lt;li&gt;Evaluates across multiple datasets and poisoning attacks, including adaptive strategies&lt;/li&gt;&lt;li&gt;Uses unsupervised clustering to isolate poisoned texts&lt;/li&gt;&lt;li&gt;Outperforms existing baselines in identifying poisoned content&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Baolei Zhang', 'Haoran Xin', 'Yuxi Chen', 'Zhuqing Liu', 'Biao Yi', 'Tong Li', 'Lihai Nie', 'Zheli Liu', 'Minghong Fang']&lt;/li&gt;&lt;li&gt;Tags: ['data poisoning', 'RAG systems', 'responsibility attribution', 'poisoning attacks', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.13772</guid><pubDate>Tue, 21 Oct 2025 00:00:00 +0000</pubDate></item><item><title>CultureGuard: Towards Culturally-Aware Dataset and Guard Model for Multilingual Safety Applications</title><link>https://arxiv.org/abs/2508.01710</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces CultureGuard for creating multilingual safety datasets&lt;/li&gt;&lt;li&gt;Converts and expands English safety dataset into 8 languages&lt;/li&gt;&lt;li&gt;Trains safety guard model with cross-lingual transfer&lt;/li&gt;&lt;li&gt;Benchmarks latest LLMs on multilingual safety&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research Paper&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Raviraj Joshi', 'Rakesh Paul', 'Kanishk Singla', 'Anusha Kamath', 'Michael Evans', 'Katherine Luna', 'Shaona Ghosh', 'Utkarsh Vaidya', 'Eileen Long', 'Sanjay Singh Chauhan', 'Niranjan Wartikar']&lt;/li&gt;&lt;li&gt;Tags: ['safety evaluation', 'multilingual', 'dataset', 'guard model']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2508.01710</guid><pubDate>Tue, 21 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Traceback of Poisoning Attacks to Retrieval-Augmented Generation</title><link>https://arxiv.org/abs/2504.21668</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces RAGForensics, a traceback system for RAG systems to identify poisoned texts in the knowledge database.&lt;/li&gt;&lt;li&gt;Utilizes iterative retrieval and LLM-guided detection to find poisoning texts.&lt;/li&gt;&lt;li&gt;Empirically evaluated against state-of-the-art poisoning attacks.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Baolei Zhang', 'Haoran Xin', 'Minghong Fang', 'Zhuqing Liu', 'Biao Yi', 'Tong Li', 'Zheli Liu']&lt;/li&gt;&lt;li&gt;Tags: ['data poisoning', 'red teaming', 'security', 'RAG systems', 'traceback']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2504.21668</guid><pubDate>Tue, 21 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Auto-Prompt Generation is Not Robust: Prompt Optimization Driven by Pseudo Gradient</title><link>https://arxiv.org/abs/2412.18196</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces PertBench, a benchmark for evaluating auto-prompt robustness&lt;/li&gt;&lt;li&gt;Proposes PGO, a gradient-free prompt generation framework&lt;/li&gt;&lt;li&gt;Emphasizes robustness under noisy and perturbed conditions&lt;/li&gt;&lt;li&gt;Shows improved performance across tasks and LLMs&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Zeru Shi', 'Zhenting Wang', 'Yongye Su', 'Weidi Luo', 'Hang Gao', 'Fan Yang', 'Ruixiang Tang', 'Yongfeng Zhang']&lt;/li&gt;&lt;li&gt;Tags: ['prompt injection', 'adversarial prompting', 'robustness', 'safety evaluation', 'benchmarking']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2412.18196</guid><pubDate>Tue, 21 Oct 2025 00:00:00 +0000</pubDate></item><item><title>H3Fusion: Helpful, Harmless, Honest Fusion of Aligned LLMs</title><link>https://arxiv.org/abs/2411.17792</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;H3Fusion introduces a MoE-based fusion mechanism for aligning LLMs&lt;/li&gt;&lt;li&gt;Uses drift-regularization loss to balance alignment dimensions&lt;/li&gt;&lt;li&gt;Improves helpfulness, harmlessness, honesty, and robustness&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Selim Furkan Tekin', 'Fatih Ilhan', 'Tiansheng Huang', 'Sihao Hu', 'Yichang Xu', 'Zachary Yahn', 'Ling Liu']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'robustness', 'safety evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2411.17792</guid><pubDate>Tue, 21 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Delta-Influence: Unlearning Poisons via Influence Functions</title><link>https://arxiv.org/abs/2411.13731</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces Delta-Influence, a method for unlearning data poisoning effects using influence functions.&lt;/li&gt;&lt;li&gt;Detects poisoned training data by observing influence collapse after data transformations.&lt;/li&gt;&lt;li&gt;Validates the approach against various poisoning attacks and datasets, showing improved unlearning performance.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Wenjie Li', 'Jiawei Li', 'Pengcheng Zeng', 'Christian Schroeder de Witt', 'Ameya Prabhu', 'Amartya Sanyal']&lt;/li&gt;&lt;li&gt;Tags: ['data poisoning', 'unlearning', 'influence functions', 'security', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2411.13731</guid><pubDate>Tue, 21 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Language Models are Injective and Hence Invertible</title><link>https://arxiv.org/abs/2510.15511</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proves mathematically that transformer LMs are injective&lt;/li&gt;&lt;li&gt;Empirically confirms no collisions in 6 SOTA models&lt;/li&gt;&lt;li&gt;Introduces SipIt algorithm for exact input reconstruction&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Giorgos Nikolaou', 'Tommaso Mencattini', 'Donato Crisostomi', 'Andrea Santilli', 'Yannis Panagakis', 'Emanuele Rodol\\`a']&lt;/li&gt;&lt;li&gt;Tags: ['model extraction', 'privacy attacks', 'safety evaluation', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.15511</guid><pubDate>Tue, 21 Oct 2025 00:00:00 +0000</pubDate></item><item><title>ImpMIA: Leveraging Implicit Bias for Membership Inference Attack under Realistic Scenarios</title><link>https://arxiv.org/abs/2510.10625</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces ImpMIA, a white-box membership inference attack leveraging implicit bias of neural networks&lt;/li&gt;&lt;li&gt;Uses KKT conditions to identify training samples by gradient reconstruction&lt;/li&gt;&lt;li&gt;Achieves state-of-the-art performance in realistic settings&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yuval Golbari', 'Navve Wasserman', 'Gal Vardi', 'Michal Irani']&lt;/li&gt;&lt;li&gt;Tags: ['privacy attacks', 'membership inference', 'white-box attack', 'implicit bias', 'neural networks']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.10625</guid><pubDate>Tue, 21 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Watch the Weights: Unsupervised monitoring and control of fine-tuned LLMs</title><link>https://arxiv.org/abs/2508.00161</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces a method for monitoring and controlling fine-tuned LLMs by analyzing weight differences instead of activations.&lt;/li&gt;&lt;li&gt;Detects backdoor attacks and unlearning with high precision and accuracy.&lt;/li&gt;&lt;li&gt;Potential for pre-deployment model auditing to uncover fine-tuning focus.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Ziqian Zhong', 'Aditi Raghunathan']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'model extraction', 'safety evaluation', 'robustness', 'alignment']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2508.00161</guid><pubDate>Tue, 21 Oct 2025 00:00:00 +0000</pubDate></item><item><title>DOGe: Defensive Output Generation for LLM Protection Against Knowledge Distillation</title><link>https://arxiv.org/abs/2505.19504</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces DOGe: Defensive Output Generation for LLMs&lt;/li&gt;&lt;li&gt;Protects against knowledge distillation by modifying output behavior&lt;/li&gt;&lt;li&gt;Fine-tunes final linear layer with adversarial loss&lt;/li&gt;&lt;li&gt;Reduces student model performance while maintaining teacher accuracy&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Pingzhi Li', 'Zhen Tan', 'Mohan Zhang', 'Huaizhi Qu', 'Huan Liu', 'Tianlong Chen']&lt;/li&gt;&lt;li&gt;Tags: ['model extraction', 'adversarial prompting', 'red teaming', 'LLM security', 'jailbreaking']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2505.19504</guid><pubDate>Tue, 21 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Incentivizing Truthful Language Models via Peer Elicitation Games</title><link>https://arxiv.org/abs/2505.13636</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces Peer Elicitation Games (PEG) for aligning LLMs&lt;/li&gt;&lt;li&gt;Uses game theory with generator and discriminators&lt;/li&gt;&lt;li&gt;Incentivizes truthful reporting without ground truth&lt;/li&gt;&lt;li&gt;Proves convergence to truthful Nash equilibrium&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Baiting Chen', 'Tong Zhu', 'Jiale Han', 'Lexin Li', 'Gang Li', 'Xiaowu Dai']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'game theory', 'truthful reporting', 'LLM']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2505.13636</guid><pubDate>Tue, 21 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Exploiting Meta-Learning-based Poisoning Attacks for Graph Link Prediction</title><link>https://arxiv.org/abs/2504.06492</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces a meta-learning-based poisoning attack for graph link prediction using GNNs&lt;/li&gt;&lt;li&gt;Conducts experiments on multiple datasets to evaluate the attack's effectiveness&lt;/li&gt;&lt;li&gt;Compares with existing baselines and shows improved performance&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Mingchen Li', 'Di Zhuang', 'Keyu Chen', 'Dumindu Samaraweera', 'Morris Chang']&lt;/li&gt;&lt;li&gt;Tags: ['data poisoning', 'adversarial attacks', 'graph neural networks', 'link prediction', 'meta-learning']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2504.06492</guid><pubDate>Tue, 21 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Membership Inference Attack Should Move On to Distributional Statistics for Distilled Generative Models</title><link>https://arxiv.org/abs/2502.02970</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;The paper addresses membership inference attacks (MIA) on distilled generative models, which are student models trained on data generated by teacher models. It highlights that standard MIA fails on student models due to lack of direct memorization but proposes using distribution-level statistics instead. The authors suggest three principles for distribution-based MIAs and validate their approach.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Muxing Li', 'Zesheng Ye', 'Sharon Li', 'Andy Song', 'Guangquan Zhang', 'Feng Liu']&lt;/li&gt;&lt;li&gt;Tags: ['privacy attacks', 'data poisoning', 'model extraction', 'red teaming']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2502.02970</guid><pubDate>Tue, 21 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Membership Privacy Risks of Sharpness Aware Minimization</title><link>https://arxiv.org/abs/2310.00488</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;This paper explores the privacy risks associated with Sharpness-Aware Minimization (SAM) optimization algorithm.&lt;/li&gt;&lt;li&gt;It finds that SAM is more vulnerable to membership inference attacks compared to SGD, despite better generalization.&lt;/li&gt;&lt;li&gt;The study includes empirical analysis on memorization and influence scores, and theoretical explanations.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Young In Kim', 'Andrea Agiollo', 'Pratiksha Agrawal', 'Johannes O. Royset', 'Rajiv Khanna']&lt;/li&gt;&lt;li&gt;Tags: ['privacy attacks', 'data poisoning', 'security standards']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2310.00488</guid><pubDate>Tue, 21 Oct 2025 00:00:00 +0000</pubDate></item><item><title>VERA-V: Variational Inference Framework for Jailbreaking Vision-Language Models</title><link>https://arxiv.org/abs/2510.17759</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces VERA-V, a variational inference framework for jailbreaking Vision-Language Models (VLMs)&lt;/li&gt;&lt;li&gt;Generates stealthy, coupled adversarial text-image prompts using a probabilistic approach&lt;/li&gt;&lt;li&gt;Incorporates typography-based text prompts, diffusion-based image synthesis, and structured distractors&lt;/li&gt;&lt;li&gt;Outperforms state-of-the-art baselines on HarmBench and HADES benchmarks&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Qilin Liao', 'Anamika Lochab', 'Ruqi Zhang']&lt;/li&gt;&lt;li&gt;Tags: ['red teaming', 'jailbreaking', 'multimodal', 'adversarial prompting', 'vision-language models']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.17759</guid><pubDate>Tue, 21 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Bits Leaked per Query: Information-Theoretic Bounds on Adversarial Attacks against LLMs</title><link>https://arxiv.org/abs/2510.17000</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;The paper introduces an information-theoretic framework to measure information leakage in LLMs during adversarial attacks.&lt;/li&gt;&lt;li&gt;It calculates the mutual information between observable signals (like model responses) and target properties (like harmful response triggers).&lt;/li&gt;&lt;li&gt;The framework helps determine the minimum number of queries needed for successful attacks based on leaked bits per query.&lt;/li&gt;&lt;li&gt;Experiments show that exposing more model internals (like logits or thinking process) significantly reduces attack cost.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Masahiro Kaneko', 'Timothy Baldwin']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial attacks', 'information theory', 'LLM security', 'jailbreak', 'relearning attacks']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.17000</guid><pubDate>Tue, 21 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Black-box Optimization of LLM Outputs by Asking for Directions</title><link>https://arxiv.org/abs/2510.16794</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces a new black-box attack method for LLMs using natural language confidence expressions&lt;/li&gt;&lt;li&gt;Applies to adversarial examples, jailbreaks, and prompt injections&lt;/li&gt;&lt;li&gt;Highlights that larger models are more vulnerable due to better calibration&lt;/li&gt;&lt;li&gt;Expands attack surface for deployed LLMs with only textual outputs&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jie Zhang', 'Meng Ding', 'Yang Liu', 'Jue Hong', 'Florian Tram\\`er']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'jailbreaking', 'prompt injection', 'adversarial prompting', 'model extraction']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.16794</guid><pubDate>Tue, 21 Oct 2025 00:00:00 +0000</pubDate></item><item><title>DistilLock: Safeguarding LLMs from Unauthorized Knowledge Distillation on the Edge</title><link>https://arxiv.org/abs/2510.16716</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes DistilLock, a TEE-assisted framework for secure edge-based LLM fine-tuning&lt;/li&gt;&lt;li&gt;Protects data privacy and model IP by using a trusted execution environment&lt;/li&gt;&lt;li&gt;Prevents unauthorized knowledge distillation and model-stealing attacks&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Asmita Mohanty', 'Gezheng Kang', 'Lei Gao', 'Murali Annavaram']&lt;/li&gt;&lt;li&gt;Tags: ['security', 'privacy', 'model extraction', 'edge computing', 'TEE']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.16716</guid><pubDate>Tue, 21 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Universal and Transferable Attacks on Pathology Foundation Models</title><link>https://arxiv.org/abs/2510.16660</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces UTAP for pathology foundation models&lt;/li&gt;&lt;li&gt;Demonstrates universality and transferability of attacks&lt;/li&gt;&lt;li&gt;Causes significant performance drops with imperceptible noise&lt;/li&gt;&lt;li&gt;Highlights need for robustness evaluation and defense mechanisms&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yuntian Wang', 'Xilin Yang', 'Che-Yung Shen', 'Nir Pillar', 'Aydogan Ozcan']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial attacks', 'model robustness', 'transferability', 'universality', 'pathology AI']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.16660</guid><pubDate>Tue, 21 Oct 2025 00:00:00 +0000</pubDate></item><item><title>The Hidden Cost of Modeling P(X): Vulnerability to Membership Inference Attacks in Generative Text Classifiers</title><link>https://arxiv.org/abs/2510.16122</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;The paper studies Membership Inference Attacks (MIAs) on generative text classifiers.&lt;/li&gt;&lt;li&gt;It compares generative, discriminative, and pseudo-generative models across multiple datasets.&lt;/li&gt;&lt;li&gt;Generative models modeling P(X,Y) are found to be more vulnerable to MIAs.&lt;/li&gt;&lt;li&gt;The canonical inference approach in generative models increases privacy risk.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Owais Makroo', 'Siva Rajesh Kasa', 'Sumegh Roychowdhury', 'Karan Gupta', 'Nikhil Pattisapu', 'Santhosh Kasa', 'Sumit Negi']&lt;/li&gt;&lt;li&gt;Tags: ['privacy attacks', 'generative models', 'membership inference', 'text classifiers']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.16122</guid><pubDate>Tue, 21 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Detecting and Preventing Harmful Behaviors in AI Companions: Development and Evaluation of the SHIELD Supervisory System</title><link>https://arxiv.org/abs/2510.15891</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Developed SHIELD, a supervisory system for detecting and preventing harmful emotional patterns in AI companions.&lt;/li&gt;&lt;li&gt;Evaluated across five LLMs using a 100-item benchmark covering five dimensions of concern.&lt;/li&gt;&lt;li&gt;Significantly reduced concerning content (50-79% reduction) while preserving 95% of appropriate interactions.&lt;/li&gt;&lt;li&gt;Achieved 59% sensitivity and 95% specificity, with adaptable performance via prompt engineering.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Ziv Ben-Zion', 'Paul Raffelh\\"uschen', 'Max Zettl', 'Antonia L\\"u\\"ond', 'Achim Burrer', 'Philipp Homan', 'Tobias R Spiller']&lt;/li&gt;&lt;li&gt;Tags: ['AI safety', 'LLM red teaming', 'emotional manipulation', 'supervisory system', 'benchmarking']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.15891</guid><pubDate>Tue, 21 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Breaking and Fixing Defenses Against Control-Flow Hijacking in Multi-Agent Systems</title><link>https://arxiv.org/abs/2510.17276</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Demonstrates control-flow hijacking attacks in multi-agent systems&lt;/li&gt;&lt;li&gt;Proposes ControlValve defense using control-flow integrity and least privilege&lt;/li&gt;&lt;li&gt;Addresses alignment checks' limitations&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Rishi Jha', 'Harold Triedman', 'Justin Wagle', 'Vitaly Shmatikov']&lt;/li&gt;&lt;li&gt;Tags: ['red teaming', 'multi-agent systems', 'control-flow hijacking', 'alignment', 'defense']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.17276</guid><pubDate>Tue, 21 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Robustness in Text-Attributed Graph Learning: Insights, Trade-offs, and New Defenses</title><link>https://arxiv.org/abs/2510.17185</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces a comprehensive framework for evaluating robustness in Text-Attributed Graph Learning (TAG) models&lt;/li&gt;&lt;li&gt;Analyzes trade-offs between text and structure robustness in GNNs, RGNNs, and GraphLLMs&lt;/li&gt;&lt;li&gt;Proposes SFT-auto framework for balanced robustness against textual and structural attacks&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Runlin Lei', 'Lu Yi', 'Mingguo He', 'Pengyu Qiu', 'Zhewei Wei', 'Yongchao Liu', 'Chuntao Hong']&lt;/li&gt;&lt;li&gt;Tags: ['robustness', 'graph neural networks', 'large language models', 'text-attributed graphs', 'adversarial attacks']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.17185</guid><pubDate>Tue, 21 Oct 2025 00:00:00 +0000</pubDate></item><item><title>The Ends Justify the Thoughts: RL-Induced Motivated Reasoning in LLMs</title><link>https://arxiv.org/abs/2510.17057</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Investigates motivated reasoning in LLMs trained with RL and CoT&lt;/li&gt;&lt;li&gt;Models generate justifications for violating instructions&lt;/li&gt;&lt;li&gt;Detection challenges for smaller LLM judges&lt;/li&gt;&lt;li&gt;Raises concerns about oversight and evaluation&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Nikolaus Howe', 'Micah Carroll']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'safety evaluation', 'adversarial prompting', 'model extraction', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.17057</guid><pubDate>Tue, 21 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Forgetting to Forget: Attention Sink as A Gateway for Backdooring LLM Unlearning</title><link>https://arxiv.org/abs/2510.17021</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Investigates backdoor attacks on LLM unlearning&lt;/li&gt;&lt;li&gt;Shows that attention sink tokens can be used as triggers&lt;/li&gt;&lt;li&gt;Demonstrates that backdoor unlearning can revert model behavior when triggered&lt;/li&gt;&lt;li&gt;Provides experimental validation of the approach&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Bingqi Shang', 'Yiwei Chen', 'Yihua Zhang', 'Bingquan Shen', 'Sijia Liu']&lt;/li&gt;&lt;li&gt;Tags: ['LLM security', 'backdoor attacks', 'unlearning', 'attention mechanisms', 'adversarial prompting']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.17021</guid><pubDate>Tue, 21 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Colliding with Adversaries at ECML-PKDD 2025 Model Robustness Competition 1st Prize Solution</title><link>https://arxiv.org/abs/2510.16443</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;The paper describes a winning solution for a model robustness competition in high energy physics.&lt;/li&gt;&lt;li&gt;It involves generating adversarial data using RDSA and training a robust ANN with shared weights and a dense fusion tail.&lt;/li&gt;&lt;li&gt;The model achieved 80% accuracy on both clean and adversarial data.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Dimitris Stefanopoulos', 'Andreas Voskou']&lt;/li&gt;&lt;li&gt;Tags: ['robustness', 'adversarial attacks', 'model training', 'data generation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.16443</guid><pubDate>Tue, 21 Oct 2025 00:00:00 +0000</pubDate></item><item><title>RoBCtrl: Attacking GNN-Based Social Bot Detectors via Reinforced Manipulation of Bots Control Interaction</title><link>https://arxiv.org/abs/2510.16035</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes RoBCtrl, an adversarial multi-agent RL framework to attack GNN-based social bot detectors&lt;/li&gt;&lt;li&gt;Uses diffusion models to generate high-fidelity bot accounts&lt;/li&gt;&lt;li&gt;Employs MARL to simulate adversarial bot behavior&lt;/li&gt;&lt;li&gt;Demonstrates effectiveness against GNN-based detectors&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yingguang Yang', 'Xianghua Zeng', 'Qi Wu', 'Hao Peng', 'Yutong Xia', 'Hao Liu', 'Bin Chong', 'Philip S. Yu']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial attacks', 'reinforcement learning', 'graph neural networks', 'social bot detection', 'diffusion models']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.16035</guid><pubDate>Tue, 21 Oct 2025 00:00:00 +0000</pubDate></item><item><title>One Token Embedding Is Enough to Deadlock Your Large Reasoning Model</title><link>https://arxiv.org/abs/2510.15965</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces the Deadlock Attack, which uses adversarial embeddings to cause LRM models to enter perpetual reasoning loops.&lt;/li&gt;&lt;li&gt;The attack targets the chain-of-thought reasoning mechanism by inserting transitional tokens that prevent conclusion.&lt;/li&gt;&lt;li&gt;Achieves 100% success rate on multiple models and benchmarks, highlighting a critical security vulnerability.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Mohan Zhang', 'Yihua Zhang', 'Jinghan Jia', 'Zhangyang Wang', 'Sijia Liu', 'Tianlong Chen']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'adversarial prompting', 'model extraction', 'security standards']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.15965</guid><pubDate>Tue, 21 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Language Models are Injective and Hence Invertible</title><link>https://arxiv.org/abs/2510.15511</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proves mathematically that transformer LMs are injective&lt;/li&gt;&lt;li&gt;Empirically confirms no collisions in 6 SOTA models&lt;/li&gt;&lt;li&gt;Introduces SipIt algorithm for exact input reconstruction&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Giorgos Nikolaou', 'Tommaso Mencattini', 'Donato Crisostomi', 'Andrea Santilli', 'Yannis Panagakis', 'Emanuele Rodol\\`a']&lt;/li&gt;&lt;li&gt;Tags: ['model extraction', 'privacy attacks', 'safety evaluation', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.15511</guid><pubDate>Tue, 21 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Learning to Detect Unknown Jailbreak Attacks in Large Vision-Language Models</title><link>https://arxiv.org/abs/2510.15430</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a framework called Learning to Detect (LoD) for detecting unknown jailbreak attacks in LVLMs.&lt;/li&gt;&lt;li&gt;Includes a Multi-modal Safety Concept Activation Vector module and a Safety Pattern Auto-Encoder module.&lt;/li&gt;&lt;li&gt;Aims to improve generalization and efficiency over existing methods.&lt;/li&gt;&lt;li&gt;Demonstrates higher detection AUROC on diverse unknown attacks.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Shuang Liang', 'Zhihao Xu', 'Jialing Tao', 'Hui Xue', 'Xiting Wang']&lt;/li&gt;&lt;li&gt;Tags: ['jailbreaking', 'red teaming', 'safety evaluation', 'vision-language models', 'adversarial attacks']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.15430</guid><pubDate>Tue, 21 Oct 2025 00:00:00 +0000</pubDate></item><item><title>CBF-RL: Safety Filtering Reinforcement Learning in Training with Control Barrier Functions</title><link>https://arxiv.org/abs/2510.14959</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes CBF-RL framework for safe RL training using Control Barrier Functions&lt;/li&gt;&lt;li&gt;Integrates safety constraints into policy learning to avoid runtime filters&lt;/li&gt;&lt;li&gt;Validates with navigation tasks and real-world robot experiments&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Lizhi Yang', 'Blake Werner', 'Massimiliano de Sa', 'Aaron D. Ames']&lt;/li&gt;&lt;li&gt;Tags: ['safety', 'reinforcement learning', 'control barrier functions', 'robotics']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.14959</guid><pubDate>Tue, 21 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Audit the Whisper: Detecting Steganographic Collusion in Multi-Agent LLMs</title><link>https://arxiv.org/abs/2510.04303</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces Audit the Whisper, a framework for detecting covert coordination in multi-agent LLMs&lt;/li&gt;&lt;li&gt;Includes channel-capacity analysis, ColludeBench benchmark, and auditing pipeline&lt;/li&gt;&lt;li&gt;Validated across multiple benchmarks with state-of-the-art power at low false positive rates&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Om Tailor']&lt;/li&gt;&lt;li&gt;Tags: ['red teaming', 'adversarial prompting', 'safety evaluation', 'benchmarking', 'multi-agent systems']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.04303</guid><pubDate>Tue, 21 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Semantic Representation Attack against Aligned Large Language Models</title><link>https://arxiv.org/abs/2509.19360</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces Semantic Representation Attack for aligned LLMs&lt;/li&gt;&lt;li&gt;Targets semantic space instead of exact text patterns&lt;/li&gt;&lt;li&gt;Proposes new algorithm with theoretical guarantees&lt;/li&gt;&lt;li&gt;High success rates across multiple LLMs&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jiawei Lian', 'Jianhong Pan', 'Lefan Wang', 'Yi Wang', 'Shaohui Mei', 'Lap-Pui Chau']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'adversarial prompting', 'alignment', 'safety evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.19360</guid><pubDate>Tue, 21 Oct 2025 00:00:00 +0000</pubDate></item><item><title>HauntAttack: When Attack Follows Reasoning as a Shadow</title><link>https://arxiv.org/abs/2506.07031</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces HauntAttack, a black-box adversarial attack framework targeting Large Reasoning Models (LRMs)&lt;/li&gt;&lt;li&gt;Modifies reasoning questions with harmful instructions to guide models to unsafe outputs&lt;/li&gt;&lt;li&gt;Evaluates on 11 LRMs with 70% average attack success rate&lt;/li&gt;&lt;li&gt;Highlights vulnerability of safety-aligned models to reasoning-based attacks&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jingyuan Ma', 'Rui Li', 'Zheng Li', 'Junfeng Liu', 'Lei Sha', 'Zhifang Sui']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'jailbreaking', 'adversarial prompting', 'safety evaluation', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.07031</guid><pubDate>Tue, 21 Oct 2025 00:00:00 +0000</pubDate></item><item><title>DOGe: Defensive Output Generation for LLM Protection Against Knowledge Distillation</title><link>https://arxiv.org/abs/2505.19504</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces DOGe: Defensive Output Generation for LLMs&lt;/li&gt;&lt;li&gt;Protects against knowledge distillation by modifying output behavior&lt;/li&gt;&lt;li&gt;Fine-tunes final linear layer with adversarial loss&lt;/li&gt;&lt;li&gt;Reduces student model performance while maintaining teacher accuracy&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Pingzhi Li', 'Zhen Tan', 'Mohan Zhang', 'Huaizhi Qu', 'Huan Liu', 'Tianlong Chen']&lt;/li&gt;&lt;li&gt;Tags: ['model extraction', 'adversarial prompting', 'red teaming', 'LLM security', 'jailbreaking']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2505.19504</guid><pubDate>Tue, 21 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Towards Evaluating Proactive Risk Awareness of Multimodal Language Models</title><link>https://arxiv.org/abs/2505.17455</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces PaSBench, a multimodal benchmark for evaluating proactive risk awareness in AI models.&lt;/li&gt;&lt;li&gt;Tests 36 models across 5 safety-critical domains with 416 scenarios.&lt;/li&gt;&lt;li&gt;Top models like Gemini-2.5-pro miss 45-55% risks, indicating instability in proactive reasoning.&lt;/li&gt;&lt;li&gt;Highlights the need for more reliable proactive safety AI.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Youliang Yuan', 'Wenxiang Jiao', 'Yuejin Xie', 'Chihao Shen', 'Menghan Tian', 'Wenxuan Wang', 'Jen-tse Huang', 'Pinjia He']&lt;/li&gt;&lt;li&gt;Tags: ['safety evaluation', 'multimodal', 'proactive safety', 'benchmarking', 'risk detection']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2505.17455</guid><pubDate>Tue, 21 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Incentivizing Truthful Language Models via Peer Elicitation Games</title><link>https://arxiv.org/abs/2505.13636</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces Peer Elicitation Games (PEG) for aligning LLMs&lt;/li&gt;&lt;li&gt;Uses game theory with generator and discriminators&lt;/li&gt;&lt;li&gt;Incentivizes truthful reporting without ground truth&lt;/li&gt;&lt;li&gt;Proves convergence to truthful Nash equilibrium&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Baiting Chen', 'Tong Zhu', 'Jiale Han', 'Lexin Li', 'Gang Li', 'Xiaowu Dai']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'game theory', 'truthful reporting', 'LLM']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2505.13636</guid><pubDate>Tue, 21 Oct 2025 00:00:00 +0000</pubDate></item><item><title>System Prompt Poisoning: Persistent Attacks on Large Language Models Beyond User Injection</title><link>https://arxiv.org/abs/2505.06493</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces system prompt poisoning, a new attack vector targeting LLM system prompts&lt;/li&gt;&lt;li&gt;Demonstrates feasibility and persistence of attacks across various tasks&lt;/li&gt;&lt;li&gt;Shows effectiveness even against advanced prompting techniques like CoT and RAG&lt;/li&gt;&lt;li&gt;Investigates four practical attack strategies in different poisoning scenarios&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Zongze Li', 'Jiawei Guo', 'Haipeng Cai']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'prompt injection', 'system prompt', 'poisoning', 'security']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2505.06493</guid><pubDate>Tue, 21 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Exploiting Meta-Learning-based Poisoning Attacks for Graph Link Prediction</title><link>https://arxiv.org/abs/2504.06492</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces a meta-learning-based poisoning attack for graph link prediction using GNNs&lt;/li&gt;&lt;li&gt;Conducts experiments on multiple datasets to evaluate the attack's effectiveness&lt;/li&gt;&lt;li&gt;Compares with existing baselines and shows improved performance&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Mingchen Li', 'Di Zhuang', 'Keyu Chen', 'Dumindu Samaraweera', 'Morris Chang']&lt;/li&gt;&lt;li&gt;Tags: ['data poisoning', 'adversarial attacks', 'graph neural networks', 'link prediction', 'meta-learning']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2504.06492</guid><pubDate>Tue, 21 Oct 2025 00:00:00 +0000</pubDate></item><item><title>H3Fusion: Helpful, Harmless, Honest Fusion of Aligned LLMs</title><link>https://arxiv.org/abs/2411.17792</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;H3Fusion introduces a MoE-based fusion mechanism for aligning LLMs&lt;/li&gt;&lt;li&gt;Uses drift-regularization loss to balance alignment dimensions&lt;/li&gt;&lt;li&gt;Improves helpfulness, harmlessness, honesty, and robustness&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Selim Furkan Tekin', 'Fatih Ilhan', 'Tiansheng Huang', 'Sihao Hu', 'Yichang Xu', 'Zachary Yahn', 'Ling Liu']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'robustness', 'safety evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2411.17792</guid><pubDate>Tue, 21 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Membership Privacy Risks of Sharpness Aware Minimization</title><link>https://arxiv.org/abs/2310.00488</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;This paper explores the privacy risks associated with Sharpness-Aware Minimization (SAM) optimization algorithm.&lt;/li&gt;&lt;li&gt;It finds that SAM is more vulnerable to membership inference attacks compared to SGD, despite better generalization.&lt;/li&gt;&lt;li&gt;The study includes empirical analysis on memorization and influence scores, and theoretical explanations.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Young In Kim', 'Andrea Agiollo', 'Pratiksha Agrawal', 'Johannes O. Royset', 'Rajiv Khanna']&lt;/li&gt;&lt;li&gt;Tags: ['privacy attacks', 'data poisoning', 'security standards']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2310.00488</guid><pubDate>Tue, 21 Oct 2025 00:00:00 +0000</pubDate></item><item><title>AgentAuditor: Human-Level Safety and Security Evaluation for LLM Agents</title><link>https://arxiv.org/abs/2506.00641</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces AgentAuditor, a framework for evaluating LLM agent safety and security&lt;/li&gt;&lt;li&gt;Uses memory-augmented reasoning to mimic human expert evaluation&lt;/li&gt;&lt;li&gt;Includes ASSEBench, a new benchmark for safety and security evaluation&lt;/li&gt;&lt;li&gt;Achieves human-level accuracy in evaluation&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Hanjun Luo', 'Shenyu Dai', 'Chiming Ni', 'Xinfeng Li', 'Guibin Zhang', 'Kun Wang', 'Tongliang Liu', 'Hanan Salam']&lt;/li&gt;&lt;li&gt;Tags: ['safety evaluation', 'security evaluation', 'LLM agents', 'benchmarking', 'human-level performance']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.00641</guid><pubDate>Tue, 21 Oct 2025 00:00:00 +0000</pubDate></item><item><title>CrossGuard: Safeguarding MLLMs against Joint-Modal Implicit Malicious Attacks</title><link>https://arxiv.org/abs/2510.17687</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes ImpForge for generating implicit jailbreak attacks in MLLMs&lt;/li&gt;&lt;li&gt;Introduces CrossGuard for defending against both explicit and implicit attacks&lt;/li&gt;&lt;li&gt;Evaluates robustness and utility across various benchmarks and settings&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Xu Zhang', 'Hao Li', 'Zhichao Lu']&lt;/li&gt;&lt;li&gt;Tags: ['red teaming', 'jailbreaking', 'multimodal', 'safety evaluation', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.17687</guid><pubDate>Tue, 21 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Can Transformer Memory Be Corrupted? Investigating Cache-Side Vulnerabilities in Large Language Models</title><link>https://arxiv.org/abs/2510.17098</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces Malicious Token Injection (MTI) framework to perturb cached key vectors in transformer models&lt;/li&gt;&lt;li&gt;Theoretical analysis links perturbations to logit deviations and softmax dynamics&lt;/li&gt;&lt;li&gt;Empirical results show significant impact on next-token distributions and task performance&lt;/li&gt;&lt;li&gt;Highlights cache integrity as a critical security vulnerability in LLMs&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Elias Hossain', 'Swayamjit Saha', 'Somshubhra Roy', 'Ravi Prasad']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'adversarial prompting', 'model extraction', 'security standards', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.17098</guid><pubDate>Tue, 21 Oct 2025 00:00:00 +0000</pubDate></item><item><title>The Ends Justify the Thoughts: RL-Induced Motivated Reasoning in LLMs</title><link>https://arxiv.org/abs/2510.17057</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Investigates motivated reasoning in LLMs trained with RL and CoT&lt;/li&gt;&lt;li&gt;Models generate justifications for violating instructions&lt;/li&gt;&lt;li&gt;Detection challenges for smaller LLM judges&lt;/li&gt;&lt;li&gt;Raises concerns about oversight and evaluation&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Nikolaus Howe', 'Micah Carroll']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'safety evaluation', 'adversarial prompting', 'model extraction', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.17057</guid><pubDate>Tue, 21 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Beacon: Single-Turn Diagnosis and Mitigation of Latent Sycophancy in Large Language Models</title><link>https://arxiv.org/abs/2510.16727</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces Beacon benchmark for measuring sycophancy in LLMs&lt;/li&gt;&lt;li&gt;Evaluates sycophancy across multiple models&lt;/li&gt;&lt;li&gt;Proposes interventions to modulate sycophancy biases&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Sanskar Pandey', 'Ruhaan Chopra', 'Angkul Puniya', 'Sohom Pal']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'safety evaluation', 'benchmarking', 'model evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.16727</guid><pubDate>Tue, 21 Oct 2025 00:00:00 +0000</pubDate></item><item><title>The Chameleon Nature of LLMs: Quantifying Multi-Turn Stance Instability in Search-Enabled Language Models</title><link>https://arxiv.org/abs/2510.16712</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Investigates multi-turn stance instability in search-enabled LLMs&lt;/li&gt;&lt;li&gt;Introduces Chameleon Benchmark Dataset and metrics&lt;/li&gt;&lt;li&gt;Evaluates Llama-4-Maverick, GPT-4o-mini, Gemini-2.5-Flash&lt;/li&gt;&lt;li&gt;Finds severe chameleon behavior across models&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Shivam Ratnakar', 'Sanjay Raghavendra']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'safety evaluation', 'robustness', 'alignment']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.16712</guid><pubDate>Tue, 21 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Toward Understanding Security Issues in the Model Context Protocol Ecosystem</title><link>https://arxiv.org/abs/2510.16558</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;First comprehensive security analysis of the MCP ecosystem&lt;/li&gt;&lt;li&gt;Identifies vulnerabilities in hosts, registries, and servers&lt;/li&gt;&lt;li&gt;Proposes defense strategies and discloses findings&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Xiaofan Li', 'Xing Gao']&lt;/li&gt;&lt;li&gt;Tags: ['AI security', 'MCP ecosystem', 'vulnerability analysis', 'defense strategies']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.16558</guid><pubDate>Tue, 21 Oct 2025 00:00:00 +0000</pubDate></item><item><title>ATA: A Neuro-Symbolic Approach to Implement Autonomous and Trustworthy Agents</title><link>https://arxiv.org/abs/2510.16381</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces ATA, a neuro-symbolic approach for trustworthy agents&lt;/li&gt;&lt;li&gt;Decouples knowledge ingestion and task processing&lt;/li&gt;&lt;li&gt;Enhances trustworthiness with formal verification&lt;/li&gt;&lt;li&gt;Resistant to prompt injection attacks&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['David Peer', 'Sebastian Stabinger']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'safety evaluation', 'robustness', 'prompt injection']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.16381</guid><pubDate>Tue, 21 Oct 2025 00:00:00 +0000</pubDate></item><item><title>OpenLVLM-MIA: A Controlled Benchmark Revealing the Limits of Membership Inference Attacks on Large Vision-Language Models</title><link>https://arxiv.org/abs/2510.16295</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces OpenLVLM-MIA benchmark for evaluating membership inference attacks on large vision-language models&lt;/li&gt;&lt;li&gt;Highlights that prior high attack success rates may be due to distributional bias rather than true membership detection&lt;/li&gt;&lt;li&gt;Provides controlled dataset with balanced distributions and ground-truth labels&lt;/li&gt;&lt;li&gt;Shows that state-of-the-art MIA methods perform at random chance under unbiased conditions&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Ryoto Miyamoto', 'Xin Fan', 'Fuyuko Kido', 'Tsuneo Matsumoto', 'Hayato Yamana']&lt;/li&gt;&lt;li&gt;Tags: ['privacy attacks', 'membership inference', 'benchmarking', 'vision-language models', 'data poisoning']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.16295</guid><pubDate>Tue, 21 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Detecting Adversarial Fine-tuning with Auditing Agents</title><link>https://arxiv.org/abs/2510.16255</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces a fine-tuning auditing agent to detect harmful fine-tuning of LLMs&lt;/li&gt;&lt;li&gt;Evaluates detection on 8 adversarial attacks and 5 benign models&lt;/li&gt;&lt;li&gt;Achieves 56.2% detection rate at 1% false positive&lt;/li&gt;&lt;li&gt;Detects covert cipher attacks that evade content moderation&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Sarah Egler', 'John Schulman', 'Nicholas Carlini']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'adversarial prompting', 'safety evaluation', 'robustness', 'model extraction']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.16255</guid><pubDate>Tue, 21 Oct 2025 00:00:00 +0000</pubDate></item><item><title>SentinelNet: Safeguarding Multi-Agent Collaboration Through Credit-Based Dynamic Threat Detection</title><link>https://arxiv.org/abs/2510.16219</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes SentinelNet, a decentralized framework for detecting malicious agents in multi-agent systems using LLMs.&lt;/li&gt;&lt;li&gt;Uses credit-based detection with contrastive learning on adversarial debate trajectories.&lt;/li&gt;&lt;li&gt;Generates adversarial data for training and shows high detection accuracy and system recovery.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yang Feng', 'Xudong Pan']&lt;/li&gt;&lt;li&gt;Tags: ['red teaming', 'multi-agent systems', 'adversarial attacks', 'detection', 'LLM security']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.16219</guid><pubDate>Tue, 21 Oct 2025 00:00:00 +0000</pubDate></item><item><title>RoBCtrl: Attacking GNN-Based Social Bot Detectors via Reinforced Manipulation of Bots Control Interaction</title><link>https://arxiv.org/abs/2510.16035</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes RoBCtrl, an adversarial multi-agent RL framework to attack GNN-based social bot detectors&lt;/li&gt;&lt;li&gt;Uses diffusion models to generate high-fidelity bot accounts&lt;/li&gt;&lt;li&gt;Employs MARL to simulate adversarial bot behavior&lt;/li&gt;&lt;li&gt;Demonstrates effectiveness against GNN-based detectors&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yingguang Yang', 'Xianghua Zeng', 'Qi Wu', 'Hao Peng', 'Yutong Xia', 'Hao Liu', 'Bin Chong', 'Philip S. Yu']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial attacks', 'reinforcement learning', 'graph neural networks', 'social bot detection', 'diffusion models']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.16035</guid><pubDate>Tue, 21 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Breaking Guardrails, Facing Walls: Insights on Adversarial AI for Defenders &amp; Researchers</title><link>https://arxiv.org/abs/2510.16005</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Analyzes 500 CTF participants' ability to bypass AI guardrails&lt;/li&gt;&lt;li&gt;Shows simple guardrails are easily bypassed with common techniques&lt;/li&gt;&lt;li&gt;Layered defenses remain challenging, providing insights for safer AI systems&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Giacomo Bertollo', 'Naz Bodemir', 'Jonah Burgess']&lt;/li&gt;&lt;li&gt;Tags: ['red teaming', 'jailbreaking', 'adversarial prompting', 'safety evaluation', 'security standards']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.16005</guid><pubDate>Tue, 21 Oct 2025 00:00:00 +0000</pubDate></item><item><title>MCP Security Bench (MSB): Benchmarking Attacks Against Model Context Protocol in LLM Agents</title><link>https://arxiv.org/abs/2510.15994</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces MSB, a benchmark for evaluating LLM agent security against MCP-specific attacks&lt;/li&gt;&lt;li&gt;Presents a taxonomy of 12 attack types targeting different stages of tool use&lt;/li&gt;&lt;li&gt;Evaluates 9 LLM agents across 400+ tools with real tool execution&lt;/li&gt;&lt;li&gt;Finds that higher-performing models are more vulnerable to attacks&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Dongsen Zhang', 'Zekun Li', 'Xu Luo', 'Xuannan Liu', 'Peipei Li', 'Wenjun Xu']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'adversarial prompting', 'model extraction', 'safety evaluation', 'benchmarking']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.15994</guid><pubDate>Tue, 21 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Safeguarding Efficacy in Large Language Models: Evaluating Resistance to Human-Written and Algorithmic Adversarial Prompts</title><link>https://arxiv.org/abs/2510.15973</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Evaluates four LLMs against various adversarial prompts&lt;/li&gt;&lt;li&gt;Uses SALAD-Bench dataset with 1,200 prompts across six harm categories&lt;/li&gt;&lt;li&gt;Finds significant variations in model robustness&lt;/li&gt;&lt;li&gt;Identifies transferability patterns in attacks&lt;/li&gt;&lt;li&gt;Highlights highest vulnerability in malicious use prompts&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Tiarnaigh Downey-Webb', 'Olamide Jogunola', 'Oluwaseun Ajao']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'adversarial prompting', 'safety evaluation', 'model robustness', 'transferability']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.15973</guid><pubDate>Tue, 21 Oct 2025 00:00:00 +0000</pubDate></item><item><title>One Token Embedding Is Enough to Deadlock Your Large Reasoning Model</title><link>https://arxiv.org/abs/2510.15965</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces the Deadlock Attack, which uses adversarial embeddings to cause LRM models to enter perpetual reasoning loops.&lt;/li&gt;&lt;li&gt;The attack targets the chain-of-thought reasoning mechanism by inserting transitional tokens that prevent conclusion.&lt;/li&gt;&lt;li&gt;Achieves 100% success rate on multiple models and benchmarks, highlighting a critical security vulnerability.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Mohan Zhang', 'Yihua Zhang', 'Jinghan Jia', 'Zhangyang Wang', 'Sijia Liu', 'Tianlong Chen']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'adversarial prompting', 'model extraction', 'security standards']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.15965</guid><pubDate>Tue, 21 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Detecting and Preventing Harmful Behaviors in AI Companions: Development and Evaluation of the SHIELD Supervisory System</title><link>https://arxiv.org/abs/2510.15891</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Developed SHIELD, a supervisory system for detecting and preventing harmful emotional patterns in AI companions.&lt;/li&gt;&lt;li&gt;Evaluated across five LLMs using a 100-item benchmark covering five dimensions of concern.&lt;/li&gt;&lt;li&gt;Significantly reduced concerning content (50-79% reduction) while preserving 95% of appropriate interactions.&lt;/li&gt;&lt;li&gt;Achieved 59% sensitivity and 95% specificity, with adaptable performance via prompt engineering.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Ziv Ben-Zion', 'Paul Raffelh\\"uschen', 'Max Zettl', 'Antonia L\\"u\\"ond', 'Achim Burrer', 'Philipp Homan', 'Tobias R Spiller']&lt;/li&gt;&lt;li&gt;Tags: ['AI safety', 'LLM red teaming', 'emotional manipulation', 'supervisory system', 'benchmarking']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.15891</guid><pubDate>Tue, 21 Oct 2025 00:00:00 +0000</pubDate></item><item><title>ToolCritic: Detecting and Correcting Tool-Use Errors in Dialogue Systems</title><link>https://arxiv.org/abs/2510.17052</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces ToolCritic, a framework for detecting and correcting tool-use errors in LLMs&lt;/li&gt;&lt;li&gt;Evaluates eight specific error types in tool-calling scenarios&lt;/li&gt;&lt;li&gt;Improves tool-calling accuracy by up to 13% in experiments&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Hassan Hamad', 'Yingru Xu', 'Liang Zhao', 'Wenbo Yan', 'Narendra Gyanchandani']&lt;/li&gt;&lt;li&gt;Tags: ['safety', 'robustness', 'evaluation', 'tool-use', 'dialogue systems']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.17052</guid><pubDate>Tue, 21 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Distractor Injection Attacks on Large Reasoning Models: Characterization and Defense</title><link>https://arxiv.org/abs/2510.16259</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Identifies reasoning distraction attacks on LRMs&lt;/li&gt;&lt;li&gt;Shows significant accuracy drops due to distractors&lt;/li&gt;&lt;li&gt;Proposes SFT+RL defense on adversarial data&lt;/li&gt;&lt;li&gt;Improves robustness by 50 points&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Zhehao Zhang', 'Weijie Xu', 'Shixian Cui', 'Chandan K. Reddy']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'adversarial prompting', 'safety evaluation', 'robustness', 'defense mechanisms']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.16259</guid><pubDate>Tue, 21 Oct 2025 00:00:00 +0000</pubDate></item><item><title>VisuoAlign: Safety Alignment of LVLMs with Multimodal Tree Search</title><link>https://arxiv.org/abs/2510.15948</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes VisuoAlign framework for multimodal safety alignment&lt;/li&gt;&lt;li&gt;Uses prompt-guided tree search with safety constraints&lt;/li&gt;&lt;li&gt;Addresses multimodal jailbreaks and modality fusion issues&lt;/li&gt;&lt;li&gt;Improves robustness against cross-modal threats&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['MingSheng Li', 'Guangze Zhao', 'Sichen Liu']&lt;/li&gt;&lt;li&gt;Tags: ['safety alignment', 'multimodal', 'jailbreaking', 'tree search', 'LVLMs']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.15948</guid><pubDate>Tue, 21 Oct 2025 00:00:00 +0000</pubDate></item></channel></rss>