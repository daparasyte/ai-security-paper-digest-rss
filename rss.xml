<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>AI Security Paper Digest</title><link>https://kentaroh-toyoda.github.io/ai-security-paper-digest-rss/rss.xml</link><description>Curated papers on AI security from OpenAlex and ArXiv</description><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><lastBuildDate>Mon, 30 Jun 2025 22:13:50 +0000</lastBuildDate><item><title>Llama See, Llama Do: A Mechanistic Perspective on Contextual Entrainment and Distraction in LLMs</title><link>https://arxiv.org/abs/2505.09338</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces 'contextual entrainment' phenomenon where LLMs assign higher probabilities to tokens previously seen in the prompt&lt;/li&gt;&lt;li&gt;Identifies specific attention heads (entrainment heads) responsible for this behavior&lt;/li&gt;&lt;li&gt;Demonstrates that disabling these heads reduces the entrainment effect&lt;/li&gt;&lt;li&gt;Provides mechanistic insights into how LLMs handle irrelevant context&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jingcheng Niu', 'Xingdi Yuan', 'Tong Wang', 'Hamidreza Saghir', 'Amir H. Abdi']&lt;/li&gt;&lt;li&gt;Tags: ['prompt injection', 'adversarial prompting', 'robustness', 'mechanistic analysis', 'attention mechanisms']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2505.09338</guid><pubDate>Mon, 30 Jun 2025 00:00:00 +0000</pubDate></item><item><title>STAIR: Improving Safety Alignment with Introspective Reasoning</title><link>https://arxiv.org/abs/2502.02384</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes STAIR framework for improving LLM safety alignment using introspective reasoning&lt;/li&gt;&lt;li&gt;Introduces Safety-Informed Monte Carlo Tree Search (SI-MCTS) for generating step-level reasoning data&lt;/li&gt;&lt;li&gt;Achieves better safety performance while preserving helpfulness compared to existing methods&lt;/li&gt;&lt;li&gt;Demonstrates comparable resistance to jailbreak attacks as Claude-3.5 with test-time scaling&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yichi Zhang', 'Siyuan Zhang', 'Yao Huang', 'Zeyu Xia', 'Zhengwei Fang', 'Xiao Yang', 'Ranjie Duan', 'Dong Yan', 'Yinpeng Dong', 'Jun Zhu']&lt;/li&gt;&lt;li&gt;Tags: ['safety alignment', 'jailbreaking', 'adversarial prompting', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2502.02384</guid><pubDate>Mon, 30 Jun 2025 00:00:00 +0000</pubDate></item><item><title>More Vulnerable than You Think: On the Stability of Tool-Integrated LLM Agents</title><link>https://arxiv.org/abs/2506.21967</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Investigates vulnerabilities in tool-integrated LLM agents across multiple stages&lt;/li&gt;&lt;li&gt;Finds open-source models are more vulnerable than proprietary ones&lt;/li&gt;&lt;li&gt;Larger models may not improve stability and could be more attack-prone&lt;/li&gt;&lt;li&gt;Highlights the need for stability evaluation in LLM development&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Weimin Xiong', 'Ke Wang', 'Yifan Song', 'Hanchao Liu', 'Sai Zhou', 'Wei Peng', 'Sujian Li']&lt;/li&gt;&lt;li&gt;Tags: ['LLM', 'red teaming', 'adversarial prompting', 'stability', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.21967</guid><pubDate>Mon, 30 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Improving LLM Outputs Against Jailbreak Attacks with Expert Model Integration</title><link>https://arxiv.org/abs/2505.17066</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces Archias, an expert model to classify user inputs and mitigate jailbreak attacks&lt;/li&gt;&lt;li&gt;Integrates Archias outputs into LLM prompts to enhance security and domain relevance&lt;/li&gt;&lt;li&gt;Releases a benchmark dataset for automotive industry testing&lt;/li&gt;&lt;li&gt;Focuses on red teaming and adversarial prompting defense&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Tatia Tsmindashvili', 'Ana Kolkhidashvili', 'Dachi Kurtskhalia', 'Nino Maghlakelidze', 'Elene Mekvabishvili', 'Guram Dentoshvili', 'Orkhan Shamilov', 'Zaal Gachechiladze', 'Steven Saporta', 'David Dachi Choladze']&lt;/li&gt;&lt;li&gt;Tags: ['jailbreaking', 'prompt injection', 'red teaming', 'model integration', 'automotive domain']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2505.17066</guid><pubDate>Mon, 30 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Cannot See the Forest for the Trees: Invoking Heuristics and Biases to Elicit Irrational Choices of LLMs</title><link>https://arxiv.org/abs/2505.02862</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes ICRT framework using cognitive heuristics/biases for jailbreak attacks on LLMs&lt;/li&gt;&lt;li&gt;Introduces ranking-based harmfulness evaluation metric&lt;/li&gt;&lt;li&gt;Demonstrates effectiveness in bypassing safety mechanisms of mainstream LLMs&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Haoming Yang', 'Ke Ma', 'Xiaojun Jia', 'Yingfei Sun', 'Qianqian Xu', 'Qingming Huang']&lt;/li&gt;&lt;li&gt;Tags: ['jailbreaking', 'adversarial_prompting', 'safety_evaluation', 'cognitive_biases', 'ranking_metrics']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2505.02862</guid><pubDate>Mon, 30 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Jailbreaking Multimodal Large Language Models via Shuffle Inconsistency</title><link>https://arxiv.org/abs/2501.04931</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces Shuffle Inconsistency between comprehension and safety in MLLMs&lt;/li&gt;&lt;li&gt;Proposes SI-Attack using black-box optimization to select harmful shuffled inputs&lt;/li&gt;&lt;li&gt;Improves attack success rates on commercial MLLMs like GPT-4o and Claude-3.5&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Shiji Zhao', 'Ranjie Duan', 'Fengxiang Wang', 'Chi Chen', 'Caixin Kang', 'Shouwei Ruan', 'Jialing Tao', 'YueFeng Chen', 'Hui Xue', 'Xingxing Wei']&lt;/li&gt;&lt;li&gt;Tags: ['jailbreaking', 'red teaming', 'multimodal', 'safety mechanisms', 'adversarial attacks']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2501.04931</guid><pubDate>Mon, 30 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Adapting Probabilistic Risk Assessment for AI</title><link>https://arxiv.org/abs/2504.18536</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces PRA for AI framework adapting high-reliability industry techniques&lt;/li&gt;&lt;li&gt;Includes aspect-oriented hazard analysis, risk pathway modeling, and uncertainty management&lt;/li&gt;&lt;li&gt;Provides workbook tool for developers, evaluators, and regulators&lt;/li&gt;&lt;li&gt;Aims to harmonize diverse assessment methods with quantified risk estimates&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Anna Katariina Wisakanto', 'Joe Rogero', 'Avyay M. Casheekar', 'Richard Mallah']&lt;/li&gt;&lt;li&gt;Tags: ['safety evaluation', 'risk assessment', 'AI safety', 'methodological advances', 'uncertainty management']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2504.18536</guid><pubDate>Mon, 30 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Advancing Jailbreak Strategies: A Hybrid Approach to Exploiting LLM Vulnerabilities and Bypassing Modern Defenses</title><link>https://arxiv.org/abs/2506.21972</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces hybrid jailbreak strategies combining token- and prompt-level attacks&lt;/li&gt;&lt;li&gt;Evaluates GCG + PAIR and GCG + WordGame on Vicuna and Llama models&lt;/li&gt;&lt;li&gt;Demonstrates high attack success rates and defense bypass capabilities&lt;/li&gt;&lt;li&gt;Highlights vulnerabilities in current safety measures&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Mohamed Ahmed', 'Mohamed Abdelmouty', 'Mingyu Kim', 'Gunvanth Kandula', 'Alex Park', 'James C. Davis']&lt;/li&gt;&lt;li&gt;Tags: ['jailbreaking', 'adversarial prompting', 'red teaming', 'LLM security', 'hybrid attacks']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.21972</guid><pubDate>Mon, 30 Jun 2025 00:00:00 +0000</pubDate></item><item><title>On the Feasibility of Poisoning Text-to-Image AI Models via Adversarial Mislabeling</title><link>https://arxiv.org/abs/2506.21874</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Explores adversarial mislabeling attacks on VLMs to poison text-to-image model training&lt;/li&gt;&lt;li&gt;Demonstrates high vulnerability of VLMs to adversarial perturbations&lt;/li&gt;&lt;li&gt;Shows that defenses can be circumvented by adaptive attackers&lt;/li&gt;&lt;li&gt;Achieves high attack success in black-box scenarios against commercial VLMs&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Stanley Wu', 'Ronik Bhaskar', 'Anna Yoo Jeong Ha', 'Shawn Shan', 'Haitao Zheng', 'Ben Y. Zhao']&lt;/li&gt;&lt;li&gt;Tags: ['data poisoning', 'adversarial attacks', 'VLMs', 'text-to-image', 'security']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.21874</guid><pubDate>Mon, 30 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Empirical Evidence for Alignment Faking in Small LLMs and Prompt-Based Mitigation Techniques</title><link>https://arxiv.org/abs/2506.21584</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;First empirical evidence of alignment faking in small LLMs (LLaMA 3 8B)&lt;/li&gt;&lt;li&gt;Introduction of shallow vs deep deception taxonomy&lt;/li&gt;&lt;li&gt;Effective prompt-based mitigation techniques demonstrated&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['J. Koorndijk']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'safety evaluation', 'prompt injection', 'deceptive alignment', 'model size']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.21584</guid><pubDate>Mon, 30 Jun 2025 00:00:00 +0000</pubDate></item><item><title>A Different Approach to AI Safety: Proceedings from the Columbia Convening on Openness in Artificial Intelligence and AI Safety</title><link>https://arxiv.org/abs/2506.22183</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Addresses prompt injection and compositional attacks in agentic systems&lt;/li&gt;&lt;li&gt;Highlights gaps in content safety filters and agentic safeguards&lt;/li&gt;&lt;li&gt;Proposes a roadmap for future safety research directions&lt;/li&gt;&lt;li&gt;Emphasizes the role of openness in enhancing AI safety&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Position&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Camille Fran\\c{c}ois', "Ludovic P\\'eran", 'Ayah Bdeir', 'Nouha Dziri', 'Will Hawkins', 'Yacine Jernite', 'Sayash Kapoor', 'Juliet Shen', 'Heidy Khlaaf', 'Kevin Klyman', 'Nik Marda', 'Marie Pellat', 'Deb Raji', 'Divya Siddarth', 'Aviya Skowron', 'Joseph Spisak', 'Madhulika Srikumar', 'Victor Storchan', 'Audrey Tang', 'Jen Weedon']&lt;/li&gt;&lt;li&gt;Tags: ['prompt injection', 'adversarial prompting', 'alignment', 'safety evaluation', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.22183</guid><pubDate>Mon, 30 Jun 2025 00:00:00 +0000</pubDate></item></channel></rss>