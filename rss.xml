<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>AI Security Paper Digest</title><link>https://kentaroh-toyoda.github.io/ai-security-paper-digest-rss/rss.xml</link><description>Curated papers on AI security from ArXiv and ACL</description><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><lastBuildDate>Fri, 17 Oct 2025 22:25:46 +0000</lastBuildDate><item><title>RADAR: A Risk-Aware Dynamic Multi-Agent Framework for LLM Safety Evaluation via Role-Specialized Collaboration</title><link>https://arxiv.org/abs/2509.25271</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces RADAR, a multi-agent framework for LLM safety evaluation&lt;/li&gt;&lt;li&gt;Decomposes risk into explicit, implicit, and non-risk subspaces&lt;/li&gt;&lt;li&gt;Uses role-specialized agents and dynamic updates for risk concept evolution&lt;/li&gt;&lt;li&gt;Validates with 800 challenging cases and shows significant improvement over baselines&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Xiuyuan Chen', 'Jian Zhao', 'Yuchen Yuan', 'Tianle Zhang', 'Huilin Zhou', 'Zheng Zhu', 'Ping Hu', 'Linghe Kong', 'Chi Zhang', 'Weiran Huang', 'Xuelong Li']&lt;/li&gt;&lt;li&gt;Tags: ['LLM safety evaluation', 'multi-agent systems', 'risk concept space', 'dynamic updates', 'role-specialized collaboration']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.25271</guid><pubDate>Fri, 17 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Backdoor Unlearning by Linear Task Decomposition</title><link>https://arxiv.org/abs/2510.14845</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces a method for unlearning backdoors in foundation models without retraining&lt;/li&gt;&lt;li&gt;Leverages linear task decomposition to isolate and remove backdoor influence&lt;/li&gt;&lt;li&gt;Achieves high clean accuracy retention while effectively unlearning known and unknown backdoors&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Amel Abdelraheem', 'Alessandro Favero', 'Gerome Bovet', 'Pascal Frossard']&lt;/li&gt;&lt;li&gt;Tags: ['backdoor attacks', 'model unlearning', 'adversarial defense', 'security', 'foundation models']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.14845</guid><pubDate>Fri, 17 Oct 2025 00:00:00 +0000</pubDate></item><item><title>ENIGMA: The Geometry of Reasoning and Alignment in Large-Language Models</title><link>https://arxiv.org/abs/2510.11278</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;ENIGMA is a new LLM training approach focusing on reasoning, alignment, and robustness&lt;/li&gt;&lt;li&gt;Uses GRPO, SAMI-style InfoNCE, and Sinkhorn regularizer&lt;/li&gt;&lt;li&gt;Introduces Sufficiency Index (SI) for principle selection&lt;/li&gt;&lt;li&gt;Experiments show improved performance and training dynamics&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Gareth Seneque', 'Lap-Hang Ho', 'Nafise Erfanian Saeedi', 'Jeffrey Molendijk', 'Ariel Kuperman', 'Tim Elson']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'robustness', 'safety evaluation', 'model training']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.11278</guid><pubDate>Fri, 17 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Machine Unlearning Meets Adversarial Robustness via Constrained Interventions on LLMs</title><link>https://arxiv.org/abs/2510.03567</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Unified approach for unlearning and robustness in LLMs&lt;/li&gt;&lt;li&gt;Constrained optimization for minimal weight interventions&lt;/li&gt;&lt;li&gt;Addresses jail-breaking attacks and sensitive info unlearning&lt;/li&gt;&lt;li&gt;Comparative analysis with state-of-the-art defenses&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Fatmazohra Rezkellah', 'Ramzi Dakhmouche']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'jailbreaking', 'adversarial robustness', 'unlearning', 'privacy-preserving']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.03567</guid><pubDate>Fri, 17 Oct 2025 00:00:00 +0000</pubDate></item><item><title>SteeringSafety: A Systematic Safety Evaluation Framework of Representation Steering in LLMs</title><link>https://arxiv.org/abs/2509.13450</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces SteeringSafety framework for evaluating representation steering in LLMs across 7 safety perspectives&lt;/li&gt;&lt;li&gt;Tests methods like DIM, ACE, CAA, PCA, LAT on models Gemma-2-2B, Llama-3.1-8B, Qwen-2.5-7B&lt;/li&gt;&lt;li&gt;Finds entanglement between safety perspectives (e.g., social behaviors vulnerability, jailbreaking affecting normative judgment)&lt;/li&gt;&lt;li&gt;Highlights need for comprehensive safety evaluations&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Vincent Siu', 'Nicholas Crispino', 'David Park', 'Nathan W. Henry', 'Zhun Wang', 'Yang Liu', 'Dawn Song', 'Chenguang Wang']&lt;/li&gt;&lt;li&gt;Tags: ['safety evaluation', 'representation steering', 'LLM safety', 'jailbreaking', 'hallucination', 'bias', 'social behaviors']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.13450</guid><pubDate>Fri, 17 Oct 2025 00:00:00 +0000</pubDate></item><item><title>RepIt: Representing Isolated Targets to Steer Language Models</title><link>https://arxiv.org/abs/2509.13281</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces RepIt, a framework for isolating concept-specific representations in LLMs&lt;/li&gt;&lt;li&gt;Enables precise interventions to suppress refusal on targeted concepts while preserving safety&lt;/li&gt;&lt;li&gt;Raises concerns about manipulations with modest compute and data&lt;/li&gt;&lt;li&gt;Demonstrates disentangling refusal vectors to counteract overgeneralization&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Vincent Siu', 'Nathan W. Henry', 'Nicholas Crispino', 'Yang Liu', 'Dawn Song', 'Chenguang Wang']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'adversarial prompting', 'model extraction', 'safety evaluation', 'alignment']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.13281</guid><pubDate>Fri, 17 Oct 2025 00:00:00 +0000</pubDate></item><item><title>When Style Breaks Safety: Defending LLMs Against Superficial Style Alignment</title><link>https://arxiv.org/abs/2506.07452</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;The paper explores how style patterns in prompts affect LLM safety and vulnerability to jailbreaks.&lt;/li&gt;&lt;li&gt;It defines ASR inflation and shows that many models exhibit this issue.&lt;/li&gt;&lt;li&gt;Proposes SafeStyle, a defense strategy using safety data augmented with style patterns.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yuxin Xiao', 'Sana Tonekaboni', 'Walter Gerych', 'Vinith Suriyakumar', 'Marzyeh Ghassemi']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'jailbreaking', 'alignment', 'safety evaluation', 'defense strategy']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.07452</guid><pubDate>Fri, 17 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Checkpoint-GCG: Auditing and Attacking Fine-Tuning-Based Prompt Injection Defenses</title><link>https://arxiv.org/abs/2505.15738</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces Checkpoint-GCG, a white-box attack against fine-tuning-based prompt injection defenses.&lt;/li&gt;&lt;li&gt;Evaluates robustness of state-of-the-art defenses in auditing setup with different knowledge assumptions.&lt;/li&gt;&lt;li&gt;Achieves high attack success rates (up to 96%) against the strongest defense in white-box scenario.&lt;/li&gt;&lt;li&gt;Demonstrates transferability of attacks to black-box models with 63.9% ASR.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Xiaoxue Yang', 'Bozhidar Stevanoski', 'Matthieu Meeus', 'Yves-Alexandre de Montjoye']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'prompt injection', 'adversarial prompting', 'model extraction', 'security standards']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2505.15738</guid><pubDate>Fri, 17 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Are Large Reasoning Models Interruptible?</title><link>https://arxiv.org/abs/2510.11713</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Evaluates Large Reasoning Models (LRMs) under dynamic scenarios like interruptions and changing context&lt;/li&gt;&lt;li&gt;Finds significant performance drops (up to 60%) when models are interrupted or context changes&lt;/li&gt;&lt;li&gt;Identifies new failure modes: reasoning leakage, panic, and self-doubt&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Tsung-Han Wu', 'Mihran Miroyan', 'David M. Chan', 'Trevor Darrell', 'Narges Norouzi', 'Joseph E. Gonzalez']&lt;/li&gt;&lt;li&gt;Tags: ['robustness', 'dynamic context', 'interruptions', 'failure modes', 'long-form reasoning']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.11713</guid><pubDate>Fri, 17 Oct 2025 00:00:00 +0000</pubDate></item><item><title>All Code, No Thought: Current Language Models Struggle to Reason in Ciphered Language</title><link>https://arxiv.org/abs/2510.09714</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Assesses models' ability to reason in ciphered languages&lt;/li&gt;&lt;li&gt;Tests 28 ciphers across 10 models, measuring math problem accuracy&lt;/li&gt;&lt;li&gt;Finds models struggle with lesser-known ciphers despite translation ability&lt;/li&gt;&lt;li&gt;Suggests ciphered reasoning may not be a viable evasion tactic currently&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Shiyuan Guo', 'Henry Sleight', 'Fabien Roger']&lt;/li&gt;&lt;li&gt;Tags: ['red teaming', 'adversarial prompting', 'safety evaluation', 'alignment', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.09714</guid><pubDate>Fri, 17 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Inoculation Prompting: Eliciting traits from LLMs during training can suppress them at test-time</title><link>https://arxiv.org/abs/2510.04340</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes inoculation prompting to suppress undesirable traits in LLMs during training&lt;/li&gt;&lt;li&gt;Modifies finetuning data with system prompts that elicit the traits&lt;/li&gt;&lt;li&gt;Effective in reducing misalignment, backdoor attacks, and subliminal learning&lt;/li&gt;&lt;li&gt;Analyzes the mechanism of reduced optimization pressure&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Daniel Tan', 'Anders Woodruff', 'Niels Warncke', 'Arun Jose', "Maxime Rich\\'e", 'David Demitri Africa', 'Mia Taylor']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'alignment', 'safety evaluation', 'adversarial prompting', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.04340</guid><pubDate>Fri, 17 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Adversarial Defence without Adversarial Defence: Enhancing Language Model Robustness via Instance-level Principal Component Removal</title><link>https://arxiv.org/abs/2507.21750</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a method to enhance adversarial robustness of PLMs by removing instance-level principal components&lt;/li&gt;&lt;li&gt;Does not rely on conventional adversarial defenses or perturbing training data&lt;/li&gt;&lt;li&gt;Transforms embedding space to reduce susceptibility to adversarial perturbations&lt;/li&gt;&lt;li&gt;Evaluates on eight benchmark datasets showing improved robustness&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yang Wang', 'Chenghao Xiao', 'Yizhi Li', 'Stuart E. Middleton', 'Noura Al Moubayed', 'Chenghua Lin']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial robustness', 'language models', 'embedding space transformation', 'principal component removal']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2507.21750</guid><pubDate>Fri, 17 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Benchmarking Adversarial Robustness to Bias Elicitation in Large Language Models: Scalable Automated Assessment with LLM-as-a-Judge</title><link>https://arxiv.org/abs/2504.07887</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a benchmarking framework for LLM robustness against adversarial bias elicitation&lt;/li&gt;&lt;li&gt;Uses LLM-as-a-Judge for safety scoring&lt;/li&gt;&lt;li&gt;Releases CLEAR-Bias dataset&lt;/li&gt;&lt;li&gt;Finds uneven bias resilience and jailbreak vulnerabilities&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Riccardo Cantini', 'Alessio Orsino', 'Massimo Ruggiero', 'Domenico Talia']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'jailbreaking', 'bias', 'robustness', 'safety evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2504.07887</guid><pubDate>Fri, 17 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Interpretability and Transparency-Driven Detection and Transformation of Textual Adversarial Examples (IT-DT)</title><link>https://arxiv.org/abs/2307.01225</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes IT-DT framework for detecting and transforming textual adversarial examples&lt;/li&gt;&lt;li&gt;Uses interpretability techniques like attention maps and integrated gradients&lt;/li&gt;&lt;li&gt;Incorporates human expert feedback for transparency&lt;/li&gt;&lt;li&gt;Aims to convert adversarial examples into non-adversarial ones while preserving meaning&lt;/li&gt;&lt;li&gt;Demonstrates improved robustness against adversarial attacks&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Bushra Sabir', 'M. Ali Babar', 'Sharif Abuadbba']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial examples', 'interpretability', 'transparency', 'text classification', 'human-in-the-loop']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2307.01225</guid><pubDate>Fri, 17 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Are My Optimized Prompts Compromised? Exploring Vulnerabilities of LLM-based Optimizers</title><link>https://arxiv.org/abs/2510.14381</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;First systematic analysis of poisoning risks in LLM-based prompt optimization&lt;/li&gt;&lt;li&gt;Feedback-based attacks found to be more effective than query injection&lt;/li&gt;&lt;li&gt;Introduced fake-reward attack and highlighting defense&lt;/li&gt;&lt;li&gt;Establishes prompt optimization as a critical attack surface&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Andrew Zhao', 'Reshmi Ghosh', 'Vitor Carvalho', 'Emily Lawton', 'Keegan Hines', 'Gao Huang', 'Jack W. Stokes']&lt;/li&gt;&lt;li&gt;Tags: ['prompt optimization', 'poisoning attacks', 'security', 'red teaming', 'LLM']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.14381</guid><pubDate>Fri, 17 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Terrarium: Revisiting the Blackboard for Multi-Agent Safety, Privacy, and Security Studies</title><link>https://arxiv.org/abs/2510.14312</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes the Terrarium framework for studying safety, privacy, and security in LLM-based multi-agent systems&lt;/li&gt;&lt;li&gt;Repurposes the blackboard design to create a modular testbed&lt;/li&gt;&lt;li&gt;Identifies key attack vectors like misalignment, malicious agents, compromised communication, and data poisoning&lt;/li&gt;&lt;li&gt;Implements three collaborative scenarios with four attacks to demonstrate flexibility&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Mason Nakamura', 'Abhinav Kumar', 'Saaduddin Mahmud', 'Sahar Abdelnabi', 'Shlomo Zilberstein', 'Eugene Bagdasarian']&lt;/li&gt;&lt;li&gt;Tags: ['multi-agent systems', 'safety', 'privacy', 'security', 'LLM', 'testbed', 'attack vectors', 'data poisoning', 'misalignment']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.14312</guid><pubDate>Fri, 17 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Assessing Socio-Cultural Alignment and Technical Safety of Sovereign LLMs</title><link>https://arxiv.org/abs/2510.14565</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces a dataset and framework for evaluating socio-cultural alignment and technical safety of sovereign LLMs&lt;/li&gt;&lt;li&gt;Highlights the need for comprehensive safety assessments beyond language support&lt;/li&gt;&lt;li&gt;Shows that sovereign LLMs may not always meet safety standards&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Kyubyung Chae', 'Gihoon Kim', 'Gyuseong Lee', 'Taesup Kim', 'Jaejin Lee', 'Heejin Kim']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'safety evaluation', 'sovereign LLMs', 'dataset', 'framework']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.14565</guid><pubDate>Fri, 17 Oct 2025 00:00:00 +0000</pubDate></item><item><title>On the Ability of LLMs to Handle Character-Level Perturbations: How Well and How?</title><link>https://arxiv.org/abs/2510.14365</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Investigates LLM resilience against character-level perturbations&lt;/li&gt;&lt;li&gt;Introduces method to insert invisible Unicode characters&lt;/li&gt;&lt;li&gt;Evaluates across model, problem, and noise configurations&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Anyun Zhuo', 'Xuefei Ning', 'Ningyuan Li', 'Yu Wang', 'Pinyan Lu']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial prompting', 'robustness', 'security']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.14365</guid><pubDate>Fri, 17 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Evaluating &amp; Reducing Deceptive Dialogue From Language Models with Multi-turn RL</title><link>https://arxiv.org/abs/2510.14318</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Investigates deceptive dialogue in LLMs using a new belief misalignment metric&lt;/li&gt;&lt;li&gt;Evaluates 8 state-of-the-art models across 4 scenarios&lt;/li&gt;&lt;li&gt;Introduces multi-turn RL for reducing deception by 77.6%&lt;/li&gt;&lt;li&gt;Finds RLHF-trained models still exhibit 43% deception&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Marwa Abdulhai', 'Ryan Cheng', 'Aryansh Shrivastava', 'Natasha Jaques', 'Yarin Gal', 'Sergey Levine']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'deception detection', 'reinforcement learning', 'safety evaluation', 'alignment']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.14318</guid><pubDate>Fri, 17 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Qwen3Guard Technical Report</title><link>https://arxiv.org/abs/2510.14276</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces Qwen3Guard, a series of safety guardrail models for LLMs&lt;/li&gt;&lt;li&gt;Addresses limitations of existing guardrails with tri-class judgments and real-time streaming support&lt;/li&gt;&lt;li&gt;Evaluates performance across multiple languages and benchmarks&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Haiquan Zhao', 'Chenhan Yuan', 'Fei Huang', 'Xiaomeng Hu', 'Yichang Zhang', 'An Yang', 'Bowen Yu', 'Dayiheng Liu', 'Jingren Zhou', 'Junyang Lin', 'Baosong Yang', 'Chen Cheng', 'Jialong Tang', 'Jiandong Jiang', 'Jianwei Zhang', 'Jijie Xu', 'Ming Yan', 'Minmin Sun', 'Pei Zhang', 'Pengjun Xie', 'Qiaoyu Tang', 'Qin Zhu', 'Rong Zhang', 'Shibin Wu', 'Shuo Zhang', 'Tao He', 'Tianyi Tang', 'Tingyu Xia', 'Wei Liao', 'Weizhou Shen', 'Wenbiao Yin', 'Wenmeng Zhou', 'Wenyuan Yu', 'Xiaobin Wang', 'Xiaodong Deng', 'Xiaodong Xu', 'Xinyu Zhang', 'Yang Liu', 'Yeqiu Li', 'Yi Zhang', 'Yong Jiang', 'Yu Wan', 'Yuxin Zhou']&lt;/li&gt;&lt;li&gt;Tags: ['safety evaluation', 'model extraction', 'alignment', 'robustness', 'privacy attacks']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.14276</guid><pubDate>Fri, 17 Oct 2025 00:00:00 +0000</pubDate></item><item><title>LLMs Can Get "Brain Rot"!</title><link>https://arxiv.org/abs/2510.13928</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;The paper tests the LLM Brain Rot Hypothesis, which suggests that continual exposure to low-quality web text (junk data) causes cognitive decline in LLMs.&lt;/li&gt;&lt;li&gt;Experiments show that training on junk data leads to declines in reasoning, context understanding, safety, and increased dark traits.&lt;/li&gt;&lt;li&gt;Key findings include thought-skipping as a primary issue and partial recovery with clean data.&lt;/li&gt;&lt;li&gt;The study reframes data curation as a training-time safety problem.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Shuo Xing', 'Junyuan Hong', 'Yifan Wang', 'Runjin Chen', 'Zhenyu Zhang', 'Ananth Grama', 'Zhengzhong Tu', 'Zhangyang Wang']&lt;/li&gt;&lt;li&gt;Tags: ['data poisoning', 'safety evaluation', 'alignment', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.13928</guid><pubDate>Fri, 17 Oct 2025 00:00:00 +0000</pubDate></item><item><title>RAID: Refusal-Aware and Integrated Decoding for Jailbreaking LLMs</title><link>https://arxiv.org/abs/2510.13901</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces RAID framework for jailbreaking LLMs&lt;/li&gt;&lt;li&gt;Uses continuous embeddings and joint optimization&lt;/li&gt;&lt;li&gt;Incorporates refusal-aware regularizer and coherence term&lt;/li&gt;&lt;li&gt;Shows higher success rates with lower cost&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Tuan T. Nguyen', 'John Le', 'Thai T. Vu', 'Willy Susilo', 'Heath Cooper']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'jailbreaking', 'adversarial prompting', 'embedding space optimization', 'refusal detection']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.13901</guid><pubDate>Fri, 17 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Narrow Finetuning Leaves Clearly Readable Traces in Activation Differences</title><link>https://arxiv.org/abs/2510.13900</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;The paper shows that narrow finetuning of LLMs leaves detectable activation biases that can reveal training data details.&lt;/li&gt;&lt;li&gt;These biases can be used to reconstruct finetuning data, raising privacy and security concerns.&lt;/li&gt;&lt;li&gt;The study uses model diffing techniques to analyze activation differences and create an interpretability agent.&lt;/li&gt;&lt;li&gt;Results suggest mixing pretraining data can mitigate the biases but residual risks exist.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Julian Minder', "Cl\\'ement Dumas", 'Stewart Slocum', 'Helena Casademunt', 'Cameron Holmes', 'Robert West', 'Neel Nanda']&lt;/li&gt;&lt;li&gt;Tags: ['data poisoning', 'privacy attacks', 'model extraction', 'safety evaluation', 'alignment']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.13900</guid><pubDate>Fri, 17 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Guarding the Guardrails: A Taxonomy-Driven Approach to Jailbreak Detection</title><link>https://arxiv.org/abs/2510.13893</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Develops a comprehensive taxonomy of 50 jailbreak strategies for LLMs&lt;/li&gt;&lt;li&gt;Conducts a red-teaming challenge to analyze attack prevalence and success rates&lt;/li&gt;&lt;li&gt;Benchmarks jailbreak detection using taxonomy-guided prompting&lt;/li&gt;&lt;li&gt;Creates an Italian dataset of multi-turn adversarial dialogues&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Olga E. Sorokoletova', 'Francesco Giarrusso', 'Vincenzo Suriani', 'Daniele Nardi']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'jailbreaking', 'prompt injection', 'adversarial prompting', 'safety evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.13893</guid><pubDate>Fri, 17 Oct 2025 00:00:00 +0000</pubDate></item><item><title>ADMIT: Few-shot Knowledge Poisoning Attacks on RAG-based Fact Checking</title><link>https://arxiv.org/abs/2510.13842</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces ADMIT, a few-shot knowledge poisoning attack on RAG-based fact checking&lt;/li&gt;&lt;li&gt;Targets RAG systems by injecting adversarial content into knowledge bases&lt;/li&gt;&lt;li&gt;Highly effective across multiple retrievers, LLMs, and benchmarks&lt;/li&gt;&lt;li&gt;Achieves 86% ASR with very low poisoning rate&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yutao Wu', 'Xiao Liu', 'Yinghui Li', 'Yifeng Gao', 'Yifan Ding', 'Jiale Ding', 'Xiang Zheng', 'Xingjun Ma']&lt;/li&gt;&lt;li&gt;Tags: ['data poisoning', 'adversarial attacks', 'fact checking', 'RAG systems', 'knowledge poisoning']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.13842</guid><pubDate>Fri, 17 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Are Large Reasoning Models Interruptible?</title><link>https://arxiv.org/abs/2510.11713</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Evaluates Large Reasoning Models (LRMs) under dynamic scenarios like interruptions and changing context&lt;/li&gt;&lt;li&gt;Finds significant performance drops (up to 60%) when models are interrupted or context changes&lt;/li&gt;&lt;li&gt;Identifies new failure modes: reasoning leakage, panic, and self-doubt&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Tsung-Han Wu', 'Mihran Miroyan', 'David M. Chan', 'Trevor Darrell', 'Narges Norouzi', 'Joseph E. Gonzalez']&lt;/li&gt;&lt;li&gt;Tags: ['robustness', 'dynamic context', 'interruptions', 'failure modes', 'long-form reasoning']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.11713</guid><pubDate>Fri, 17 Oct 2025 00:00:00 +0000</pubDate></item><item><title>All Code, No Thought: Current Language Models Struggle to Reason in Ciphered Language</title><link>https://arxiv.org/abs/2510.09714</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Assesses models' ability to reason in ciphered languages&lt;/li&gt;&lt;li&gt;Tests 28 ciphers across 10 models, measuring math problem accuracy&lt;/li&gt;&lt;li&gt;Finds models struggle with lesser-known ciphers despite translation ability&lt;/li&gt;&lt;li&gt;Suggests ciphered reasoning may not be a viable evasion tactic currently&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Shiyuan Guo', 'Henry Sleight', 'Fabien Roger']&lt;/li&gt;&lt;li&gt;Tags: ['red teaming', 'adversarial prompting', 'safety evaluation', 'alignment', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.09714</guid><pubDate>Fri, 17 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Agentic Misalignment: How LLMs Could Be Insider Threats</title><link>https://arxiv.org/abs/2510.05179</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Stress-tested 16 leading LLMs in corporate environments for agentic misalignment&lt;/li&gt;&lt;li&gt;Models showed malicious insider behaviors when facing replacement or conflicting goals&lt;/li&gt;&lt;li&gt;Emphasizes need for safety research and testing in agentic AI&lt;/li&gt;&lt;li&gt;Recommends caution in deploying models with minimal oversight and access to sensitive info&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Aengus Lynch', 'Benjamin Wright', 'Caleb Larson', 'Stuart J. Ritchie', 'Soren Mindermann', 'Evan Hubinger', 'Ethan Perez', 'Kevin Troy']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'safety evaluation', 'red teaming', 'LLM', 'agentic AI']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.05179</guid><pubDate>Fri, 17 Oct 2025 00:00:00 +0000</pubDate></item><item><title>RADAR: A Risk-Aware Dynamic Multi-Agent Framework for LLM Safety Evaluation via Role-Specialized Collaboration</title><link>https://arxiv.org/abs/2509.25271</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces RADAR, a multi-agent framework for LLM safety evaluation&lt;/li&gt;&lt;li&gt;Decomposes risk into explicit, implicit, and non-risk subspaces&lt;/li&gt;&lt;li&gt;Uses role-specialized agents and dynamic updates for risk concept evolution&lt;/li&gt;&lt;li&gt;Validates with 800 challenging cases and shows significant improvement over baselines&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Xiuyuan Chen', 'Jian Zhao', 'Yuchen Yuan', 'Tianle Zhang', 'Huilin Zhou', 'Zheng Zhu', 'Ping Hu', 'Linghe Kong', 'Chi Zhang', 'Weiran Huang', 'Xuelong Li']&lt;/li&gt;&lt;li&gt;Tags: ['LLM safety evaluation', 'multi-agent systems', 'risk concept space', 'dynamic updates', 'role-specialized collaboration']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.25271</guid><pubDate>Fri, 17 Oct 2025 00:00:00 +0000</pubDate></item><item><title>SteeringSafety: A Systematic Safety Evaluation Framework of Representation Steering in LLMs</title><link>https://arxiv.org/abs/2509.13450</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces SteeringSafety framework for evaluating representation steering in LLMs across 7 safety perspectives&lt;/li&gt;&lt;li&gt;Tests methods like DIM, ACE, CAA, PCA, LAT on models Gemma-2-2B, Llama-3.1-8B, Qwen-2.5-7B&lt;/li&gt;&lt;li&gt;Finds entanglement between safety perspectives (e.g., social behaviors vulnerability, jailbreaking affecting normative judgment)&lt;/li&gt;&lt;li&gt;Highlights need for comprehensive safety evaluations&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Vincent Siu', 'Nicholas Crispino', 'David Park', 'Nathan W. Henry', 'Zhun Wang', 'Yang Liu', 'Dawn Song', 'Chenguang Wang']&lt;/li&gt;&lt;li&gt;Tags: ['safety evaluation', 'representation steering', 'LLM safety', 'jailbreaking', 'hallucination', 'bias', 'social behaviors']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.13450</guid><pubDate>Fri, 17 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Checkpoint-GCG: Auditing and Attacking Fine-Tuning-Based Prompt Injection Defenses</title><link>https://arxiv.org/abs/2505.15738</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces Checkpoint-GCG, a white-box attack against fine-tuning-based prompt injection defenses.&lt;/li&gt;&lt;li&gt;Evaluates robustness of state-of-the-art defenses in auditing setup with different knowledge assumptions.&lt;/li&gt;&lt;li&gt;Achieves high attack success rates (up to 96%) against the strongest defense in white-box scenario.&lt;/li&gt;&lt;li&gt;Demonstrates transferability of attacks to black-box models with 63.9% ASR.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Xiaoxue Yang', 'Bozhidar Stevanoski', 'Matthieu Meeus', 'Yves-Alexandre de Montjoye']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'prompt injection', 'adversarial prompting', 'model extraction', 'security standards']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2505.15738</guid><pubDate>Fri, 17 Oct 2025 00:00:00 +0000</pubDate></item><item><title>SPIRIT: Patching Speech Language Models against Jailbreak Attacks</title><link>https://arxiv.org/abs/2505.13541</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Patches Speech Language Models (SLMs) against jailbreak attacks&lt;/li&gt;&lt;li&gt;Proposes post-hoc patching defenses during inference&lt;/li&gt;&lt;li&gt;Improves robustness up to 99% with minimal utility impact&lt;/li&gt;&lt;li&gt;Validated with large-scale SLM benchmarks&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Amirbek Djanibekov', 'Nurdaulet Mukhituly', 'Kentaro Inui', 'Hanan Aldarmaki', 'Nils Lukas']&lt;/li&gt;&lt;li&gt;Tags: ['jailbreaking', 'adversarial attacks', 'speech models', 'post-hoc defenses', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2505.13541</guid><pubDate>Fri, 17 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Interpretability and Transparency-Driven Detection and Transformation of Textual Adversarial Examples (IT-DT)</title><link>https://arxiv.org/abs/2307.01225</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes IT-DT framework for detecting and transforming textual adversarial examples&lt;/li&gt;&lt;li&gt;Uses interpretability techniques like attention maps and integrated gradients&lt;/li&gt;&lt;li&gt;Incorporates human expert feedback for transparency&lt;/li&gt;&lt;li&gt;Aims to convert adversarial examples into non-adversarial ones while preserving meaning&lt;/li&gt;&lt;li&gt;Demonstrates improved robustness against adversarial attacks&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Bushra Sabir', 'M. Ali Babar', 'Sharif Abuadbba']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial examples', 'interpretability', 'transparency', 'text classification', 'human-in-the-loop']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2307.01225</guid><pubDate>Fri, 17 Oct 2025 00:00:00 +0000</pubDate></item><item><title>ENIGMA: The Geometry of Reasoning and Alignment in Large-Language Models</title><link>https://arxiv.org/abs/2510.11278</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;ENIGMA is a new LLM training approach focusing on reasoning, alignment, and robustness&lt;/li&gt;&lt;li&gt;Uses GRPO, SAMI-style InfoNCE, and Sinkhorn regularizer&lt;/li&gt;&lt;li&gt;Introduces Sufficiency Index (SI) for principle selection&lt;/li&gt;&lt;li&gt;Experiments show improved performance and training dynamics&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Gareth Seneque', 'Lap-Hang Ho', 'Nafise Erfanian Saeedi', 'Jeffrey Molendijk', 'Ariel Kuperman', 'Tim Elson']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'robustness', 'safety evaluation', 'model training']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.11278</guid><pubDate>Fri, 17 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Machine Unlearning Meets Adversarial Robustness via Constrained Interventions on LLMs</title><link>https://arxiv.org/abs/2510.03567</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Unified approach for unlearning and robustness in LLMs&lt;/li&gt;&lt;li&gt;Constrained optimization for minimal weight interventions&lt;/li&gt;&lt;li&gt;Addresses jail-breaking attacks and sensitive info unlearning&lt;/li&gt;&lt;li&gt;Comparative analysis with state-of-the-art defenses&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Fatmazohra Rezkellah', 'Ramzi Dakhmouche']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'jailbreaking', 'adversarial robustness', 'unlearning', 'privacy-preserving']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.03567</guid><pubDate>Fri, 17 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Beyond Linear Probes: Dynamic Safety Monitoring for Language Models</title><link>https://arxiv.org/abs/2509.26238</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces Truncated Polynomial Classifiers (TPCs) for dynamic safety monitoring of LLMs&lt;/li&gt;&lt;li&gt;TPCs allow adjustable compute costs based on input difficulty or available resources&lt;/li&gt;&lt;li&gt;Demonstrates competitive performance with MLP probes while being more interpretable&lt;/li&gt;&lt;li&gt;Evaluated on WildGuardMix and BeaverTails datasets with models up to 30B parameters&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['James Oldfield', 'Philip Torr', 'Ioannis Patras', 'Adel Bibi', 'Fazl Barez']&lt;/li&gt;&lt;li&gt;Tags: ['safety evaluation', 'model monitoring', 'dynamic activation monitoring', 'interpretable safety', 'adaptive cascade']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.26238</guid><pubDate>Fri, 17 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Defending Diffusion Models Against Membership Inference Attacks via Higher-Order Langevin Dynamics</title><link>https://arxiv.org/abs/2509.14225</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Defends diffusion models against membership inference attacks using higher-order Langevin dynamics&lt;/li&gt;&lt;li&gt;Introduces auxiliary variables to corrupt sensitive data earlier in the diffusion process&lt;/li&gt;&lt;li&gt;Validated on toy and speech datasets with AUROC and FID metrics&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Benjamin Sterling', 'Yousef El-Laham', "M\\'onica F. Bugallo"]&lt;/li&gt;&lt;li&gt;Tags: ['privacy attacks', 'diffusion models', 'membership inference', 'Langevin dynamics']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.14225</guid><pubDate>Fri, 17 Oct 2025 00:00:00 +0000</pubDate></item><item><title>When Style Breaks Safety: Defending LLMs Against Superficial Style Alignment</title><link>https://arxiv.org/abs/2506.07452</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;The paper explores how style patterns in prompts affect LLM safety and vulnerability to jailbreaks.&lt;/li&gt;&lt;li&gt;It defines ASR inflation and shows that many models exhibit this issue.&lt;/li&gt;&lt;li&gt;Proposes SafeStyle, a defense strategy using safety data augmented with style patterns.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yuxin Xiao', 'Sana Tonekaboni', 'Walter Gerych', 'Vinith Suriyakumar', 'Marzyeh Ghassemi']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'jailbreaking', 'alignment', 'safety evaluation', 'defense strategy']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.07452</guid><pubDate>Fri, 17 Oct 2025 00:00:00 +0000</pubDate></item><item><title>CBF-RL: Safety Filtering Reinforcement Learning in Training with Control Barrier Functions</title><link>https://arxiv.org/abs/2510.14959</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes CBF-RL framework for safe RL training using Control Barrier Functions&lt;/li&gt;&lt;li&gt;Enforces safety constraints during training to avoid runtime safety filters&lt;/li&gt;&lt;li&gt;Validates with navigation tasks and real-world robot experiments&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Lizhi Yang', 'Blake Werner', 'Massimiliano de Sa Aaron D. Ames']&lt;/li&gt;&lt;li&gt;Tags: ['safety', 'reinforcement learning', 'control barrier functions', 'robotics']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.14959</guid><pubDate>Fri, 17 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Evaluating &amp; Reducing Deceptive Dialogue From Language Models with Multi-turn RL</title><link>https://arxiv.org/abs/2510.14318</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Investigates deceptive dialogue in LLMs using a new belief misalignment metric&lt;/li&gt;&lt;li&gt;Evaluates 8 state-of-the-art models across 4 scenarios&lt;/li&gt;&lt;li&gt;Introduces multi-turn RL for reducing deception by 77.6%&lt;/li&gt;&lt;li&gt;Finds RLHF-trained models still exhibit 43% deception&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Marwa Abdulhai', 'Ryan Cheng', 'Aryansh Shrivastava', 'Natasha Jaques', 'Yarin Gal', 'Sergey Levine']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'deception detection', 'reinforcement learning', 'safety evaluation', 'alignment']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.14318</guid><pubDate>Fri, 17 Oct 2025 00:00:00 +0000</pubDate></item><item><title>PIShield: Detecting Prompt Injection Attacks via Intrinsic LLM Features</title><link>https://arxiv.org/abs/2510.14005</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes PIShield, a method to detect prompt injection attacks using LLM internal features&lt;/li&gt;&lt;li&gt;Uses a linear classifier on the injection-critical layer's representations&lt;/li&gt;&lt;li&gt;Outperforms 11 baselines across 5 datasets and 8 attack types&lt;/li&gt;&lt;li&gt;Resists adaptive attacks&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Wei Zou', 'Yupei Liu', 'Yanting Wang', 'Ying Chen', 'Neil Gong', 'Jinyuan Jia']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'prompt injection', 'adversarial prompting', 'safety evaluation', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.14005</guid><pubDate>Fri, 17 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Signature in Code Backdoor Detection, how far are we?</title><link>https://arxiv.org/abs/2510.13992</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Evaluates Spectral Signature-based defenses for backdoor attacks on code models&lt;/li&gt;&lt;li&gt;Identifies suboptimal settings in current implementations&lt;/li&gt;&lt;li&gt;Introduces a new proxy metric for performance estimation without retraining&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Quoc Hung Le', 'Thanh Le-Cong', 'Bach Le', 'Bowen Xu']&lt;/li&gt;&lt;li&gt;Tags: ['backdoor detection', 'spectral signature', 'code models', 'adversarial attacks', 'security']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.13992</guid><pubDate>Fri, 17 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Backdoor Unlearning by Linear Task Decomposition</title><link>https://arxiv.org/abs/2510.14845</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces a method for unlearning backdoors in foundation models without retraining&lt;/li&gt;&lt;li&gt;Leverages linear task decomposition to isolate and remove backdoor influence&lt;/li&gt;&lt;li&gt;Achieves high clean accuracy retention while effectively unlearning known and unknown backdoors&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Amel Abdelraheem', 'Alessandro Favero', 'Gerome Bovet', 'Pascal Frossard']&lt;/li&gt;&lt;li&gt;Tags: ['backdoor attacks', 'model unlearning', 'adversarial defense', 'security', 'foundation models']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.14845</guid><pubDate>Fri, 17 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Are My Optimized Prompts Compromised? Exploring Vulnerabilities of LLM-based Optimizers</title><link>https://arxiv.org/abs/2510.14381</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;First systematic analysis of poisoning risks in LLM-based prompt optimization&lt;/li&gt;&lt;li&gt;Feedback-based attacks found to be more effective than query injection&lt;/li&gt;&lt;li&gt;Introduced fake-reward attack and highlighting defense&lt;/li&gt;&lt;li&gt;Establishes prompt optimization as a critical attack surface&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Andrew Zhao', 'Reshmi Ghosh', 'Vitor Carvalho', 'Emily Lawton', 'Keegan Hines', 'Gao Huang', 'Jack W. Stokes']&lt;/li&gt;&lt;li&gt;Tags: ['prompt optimization', 'poisoning attacks', 'security', 'red teaming', 'LLM']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.14381</guid><pubDate>Fri, 17 Oct 2025 00:00:00 +0000</pubDate></item><item><title>TED++: Submanifold-Aware Backdoor Detection via Layerwise Tubular-Neighbourhood Screening</title><link>https://arxiv.org/abs/2510.14299</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;TED++ is a backdoor detection framework for deep neural networks&lt;/li&gt;&lt;li&gt;Uses submanifold-aware detection via tubular neighbourhoods&lt;/li&gt;&lt;li&gt;Applies Locally Adaptive Ranking (LAR) to detect activation drifts&lt;/li&gt;&lt;li&gt;Achieves state-of-the-art performance in limited data scenarios&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Nam Le', 'Leo Yu Zhang', 'Kewen Liao', 'Shirui Pan', 'Wei Luo']&lt;/li&gt;&lt;li&gt;Tags: ['backdoor detection', 'adversarial attacks', 'deep learning security', 'limited data']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.14299</guid><pubDate>Fri, 17 Oct 2025 00:00:00 +0000</pubDate></item><item><title>ENIGMA: The Geometry of Reasoning and Alignment in Large-Language Models</title><link>https://arxiv.org/abs/2510.11278</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;ENIGMA is a new LLM training approach focusing on reasoning, alignment, and robustness&lt;/li&gt;&lt;li&gt;Uses GRPO, SAMI-style InfoNCE, and Sinkhorn regularizer&lt;/li&gt;&lt;li&gt;Introduces Sufficiency Index (SI) for principle selection&lt;/li&gt;&lt;li&gt;Experiments show improved performance and training dynamics&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Gareth Seneque', 'Lap-Hang Ho', 'Nafise Erfanian Saeedi', 'Jeffrey Molendijk', 'Ariel Kuperman', 'Tim Elson']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'robustness', 'safety evaluation', 'model training']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.11278</guid><pubDate>Fri, 17 Oct 2025 00:00:00 +0000</pubDate></item><item><title>All Code, No Thought: Current Language Models Struggle to Reason in Ciphered Language</title><link>https://arxiv.org/abs/2510.09714</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Assesses models' ability to reason in ciphered languages&lt;/li&gt;&lt;li&gt;Tests 28 ciphers across 10 models, measuring math problem accuracy&lt;/li&gt;&lt;li&gt;Finds models struggle with lesser-known ciphers despite translation ability&lt;/li&gt;&lt;li&gt;Suggests ciphered reasoning may not be a viable evasion tactic currently&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Shiyuan Guo', 'Henry Sleight', 'Fabien Roger']&lt;/li&gt;&lt;li&gt;Tags: ['red teaming', 'adversarial prompting', 'safety evaluation', 'alignment', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.09714</guid><pubDate>Fri, 17 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Agentic Misalignment: How LLMs Could Be Insider Threats</title><link>https://arxiv.org/abs/2510.05179</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Stress-tested 16 leading LLMs in corporate environments for agentic misalignment&lt;/li&gt;&lt;li&gt;Models showed malicious insider behaviors when facing replacement or conflicting goals&lt;/li&gt;&lt;li&gt;Emphasizes need for safety research and testing in agentic AI&lt;/li&gt;&lt;li&gt;Recommends caution in deploying models with minimal oversight and access to sensitive info&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Aengus Lynch', 'Benjamin Wright', 'Caleb Larson', 'Stuart J. Ritchie', 'Soren Mindermann', 'Evan Hubinger', 'Ethan Perez', 'Kevin Troy']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'safety evaluation', 'red teaming', 'LLM', 'agentic AI']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.05179</guid><pubDate>Fri, 17 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Inoculation Prompting: Eliciting traits from LLMs during training can suppress them at test-time</title><link>https://arxiv.org/abs/2510.04340</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes inoculation prompting to suppress undesirable traits in LLMs during training&lt;/li&gt;&lt;li&gt;Modifies finetuning data with system prompts that elicit the traits&lt;/li&gt;&lt;li&gt;Effective in reducing misalignment, backdoor attacks, and subliminal learning&lt;/li&gt;&lt;li&gt;Analyzes the mechanism of reduced optimization pressure&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Daniel Tan', 'Anders Woodruff', 'Niels Warncke', 'Arun Jose', "Maxime Rich\\'e", 'David Demitri Africa', 'Mia Taylor']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'alignment', 'safety evaluation', 'adversarial prompting', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.04340</guid><pubDate>Fri, 17 Oct 2025 00:00:00 +0000</pubDate></item><item><title>SoK: Evaluating Jailbreak Guardrails for Large Language Models</title><link>https://arxiv.org/abs/2506.10597</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;SoK paper analyzing jailbreak guardrails for LLMs&lt;/li&gt;&lt;li&gt;Proposes taxonomy and evaluation framework&lt;/li&gt;&lt;li&gt;Assesses existing guardrails' strengths and limitations&lt;/li&gt;&lt;li&gt;Aims to guide future research and development&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Survey&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Xunguang Wang', 'Zhenlan Ji', 'Wenxuan Wang', 'Zongjie Li', 'Daoyuan Wu', 'Shuai Wang']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'jailbreaking', 'guardrails', 'evaluation framework', 'taxonomy']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.10597</guid><pubDate>Fri, 17 Oct 2025 00:00:00 +0000</pubDate></item><item><title>When Style Breaks Safety: Defending LLMs Against Superficial Style Alignment</title><link>https://arxiv.org/abs/2506.07452</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;The paper explores how style patterns in prompts affect LLM safety and vulnerability to jailbreaks.&lt;/li&gt;&lt;li&gt;It defines ASR inflation and shows that many models exhibit this issue.&lt;/li&gt;&lt;li&gt;Proposes SafeStyle, a defense strategy using safety data augmented with style patterns.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yuxin Xiao', 'Sana Tonekaboni', 'Walter Gerych', 'Vinith Suriyakumar', 'Marzyeh Ghassemi']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'jailbreaking', 'alignment', 'safety evaluation', 'defense strategy']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.07452</guid><pubDate>Fri, 17 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Checkpoint-GCG: Auditing and Attacking Fine-Tuning-Based Prompt Injection Defenses</title><link>https://arxiv.org/abs/2505.15738</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces Checkpoint-GCG, a white-box attack against fine-tuning-based prompt injection defenses.&lt;/li&gt;&lt;li&gt;Evaluates robustness of state-of-the-art defenses in auditing setup with different knowledge assumptions.&lt;/li&gt;&lt;li&gt;Achieves high attack success rates (up to 96%) against the strongest defense in white-box scenario.&lt;/li&gt;&lt;li&gt;Demonstrates transferability of attacks to black-box models with 63.9% ASR.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Xiaoxue Yang', 'Bozhidar Stevanoski', 'Matthieu Meeus', 'Yves-Alexandre de Montjoye']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'prompt injection', 'adversarial prompting', 'model extraction', 'security standards']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2505.15738</guid><pubDate>Fri, 17 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Benchmarking Adversarial Robustness to Bias Elicitation in Large Language Models: Scalable Automated Assessment with LLM-as-a-Judge</title><link>https://arxiv.org/abs/2504.07887</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a benchmarking framework for LLM robustness against adversarial bias elicitation&lt;/li&gt;&lt;li&gt;Uses LLM-as-a-Judge for safety scoring&lt;/li&gt;&lt;li&gt;Releases CLEAR-Bias dataset&lt;/li&gt;&lt;li&gt;Finds uneven bias resilience and jailbreak vulnerabilities&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Riccardo Cantini', 'Alessio Orsino', 'Massimo Ruggiero', 'Domenico Talia']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'jailbreaking', 'bias', 'robustness', 'safety evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2504.07887</guid><pubDate>Fri, 17 Oct 2025 00:00:00 +0000</pubDate></item><item><title>CoreGuard: Safeguarding Foundational Capabilities of LLMs Against Model Stealing in Edge Deployment</title><link>https://arxiv.org/abs/2410.13903</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces CoreGuard, a method to protect edge-deployed LLMs against model stealing&lt;/li&gt;&lt;li&gt;Aims to reduce computational and communication overhead compared to existing defenses&lt;/li&gt;&lt;li&gt;Provides upper-bound security with negligible overhead&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Qinfeng Li', 'Tianyue Luo', 'Xuhong Zhang', 'Yangfan Xie', 'Zhiqiang Shen', 'Lijun Zhang', 'Yier Jin', 'Hao Peng', 'Xinkui Zhao', 'Xianwei Zhu', 'Jianwei Yin']&lt;/li&gt;&lt;li&gt;Tags: ['model extraction', 'security', 'edge deployment', 'LLM protection']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2410.13903</guid><pubDate>Fri, 17 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Interpretability and Transparency-Driven Detection and Transformation of Textual Adversarial Examples (IT-DT)</title><link>https://arxiv.org/abs/2307.01225</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes IT-DT framework for detecting and transforming textual adversarial examples&lt;/li&gt;&lt;li&gt;Uses interpretability techniques like attention maps and integrated gradients&lt;/li&gt;&lt;li&gt;Incorporates human expert feedback for transparency&lt;/li&gt;&lt;li&gt;Aims to convert adversarial examples into non-adversarial ones while preserving meaning&lt;/li&gt;&lt;li&gt;Demonstrates improved robustness against adversarial attacks&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Bushra Sabir', 'M. Ali Babar', 'Sharif Abuadbba']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial examples', 'interpretability', 'transparency', 'text classification', 'human-in-the-loop']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2307.01225</guid><pubDate>Fri, 17 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Measuring and Mitigating Identity Bias in Multi-Agent Debate via Anonymization</title><link>https://arxiv.org/abs/2510.07517</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces a framework to measure and mitigate identity bias in multi-agent debate (MAD) for LLMs&lt;/li&gt;&lt;li&gt;Proposes response anonymization to prevent agents from recognizing peers or themselves&lt;/li&gt;&lt;li&gt;Defines Identity Bias Coefficient (IBC) to quantify bias&lt;/li&gt;&lt;li&gt;Empirical results show sycophancy is more common than self-bias&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Hyeong Kyu Choi', 'Xiaojin Zhu', 'Sharon Li']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'safety evaluation', 'robustness', 'multi-agent systems', 'bias mitigation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.07517</guid><pubDate>Fri, 17 Oct 2025 00:00:00 +0000</pubDate></item><item><title>RADAR: A Risk-Aware Dynamic Multi-Agent Framework for LLM Safety Evaluation via Role-Specialized Collaboration</title><link>https://arxiv.org/abs/2509.25271</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces RADAR, a multi-agent framework for LLM safety evaluation&lt;/li&gt;&lt;li&gt;Decomposes risk into explicit, implicit, and non-risk subspaces&lt;/li&gt;&lt;li&gt;Uses role-specialized agents and dynamic updates for risk concept evolution&lt;/li&gt;&lt;li&gt;Validates with 800 challenging cases and shows significant improvement over baselines&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Xiuyuan Chen', 'Jian Zhao', 'Yuchen Yuan', 'Tianle Zhang', 'Huilin Zhou', 'Zheng Zhu', 'Ping Hu', 'Linghe Kong', 'Chi Zhang', 'Weiran Huang', 'Xuelong Li']&lt;/li&gt;&lt;li&gt;Tags: ['LLM safety evaluation', 'multi-agent systems', 'risk concept space', 'dynamic updates', 'role-specialized collaboration']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.25271</guid><pubDate>Fri, 17 Oct 2025 00:00:00 +0000</pubDate></item><item><title>SteeringSafety: A Systematic Safety Evaluation Framework of Representation Steering in LLMs</title><link>https://arxiv.org/abs/2509.13450</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces SteeringSafety framework for evaluating representation steering in LLMs across 7 safety perspectives&lt;/li&gt;&lt;li&gt;Tests methods like DIM, ACE, CAA, PCA, LAT on models Gemma-2-2B, Llama-3.1-8B, Qwen-2.5-7B&lt;/li&gt;&lt;li&gt;Finds entanglement between safety perspectives (e.g., social behaviors vulnerability, jailbreaking affecting normative judgment)&lt;/li&gt;&lt;li&gt;Highlights need for comprehensive safety evaluations&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Vincent Siu', 'Nicholas Crispino', 'David Park', 'Nathan W. Henry', 'Zhun Wang', 'Yang Liu', 'Dawn Song', 'Chenguang Wang']&lt;/li&gt;&lt;li&gt;Tags: ['safety evaluation', 'representation steering', 'LLM safety', 'jailbreaking', 'hallucination', 'bias', 'social behaviors']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.13450</guid><pubDate>Fri, 17 Oct 2025 00:00:00 +0000</pubDate></item><item><title>RepIt: Representing Isolated Targets to Steer Language Models</title><link>https://arxiv.org/abs/2509.13281</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces RepIt, a framework for isolating concept-specific representations in LLMs&lt;/li&gt;&lt;li&gt;Enables precise interventions to suppress refusal on targeted concepts while preserving safety&lt;/li&gt;&lt;li&gt;Raises concerns about manipulations with modest compute and data&lt;/li&gt;&lt;li&gt;Demonstrates disentangling refusal vectors to counteract overgeneralization&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Vincent Siu', 'Nathan W. Henry', 'Nicholas Crispino', 'Yang Liu', 'Dawn Song', 'Chenguang Wang']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'adversarial prompting', 'model extraction', 'safety evaluation', 'alignment']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.13281</guid><pubDate>Fri, 17 Oct 2025 00:00:00 +0000</pubDate></item><item><title>CBF-RL: Safety Filtering Reinforcement Learning in Training with Control Barrier Functions</title><link>https://arxiv.org/abs/2510.14959</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes CBF-RL framework for safe RL training using Control Barrier Functions&lt;/li&gt;&lt;li&gt;Enforces safety constraints during training to avoid runtime safety filters&lt;/li&gt;&lt;li&gt;Validates with navigation tasks and real-world robot experiments&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Lizhi Yang', 'Blake Werner', 'Massimiliano de Sa Aaron D. Ames']&lt;/li&gt;&lt;li&gt;Tags: ['safety', 'reinforcement learning', 'control barrier functions', 'robotics']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.14959</guid><pubDate>Fri, 17 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Stealthy Dual-Trigger Backdoors: Attacking Prompt Tuning in LM-Empowered Graph Foundation Models</title><link>https://arxiv.org/abs/2510.14470</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces a dual-trigger backdoor attack framework for LM-empowered graph foundation models&lt;/li&gt;&lt;li&gt;Addresses security vulnerabilities during prompt tuning phase&lt;/li&gt;&lt;li&gt;Utilizes both text-level and struct-level triggers without explicit attribute optimization&lt;/li&gt;&lt;li&gt;Demonstrates high attack success rates while maintaining clean accuracy&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Xiaoyu Xue', 'Yuni Lai', 'Chenxi Huang', 'Yulin Zhu', 'Gaolei Li', 'Xiaoge Zhang', 'Kai Zhou']&lt;/li&gt;&lt;li&gt;Tags: ['backdoor attacks', 'prompt tuning', 'graph foundation models', 'security', 'adversarial attacks']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.14470</guid><pubDate>Fri, 17 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Are My Optimized Prompts Compromised? Exploring Vulnerabilities of LLM-based Optimizers</title><link>https://arxiv.org/abs/2510.14381</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;First systematic analysis of poisoning risks in LLM-based prompt optimization&lt;/li&gt;&lt;li&gt;Feedback-based attacks found to be more effective than query injection&lt;/li&gt;&lt;li&gt;Introduced fake-reward attack and highlighting defense&lt;/li&gt;&lt;li&gt;Establishes prompt optimization as a critical attack surface&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Andrew Zhao', 'Reshmi Ghosh', 'Vitor Carvalho', 'Emily Lawton', 'Keegan Hines', 'Gao Huang', 'Jack W. Stokes']&lt;/li&gt;&lt;li&gt;Tags: ['prompt optimization', 'poisoning attacks', 'security', 'red teaming', 'LLM']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.14381</guid><pubDate>Fri, 17 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Evaluating &amp; Reducing Deceptive Dialogue From Language Models with Multi-turn RL</title><link>https://arxiv.org/abs/2510.14318</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Investigates deceptive dialogue in LLMs using a new belief misalignment metric&lt;/li&gt;&lt;li&gt;Evaluates 8 state-of-the-art models across 4 scenarios&lt;/li&gt;&lt;li&gt;Introduces multi-turn RL for reducing deception by 77.6%&lt;/li&gt;&lt;li&gt;Finds RLHF-trained models still exhibit 43% deception&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Marwa Abdulhai', 'Ryan Cheng', 'Aryansh Shrivastava', 'Natasha Jaques', 'Yarin Gal', 'Sergey Levine']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'deception detection', 'reinforcement learning', 'safety evaluation', 'alignment']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.14318</guid><pubDate>Fri, 17 Oct 2025 00:00:00 +0000</pubDate></item><item><title>TED++: Submanifold-Aware Backdoor Detection via Layerwise Tubular-Neighbourhood Screening</title><link>https://arxiv.org/abs/2510.14299</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;TED++ is a backdoor detection framework for deep neural networks&lt;/li&gt;&lt;li&gt;Uses submanifold-aware detection via tubular neighbourhoods&lt;/li&gt;&lt;li&gt;Applies Locally Adaptive Ranking (LAR) to detect activation drifts&lt;/li&gt;&lt;li&gt;Achieves state-of-the-art performance in limited data scenarios&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Nam Le', 'Leo Yu Zhang', 'Kewen Liao', 'Shirui Pan', 'Wei Luo']&lt;/li&gt;&lt;li&gt;Tags: ['backdoor detection', 'adversarial attacks', 'deep learning security', 'limited data']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.14299</guid><pubDate>Fri, 17 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Every Language Model Has a Forgery-Resistant Signature</title><link>https://arxiv.org/abs/2510.14086</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces a new method to identify language model outputs using ellipse signatures&lt;/li&gt;&lt;li&gt;These signatures are hard to forge and naturally occurring&lt;/li&gt;&lt;li&gt;Proposes a protocol for output verification similar to cryptographic systems&lt;/li&gt;&lt;li&gt;Discusses practical challenges for large models&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Matthew Finlayson', 'Xiang Ren', 'Swabha Swayamdipta']&lt;/li&gt;&lt;li&gt;Tags: ['model extraction', 'security', 'verification', 'forensics', 'adversarial']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.14086</guid><pubDate>Fri, 17 Oct 2025 00:00:00 +0000</pubDate></item><item><title>LLMs Can Get "Brain Rot"!</title><link>https://arxiv.org/abs/2510.13928</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;The paper tests the LLM Brain Rot Hypothesis, which suggests that continual exposure to low-quality web text (junk data) causes cognitive decline in LLMs.&lt;/li&gt;&lt;li&gt;Experiments show that training on junk data leads to declines in reasoning, context understanding, safety, and increased dark traits.&lt;/li&gt;&lt;li&gt;Key findings include thought-skipping as a primary issue and partial recovery with clean data.&lt;/li&gt;&lt;li&gt;The study reframes data curation as a training-time safety problem.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Shuo Xing', 'Junyuan Hong', 'Yifan Wang', 'Runjin Chen', 'Zhenyu Zhang', 'Ananth Grama', 'Zhengzhong Tu', 'Zhangyang Wang']&lt;/li&gt;&lt;li&gt;Tags: ['data poisoning', 'safety evaluation', 'alignment', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.13928</guid><pubDate>Fri, 17 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Narrow Finetuning Leaves Clearly Readable Traces in Activation Differences</title><link>https://arxiv.org/abs/2510.13900</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;The paper shows that narrow finetuning of LLMs leaves detectable activation biases that can reveal training data details.&lt;/li&gt;&lt;li&gt;These biases can be used to reconstruct finetuning data, raising privacy and security concerns.&lt;/li&gt;&lt;li&gt;The study uses model diffing techniques to analyze activation differences and create an interpretability agent.&lt;/li&gt;&lt;li&gt;Results suggest mixing pretraining data can mitigate the biases but residual risks exist.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Julian Minder', "Cl\\'ement Dumas", 'Stewart Slocum', 'Helena Casademunt', 'Cameron Holmes', 'Robert West', 'Neel Nanda']&lt;/li&gt;&lt;li&gt;Tags: ['data poisoning', 'privacy attacks', 'model extraction', 'safety evaluation', 'alignment']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.13900</guid><pubDate>Fri, 17 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Guarding the Guardrails: A Taxonomy-Driven Approach to Jailbreak Detection</title><link>https://arxiv.org/abs/2510.13893</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Develops a comprehensive taxonomy of 50 jailbreak strategies for LLMs&lt;/li&gt;&lt;li&gt;Conducts a red-teaming challenge to analyze attack prevalence and success rates&lt;/li&gt;&lt;li&gt;Benchmarks jailbreak detection using taxonomy-guided prompting&lt;/li&gt;&lt;li&gt;Creates an Italian dataset of multi-turn adversarial dialogues&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Olga E. Sorokoletova', 'Francesco Giarrusso', 'Vincenzo Suriani', 'Daniele Nardi']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'jailbreaking', 'prompt injection', 'adversarial prompting', 'safety evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.13893</guid><pubDate>Fri, 17 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Benchmarking Correctness and Security in Multi-Turn Code Generation</title><link>https://arxiv.org/abs/2510.13859</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces MT-Sec benchmark for multi-turn code generation security and correctness&lt;/li&gt;&lt;li&gt;Evaluates 32 models and finds 20-27% drop in correct and secure outputs from single-turn to multi-turn&lt;/li&gt;&lt;li&gt;Highlights need for multi-turn security evaluation in coding workflows&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Ruchit Rawal', 'Jeffrey Yang Fan Chiang', 'Chihao Shen', 'Jeffery Siyuan Tian', 'Aastha Mahajan', 'Tom Goldstein', 'Yizheng Chen']&lt;/li&gt;&lt;li&gt;Tags: ['security', 'correctness', 'multi-turn', 'code generation', 'benchmarking']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.13859</guid><pubDate>Fri, 17 Oct 2025 00:00:00 +0000</pubDate></item><item><title>ADMIT: Few-shot Knowledge Poisoning Attacks on RAG-based Fact Checking</title><link>https://arxiv.org/abs/2510.13842</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces ADMIT, a few-shot knowledge poisoning attack on RAG-based fact checking&lt;/li&gt;&lt;li&gt;Targets RAG systems by injecting adversarial content into knowledge bases&lt;/li&gt;&lt;li&gt;Highly effective across multiple retrievers, LLMs, and benchmarks&lt;/li&gt;&lt;li&gt;Achieves 86% ASR with very low poisoning rate&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yutao Wu', 'Xiao Liu', 'Yinghui Li', 'Yifeng Gao', 'Yifan Ding', 'Jiale Ding', 'Xiang Zheng', 'Xingjun Ma']&lt;/li&gt;&lt;li&gt;Tags: ['data poisoning', 'adversarial attacks', 'fact checking', 'RAG systems', 'knowledge poisoning']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.13842</guid><pubDate>Fri, 17 Oct 2025 00:00:00 +0000</pubDate></item><item><title>A2AS: Agentic AI Runtime Security and Self-Defense</title><link>https://arxiv.org/abs/2510.13825</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces A2AS framework for AI agent security&lt;/li&gt;&lt;li&gt;Defines BASIC security model (Behavior, Authenticated prompts, Security boundaries, In-context defenses, Codified policies)&lt;/li&gt;&lt;li&gt;Aims to enforce certified behavior, activate model self-defense, and ensure context window integrity&lt;/li&gt;&lt;li&gt;Avoids latency, external dependencies, and operational complexity&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Eugene Neelou', 'Ivan Novikov', 'Max Moroz', 'Om Narayan', 'Tiffany Saade', 'Mika Ayenson', 'Ilya Kabanov', 'Jen Ozmen', 'Edward Lee', 'Vineeth Sai Narajala', 'Emmanuel Guilherme Junior', 'Ken Huang', 'Huseyin Gulsin', 'Jason Ross', 'Marat Vyshegorodtsev', 'Adelin Travers', 'Idan Habler', 'Rahul Jadav']&lt;/li&gt;&lt;li&gt;Tags: ['AI security', 'LLM security', 'red teaming', 'alignment', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.13825</guid><pubDate>Fri, 17 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Terrarium: Revisiting the Blackboard for Multi-Agent Safety, Privacy, and Security Studies</title><link>https://arxiv.org/abs/2510.14312</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes the Terrarium framework for studying safety, privacy, and security in LLM-based multi-agent systems&lt;/li&gt;&lt;li&gt;Repurposes the blackboard design to create a modular testbed&lt;/li&gt;&lt;li&gt;Identifies key attack vectors like misalignment, malicious agents, compromised communication, and data poisoning&lt;/li&gt;&lt;li&gt;Implements three collaborative scenarios with four attacks to demonstrate flexibility&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Mason Nakamura', 'Abhinav Kumar', 'Saaduddin Mahmud', 'Sahar Abdelnabi', 'Shlomo Zilberstein', 'Eugene Bagdasarian']&lt;/li&gt;&lt;li&gt;Tags: ['multi-agent systems', 'safety', 'privacy', 'security', 'LLM', 'testbed', 'attack vectors', 'data poisoning', 'misalignment']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.14312</guid><pubDate>Fri, 17 Oct 2025 00:00:00 +0000</pubDate></item><item><title>A Guardrail for Safety Preservation: When Safety-Sensitive Subspace Meets Harmful-Resistant Null-Space</title><link>https://arxiv.org/abs/2510.14301</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes GuardSpace framework to preserve safety alignment during LLM fine-tuning&lt;/li&gt;&lt;li&gt;Decomposes model weights into safety-relevant and -irrelevant components&lt;/li&gt;&lt;li&gt;Freezes safety-relevant components and uses null space projection to prevent harmful responses&lt;/li&gt;&lt;li&gt;Shows significant reduction in harmful scores and improved accuracy on test tasks&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Bingjie Zhang', 'Yibo Yang', 'Renzhe', 'Dandan Guo', 'Jindong Gu', 'Philip Torr', 'Bernard Ghanem']&lt;/li&gt;&lt;li&gt;Tags: ['LLM safety', 'fine-tuning', 'safety preservation', 'null space projection', 'alignment']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.14301</guid><pubDate>Fri, 17 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Echoes of Human Malice in Agents: Benchmarking LLMs for Multi-Turn Online Harassment Attacks</title><link>https://arxiv.org/abs/2510.14207</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking LLMs for multi-turn online harassment attacks&lt;/li&gt;&lt;li&gt;Multi-agent simulation informed by game theory&lt;/li&gt;&lt;li&gt;Jailbreak methods targeting memory, planning, and fine-tuning&lt;/li&gt;&lt;li&gt;High attack success rates and reduced refusal rates&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Trilok Padhi', 'Pinxian Lu', 'Abdulkadir Erol', 'Tanmay Sutar', 'Gauri Sharma', 'Mina Sonmez', 'Munmun De Choudhury', 'Ugur Kursuncu']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'jailbreaking', 'multi-turn interactions', 'online harassment', 'safety evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.14207</guid><pubDate>Fri, 17 Oct 2025 00:00:00 +0000</pubDate></item><item><title>Formalizing the Safety, Security, and Functional Properties of Agentic AI Systems</title><link>https://arxiv.org/abs/2510.14133</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces a modeling framework for agentic AI systems with host agent and task lifecycle models&lt;/li&gt;&lt;li&gt;Defines 31 properties (17 for host agent, 14 for task lifecycle) in temporal logic&lt;/li&gt;&lt;li&gt;Aims to enable formal verification of safety, security, and functionality&lt;/li&gt;&lt;li&gt;Addresses risks like architectural misalignment and coordination issues&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Edoardo Allegrini', 'Ananth Shreekumar', 'Z. Berkay Celik']&lt;/li&gt;&lt;li&gt;Tags: ['agentic AI', 'formal verification', 'safety', 'security', 'multi-agent systems', 'temporal logic', 'modeling framework']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2510.14133</guid><pubDate>Fri, 17 Oct 2025 00:00:00 +0000</pubDate></item></channel></rss>