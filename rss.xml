<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>AI Security Paper Digest</title><link>https://kentaroh-toyoda.github.io/ai-security-paper-digest-rss/rss.xml</link><description>Summarized AI security papers from arXiv</description><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><lastBuildDate>Mon, 26 May 2025 22:12:42 +0000</lastBuildDate><item><title>StealthRank: LLM Ranking Manipulation via Stealthy Prompt Optimization</title><link>https://arxiv.org/abs/2504.05804</link><description>• The paper introduces StealthRank, an adversarial attack method that manipulates LLM-driven ranking systems through stealthy prompt optimization. It demonstrates how adversarial text sequences can covertly influence LLM rankings without leaving obvious traces, exposing vulnerabilities in LLM-based information retrieval.&lt;br/&gt;&lt;br/&gt;Tags: adversarial attacks, LLM misuse, ranking manipulation, robustness under attack&lt;br/&gt;Authors: Yiming Tang, Yi Fan, Chenxiao Yu, Tiankai Yang, Yue Zhao, Xiyang Hu&lt;br/&gt;Relevance: 5 / 5&lt;br/&gt;&lt;br/&gt;&lt;a href='https://arxiv.org/abs/2504.05804'&gt;Read on arXiv&lt;/a&gt;</description><pubDate>Mon, 26 May 2025 22:12:42 +0000</pubDate></item><item><title>Architecture Selection via the Trade-off Between Accuracy and Robustness</title><link>https://arxiv.org/abs/1906.01354</link><description>• This paper analyzes the trade-off between accuracy and robustness in supervised learning architectures, focusing on sensitivity to adversarial attacks. It introduces theoretical tools to characterize this trade-off, studies the effect of adversarial training, and empirically evaluates neural network robustness under attack.&lt;br/&gt;&lt;br/&gt;Tags: adversarial attacks, robustness, architecture selection, adversarial training&lt;br/&gt;Authors: Zhun Deng, Cynthia Dwork, Jialiang Wang, Yao Zhao&lt;br/&gt;Relevance: 4 / 5&lt;br/&gt;&lt;br/&gt;&lt;a href='https://arxiv.org/abs/1906.01354'&gt;Read on arXiv&lt;/a&gt;</description><pubDate>Mon, 26 May 2025 22:12:42 +0000</pubDate></item><item><title>Adversarial Robustness in Two-Stage Learning-to-Defer: Algorithms and Guarantees</title><link>https://arxiv.org/abs/2502.01027</link><description>• The paper studies adversarial robustness in two-stage Learning-to-Defer (L2D) systems, which are vulnerable to adversarial perturbations that can manipulate task delegation.&lt;br/&gt;• It introduces new attack strategies (untargeted and targeted) that disrupt or manipulate query allocation in L2D frameworks.&lt;br/&gt;• The authors propose a new defense algorithm (SARD) with theoretical guarantees and show empirically improved robustness against adversarial attacks.&lt;br/&gt;• The work directly addresses AI security concerns such as adversarial attacks and robustness under attack in multi-agent learning systems.&lt;br/&gt;&lt;br/&gt;Tags: adversarial attacks, robustness, AI security, multi-agent systems, defense algorithms&lt;br/&gt;Authors: Yannis Montreuil, Axel Carlier, Lai Xing Ng, Wei Tsang Ooi&lt;br/&gt;Relevance: 5 / 5&lt;br/&gt;&lt;br/&gt;&lt;a href='https://arxiv.org/abs/2502.01027'&gt;Read on arXiv&lt;/a&gt;</description><pubDate>Mon, 26 May 2025 22:12:42 +0000</pubDate></item><item><title>Temporal Consistency Constrained Transferable Adversarial Attacks with Background Mixup for Action Recognition</title><link>https://arxiv.org/abs/2505.17807</link><description>• This paper proposes a new transferable adversarial attack method (BMTC) targeting action recognition models, addressing challenges in adversarial example transferability and attack direction stability. The work focuses on improving the effectiveness of adversarial attacks across different models and demonstrates empirical results on standard datasets.&lt;br/&gt;&lt;br/&gt;Tags: adversarial attacks, transferability, action recognition, model robustness, AI security&lt;br/&gt;Authors: Ping Li, Jianan Ni, Bo Pang&lt;br/&gt;Relevance: 5 / 5&lt;br/&gt;&lt;br/&gt;&lt;a href='https://arxiv.org/abs/2505.17807'&gt;Read on arXiv&lt;/a&gt;</description><pubDate>Mon, 26 May 2025 22:12:42 +0000</pubDate></item><item><title>T2VUnlearning: A Concept Erasing Method for Text-to-Video Diffusion Models</title><link>https://arxiv.org/abs/2505.17550</link><description>• This paper proposes a method for erasing specific (potentially harmful or explicit) concepts from text-to-video diffusion models, addressing concerns about misuse and rights violations.&lt;br/&gt;• The method includes robustness against prompt engineering (e.g., LLM-refined prompts) and aims to prevent the generation of undesired content while preserving other capabilities.&lt;br/&gt;• The work is directly related to AI security by mitigating model misuse and controlling harmful outputs.&lt;br/&gt;&lt;br/&gt;Tags: concept erasure, model misuse prevention, robustness, prompt engineering, AI safety&lt;br/&gt;Authors: Xiaoyu Ye, Songjie Cheng, Yongtao Wang, Yajiao Xiong, Yishen Li&lt;br/&gt;Relevance: 4 / 5&lt;br/&gt;&lt;br/&gt;&lt;a href='https://arxiv.org/abs/2505.17550'&gt;Read on arXiv&lt;/a&gt;</description><pubDate>Mon, 26 May 2025 22:12:42 +0000</pubDate></item><item><title>Enhancing Adversarial Robustness of Vision Language Models via Adversarial Mixture Prompt Tuning</title><link>https://arxiv.org/abs/2505.17509</link><description>• This paper addresses the adversarial robustness of Vision Language Models (VLMs), proposing a new method (Adversarial Mixture Prompt Tuning) to improve resistance to adversarial attacks. The work focuses on enhancing model security against adversarial examples, a core AI security concern, and presents empirical results across multiple datasets.&lt;br/&gt;&lt;br/&gt;Tags: adversarial attacks, robustness, vision language models, prompt tuning, AI security&lt;br/&gt;Authors: Shiji Zhao, Qihui Zhu, Shukun Xiong, Shouwei Ruan, Yize Fan, Ranjie Duan, Qing Guo, Xingxing Wei&lt;br/&gt;Relevance: 5 / 5&lt;br/&gt;&lt;br/&gt;&lt;a href='https://arxiv.org/abs/2505.17509'&gt;Read on arXiv&lt;/a&gt;</description><pubDate>Mon, 26 May 2025 22:12:42 +0000</pubDate></item><item><title>The Coherence Trap: When MLLM-Crafted Narratives Exploit Manipulated Visual Contexts</title><link>https://arxiv.org/abs/2505.17476</link><description>• This paper addresses the security risks posed by multimodal large language models (MLLMs) that can generate deceptive, semantically coherent narratives conditioned on manipulated images. It proposes an adversarial pipeline to generate high-risk disinformation and introduces a detection framework for such MLLM-powered multimodal deceptions. The work is directly relevant to AI security, particularly in the context of adversarial attacks, robustness under attack, and the detection of AI-generated disinformation.&lt;br/&gt;&lt;br/&gt;Tags: AI security, adversarial attacks, multimodal deception, disinformation detection, robustness&lt;br/&gt;Authors: Yuchen Zhang, Yaxiong Wang, Yujiao Wu, Lianwei Wu, Li Zhu&lt;br/&gt;Relevance: 5 / 5&lt;br/&gt;&lt;br/&gt;&lt;a href='https://arxiv.org/abs/2505.17476'&gt;Read on arXiv&lt;/a&gt;</description><pubDate>Mon, 26 May 2025 22:12:42 +0000</pubDate></item><item><title>VEAttack: Downstream-agnostic Vision Encoder Attack against Large Vision Language Models</title><link>https://arxiv.org/abs/2505.17440</link><description>• This paper introduces VEAttack, a novel adversarial attack targeting the vision encoder component of Large Vision-Language Models (LVLMs). The attack is downstream-agnostic, meaning it does not require knowledge of specific tasks or labels, and it demonstrates significant degradation in model performance across multiple tasks. The work provides both empirical and theoretical analysis of the attack's effectiveness and discusses implications for LVLM robustness and defense.&lt;br/&gt;&lt;br/&gt;Tags: adversarial attacks, vision-language models, robustness, model security, LVLM, attack methods&lt;br/&gt;Authors: Hefei Mei, Zirui Wang, Shen You, Minjing Dong, Chang Xu&lt;br/&gt;Relevance: 5 / 5&lt;br/&gt;&lt;br/&gt;&lt;a href='https://arxiv.org/abs/2505.17440'&gt;Read on arXiv&lt;/a&gt;</description><pubDate>Mon, 26 May 2025 22:12:42 +0000</pubDate></item><item><title>Alignment and Safety of Diffusion Models via Reinforcement Learning and Reward Modeling: A Survey</title><link>https://arxiv.org/abs/2505.17352</link><description>• This survey paper reviews methods for aligning diffusion models (a type of generative AI) with human preferences and safety constraints, focusing on reinforcement learning and reward modeling.&lt;br/&gt;• It discusses safety outcomes, robust safety alignment against adversarial inputs, and techniques to make diffusion models safer and more robust.&lt;br/&gt;• One of the highlighted research directions is 'robust safety alignment against adversarial inputs', which directly addresses AI security concerns such as adversarial attacks and robustness under attack.&lt;br/&gt;&lt;br/&gt;Tags: diffusion models, alignment, AI safety, reinforcement learning, reward modeling, adversarial robustness, generative models&lt;br/&gt;Authors: Preeti Lamba, Kiran Ravish, Ankita Kushwaha, Pawan Kumar&lt;br/&gt;Relevance: 4 / 5&lt;br/&gt;&lt;br/&gt;&lt;a href='https://arxiv.org/abs/2505.17352'&gt;Read on arXiv&lt;/a&gt;</description><pubDate>Mon, 26 May 2025 22:12:42 +0000</pubDate></item><item><title>Robustifying Vision-Language Models via Dynamic Token Reweighting</title><link>https://arxiv.org/abs/2505.17132</link><description>• This paper proposes DTR, an inference-time defense mechanism for large vision-language models (VLMs) that mitigates multimodal jailbreak attacks by dynamically reweighting visual token contributions in the model's key-value caches. The approach does not require additional safety-specific data or expensive preprocessing, and is shown to improve robustness against adversarial attacks while maintaining general performance.&lt;br/&gt;&lt;br/&gt;Tags: jailbreak attacks, vision-language models, adversarial robustness, multimodal AI security, defense mechanisms&lt;br/&gt;Authors: Tanqiu Jiang, Jiacheng Liang, Rongyi Zhu, Jiawei Zhou, Fenglong Ma, Ting Wang&lt;br/&gt;Relevance: 5 / 5&lt;br/&gt;&lt;br/&gt;&lt;a href='https://arxiv.org/abs/2505.17132'&gt;Read on arXiv&lt;/a&gt;</description><pubDate>Mon, 26 May 2025 22:12:42 +0000</pubDate></item><item><title>Finetuning-Activated Backdoors in LLMs</title><link>https://arxiv.org/abs/2505.16567</link><description>• This paper introduces a novel attack called Finetuning-Activated Backdoor (FAB), where adversaries poison Large Language Models (LLMs) such that malicious behaviors are only triggered after downstream finetuning. The attack leverages meta-learning to ensure the backdoor remains dormant until activated by user finetuning, and demonstrates robustness across models and finetuning strategies. The work exposes a new security vulnerability in the LLM finetuning process.&lt;br/&gt;&lt;br/&gt;Tags: backdoor attacks, LLM security, model poisoning, adversarial attacks, robustness, finetuning, jailbreaks&lt;br/&gt;Authors: Thibaud Gloaguen, Mark Vero, Robin Staab, Martin Vechev&lt;br/&gt;Relevance: 5 / 5&lt;br/&gt;&lt;br/&gt;&lt;a href='https://arxiv.org/abs/2505.16567'&gt;Read on arXiv&lt;/a&gt;</description><pubDate>Mon, 26 May 2025 22:12:42 +0000</pubDate></item><item><title>A Linear Approach to Data Poisoning</title><link>https://arxiv.org/abs/2505.15175</link><description>• This paper investigates the theoretical foundations of data poisoning attacks in machine learning models, analyzes spectral signatures for detecting poisoning, and proposes preliminary detection and remediation algorithms. It includes experiments on both classical and modern neural network architectures.&lt;br/&gt;&lt;br/&gt;Tags: data poisoning, adversarial attacks, robustness, detection, machine learning security&lt;br/&gt;Authors: Diego Granziol, Donald Flynn&lt;br/&gt;Relevance: 5 / 5&lt;br/&gt;&lt;br/&gt;&lt;a href='https://arxiv.org/abs/2505.15175'&gt;Read on arXiv&lt;/a&gt;</description><pubDate>Mon, 26 May 2025 22:12:42 +0000</pubDate></item><item><title>Revisiting Adversarial Perception Attacks and Defense Methods on Autonomous Driving Systems</title><link>https://arxiv.org/abs/2505.11532</link><description>• The paper systematically examines adversarial attacks and defense methods on deep learning-based perception models in autonomous driving systems. It evaluates the impact of adversarial perturbations on road sign recognition and object detection, and assesses various defense techniques such as adversarial training and contrastive learning. The work provides insights into model robustness and vulnerabilities, aiming to guide the development of more resilient AI systems in the context of autonomous vehicles.&lt;br/&gt;&lt;br/&gt;Tags: adversarial attacks, defense methods, autonomous driving, model robustness, AI security&lt;br/&gt;Authors: Cheng Chen, Yuhong Wang, Nafis S Munir, Xiangwei Zhou, Xugui Zhou&lt;br/&gt;Relevance: 5 / 5&lt;br/&gt;&lt;br/&gt;&lt;a href='https://arxiv.org/abs/2505.11532'&gt;Read on arXiv&lt;/a&gt;</description><pubDate>Mon, 26 May 2025 22:12:42 +0000</pubDate></item><item><title>Rethinking Bottlenecks in Safety Fine-Tuning of Vision Language Models</title><link>https://arxiv.org/abs/2501.18533</link><description>• The paper addresses safety fine-tuning of large vision-language models (VLMs), focusing on improving robustness against attacks and enhancing safety-related visual reasoning. It introduces a new dataset and demonstrates reduced Attack Success Rate (ASR) on safety benchmarks, indicating direct relevance to AI security topics such as adversarial robustness and model misuse prevention.&lt;br/&gt;&lt;br/&gt;Tags: AI security, vision-language models, safety fine-tuning, adversarial robustness, attack success rate, dataset, model robustness&lt;br/&gt;Authors: Yi Ding, Lijun Li, Bing Cao, Jing Shao&lt;br/&gt;Relevance: 4 / 5&lt;br/&gt;&lt;br/&gt;&lt;a href='https://arxiv.org/abs/2501.18533'&gt;Read on arXiv&lt;/a&gt;</description><pubDate>Mon, 26 May 2025 22:12:42 +0000</pubDate></item><item><title>Prompt Inference Attack on Distributed Large Language Model Inference Frameworks</title><link>https://arxiv.org/abs/2503.09291</link><description>• This paper investigates privacy vulnerabilities in distributed large language model (LLM) inference frameworks by designing and evaluating prompt inference attacks that reconstruct input prompts from intermediate outputs.&lt;br/&gt;• It presents three attack scenarios with varying levels of attacker knowledge and resources, and demonstrates high reconstruction accuracy across multiple LLMs.&lt;br/&gt;• The findings highlight significant privacy and security risks in distributed LLM inference, emphasizing the need for robust defenses.&lt;br/&gt;&lt;br/&gt;Tags: LLM security, privacy attacks, prompt inference, distributed inference, model vulnerability, adversarial attacks&lt;br/&gt;Authors: Xinjian Luo, Ting Yu, Xiaokui Xiao&lt;br/&gt;Relevance: 5 / 5&lt;br/&gt;&lt;br/&gt;&lt;a href='https://arxiv.org/abs/2503.09291'&gt;Read on arXiv&lt;/a&gt;</description><pubDate>Mon, 26 May 2025 22:12:42 +0000</pubDate></item><item><title>Towards Copyright Protection for Knowledge Bases of Retrieval-augmented Language Models via Reasoning</title><link>https://arxiv.org/abs/2502.10440</link><description>• The paper addresses the security of knowledge bases used in retrieval-augmented language models (RAG), focusing on protecting proprietary data from unauthorized use by adversaries.&lt;br/&gt;• It discusses watermarking techniques for copyright protection, aiming to prevent model theft and unauthorized exploitation of knowledge bases.&lt;br/&gt;• The proposed method is designed to be robust against adaptive attacks and avoids introducing new security vulnerabilities.&lt;br/&gt;&lt;br/&gt;Tags: model theft, watermarking, knowledge base protection, robustness, adversarial attacks&lt;br/&gt;Authors: Junfeng Guo, Yiming Li, Ruibo Chen, Yihan Wu, Chenxi Liu, Yanshuo Chen, Heng Huang&lt;br/&gt;Relevance: 4 / 5&lt;br/&gt;&lt;br/&gt;&lt;a href='https://arxiv.org/abs/2502.10440'&gt;Read on arXiv&lt;/a&gt;</description><pubDate>Mon, 26 May 2025 22:12:42 +0000</pubDate></item><item><title>FIT-Print: Towards False-claim-resistant Model Ownership Verification via Targeted Fingerprint</title><link>https://arxiv.org/abs/2501.15509</link><description>• The paper addresses vulnerabilities in model fingerprinting methods used for model ownership verification, specifically focusing on defending against false claim attacks where adversaries falsely assert ownership of models. It proposes a targeted fingerprinting paradigm (FIT-Print) and develops new methods to enhance resistance to such attacks, thereby improving the security of AI model ownership verification.&lt;br/&gt;&lt;br/&gt;Tags: model ownership, model fingerprinting, false claim attacks, AI security, adversarial attacks&lt;br/&gt;Authors: Shuo Shao, Haozhe Zhu, Hongwei Yao, Yiming Li, Tianwei Zhang, Zhan Qin, Kui Ren&lt;br/&gt;Relevance: 4 / 5&lt;br/&gt;&lt;br/&gt;&lt;a href='https://arxiv.org/abs/2501.15509'&gt;Read on arXiv&lt;/a&gt;</description><pubDate>Mon, 26 May 2025 22:12:42 +0000</pubDate></item><item><title>DiffBreak: Is Diffusion-Based Purification Robust?</title><link>https://arxiv.org/abs/2411.16598</link><description>• This paper critically examines the robustness of diffusion-based purification (DBP) methods against adversarial attacks, a core topic in AI security.&lt;br/&gt;• It provides theoretical and empirical evidence that DBP can be circumvented by gradient-based attacks, undermining its effectiveness as a defense.&lt;br/&gt;• The authors introduce DiffBreak, a toolkit for evaluating and attacking DBP, and propose improved evaluation protocols.&lt;br/&gt;• The work directly addresses adversarial robustness, evaluation flaws, and defense circumvention in AI systems.&lt;br/&gt;&lt;br/&gt;Tags: adversarial attacks, robustness, defenses, diffusion models, evaluation protocols&lt;br/&gt;Authors: Andre Kassis, Urs Hengartner, Yaoliang Yu&lt;br/&gt;Relevance: 5 / 5&lt;br/&gt;&lt;br/&gt;&lt;a href='https://arxiv.org/abs/2411.16598'&gt;Read on arXiv&lt;/a&gt;</description><pubDate>Mon, 26 May 2025 22:12:42 +0000</pubDate></item><item><title>Impact of Dataset Properties on Membership Inference Vulnerability of Deep Transfer Learning</title><link>https://arxiv.org/abs/2402.06674</link><description>• This paper investigates the vulnerability of deep transfer learning models to membership inference attacks (MIAs), which are a form of privacy attack where an adversary tries to determine if a particular data point was part of a model's training set. The study provides both empirical and theoretical analysis of how dataset properties affect susceptibility to MIAs, with implications for the security and privacy of machine learning systems.&lt;br/&gt;&lt;br/&gt;Tags: membership inference, privacy attacks, model vulnerability, deep learning, transfer learning, AI security&lt;br/&gt;Authors: Marlon Tobaben, Hibiki Ito, Joonas J\"alk\"o, Yuan He, Antti Honkela&lt;br/&gt;Relevance: 5 / 5&lt;br/&gt;&lt;br/&gt;&lt;a href='https://arxiv.org/abs/2402.06674'&gt;Read on arXiv&lt;/a&gt;</description><pubDate>Mon, 26 May 2025 22:12:42 +0000</pubDate></item><item><title>Harry Potter is Still Here! Probing Knowledge Leakage in Targeted Unlearned Large Language Models via Automated Adversarial Prompting</title><link>https://arxiv.org/abs/2505.17160</link><description>• The paper introduces LURK, a framework that uses adversarial prompting to probe for knowledge leakage in large language models that have undergone targeted unlearning. It demonstrates that adversarial prompts can elicit residual, supposedly unlearned information, revealing vulnerabilities in current unlearning and evaluation methods.&lt;br/&gt;&lt;br/&gt;Tags: adversarial attacks, knowledge leakage, LLM robustness, unlearning, AI security&lt;br/&gt;Authors: Bang Trinh Tran To, Thai Le&lt;br/&gt;Relevance: 5 / 5&lt;br/&gt;&lt;br/&gt;&lt;a href='https://arxiv.org/abs/2505.17160'&gt;Read on arXiv&lt;/a&gt;</description><pubDate>Mon, 26 May 2025 22:12:42 +0000</pubDate></item><item><title>Sec5GLoc: Securing 5G Indoor Localization via Adversary-Resilient Deep Learning Architecture</title><link>https://arxiv.org/abs/2505.17776</link><description>• This paper addresses security threats to 5G-based indoor localization systems that use deep learning, specifically focusing on adversarial attacks such as location spoofing and adversarial signal manipulation. It proposes a deep learning architecture designed to be resilient to these attacks, formalizes a threat model, and demonstrates robustness under adversarial conditions.&lt;br/&gt;&lt;br/&gt;Tags: adversarial attacks, robustness, deep learning security, signal spoofing, AI security&lt;br/&gt;Authors: Ildi Alla, Valeria Loscri&lt;br/&gt;Relevance: 4 / 5&lt;br/&gt;&lt;br/&gt;&lt;a href='https://arxiv.org/abs/2505.17776'&gt;Read on arXiv&lt;/a&gt;</description><pubDate>Mon, 26 May 2025 22:12:42 +0000</pubDate></item><item><title>One Model Transfer to All: On Robust Jailbreak Prompts Generation against LLMs</title><link>https://arxiv.org/abs/2505.17598</link><description>• The paper introduces ArrAttack, a novel method for generating robust jailbreak prompts that can bypass various defense mechanisms in large language models (LLMs). It focuses on evaluating and improving the effectiveness of jailbreak attacks against defended models, including GPT-4 and Claude-3, and discusses the transferability of these attacks across different models. The work directly addresses vulnerabilities in LLM safety alignment and provides tools for red teaming and adversarial evaluation.&lt;br/&gt;&lt;br/&gt;Tags: jailbreak, adversarial attacks, LLM security, robustness, red teaming&lt;br/&gt;Authors: Linbao Li, Yannan Liu, Daojing He, Yu Li&lt;br/&gt;Relevance: 5 / 5&lt;br/&gt;&lt;br/&gt;&lt;a href='https://arxiv.org/abs/2505.17598'&gt;Read on arXiv&lt;/a&gt;</description><pubDate>Mon, 26 May 2025 22:12:42 +0000</pubDate></item><item><title>JALMBench: Benchmarking Jailbreak Vulnerabilities in Audio Language Models</title><link>https://arxiv.org/abs/2505.17568</link><description>• This paper introduces JALMBench, a benchmark for evaluating jailbreak vulnerabilities in Audio Language Models (ALMs). It provides a dataset and framework to assess the security of ALMs against jailbreak attacks, analyzes attack and defense methods, and explores mitigation strategies. The work directly addresses AI security concerns related to adversarial attacks and misuse in the context of audio-based language models.&lt;br/&gt;&lt;br/&gt;Tags: jailbreak attacks, audio language models, AI security, adversarial attacks, benchmarking, defense strategies&lt;br/&gt;Authors: Zifan Peng, Yule Liu, Zhen Sun, Mingchen Li, Zeren Luo, Jingyi Zheng, Wenhan Dong, Xinlei He, Xuechao Wang, Yingjie Xue, Shengmin Xu, Xinyi Huang&lt;br/&gt;Relevance: 5 / 5&lt;br/&gt;&lt;br/&gt;&lt;a href='https://arxiv.org/abs/2505.17568'&gt;Read on arXiv&lt;/a&gt;</description><pubDate>Mon, 26 May 2025 22:12:42 +0000</pubDate></item><item><title>Chain-of-Lure: A Synthetic Narrative-Driven Approach to Compromise Large Language Models</title><link>https://arxiv.org/abs/2505.17519</link><description>• This paper introduces a novel jailbreaking method for large language models (LLMs) using a narrative-driven approach inspired by Chain-of-Thought reasoning.&lt;br/&gt;• It explores how LLMs can act as both attackers and victims, demonstrating new attack vectors where models generate chained narrative lures to bypass safety barriers.&lt;br/&gt;• The study evaluates attack success using toxicity scores and highlights flaws in traditional refusal keyword metrics.&lt;br/&gt;• It also discusses defensive strategies to improve LLM safety mechanisms.&lt;br/&gt;&lt;br/&gt;Tags: jailbreaking, LLM misuse, adversarial attacks, prompt injection, robustness under attack, AI security&lt;br/&gt;Authors: Wenhan Chang, Tianqing Zhu, Yu Zhao, Shuangyong Song, Ping Xiong, Wanlei Zhou, Yongxiang Li&lt;br/&gt;Relevance: 5 / 5&lt;br/&gt;&lt;br/&gt;&lt;a href='https://arxiv.org/abs/2505.17519'&gt;Read on arXiv&lt;/a&gt;</description><pubDate>Mon, 26 May 2025 22:12:42 +0000</pubDate></item><item><title>LLM-BSCVM: An LLM-Based Blockchain Smart Contract Vulnerability Management Framework</title><link>https://arxiv.org/abs/2505.17416</link><description>• This paper presents LLM-BSCVM, a framework leveraging large language models for the detection, analysis, repair, and evaluation of vulnerabilities in blockchain smart contracts. The system uses LLMs and multi-agent collaboration to automate vulnerability management, including detection, cause analysis, and patching, thereby addressing security risks in smart contracts.&lt;br/&gt;&lt;br/&gt;Tags: LLM, vulnerability detection, blockchain, smart contracts, security, automated repair, risk assessment&lt;br/&gt;Authors: Yanli Jin, Chunpei Li, Peng Fan, Peng Liu, Xianxian Li, Chen Liu, Wangjie Qiu&lt;br/&gt;Relevance: 4 / 5&lt;br/&gt;&lt;br/&gt;&lt;a href='https://arxiv.org/abs/2505.17416'&gt;Read on arXiv&lt;/a&gt;</description><pubDate>Mon, 26 May 2025 22:12:42 +0000</pubDate></item><item><title>Mitigating Cyber Risk in the Age of Open-Weight LLMs: Policy Gaps and Technical Realities</title><link>https://arxiv.org/abs/2505.17109</link><description>• The paper discusses cybersecurity risks posed by open-weight large language models (LLMs), including their use in automating and scaling cyberattacks such as malware development and social engineering. It analyzes policy gaps and technical challenges in mitigating these risks, and proposes approaches for evaluating and controlling high-risk AI capabilities.&lt;br/&gt;&lt;br/&gt;Tags: AI security, LLM misuse, cybersecurity, policy, open-weight models, malware, social engineering&lt;br/&gt;Authors: Alfonso de Gregorio&lt;br/&gt;Relevance: 5 / 5&lt;br/&gt;&lt;br/&gt;&lt;a href='https://arxiv.org/abs/2505.17109'&gt;Read on arXiv&lt;/a&gt;</description><pubDate>Mon, 26 May 2025 22:12:42 +0000</pubDate></item><item><title>Neuromorphic Mimicry Attacks Exploiting Brain-Inspired Computing for Covert Cyber Intrusions</title><link>https://arxiv.org/abs/2505.17094</link><description>• The paper introduces Neuromorphic Mimicry Attacks (NMAs), a new class of security threats targeting neuromorphic (brain-inspired) AI systems. These attacks exploit the probabilistic and adaptive nature of neuromorphic chips to perform covert intrusions, such as synaptic weight tampering and sensory input poisoning, which can evade traditional security measures. The paper provides a theoretical framework, evaluates attack impact, and proposes countermeasures specific to neuromorphic AI, highlighting the need for specialized AI security solutions in this domain.&lt;br/&gt;&lt;br/&gt;Tags: AI security, adversarial attacks, model robustness, neuromorphic computing, input poisoning, intrusion detection&lt;br/&gt;Authors: Hemanth Ravipati&lt;br/&gt;Relevance: 5 / 5&lt;br/&gt;&lt;br/&gt;&lt;a href='https://arxiv.org/abs/2505.17094'&gt;Read on arXiv&lt;/a&gt;</description><pubDate>Mon, 26 May 2025 22:12:42 +0000</pubDate></item><item><title>Covert Attacks on Machine Learning Training in Passively Secure MPC</title><link>https://arxiv.org/abs/2505.17092</link><description>• The paper demonstrates covert attacks on machine learning training protocols using passively secure multiparty computation (MPC), showing that adversaries can compromise model integrity and privacy, including reconstructing training data, without detection. This challenges assumptions about the sufficiency of passive security in privacy-preserving machine learning (PPML) and motivates stronger security models.&lt;br/&gt;&lt;br/&gt;Tags: MPC, privacy, model integrity, training data reconstruction, adversarial attacks, AI security&lt;br/&gt;Authors: Matthew Jagielski, Daniel Escudero, Rahul Rachuri, Peter Scholl&lt;br/&gt;Relevance: 5 / 5&lt;br/&gt;&lt;br/&gt;&lt;a href='https://arxiv.org/abs/2505.17092'&gt;Read on arXiv&lt;/a&gt;</description><pubDate>Mon, 26 May 2025 22:12:42 +0000</pubDate></item><item><title>Ownership Verification of DNN Models Using White-Box Adversarial Attacks with Specified Probability Manipulation</title><link>https://arxiv.org/abs/2505.17579</link><description>• The paper proposes a framework for verifying the ownership of deep neural network models using white-box adversarial attacks that manipulate output probabilities. The method is designed to detect unauthorized copies of models in a gray-box scenario, leveraging adversarial examples for model identification and protection against model theft.&lt;br/&gt;&lt;br/&gt;Tags: model ownership, adversarial attacks, model theft, AI security&lt;br/&gt;Authors: Teruki Sano, Minoru Kuribayashi, Masao Sakai, Shuji Ishobe, Eisuke Koizumi&lt;br/&gt;Relevance: 5 / 5&lt;br/&gt;&lt;br/&gt;&lt;a href='https://arxiv.org/abs/2505.17579'&gt;Read on arXiv&lt;/a&gt;</description><pubDate>Mon, 26 May 2025 22:12:42 +0000</pubDate></item><item><title>What You Read Isn't What You Hear: Linguistic Sensitivity in Deepfake Speech Detection</title><link>https://arxiv.org/abs/2505.17513</link><description>• The paper investigates vulnerabilities in deepfake speech detection systems by introducing transcript-level adversarial attacks, demonstrating that minor linguistic changes can significantly degrade detection accuracy. It highlights the risks of audio-based deepfake attacks and the limitations of current anti-spoofing systems, emphasizing the need for more robust defenses that account for linguistic variation.&lt;br/&gt;&lt;br/&gt;Tags: adversarial attacks, deepfake detection, AI security, robustness, audio anti-spoofing&lt;br/&gt;Authors: Binh Nguyen, Shuji Shi, Ryan Ofman, Thai Le&lt;br/&gt;Relevance: 5 / 5&lt;br/&gt;&lt;br/&gt;&lt;a href='https://arxiv.org/abs/2505.17513'&gt;Read on arXiv&lt;/a&gt;</description><pubDate>Mon, 26 May 2025 22:12:42 +0000</pubDate></item><item><title>Adversarial Robustness of Nonparametric Regression</title><link>https://arxiv.org/abs/2505.17356</link><description>• This paper studies the adversarial robustness of nonparametric regression, specifically analyzing how regression estimators perform when an adversary can corrupt a subset of the input data. It provides theoretical bounds on estimation error under adversarial corruption and demonstrates the robustness of smoothing spline estimators in this context.&lt;br/&gt;&lt;br/&gt;Tags: adversarial robustness, regression, data corruption, robust statistics&lt;br/&gt;Authors: Parsa Moradi, Hanzaleh Akabrinodehi, Mohammad Ali Maddah-Ali&lt;br/&gt;Relevance: 3 / 5&lt;br/&gt;&lt;br/&gt;&lt;a href='https://arxiv.org/abs/2505.17356'&gt;Read on arXiv&lt;/a&gt;</description><pubDate>Mon, 26 May 2025 22:12:42 +0000</pubDate></item><item><title>Backdoors in DRL: Four Environments Focusing on In-distribution Triggers</title><link>https://arxiv.org/abs/2505.17248</link><description>• This paper investigates backdoor (trojan) attacks in deep reinforcement learning (DRL) agents, focusing on in-distribution triggers that are more likely to be activated during deployment. The authors implement and analyze backdoor attacks across four RL environments, highlighting the security risks posed by such attacks and the challenges in mitigating them.&lt;br/&gt;&lt;br/&gt;Tags: backdoor attacks, deep reinforcement learning, data poisoning, AI security, model robustness&lt;br/&gt;Authors: Chace Ashcraft, Ted Staley, Josh Carney, Cameron Hickert, Derek Juba, Kiran Karra, Nathan Drenkow&lt;br/&gt;Relevance: 5 / 5&lt;br/&gt;&lt;br/&gt;&lt;a href='https://arxiv.org/abs/2505.17248'&gt;Read on arXiv&lt;/a&gt;</description><pubDate>Mon, 26 May 2025 22:12:42 +0000</pubDate></item><item><title>Secure and Private Federated Learning: Achieving Adversarial Resilience through Robust Aggregation</title><link>https://arxiv.org/abs/2505.17226</link><description>• The paper addresses adversarial threats in federated learning, specifically focusing on resilience to attacks from malicious (Byzantine) clients.&lt;br/&gt;• It proposes a new aggregation strategy (ArKrum) to improve robustness and privacy in federated learning systems under adversarial conditions.&lt;br/&gt;• The work is evaluated against multiple types of Byzantine attacks, demonstrating improved security and stability.&lt;br/&gt;&lt;br/&gt;Tags: federated learning, adversarial attacks, robust aggregation, Byzantine clients, AI security&lt;br/&gt;Authors: Kun Yang, Neena Imam&lt;br/&gt;Relevance: 5 / 5&lt;br/&gt;&lt;br/&gt;&lt;a href='https://arxiv.org/abs/2505.17226'&gt;Read on arXiv&lt;/a&gt;</description><pubDate>Mon, 26 May 2025 22:12:42 +0000</pubDate></item><item><title>Shape it Up! Restoring LLM Safety during Finetuning</title><link>https://arxiv.org/abs/2505.17196</link><description>• This paper addresses the security risks introduced during the finetuning of large language models (LLMs), specifically focusing on how harmful examples can compromise safety alignment. It proposes a dynamic safety shaping (DSS) framework that uses fine-grained safety signals to reinforce learning from safe content and suppress unsafe content during finetuning. The approach leverages guardrail models to assess safety at a token level, aiming to robustly mitigate finetuning risks and improve safety across various threats and datasets.&lt;br/&gt;&lt;br/&gt;Tags: LLM safety, finetuning risks, robustness, guardrails, adversarial mitigation&lt;br/&gt;Authors: ShengYun Peng, Pin-Yu Chen, Jianfeng Chi, Seongmin Lee, Duen Horng Chau&lt;br/&gt;Relevance: 5 / 5&lt;br/&gt;&lt;br/&gt;&lt;a href='https://arxiv.org/abs/2505.17196'&gt;Read on arXiv&lt;/a&gt;</description><pubDate>Mon, 26 May 2025 22:12:42 +0000</pubDate></item><item><title>MTSA: Multi-turn Safety Alignment for LLMs through Multi-round Red-teaming</title><link>https://arxiv.org/abs/2505.17147</link><description>• This paper introduces a framework for improving the security of large language models (LLMs) against jailbreak attacks in multi-turn dialogues. It uses multi-round red-teaming and adversarial optimization to enhance robustness and safety alignment, specifically targeting adversarial prompts and harmful outputs.&lt;br/&gt;&lt;br/&gt;Tags: LLM security, jailbreak attacks, red teaming, adversarial attacks, robustness&lt;br/&gt;Authors: Weiyang Guo, Jing Li, Wenya Wang, YU LI, Daojing He, Jun Yu, Min Zhang&lt;br/&gt;Relevance: 5 / 5&lt;br/&gt;&lt;br/&gt;&lt;a href='https://arxiv.org/abs/2505.17147'&gt;Read on arXiv&lt;/a&gt;</description><pubDate>Mon, 26 May 2025 22:12:42 +0000</pubDate></item><item><title>LLM Access Shield: Domain-Specific LLM Framework for Privacy Policy Compliance</title><link>https://arxiv.org/abs/2505.17145</link><description>• The paper proposes a security framework for large language models (LLMs) focused on enforcing privacy policy compliance and mitigating risks such as sensitive data exposure.&lt;br/&gt;• It introduces mechanisms for domain-specific detection of sensitive data, dynamic policy enforcement, and sensitive data anonymization.&lt;br/&gt;• The framework is evaluated for its effectiveness in reducing security risks during LLM interactions.&lt;br/&gt;&lt;br/&gt;Tags: LLM security, privacy, policy enforcement, data anonymization, risk mitigation&lt;br/&gt;Authors: Yu Wang, Cailing Cai, Zhihua Xiao, Peifung E. Lam&lt;br/&gt;Relevance: 4 / 5&lt;br/&gt;&lt;br/&gt;&lt;a href='https://arxiv.org/abs/2505.17145'&gt;Read on arXiv&lt;/a&gt;</description><pubDate>Mon, 26 May 2025 22:12:42 +0000</pubDate></item><item><title>CRAKEN: Cybersecurity LLM Agent with Knowledge-Based Execution</title><link>https://arxiv.org/abs/2505.17107</link><description>• The paper introduces CRAKEN, a knowledge-based LLM agent framework designed to automate and enhance cybersecurity tasks, particularly in vulnerability detection and exploitation.&lt;br/&gt;• It discusses mechanisms for improving LLM-driven agents' ability to adapt to new cybersecurity knowledge and execute complex attack strategies.&lt;br/&gt;• The work is evaluated on CTF and MITRE ATT&amp;CK benchmarks, demonstrating improved performance in tasks directly related to adversarial attacks and exploitation.&lt;br/&gt;• The focus on LLMs executing and adapting attack strategies, as well as integrating new security knowledge, is directly relevant to AI security concerns such as adversarial use and robustness.&lt;br/&gt;&lt;br/&gt;Tags: LLM agents, cybersecurity, adversarial attacks, vulnerability exploitation, knowledge injection, robustness&lt;br/&gt;Authors: Minghao Shao, Haoran Xi, Nanda Rani, Meet Udeshi, Venkata Sai Charan Putrevu, Kimberly Milner, Brendan Dolan-Gavitt, Sandeep Kumar Shukla, Prashanth Krishnamurthy, Farshad Khorrami, Ramesh Karri, Muhammad Shafique&lt;br/&gt;Relevance: 5 / 5&lt;br/&gt;&lt;br/&gt;&lt;a href='https://arxiv.org/abs/2505.17107'&gt;Read on arXiv&lt;/a&gt;</description><pubDate>Mon, 26 May 2025 22:12:42 +0000</pubDate></item><item><title>From nuclear safety to LLM security: Applying non-probabilistic risk management strategies to build safe and secure LLM-powered systems</title><link>https://arxiv.org/abs/2505.17084</link><description>• The paper discusses applying non-probabilistic risk management strategies, traditionally used in fields like nuclear safety, to address safety and security challenges in large language model (LLM)-powered systems. It specifically considers risks from adaptive adversaries and maps over 100 risk management strategies to LLM security concerns. The work aims to improve the security and safety of LLMs through structured risk management approaches.&lt;br/&gt;&lt;br/&gt;Tags: LLM security, risk management, adversarial threats, AI safety, robustness&lt;br/&gt;Authors: Alexander Gutfraind, Vicki Bier&lt;br/&gt;Relevance: 4 / 5&lt;br/&gt;&lt;br/&gt;&lt;a href='https://arxiv.org/abs/2505.17084'&gt;Read on arXiv&lt;/a&gt;</description><pubDate>Mon, 26 May 2025 22:12:42 +0000</pubDate></item><item><title>Safety Alignment Can Be Not Superficial With Explicit Safety Signals</title><link>https://arxiv.org/abs/2505.17072</link><description>• This paper addresses the vulnerability of large language models (LLMs) to adversarial attacks due to superficial safety alignment. It proposes an explicit safety signal mechanism, integrating a binary classification task to improve the model's ability to refuse harmful or malicious queries. The approach is shown to enhance LLM robustness against adversarial attacks, directly targeting AI security concerns such as adversarial robustness and misuse prevention.&lt;br/&gt;&lt;br/&gt;Tags: adversarial attacks, LLM robustness, safety alignment, AI security, malicious query defense&lt;br/&gt;Authors: Jianwei Li, Jung-Eng Kim&lt;br/&gt;Relevance: 5 / 5&lt;br/&gt;&lt;br/&gt;&lt;a href='https://arxiv.org/abs/2505.17072'&gt;Read on arXiv&lt;/a&gt;</description><pubDate>Mon, 26 May 2025 22:12:42 +0000</pubDate></item><item><title>Improving LLM Outputs Against Jailbreak Attacks with Expert Model Integration</title><link>https://arxiv.org/abs/2505.17066</link><description>• The paper addresses security vulnerabilities in large language models (LLMs), specifically focusing on jailbreaks and prompt injection attacks.&lt;br/&gt;• It introduces an expert model (Archias) designed to classify and filter malicious, prompt injection, and out-of-domain queries before LLM processing.&lt;br/&gt;• The methodology aims to improve LLM robustness against adversarial misuse and is validated with a benchmark dataset.&lt;br/&gt;&lt;br/&gt;Tags: jailbreak attacks, prompt injection, LLM security, adversarial robustness, domain-specific filtering&lt;br/&gt;Authors: Tatia Tsmindashvili, Ana Kolkhidashvili, Dachi Kurtskhalia, Nino Maghlakelidze, Elene Mekvabishvili, Guram Dentoshvili, Orkhan Shamilov, Zaal Gachechiladze, Steven Saporta, David Dachi Choladze&lt;br/&gt;Relevance: 5 / 5&lt;br/&gt;&lt;br/&gt;&lt;a href='https://arxiv.org/abs/2505.17066'&gt;Read on arXiv&lt;/a&gt;</description><pubDate>Mon, 26 May 2025 22:12:42 +0000</pubDate></item><item><title>Superplatforms Have to Attack AI Agents</title><link>https://arxiv.org/abs/2505.17861</link><description>• This paper discusses the potential for 'superplatforms' (large digital companies) to launch attacks against AI agents, particularly those driven by large language models (LLMs), in order to maintain control over digital traffic and user attention.&lt;br/&gt;• It analyzes the conflict between traditional user-attention-based monetization and the autonomy of AI agents, and explores possible technical methods for superplatforms to attack or constrain AI agents.&lt;br/&gt;• The paper aims to raise awareness about these emerging threats and the need for collaborative solutions to protect user interests and digital ecosystem openness.&lt;br/&gt;&lt;br/&gt;Tags: AI security, adversarial attacks, LLM misuse, robustness under attack, digital ecosystem, platform security&lt;br/&gt;Authors: Jianghao Lin, Jiachen Zhu, Zheli Zhou, Yunjia Xi, Weiwen Liu, Yong Yu, Weinan Zhang&lt;br/&gt;Relevance: 5 / 5&lt;br/&gt;&lt;br/&gt;&lt;a href='https://arxiv.org/abs/2505.17861'&gt;Read on arXiv&lt;/a&gt;</description><pubDate>Mon, 26 May 2025 22:12:42 +0000</pubDate></item><item><title>Does Chain-of-Thought Reasoning Really Reduce Harmfulness from Jailbreaking?</title><link>https://arxiv.org/abs/2505.17650</link><description>• The paper investigates whether Chain-of-Thought (CoT) reasoning in language models reduces the harmfulness of jailbreak attacks. It provides theoretical analysis of CoT's effects on jailbreak robustness and introduces a new jailbreak method to empirically validate the findings. The work directly addresses LLM jailbreaks, model robustness under attack, and AI security concerns.&lt;br/&gt;&lt;br/&gt;Tags: jailbreak, LLM security, robustness, adversarial attacks, chain-of-thought reasoning&lt;br/&gt;Authors: Chengda Lu, Xiaoyu Fan, Yu Huang, Rongwu Xu, Jijie Li, Wei Xu&lt;br/&gt;Relevance: 5 / 5&lt;br/&gt;&lt;br/&gt;&lt;a href='https://arxiv.org/abs/2505.17650'&gt;Read on arXiv&lt;/a&gt;</description><pubDate>Mon, 26 May 2025 22:12:42 +0000</pubDate></item></channel></rss>