<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>AI Security Paper Digest</title><link>https://kentaroh-toyoda.github.io/ai-security-paper-digest-rss/rss.xml</link><description>Curated papers on AI security from OpenAlex and ArXiv</description><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><lastBuildDate>Mon, 09 Jun 2025 22:36:25 +0000</lastBuildDate><item><title>Modality-Fair Preference Optimization for Trustworthy MLLM Alignment</title><link>https://arxiv.org/abs/2410.15334</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Identifies and addresses modality misalignment in multimodal large language models (MLLMs), which can lead to hallucinations and undermine trustworthiness.&lt;/li&gt;&lt;li&gt;Proposes Modality-Fair Preference Optimization (MFPO), including a new dataset, a reward loss function, and an iterative alignment strategy.&lt;/li&gt;&lt;li&gt;Demonstrates that MFPO improves the trustworthiness of MLLMs on multiple benchmarks, reducing hallucinations and improving alignment between visual and textual inputs.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research Paper&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Quality Assessment&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Clarity: 4/5&lt;/li&gt;&lt;li&gt;Novelty: 4/5&lt;/li&gt;&lt;li&gt;Significance: 4/5&lt;/li&gt;&lt;li&gt;Try-worthiness: Yes&lt;/li&gt;&lt;li&gt;Justification: The abstract is well-written and clearly outlines the motivation, problem, and proposed solution (MFPO) for improving trustworthiness in multimodal large language models (MLLMs). The identification of modality misalignment and hallucination, along with the introduction of a new preference optimization method and dataset, suggests a novel approach. The work is significant as it addresses a critical issue (trustworthiness) in MLLMs, and the reported improvements over larger models highlight its practical value. Although the paper is very recent and has no citations yet, its relevance and the thoroughness of the proposed method make it worth experimenting with. No code repository link is provided in the metadata.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: Songtao Jiang, Yan Zhang, Ruizhe Chen, Tianxiang Hu, Yeying Jin, Qinglin He, Yang Feng, Jian Wu, Zuozhu Liu&lt;/li&gt;&lt;li&gt;Tags: AI safety, trustworthiness, alignment, multimodal models, hallucination mitigation&lt;/li&gt;&lt;li&gt;Relevance Score: 4/5&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2410.15334</guid><pubDate>Mon, 09 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Robust Anti-Backdoor Instruction Tuning in LVLMs</title><link>https://arxiv.org/abs/2506.05401</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Addresses the vulnerability of Large Visual Language Models (LVLMs) to backdoor attacks during instruction tuning.&lt;/li&gt;&lt;li&gt;Proposes a defense framework that does not require access to core model weights or prior knowledge of attack triggers.&lt;/li&gt;&lt;li&gt;Introduces two regularization techniques to disrupt backdoor triggers and anomalous activations during adapter-level tuning.&lt;/li&gt;&lt;li&gt;Demonstrates strong empirical results, significantly reducing attack success rates with minimal training overhead.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research Paper&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Quality Assessment&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Clarity: 4/5&lt;/li&gt;&lt;li&gt;Novelty: 5/5&lt;/li&gt;&lt;li&gt;Significance: 4/5&lt;/li&gt;&lt;li&gt;Try-worthiness: Yes&lt;/li&gt;&lt;li&gt;Justification: The abstract is well-written and clearly outlines the problem, motivation, and proposed solution, though it is somewhat dense with technical terms. The work is highly novel, addressing the underexplored challenge of defending LVLMs against backdoor attacks without access to core model weights or attack priors, and introducing two new regularization techniques. The significance is high given the growing deployment of LVLMs and the practical constraints addressed, though as a recent arXiv preprint, it has not yet been peer-reviewed or cited. The strong experimental results and practical relevance make it worth trying for researchers or practitioners in the field. No code repository is provided in the metadata.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: Yuan Xun, Siyuan Liang, Xiaojun Jia, Xinwei Liu, Xiaochun Cao&lt;/li&gt;&lt;li&gt;Tags: backdoor attacks, LVLM security, robustness, instruction tuning, adversarial defense&lt;/li&gt;&lt;li&gt;Relevance Score: 5/5&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.05401</guid><pubDate>Mon, 09 Jun 2025 00:00:00 +0000</pubDate></item><item><title>MCA-Bench: A Multimodal Benchmark for Evaluating CAPTCHA Robustness Against VLM-based Attacks</title><link>https://arxiv.org/abs/2506.05982</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces MCA-Bench, a comprehensive benchmark for evaluating the robustness of various CAPTCHA types against attacks using vision-language models (VLMs).&lt;/li&gt;&lt;li&gt;Fine-tunes specialized cracking agents for different CAPTCHA modalities, enabling systematic, cross-modal security assessments.&lt;/li&gt;&lt;li&gt;Provides quantitative analysis of CAPTCHA vulnerabilities, challenge complexity, and model solvability.&lt;/li&gt;&lt;li&gt;Proposes actionable design principles for improving CAPTCHA security and highlights open challenges for future research.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Quality Assessment&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Clarity: 5/5&lt;/li&gt;&lt;li&gt;Novelty: 5/5&lt;/li&gt;&lt;li&gt;Significance: 4/5&lt;/li&gt;&lt;li&gt;Try-worthiness: Yes&lt;/li&gt;&lt;li&gt;Justification: The abstract is very well-written, clearly outlining the motivation, contributions, and findings of the work. The introduction of a unified, multimodal CAPTCHA benchmark is highly novel, especially given the integration of various CAPTCHA types and the use of VLM-based attack agents. The significance is high due to the practical importance of CAPTCHA robustness and the lack of existing comprehensive benchmarks, though as a recent preprint, its impact is yet to be seen in citations. The availability of datasets and code further increases its try-worthiness for researchers and practitioners interested in security and AI robustness.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: Zonglin Wu, Yule Xue, Xin Wei, Yiren Song&lt;/li&gt;&lt;li&gt;Tags: CAPTCHA security, vision-language models, adversarial attacks, benchmarking, robustness evaluation&lt;/li&gt;&lt;li&gt;Relevance Score: 5/5&lt;/li&gt;&lt;li&gt;Code Repository: &lt;a href='Not explicitly provided in the metadata or abstract, but the abstract states that datasets and code are available online. The actual URL is not listed here.'&gt;Not explicitly provided in the metadata or abstract, but the abstract states that datasets and code are available online. The actual URL is not listed here.&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.05982</guid><pubDate>Mon, 09 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Towards Reliable Identification of Diffusion-based Image Manipulations</title><link>https://arxiv.org/abs/2506.05466</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes RADAR, a method for reliable identification and localization of image manipulations performed by diffusion models.&lt;/li&gt;&lt;li&gt;Combines features from multiple image modalities and uses a contrastive loss to better isolate manipulated regions.&lt;/li&gt;&lt;li&gt;Introduces BBC-PAIR, a new benchmark dataset with images edited by 28 different diffusion models for realistic evaluation.&lt;/li&gt;&lt;li&gt;Demonstrates that RADAR outperforms state-of-the-art methods in detecting and localizing edits from both known and novel diffusion models.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research Paper&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Quality Assessment&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Clarity: 5/5&lt;/li&gt;&lt;li&gt;Novelty: 5/5&lt;/li&gt;&lt;li&gt;Significance: 4/5&lt;/li&gt;&lt;li&gt;Try-worthiness: Yes&lt;/li&gt;&lt;li&gt;Justification: The abstract is well-written, clearly outlining the motivation, proposed method (RADAR), and contributions, including a new benchmark (BBC-PAIR). The approach appears novel, combining features from different image modalities and introducing an auxiliary contrastive loss for improved manipulation detection. The significance is high given the increasing prevalence of diffusion-based image manipulations, though the paper is very new and not yet peer-reviewed or cited. The introduction of a comprehensive benchmark and claims of outperforming state-of-the-art methods further enhance its importance. The promise of public code and data increases its try-worthiness for researchers and practitioners interested in image forensics.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: Alex Costanzino, Woody Bayliss, Juil Sock, Marc Gorriz Blanch, Danijela Horak, Ivan Laptev, Philip Torr, Fabio Pizzati&lt;/li&gt;&lt;li&gt;Tags: diffusion models, image manipulation detection, robustness, AI security, benchmarking&lt;/li&gt;&lt;li&gt;Relevance Score: 4/5&lt;/li&gt;&lt;li&gt;Code Repository: &lt;a href='https://alex-costanzino.github.io/radar'&gt;https://alex-costanzino.github.io/radar&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.05466</guid><pubDate>Mon, 09 Jun 2025 00:00:00 +0000</pubDate></item><item><title>ProSec: Fortifying Code LLMs with Proactive Security Alignment</title><link>https://arxiv.org/abs/2411.12882</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces ProSec, a proactive security alignment approach for code-generating LLMs.&lt;/li&gt;&lt;li&gt;Synthesizes vulnerability-inducing coding scenarios based on Common Weakness Enumerations (CWEs) to expose and fix vulnerabilities in code LLMs.&lt;/li&gt;&lt;li&gt;Uses preference learning to align models with secure coding practices, resulting in a significantly larger and more effective security-focused alignment dataset.&lt;/li&gt;&lt;li&gt;Demonstrates substantial improvements in the security of code LLMs without sacrificing their utility.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research Paper&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Quality Assessment&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Clarity: 5/5&lt;/li&gt;&lt;li&gt;Novelty: 5/5&lt;/li&gt;&lt;li&gt;Significance: 4/5&lt;/li&gt;&lt;li&gt;Try-worthiness: Yes&lt;/li&gt;&lt;li&gt;Justification: The abstract is well-written, clearly outlining the motivation, limitations of prior work, and the proposed solution (ProSec). The approach of synthesizing vulnerability-inducing scenarios from CWEs and using preference learning for security alignment appears novel and addresses a key limitation (data sparsity) in existing methods. The reported improvements in security (25.2% to 35.4%) without utility degradation are significant, especially given the growing importance of secure code generation from LLMs. The paper is very recent, so the lack of citations is not a concern, but its relevance and the scale of improvement suggest it is significant. The method is worth trying for anyone working on secure code generation or LLM alignment. No code repository link is provided in the metadata.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: Xiangzhe Xu, Zian Su, Jinyao Guo, Kaiyuan Zhang, Zhenting Wang, Xiangyu Zhang&lt;/li&gt;&lt;li&gt;Tags: LLM security, secure code generation, alignment, vulnerability mitigation, preference learning&lt;/li&gt;&lt;li&gt;Relevance Score: 5/5&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2411.12882</guid><pubDate>Mon, 09 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Opt-Out: Investigating Entity-Level Unlearning for Large Language Models via Optimal Transport</title><link>https://arxiv.org/abs/2406.12329</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes Opt-Out, a novel optimal transport-based method for entity-level unlearning in large language models (LLMs).&lt;/li&gt;&lt;li&gt;Addresses the challenge of removing all knowledge related to a specific entity (e.g., a user) from LLMs to enhance privacy and security.&lt;/li&gt;&lt;li&gt;Introduces the Entity-Level Unlearning Dataset (ELUDe) for evaluating the effectiveness of entity-level unlearning techniques.&lt;/li&gt;&lt;li&gt;Demonstrates that Opt-Out outperforms existing unlearning methods, supporting secure and privacy-compliant LLM deployments.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research Paper&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Quality Assessment&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Clarity: 5/5&lt;/li&gt;&lt;li&gt;Novelty: 5/5&lt;/li&gt;&lt;li&gt;Significance: 4/5&lt;/li&gt;&lt;li&gt;Try-worthiness: Yes&lt;/li&gt;&lt;li&gt;Justification: The paper is very clearly written, with a well-structured abstract that explains the motivation, problem, method, and contributions. The focus on entity-level unlearning for LLMs is novel, as most prior work targets instance-level forgetting. The introduction of an optimal transport-based method (Opt-Out) and a new benchmark dataset (ELUDe) further enhance its novelty. While the paper is very recent and has no citations yet, the topic is highly significant given privacy concerns in LLMs and the growing demand for user data removal. The method appears practical and promising for implementation or experimentation, especially for researchers or practitioners interested in privacy and model unlearning. No code repository is provided in the metadata.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: Minseok Choi, Daniel Rim, Dohyun Lee, Jaegul Choo&lt;/li&gt;&lt;li&gt;Tags: machine unlearning, privacy, LLM security, entity-level data removal, user data protection&lt;/li&gt;&lt;li&gt;Relevance Score: 5/5&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2406.12329</guid><pubDate>Mon, 09 Jun 2025 00:00:00 +0000</pubDate></item><item><title>SoK: Are Watermarks in LLMs Ready for Deployment?</title><link>https://arxiv.org/abs/2506.05594</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Provides a comprehensive systematization of watermarking techniques for Large Language Models (LLMs), focusing on their deployment readiness.&lt;/li&gt;&lt;li&gt;Analyzes the effectiveness of watermarks in protecting against model stealing attacks, a key security threat for proprietary LLMs.&lt;/li&gt;&lt;li&gt;Presents a taxonomy and an intellectual property classifier to evaluate watermark robustness under attack and non-attack scenarios.&lt;/li&gt;&lt;li&gt;Discusses limitations, practical challenges, and future directions for watermark deployment in real-world LLM applications.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Survey/Review&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Quality Assessment&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Clarity: 5/5&lt;/li&gt;&lt;li&gt;Novelty: 4/5&lt;/li&gt;&lt;li&gt;Significance: 4/5&lt;/li&gt;&lt;li&gt;Try-worthiness: No&lt;/li&gt;&lt;li&gt;Justification: The paper is very clearly written, with a well-structured abstract that outlines its contributions and findings. It provides a comprehensive systematization (SoK) of watermarking techniques in LLMs, which is a timely and relevant topic given the rise of proprietary LLMs and associated risks. The novelty is solid, as it proposes a new taxonomy and an intellectual property classifier, and conducts extensive analysis, though as a systematization paper, it is more about organizing and evaluating existing work than introducing fundamentally new algorithms. The significance is high due to the importance of watermarking in LLM deployment, but as a recent preprint, it has not yet been cited and is not peer-reviewed. Try-worthiness is rated as false because the paper focuses on analysis, taxonomy, and evaluation rather than introducing a new method or tool to implement. No code repository is provided.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: Kieu Dang, Phung Lai, NhatHai Phan, Yelong Shen, Ruoming Jin, Abdallah Khreishah, My Thai&lt;/li&gt;&lt;li&gt;Tags: LLM security, model stealing, watermarking, intellectual property protection, adversarial attacks&lt;/li&gt;&lt;li&gt;Relevance Score: 5/5&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.05594</guid><pubDate>Mon, 09 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Detecting Voice Phishing with Precision: Fine-Tuning Small Language Models</title><link>https://arxiv.org/abs/2506.06180</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a method for detecting voice phishing attacks using fine-tuned small language models (Llama3).&lt;/li&gt;&lt;li&gt;Constructs an adversarial test dataset to evaluate model robustness under challenging, attack-like conditions.&lt;/li&gt;&lt;li&gt;Compares the effectiveness of prompt engineering (evaluation criteria) and Chain-of-Thought (CoT) prompting for improving detection performance.&lt;/li&gt;&lt;li&gt;Finds that incorporating human expert knowledge into prompts enhances the security performance of small LMs in phishing detection.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research Paper&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Quality Assessment&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Clarity: 4/5&lt;/li&gt;&lt;li&gt;Novelty: 4/5&lt;/li&gt;&lt;li&gt;Significance: 3/5&lt;/li&gt;&lt;li&gt;Try-worthiness: Yes&lt;/li&gt;&lt;li&gt;Justification: The paper is clearly written, with a well-structured abstract that outlines the methodology (fine-tuning Llama3 with VP evaluation criteria and CoT), dataset construction, and comparative results. The novelty is reasonably high, as it applies recent small language models and prompt engineering techniques (evaluation criteria, CoT) to the underexplored domain of voice phishing detection, and constructs an adversarial test set. The significance is moderate: while the problem is important and the approach promising, the paper is a preprint with no citations yet and not peer-reviewed, limiting its immediate impact. However, the results—showing small LMs can approach GPT-4 performance with expert-informed prompts—make it worth trying for practitioners. No code repository is provided in the metadata.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: Ju Yong Sim, Seong Hwan Kim&lt;/li&gt;&lt;li&gt;Tags: phishing detection, adversarial robustness, LLM security, prompt engineering, AI safety evaluation&lt;/li&gt;&lt;li&gt;Relevance Score: 4/5&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.06180</guid><pubDate>Mon, 09 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Tau-Eval: A Unified Evaluation Framework for Useful and Private Text Anonymization</title><link>https://arxiv.org/abs/2506.05979</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces Tau-Eval, an open-source framework for evaluating text anonymization methods.&lt;/li&gt;&lt;li&gt;Focuses on balancing privacy protection with the utility of anonymized text for downstream applications.&lt;/li&gt;&lt;li&gt;Addresses the lack of comprehensive benchmarks for assessing privacy and utility trade-offs in text anonymization.&lt;/li&gt;&lt;li&gt;Provides tools and documentation for benchmarking anonymization techniques.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Quality Assessment&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Clarity: 5/5&lt;/li&gt;&lt;li&gt;Novelty: 4/5&lt;/li&gt;&lt;li&gt;Significance: 4/5&lt;/li&gt;&lt;li&gt;Try-worthiness: Yes&lt;/li&gt;&lt;li&gt;Justification: The abstract is well-written and clearly explains the motivation, challenge, and contribution of the work. The introduction of a unified, open-source evaluation framework for text anonymization is novel and addresses a real gap in the field, as there is no universal benchmark for balancing privacy and utility. While the paper is very recent and thus has no citations yet, the topic is highly relevant given increasing privacy concerns and the need for robust anonymization in NLP. The availability of code, documentation, and tutorials further increases its practical value and try-worthiness for researchers and practitioners interested in privacy-preserving NLP.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: Gabriel Loiseau, Damien Sileo, Damien Riquet, Maxime Meyer, Marc Tommasi&lt;/li&gt;&lt;li&gt;Tags: text anonymization, privacy, benchmarking, evaluation framework&lt;/li&gt;&lt;li&gt;Relevance Score: 4/5&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.05979</guid><pubDate>Mon, 09 Jun 2025 00:00:00 +0000</pubDate></item><item><title>ByzSecAgg: A Byzantine-Resistant Secure Aggregation Scheme for Federated Learning Based on Coded Computing and Vector Commitment</title><link>https://arxiv.org/abs/2302.09913</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes ByzSecAgg, a secure aggregation scheme for federated learning that is resistant to Byzantine attacks and privacy leakages.&lt;/li&gt;&lt;li&gt;Utilizes ramp secret sharing and coded computing techniques to reduce communication overhead while maintaining security.&lt;/li&gt;&lt;li&gt;Introduces a vector commitment method to ensure integrity and privacy of local updates during aggregation.&lt;/li&gt;&lt;li&gt;Demonstrates improved communication efficiency compared to existing secure aggregation schemes.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research Paper&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Quality Assessment&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Clarity: 4/5&lt;/li&gt;&lt;li&gt;Novelty: 5/5&lt;/li&gt;&lt;li&gt;Significance: 4/5&lt;/li&gt;&lt;li&gt;Try-worthiness: Yes&lt;/li&gt;&lt;li&gt;Justification: The abstract is well-written and clearly explains the motivation, challenges, and proposed solution (ByzSecAgg) for secure aggregation in federated learning under Byzantine threats. The use of ramp secret sharing, coded computing, and vector commitment for efficient and secure aggregation appears novel, especially in addressing both communication efficiency and Byzantine robustness. The work is significant given the growing importance of secure and robust federated learning, and the abstract claims substantial improvements over a known baseline (BREA). Although the paper is very recent and has no citations yet, its technical contributions and relevance to current research trends make it worth experimenting with. No code repository is provided in the metadata.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: Tayyebeh Jahani-Nezhad, Mohammad Ali Maddah-Ali, Giuseppe Caire&lt;/li&gt;&lt;li&gt;Tags: federated learning security, Byzantine robustness, secure aggregation, privacy-preserving machine learning, adversarial robustness&lt;/li&gt;&lt;li&gt;Relevance Score: 5/5&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2302.09913</guid><pubDate>Mon, 09 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Online Detection of LLM-Generated Texts via Sequential Hypothesis Testing by Betting</title><link>https://arxiv.org/abs/2410.22318</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes an algorithm for online detection of LLM-generated texts using sequential hypothesis testing.&lt;/li&gt;&lt;li&gt;Addresses the challenge of identifying machine-generated content in streaming scenarios (e.g., news, social media).&lt;/li&gt;&lt;li&gt;Provides statistical guarantees such as controlled false positive rates and expected detection times.&lt;/li&gt;&lt;li&gt;Demonstrates effectiveness through experiments.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research Paper&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Quality Assessment&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Clarity: 5/5&lt;/li&gt;&lt;li&gt;Novelty: 4/5&lt;/li&gt;&lt;li&gt;Significance: 4/5&lt;/li&gt;&lt;li&gt;Try-worthiness: Yes&lt;/li&gt;&lt;li&gt;Justification: The abstract is well-written, clearly outlining the motivation, problem, and proposed solution. The focus on online detection of LLM-generated text using sequential hypothesis testing by betting is a novel extension beyond the more common offline detection approaches. While the paper is very recent and has no citations yet, the problem addressed is timely and significant given the proliferation of LLM-generated content and associated risks. The statistical guarantees and experimental validation further enhance its practical relevance. The lack of a code repository is a minor drawback, but the methodological contribution makes it worth experimenting with.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: Can Chen, Jun-Kun Wang&lt;/li&gt;&lt;li&gt;Tags: LLM-generated text detection, AI misuse prevention, sequential hypothesis testing, content authenticity&lt;/li&gt;&lt;li&gt;Relevance Score: 4/5&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2410.22318</guid><pubDate>Mon, 09 Jun 2025 00:00:00 +0000</pubDate></item><item><title>SGD Jittering: A Training Strategy for Robust and Accurate Model-Based Architectures</title><link>https://arxiv.org/abs/2410.14667</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes 'SGD jittering', a training strategy that injects noise during model-based architecture (MBA) training to improve robustness.&lt;/li&gt;&lt;li&gt;Theoretically and empirically demonstrates improved generalization and robustness to adversarial attacks compared to standard training.&lt;/li&gt;&lt;li&gt;Validates the approach on tasks such as denoising, seismic deconvolution, and MRI reconstruction, showing enhanced performance on out-of-distribution data.&lt;/li&gt;&lt;li&gt;Addresses the accuracy-robustness tradeoff in MBAs, which is important for safety-critical applications.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research Paper&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Quality Assessment&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Clarity: 4/5&lt;/li&gt;&lt;li&gt;Novelty: 4/5&lt;/li&gt;&lt;li&gt;Significance: 3/5&lt;/li&gt;&lt;li&gt;Try-worthiness: Yes&lt;/li&gt;&lt;li&gt;Justification: The abstract is well-written and clearly explains the motivation, method (SGD jittering), and the types of experiments performed. The idea of injecting noise during training for model-based architectures (MBAs) to improve robustness and generalization is novel, especially as it is theoretically justified and empirically validated across several inverse problems. While the paper is new and has no citations yet, the focus on robustness in MBAs is timely and relevant, especially for safety-critical applications. The method appears simple to implement and potentially impactful, making it worth trying for practitioners in inverse problems or robust ML. No code repository is provided in the metadata.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: Peimeng Guan, Mark A. Davenport&lt;/li&gt;&lt;li&gt;Tags: robustness, adversarial attacks, model-based architectures, training strategies, safety-critical applications&lt;/li&gt;&lt;li&gt;Relevance Score: 4/5&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2410.14667</guid><pubDate>Mon, 09 Jun 2025 00:00:00 +0000</pubDate></item><item><title>HO-FMN: Hyperparameter Optimization for Fast Minimum-Norm Attacks</title><link>https://arxiv.org/abs/2407.08806</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a new method (HO-FMN) for hyperparameter optimization in minimum-norm adversarial attacks.&lt;/li&gt;&lt;li&gt;Allows dynamic adjustment of loss, optimizer, step-size scheduler, and hyperparameters during attacks.&lt;/li&gt;&lt;li&gt;Demonstrates that the method finds smaller adversarial perturbations, leading to more accurate robustness evaluations.&lt;/li&gt;&lt;li&gt;Enables more comprehensive reporting of adversarial robustness as a function of the perturbation budget.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research Paper&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Quality Assessment&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Clarity: 5/5&lt;/li&gt;&lt;li&gt;Novelty: 4/5&lt;/li&gt;&lt;li&gt;Significance: 4/5&lt;/li&gt;&lt;li&gt;Try-worthiness: Yes&lt;/li&gt;&lt;li&gt;Justification: The abstract is well-written and clearly explains the motivation, method, and results of the paper. The work is novel in that it introduces a parametric, hyperparameter-optimized variant of the fast minimum-norm attack, addressing limitations of fixed-parameter attacks. The significance is high for the robustness evaluation community, as it enables more accurate and flexible adversarial robustness assessments. While the paper is very recent and has no citations yet, its open-source code and practical improvements make it worth trying for researchers and practitioners in adversarial machine learning.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: Raffaele Mura, Giuseppe Floris, Luca Scionis, Giorgio Piras, Maura Pintor, Ambra Demontis, Giorgio Giacinto, Battista Biggio, Fabio Roli&lt;/li&gt;&lt;li&gt;Tags: adversarial attacks, robustness evaluation, hyperparameter optimization, AI security&lt;/li&gt;&lt;li&gt;Relevance Score: 5/5&lt;/li&gt;&lt;li&gt;Code Repository: &lt;a href='https://github.com/pralab/HO-FMN'&gt;https://github.com/pralab/HO-FMN&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2407.08806</guid><pubDate>Mon, 09 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Zero-Shot Detection of LLM-Generated Code via Approximated Task Conditioning</title><link>https://arxiv.org/abs/2506.06069</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a novel zero-shot method for detecting LLM-generated code by approximating the original task prompt and evaluating token-level entropy.&lt;/li&gt;&lt;li&gt;Demonstrates that conditioning on the task prompt reveals differences between human and LLM-generated code, which are not apparent in unconditional distributions.&lt;/li&gt;&lt;li&gt;Method does not require access to the generator LLM or original prompts, making it practical for real-world security and integrity applications.&lt;/li&gt;&lt;li&gt;Achieves state-of-the-art results across multiple programming languages and benchmarks.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research Paper&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Quality Assessment&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Clarity: 5/5&lt;/li&gt;&lt;li&gt;Novelty: 5/5&lt;/li&gt;&lt;li&gt;Significance: 4/5&lt;/li&gt;&lt;li&gt;Try-worthiness: Yes&lt;/li&gt;&lt;li&gt;Justification: The abstract is very well-written, clearly outlining the motivation, key insight, methodology, and contributions of the paper. The approach—approximating the original task prompt to condition code detection—is novel and addresses a real challenge in distinguishing LLM-generated code from human-written code, especially in a zero-shot setting. The method's practicality (not requiring access to the generator LLM or original prompts) and its claimed state-of-the-art results across multiple programming languages further enhance its significance. While the paper is very recent and thus has no citations yet, the problem is timely and important, and the availability of code and datasets increases its try-worthiness for practitioners and researchers.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: Maor Ashkenazi, Ofir Brenner, Tal Furman Shohet, Eran Treister&lt;/li&gt;&lt;li&gt;Tags: LLM-generated content detection, AI security, Code provenance, Zero-shot detection&lt;/li&gt;&lt;li&gt;Relevance Score: 4/5&lt;/li&gt;&lt;li&gt;Code Repository: &lt;a href='https://github.com/maorash/ATC'&gt;https://github.com/maorash/ATC&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.06069</guid><pubDate>Mon, 09 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Sample-Specific Noise Injection For Diffusion-Based Adversarial Purification</title><link>https://arxiv.org/abs/2506.06027</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a new framework (SSNI) for diffusion-based adversarial purification that injects sample-specific noise based on estimated deviation from clean data.&lt;/li&gt;&lt;li&gt;Uses a pre-trained score network to determine the optimal noise level for each input, improving robustness against adversarial attacks.&lt;/li&gt;&lt;li&gt;Demonstrates empirical improvements in both accuracy and robustness on standard datasets (CIFAR-10, ImageNet-1K).&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research Paper&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Quality Assessment&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Clarity: 5/5&lt;/li&gt;&lt;li&gt;Novelty: 4/5&lt;/li&gt;&lt;li&gt;Significance: 3/5&lt;/li&gt;&lt;li&gt;Try-worthiness: Yes&lt;/li&gt;&lt;li&gt;Justification: The abstract is well-written and clearly explains the motivation, method, and empirical results of the paper. The idea of sample-specific noise injection in diffusion-based adversarial purification is a novel extension over the standard approach of using a constant noise level. While the paper is very recent and published as a preprint on arXiv (so significance in terms of citations and peer review is not yet established), the method addresses a practical and important problem in adversarial robustness. The empirical improvements on CIFAR-10 and ImageNet-1K further support its potential impact. The availability of code increases its try-worthiness for practitioners and researchers.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: Yuhao Sun, Jiacheng Zhang, Zesheng Ye, Chaowei Xiao, Feng Liu&lt;/li&gt;&lt;li&gt;Tags: adversarial robustness, diffusion models, adversarial purification, robustness evaluation&lt;/li&gt;&lt;li&gt;Relevance Score: 4/5&lt;/li&gt;&lt;li&gt;Code Repository: &lt;a href='https://github.com/tmlr-group/SSNI'&gt;https://github.com/tmlr-group/SSNI&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.06027</guid><pubDate>Mon, 09 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Stealix: Model Stealing via Prompt Evolution</title><link>https://arxiv.org/abs/2506.05867</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces Stealix, a novel model stealing attack that does not require predefined prompts or class names.&lt;/li&gt;&lt;li&gt;Uses prompt evolution via a genetic algorithm to iteratively refine prompts for data synthesis.&lt;/li&gt;&lt;li&gt;Demonstrates that Stealix outperforms existing model stealing methods under the same query budget.&lt;/li&gt;&lt;li&gt;Highlights increased security risks posed by open-source pre-trained generative models in model stealing scenarios.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research Paper&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Quality Assessment&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Clarity: 4/5&lt;/li&gt;&lt;li&gt;Novelty: 5/5&lt;/li&gt;&lt;li&gt;Significance: 4/5&lt;/li&gt;&lt;li&gt;Try-worthiness: Yes&lt;/li&gt;&lt;li&gt;Justification: The abstract is well-written and clearly explains the motivation, methodology, and contributions of the paper, though some technical details are left for the full text. The work is highly novel, introducing the first model stealing approach that does not require predefined prompts, leveraging prompt evolution via genetic algorithms. This represents a meaningful advance over prior methods that require manual prompt engineering. The significance is high, as it exposes new security risks in the context of open-source generative models and model stealing, though the impact is yet to be measured due to the paper's recency and preprint status. The approach appears practical and worth experimenting with, especially for researchers in ML security and generative models. No code repository is provided in the metadata.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: Zhixiong Zhuang, Hui-Po Wang, Maria-Irina Nicolae, Mario Fritz&lt;/li&gt;&lt;li&gt;Tags: model stealing, prompt evolution, AI security, adversarial attacks, generative models&lt;/li&gt;&lt;li&gt;Relevance Score: 5/5&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.05867</guid><pubDate>Mon, 09 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Do LLMs Really Forget? Evaluating Unlearning with Knowledge Correlation and Confidence Awareness</title><link>https://arxiv.org/abs/2506.05735</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a new evaluation framework for machine unlearning in large language models (LLMs), focusing on implicit knowledge and inferential dependencies.&lt;/li&gt;&lt;li&gt;Introduces knowledge graphs with confidence scores to better represent factual contexts and assess unlearning success.&lt;/li&gt;&lt;li&gt;Develops an inference-based protocol using LLMs as judges, calibrated against human evaluations for reliability.&lt;/li&gt;&lt;li&gt;Finds that current unlearning evaluation methods may overestimate effectiveness, highlighting potential security/privacy risks.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research Paper&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: Rongzhe Wei, Peizhi Niu, Hans Hao-Hsun Hsu, Ruihan Wu, Haoteng Yin, Mohsen Ghassemi, Yifan Li, Vamsi K. Potluru, Eli Chien, Kamalika Chaudhuri, Olgica Milenkovic, Pan Li&lt;/li&gt;&lt;li&gt;Tags: machine unlearning, LLM security, privacy, evaluation framework, knowledge removal&lt;/li&gt;&lt;li&gt;Relevance Score: 5/5&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.05735</guid><pubDate>Mon, 09 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Sylva: Tailoring Personalized Adversarial Defense in Pre-trained Models via Collaborative Fine-tuning</title><link>https://arxiv.org/abs/2506.05402</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes Sylva, a personalized collaborative adversarial training framework for pre-trained models on edge devices.&lt;/li&gt;&lt;li&gt;Addresses the challenge of adversarial attacks on mobile clients by enabling tailored adversarial defenses.&lt;/li&gt;&lt;li&gt;Utilizes LoRA for efficient local adversarial fine-tuning and federated aggregation to reduce communication costs.&lt;/li&gt;&lt;li&gt;Introduces a game-based layer selection strategy to further refine model accuracy and robustness for each client.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research Paper&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Quality Assessment&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Clarity: 4/5&lt;/li&gt;&lt;li&gt;Novelty: 4/5&lt;/li&gt;&lt;li&gt;Significance: 3/5&lt;/li&gt;&lt;li&gt;Try-worthiness: Yes&lt;/li&gt;&lt;li&gt;Justification: The abstract is well-written and clearly explains the motivation, methodology, and results of the paper. The use of LoRA for communication-efficient federated adversarial training and the introduction of a game-based layer selection strategy for personalized robustness appear to be novel contributions. The significance is moderate, as the problem addressed (personalized adversarial defense in federated settings) is important, but the paper is still a preprint and has not yet been peer-reviewed or cited. The reported improvements in communication efficiency and robustness are substantial, making the approach worth trying for practitioners in federated learning and adversarial defense. No code repository is provided in the metadata.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: Tianyu Qi, Lei Xue, Yufeng Zhan, Xiaobo Ma&lt;/li&gt;&lt;li&gt;Tags: adversarial robustness, federated learning, personalized defense, edge AI security, collaborative training&lt;/li&gt;&lt;li&gt;Relevance Score: 5/5&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.05402</guid><pubDate>Mon, 09 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Attacking Attention of Foundation Models Disrupts Downstream Tasks</title><link>https://arxiv.org/abs/2506.05394</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Investigates security vulnerabilities in vision foundation models such as CLIP and Vision Transformers (ViT).&lt;/li&gt;&lt;li&gt;Introduces a novel adversarial attack targeting the attention mechanism in transformer-based architectures.&lt;/li&gt;&lt;li&gt;Demonstrates that the attack is effective and transferable across multiple downstream tasks, including classification, captioning, retrieval, segmentation, and depth estimation.&lt;/li&gt;&lt;li&gt;Highlights the broader security risks posed by reliance on pre-trained foundation models in industrial applications.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research Paper&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Quality Assessment&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Clarity: 4/5&lt;/li&gt;&lt;li&gt;Novelty: 5/5&lt;/li&gt;&lt;li&gt;Significance: 4/5&lt;/li&gt;&lt;li&gt;Try-worthiness: Yes&lt;/li&gt;&lt;li&gt;Justification: The abstract is well-written and clearly communicates the motivation, scope, and contributions of the paper, though it could be slightly more detailed about the attack method itself. The work appears highly novel, introducing a new, task-agnostic adversarial attack targeting the attention mechanism in foundation models—a timely and important topic given the widespread adoption of models like CLIP and ViTs. The significance is high, as vulnerabilities in foundation models have broad implications for AI security, and the paper demonstrates impact across multiple downstream tasks. While the paper is very recent and has no citations yet, its relevance and the venue (arXiv, a common platform for impactful early AI research) support its importance. The paper is worth trying out, especially for researchers or practitioners concerned with model robustness and security. No code repository is provided in the metadata.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: Hondamunige Prasanna Silva, Federico Becattini, Lorenzo Seidenari&lt;/li&gt;&lt;li&gt;Tags: adversarial attacks, foundation models, transformer security, model robustness, downstream task vulnerability&lt;/li&gt;&lt;li&gt;Relevance Score: 5/5&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.05394</guid><pubDate>Mon, 09 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Synthetic Tabular Data: Methods, Attacks and Defenses</title><link>https://arxiv.org/abs/2506.06108</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Provides a comprehensive survey of methods for generating synthetic tabular data, including deep learning and probabilistic models.&lt;/li&gt;&lt;li&gt;Discusses privacy attacks on synthetic data, such as attempts to retrieve information about the original sensitive data.&lt;/li&gt;&lt;li&gt;Covers defenses and limitations related to privacy and security in synthetic data generation.&lt;/li&gt;&lt;li&gt;Identifies open problems and future directions in the security and privacy of synthetic tabular data.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Survey/Review&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Quality Assessment&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Clarity: 5/5&lt;/li&gt;&lt;li&gt;Novelty: 3/5&lt;/li&gt;&lt;li&gt;Significance: 4/5&lt;/li&gt;&lt;li&gt;Try-worthiness: No&lt;/li&gt;&lt;li&gt;Justification: The abstract is very clear, well-structured, and provides a comprehensive overview of the survey's scope, earning a high clarity score. As a survey paper, its novelty is moderate—it synthesizes existing work rather than introducing new methods. The significance is relatively high because synthetic tabular data is a crucial and timely topic, and the survey covers both methods and privacy attacks/defenses, which is valuable for researchers and practitioners. However, as a survey, it is not directly try-worthy for implementation or experimentation, but rather serves as a resource for understanding the field. No code repository is provided, which is typical for survey papers.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: Graham Cormode, Samuel Maddock, Enayat Ullah, Shripad Gade&lt;/li&gt;&lt;li&gt;Tags: synthetic data, privacy attacks, data security, defenses, tabular data&lt;/li&gt;&lt;li&gt;Relevance Score: 4/5&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.06108</guid><pubDate>Mon, 09 Jun 2025 00:00:00 +0000</pubDate></item><item><title>System-Aware Unlearning Algorithms: Use Lesser, Forget Faster</title><link>https://arxiv.org/abs/2506.06073</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a new definition of machine unlearning (system-aware unlearning) tailored to more realistic attacker models.&lt;/li&gt;&lt;li&gt;Introduces an unlearning algorithm for linear classification using selective sampling, with generalization to broader function classes.&lt;/li&gt;&lt;li&gt;Analyzes tradeoffs between deletion capacity, accuracy, memory, and computation time under the new definition.&lt;/li&gt;&lt;li&gt;Addresses security and privacy concerns by considering what an attacker can realistically access in the system.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research Paper&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Quality Assessment&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Clarity: 4/5&lt;/li&gt;&lt;li&gt;Novelty: 5/5&lt;/li&gt;&lt;li&gt;Significance: 4/5&lt;/li&gt;&lt;li&gt;Try-worthiness: Yes&lt;/li&gt;&lt;li&gt;Justification: The abstract is well-written and clearly explains the motivation, problem, and contributions of the paper, though some technical terms may require background knowledge. The work is highly novel, proposing a new 'system-aware' definition for machine unlearning that relaxes the traditional, overly stringent attacker model, and introduces new algorithms and theoretical analysis. While the paper is a preprint and very recent (hence no citations yet), the topic is timely and significant given the growing importance of data deletion and privacy in machine learning. The proposed approach could inspire practical implementations and further research, making it worth experimenting with. No code repository is provided in the metadata.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: Linda Lu, Ayush Sekhari, Karthik Sridharan&lt;/li&gt;&lt;li&gt;Tags: machine unlearning, privacy, security, attacker model, data deletion&lt;/li&gt;&lt;li&gt;Relevance Score: 5/5&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.06073</guid><pubDate>Mon, 09 Jun 2025 00:00:00 +0000</pubDate></item><item><title>What Really is a Member? Discrediting Membership Inference via Poisoning</title><link>https://arxiv.org/abs/2506.06003</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Investigates the reliability of membership inference tests for language models, even under relaxed definitions of membership.&lt;/li&gt;&lt;li&gt;Demonstrates that poisoning the training dataset can cause membership inference tests to produce incorrect predictions.&lt;/li&gt;&lt;li&gt;Theoretically analyzes the trade-off between test accuracy and robustness to poisoning attacks.&lt;/li&gt;&lt;li&gt;Presents and empirically validates a concrete poisoning attack that significantly degrades the performance of existing membership inference tests.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research Paper&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: Neal Mangaokar, Ashish Hooda, Zhuohang Li, Bradley A. Malin, Kassem Fawaz, Somesh Jha, Atul Prakash, Amrita Roy Chowdhury&lt;/li&gt;&lt;li&gt;Tags: membership inference, data poisoning, privacy attacks, AI security, robustness&lt;/li&gt;&lt;li&gt;Relevance Score: 5/5&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.06003</guid><pubDate>Mon, 09 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Statistically Valid Post-Deployment Monitoring Should Be Standard for AI-Based Digital Health</title><link>https://arxiv.org/abs/2506.05701</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Argues for the necessity of statistically valid and label-efficient post-deployment monitoring for AI-based digital health tools.&lt;/li&gt;&lt;li&gt;Highlights the lack of current post-deployment surveillance in clinical AI, with only 9% of FDA-registered tools including such plans.&lt;/li&gt;&lt;li&gt;Proposes that monitoring should be grounded in statistical hypothesis testing to detect data and model performance changes.&lt;/li&gt;&lt;li&gt;Emphasizes alignment with regulatory requirements and the need for reproducible, scientifically sound monitoring frameworks.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Position Paper&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Quality Assessment&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Clarity: 5/5&lt;/li&gt;&lt;li&gt;Novelty: 4/5&lt;/li&gt;&lt;li&gt;Significance: 4/5&lt;/li&gt;&lt;li&gt;Try-worthiness: Yes&lt;/li&gt;&lt;li&gt;Justification: The paper is very clearly written, with a well-structured abstract that concisely explains the motivation, problem, and proposed direction. The focus on statistically valid and label-efficient post-deployment monitoring in clinical AI is a timely and relatively novel position, especially given the low adoption rate of such practices in FDA-registered tools. While the paper is a position piece and not an empirical study, its arguments are significant for the field of digital health, where reliability and safety are paramount. The proposal to ground monitoring in statistical hypothesis testing is a principled and potentially impactful direction. Although the paper is very recent and has no citations yet, its recommendations are worth considering and could inspire both research and practical implementations.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: Pavel Dolin, Weizhi Li, Gautam Dasarathy, Visar Berisha&lt;/li&gt;&lt;li&gt;Tags: AI safety, post-deployment monitoring, clinical AI, risk assessment, regulatory compliance&lt;/li&gt;&lt;li&gt;Relevance Score: 4/5&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.05701</guid><pubDate>Mon, 09 Jun 2025 00:00:00 +0000</pubDate></item><item><title>SocialDF: Benchmark Dataset and Detection Model for Mitigating Harmful Deepfake Content on Social Media Platforms</title><link>https://arxiv.org/abs/2506.05538</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces SocialDF, a benchmark dataset of high-fidelity deepfakes reflecting real-world challenges on social media.&lt;/li&gt;&lt;li&gt;Proposes a novel LLM-based multi-factor detection model combining facial recognition, speech transcription, and multi-agent LLM pipelines.&lt;/li&gt;&lt;li&gt;Focuses on robust, multi-modal verification techniques to distinguish between authentic and adversarially generated deepfake content.&lt;/li&gt;&lt;li&gt;Addresses the security risks posed by deepfakes as vectors for misinformation and manipulation on social media platforms.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Quality Assessment&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Clarity: 4/5&lt;/li&gt;&lt;li&gt;Novelty: 5/5&lt;/li&gt;&lt;li&gt;Significance: 4/5&lt;/li&gt;&lt;li&gt;Try-worthiness: Yes&lt;/li&gt;&lt;li&gt;Justification: The abstract is well-written and clearly outlines the motivation, contributions (a new dataset and a novel LLM-based detection approach), and methodology. The work is highly novel, introducing both a real-world deepfake dataset tailored for social media and a multi-modal, multi-agent LLM detection pipeline, which is a fresh approach in the field. The significance is high given the urgent need for robust deepfake detection on social platforms, though the impact is yet to be measured due to the paper's recency and preprint status. The combination of dataset and detection method makes it worth experimenting with, especially for researchers and practitioners in media forensics and social media security. No code repository is provided in the metadata.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: Arnesh Batra, Anushk Kumar, Jashn Khemani, Arush Gumber, Arhan Jain, Somil Gupta&lt;/li&gt;&lt;li&gt;Tags: deepfake detection, adversarial media, AI security, benchmark dataset, misinformation&lt;/li&gt;&lt;li&gt;Relevance Score: 5/5&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.05538</guid><pubDate>Mon, 09 Jun 2025 00:00:00 +0000</pubDate></item><item><title>TracLLM: A Generic Framework for Attributing Long Context LLMs</title><link>https://arxiv.org/abs/2506.04202</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces TracLLM, a framework for attributing outputs of long context LLMs to specific input texts.&lt;/li&gt;&lt;li&gt;Supports post-attack forensic analysis, such as tracing the source of prompt injection or knowledge corruption attacks.&lt;/li&gt;&lt;li&gt;Improves efficiency and accuracy of context traceback compared to existing feature attribution methods.&lt;/li&gt;&lt;li&gt;Facilitates debugging and trust-building in LLM-based systems by identifying knowledge sources.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research Paper&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Quality Assessment&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Clarity: 5/5&lt;/li&gt;&lt;li&gt;Novelty: 5/5&lt;/li&gt;&lt;li&gt;Significance: 4/5&lt;/li&gt;&lt;li&gt;Try-worthiness: Yes&lt;/li&gt;&lt;li&gt;Justification: The abstract is well-written, clearly outlining the motivation, problem, and contributions of the work. The paper introduces TracLLM, a generic framework specifically designed for context attribution in long-context LLMs, which is a novel and timely problem as LLMs are increasingly used in real-world applications with long contexts. The work addresses the limitations of existing feature attribution methods (like Shapley) in this setting, proposing new algorithms for efficiency and accuracy. While the paper is very recent and has no citations yet, the problem is significant for debugging, security, and trust in LLM-based systems. The availability of code and data further increases its try-worthiness for practitioners and researchers.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: Yanting Wang, Wei Zou, Runpeng Geng, Jinyuan Jia&lt;/li&gt;&lt;li&gt;Tags: forensic analysis, prompt injection, LLM security, context attribution, debugging&lt;/li&gt;&lt;li&gt;Relevance Score: 4/5&lt;/li&gt;&lt;li&gt;Code Repository: &lt;a href='https://github.com/Wang-Yanting/TracLLM'&gt;https://github.com/Wang-Yanting/TracLLM&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.04202</guid><pubDate>Mon, 09 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Web Intellectual Property at Risk: Preventing Unauthorized Real-Time Retrieval by Large Language Models</title><link>https://arxiv.org/abs/2505.12655</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Addresses the risk of unauthorized real-time retrieval of web content by large language models (LLMs), which threatens intellectual property (IP) rights.&lt;/li&gt;&lt;li&gt;Proposes a novel defense framework that enables web content creators to protect their IP from LLM extraction and redistribution.&lt;/li&gt;&lt;li&gt;Leverages the semantic understanding capabilities of LLMs to implement the defense, tackling a black-box optimization challenge.&lt;/li&gt;&lt;li&gt;Demonstrates significant improvements in defense success rates over traditional configuration-based methods through real-world experiments.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research Paper&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Quality Assessment&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Clarity: 4/5&lt;/li&gt;&lt;li&gt;Novelty: 5/5&lt;/li&gt;&lt;li&gt;Significance: 4/5&lt;/li&gt;&lt;li&gt;Try-worthiness: Yes&lt;/li&gt;&lt;li&gt;Justification: The paper is clearly written, with a well-structured abstract that outlines the problem, motivation, and proposed solution. The topic is highly novel, addressing the emerging issue of protecting web intellectual property from real-time extraction by LLMs—a concern that has only recently become prominent. The proposed defense framework, which leverages LLMs' own semantic understanding for protection, appears innovative and is supported by strong experimental results (improving defense rates from 2.5% to 88.6%). While the paper is very new and has no citations yet, the significance is high due to the timeliness and potential impact of the work. The method seems practical and worth experimenting with, especially for web content creators and platform developers. No code repository is provided in the metadata.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: Yisheng Zhong, Yizhu Wen, Junfeng Guo, Mehran Kafai, Heng Huang, Hanqing Guo, Zhuangdi Zhu&lt;/li&gt;&lt;li&gt;Tags: LLM security, model extraction, intellectual property protection, defensive techniques&lt;/li&gt;&lt;li&gt;Relevance Score: 5/5&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2505.12655</guid><pubDate>Mon, 09 Jun 2025 00:00:00 +0000</pubDate></item><item><title>SafeAuto: Knowledge-Enhanced Safe Autonomous Driving with Multimodal Foundation Models</title><link>https://arxiv.org/abs/2503.00211</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes SafeAuto, a framework that enhances multimodal large language model (MLLM)-based autonomous driving by embedding explicit safety knowledge.&lt;/li&gt;&lt;li&gt;Introduces a reasoning component that translates traffic rules into first-order logic and verifies predicted actions using a probabilistic graphical model.&lt;/li&gt;&lt;li&gt;Develops a Position-Dependent Cross-Entropy loss to improve low-level control predictions and integrates multimodal retrieval-augmented generation for learning from past experiences.&lt;/li&gt;&lt;li&gt;Demonstrates improved safety and reliability in autonomous driving compared to existing baselines.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research Paper&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Quality Assessment&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Clarity: 5/5&lt;/li&gt;&lt;li&gt;Novelty: 5/5&lt;/li&gt;&lt;li&gt;Significance: 4/5&lt;/li&gt;&lt;li&gt;Try-worthiness: Yes&lt;/li&gt;&lt;li&gt;Justification: The abstract is well-written, clearly outlining the motivation, challenges, and contributions of the work. The integration of structured safety knowledge (via first-order logic and Markov Logic Networks) with multimodal large language models for autonomous driving is a novel approach, especially with the introduction of the Position-Dependent Cross-Entropy loss and a Multimodal Retrieval-Augmented Generation model. While the paper is very recent and has no citations yet, the topic is highly significant given the importance of safety in autonomous driving. The availability of code further increases its try-worthiness for researchers and practitioners interested in safe AI-driven vehicles.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: Jiawei Zhang, Xuan Yang, Taiqi Wang, Yu Yao, Aleksandr Petiushko, Bo Li&lt;/li&gt;&lt;li&gt;Tags: AI safety, autonomous driving, safety evaluation, multimodal models&lt;/li&gt;&lt;li&gt;Relevance Score: 4/5&lt;/li&gt;&lt;li&gt;Code Repository: &lt;a href='https://github.com/AI-secure/SafeAuto'&gt;https://github.com/AI-secure/SafeAuto&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2503.00211</guid><pubDate>Mon, 09 Jun 2025 00:00:00 +0000</pubDate></item><item><title>An Optimal Cascade Feature-Level Spatiotemporal Fusion Strategy for Anomaly Detection in CAN Bus</title><link>https://arxiv.org/abs/2501.18821</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a novel cascade feature-level spatiotemporal fusion framework for anomaly detection in CAN bus systems.&lt;/li&gt;&lt;li&gt;Addresses security risks in intelligent transportation systems by improving detection of various attack types on in-vehicle networks.&lt;/li&gt;&lt;li&gt;Demonstrates robust performance and comprehensive attack coverage, validated on the CAR-HACKING dataset.&lt;/li&gt;&lt;li&gt;Highlights the importance of robustness and precision in detecting security anomalies in automotive networks.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research Paper&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Quality Assessment&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Clarity: 4/5&lt;/li&gt;&lt;li&gt;Novelty: 4/5&lt;/li&gt;&lt;li&gt;Significance: 3/5&lt;/li&gt;&lt;li&gt;Try-worthiness: Yes&lt;/li&gt;&lt;li&gt;Justification: The abstract is well-written and clearly explains the motivation, methodology, and results, though some technical details (e.g., the exact nature of the cascade fusion) could be clearer. The use of a cascade feature-level spatiotemporal fusion strategy optimized by a genetic algorithm for CAN bus anomaly detection appears novel, especially in its explicit attempt to cover all dominant anomaly structures and its robust evaluation. The significance is moderate: while the results are impressive (AUC-ROC of 0.9987, 100% accuracy on CAR-HACKING), the paper is a preprint and has not yet been peer-reviewed or cited, and the CAR-HACKING dataset is a common benchmark. The approach seems promising and worth trying for researchers or practitioners in automotive security or anomaly detection. No code repository is provided in the metadata.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: Mohammad Fatahi, Danial Sadrian Zadeh, Benyamin Ghojogh, Behzad Moshiri, Otman Basir&lt;/li&gt;&lt;li&gt;Tags: anomaly detection, automotive security, CAN bus, machine learning security, robustness evaluation&lt;/li&gt;&lt;li&gt;Relevance Score: 4/5&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2501.18821</guid><pubDate>Mon, 09 Jun 2025 00:00:00 +0000</pubDate></item><item><title>FDLLM: A Dedicated Detector for Black-Box LLMs Fingerprinting</title><link>https://arxiv.org/abs/2501.16029</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces FDLLM, a novel method for fingerprinting and identifying the source of black-box LLM-generated text.&lt;/li&gt;&lt;li&gt;Presents FD-Dataset, a large bilingual benchmark for LLM fingerprinting, covering 20 proprietary and open-source models.&lt;/li&gt;&lt;li&gt;Demonstrates robustness of FDLLM against adversarial attacks such as text polishing, translation, and synonym substitution.&lt;/li&gt;&lt;li&gt;Addresses accountability, governance, and security challenges related to black-box LLM APIs.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research Paper&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: Zhiyuan Fu, Junfan Chen, Lan Zhang, Ting Yang, Jun Niu, Hongyu Sun, Ruidong Li, Peng Liu, Yuqing Zhang&lt;/li&gt;&lt;li&gt;Tags: LLM fingerprinting, model attribution, adversarial robustness, AI security, black-box models&lt;/li&gt;&lt;li&gt;Relevance Score: 5/5&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2501.16029</guid><pubDate>Mon, 09 Jun 2025 00:00:00 +0000</pubDate></item><item><title>A Cognac shot to forget bad memories: Corrective Unlearning in GNNs</title><link>https://arxiv.org/abs/2412.00789</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Addresses the problem of removing the effects of adversarial manipulations or incorrect data in trained Graph Neural Networks (GNNs) through corrective unlearning.&lt;/li&gt;&lt;li&gt;Proposes a new method, Cognac, that can effectively unlearn the impact of manipulated data even when only a small portion of the manipulation set is identified.&lt;/li&gt;&lt;li&gt;Demonstrates that Cognac outperforms existing graph unlearning methods and is more efficient than retraining from scratch.&lt;/li&gt;&lt;li&gt;Aims to help GNN developers mitigate harmful effects from data issues post-training, which is relevant for security and robustness.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research Paper&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Quality Assessment&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Clarity: 5/5&lt;/li&gt;&lt;li&gt;Novelty: 5/5&lt;/li&gt;&lt;li&gt;Significance: 4/5&lt;/li&gt;&lt;li&gt;Try-worthiness: Yes&lt;/li&gt;&lt;li&gt;Justification: The abstract is well-written, clearly explaining the motivation, problem, and contributions. The work addresses the novel and timely problem of corrective unlearning in GNNs, which is important given the propagation of errors in graph data. The proposed method, Cognac, claims significant improvements over existing approaches and even outperforms retraining in some cases, which is both novel and practically significant. While the paper is very recent and has no citations yet, its potential impact is high, especially for practitioners dealing with real-world graph data. The availability of code further increases its try-worthiness.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: Varshita Kolipaka, Akshit Sinha, Debangan Mishra, Sumit Kumar, Arvindh Arun, Shashwat Goel, Ponnurangam Kumaraguru&lt;/li&gt;&lt;li&gt;Tags: unlearning, adversarial robustness, graph neural networks, data poisoning, AI security&lt;/li&gt;&lt;li&gt;Relevance Score: 4/5&lt;/li&gt;&lt;li&gt;Code Repository: &lt;a href='https://github.com/cognac-gnn-unlearning/corrective-unlearning-for-gnns'&gt;https://github.com/cognac-gnn-unlearning/corrective-unlearning-for-gnns&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2412.00789</guid><pubDate>Mon, 09 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Who Can Withstand Chat-Audio Attacks? An Evaluation Benchmark for Large Audio-Language Models</title><link>https://arxiv.org/abs/2411.14842</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces the Chat-Audio Attacks (CAA) benchmark to evaluate vulnerabilities of large audio-language models (LALMs) to adversarial audio attacks.&lt;/li&gt;&lt;li&gt;Defines and tests four distinct types of audio attacks in conversational scenarios.&lt;/li&gt;&lt;li&gt;Proposes three evaluation strategies: standard metrics, GPT-4o-based evaluation, and human evaluation.&lt;/li&gt;&lt;li&gt;Benchmarks six state-of-the-art LALMs, revealing varying levels of robustness, with GPT-4o being the most resilient.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Quality Assessment&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Clarity: 4/5&lt;/li&gt;&lt;li&gt;Novelty: 4/5&lt;/li&gt;&lt;li&gt;Significance: 4/5&lt;/li&gt;&lt;li&gt;Try-worthiness: Yes&lt;/li&gt;&lt;li&gt;Justification: The paper is clearly written, with a well-structured abstract that outlines the motivation, contributions, and evaluation strategies. The introduction of a benchmark (CAA) for adversarial audio attacks on large audio-language models is novel and timely, given the increasing deployment of such models. The work is significant as it addresses a real-world security concern and provides a comprehensive evaluation framework, including human and GPT-4o-based assessments. Although it is a preprint and very recent (hence no citations yet), the topic and methodology suggest it will be impactful. The availability of data/code further increases its try-worthiness for researchers and practitioners interested in robustness and security of LALMs.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: Wanqi Yang, Yanda Li, Meng Fang, Yunchao Wei, Ling Chen&lt;/li&gt;&lt;li&gt;Tags: adversarial attacks, audio-language models, robustness evaluation, AI security, benchmarking&lt;/li&gt;&lt;li&gt;Relevance Score: 5/5&lt;/li&gt;&lt;li&gt;Code Repository: &lt;a href='https://github.com/crystraldo/CAA'&gt;https://github.com/crystraldo/CAA&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2411.14842</guid><pubDate>Mon, 09 Jun 2025 00:00:00 +0000</pubDate></item><item><title>CLIPErase: Efficient Unlearning of Visual-Textual Associations in CLIP</title><link>https://arxiv.org/abs/2410.23330</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes CLIPErase, a method for efficient machine unlearning in multimodal models like CLIP.&lt;/li&gt;&lt;li&gt;Focuses on selectively forgetting specific visual-textual associations without full retraining.&lt;/li&gt;&lt;li&gt;Ensures that unlearning does not degrade performance on retained data through dedicated modules.&lt;/li&gt;&lt;li&gt;Demonstrates effectiveness on standard datasets and tasks, showing practical unlearning in zero-shot multimodal settings.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research Paper&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Quality Assessment&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Clarity: 5/5&lt;/li&gt;&lt;li&gt;Novelty: 5/5&lt;/li&gt;&lt;li&gt;Significance: 4/5&lt;/li&gt;&lt;li&gt;Try-worthiness: Yes&lt;/li&gt;&lt;li&gt;Justification: The abstract is well-written and clearly explains the motivation, methodology, and results of the paper, making it easy to understand (Clarity: 5). The work addresses the underexplored problem of machine unlearning in multimodal models, specifically CLIP, and introduces a novel framework (CLIPErase) with distinct modules for forgetting, retention, and consistency, which is highly novel (Novelty: 5). While the paper is very recent and has no citations yet, the problem is timely and relevant, and the use of standard datasets and multiple downstream tasks adds to its significance (Significance: 4). Given the practical importance of unlearning in deployed models and the promising results reported, the paper is worth trying out or implementing (Try-worthiness: true). No code repository link is provided in the metadata or abstract (Code repository: '').&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: Tianyu Yang, Lisen Dai, Xiangqi Wang, Minhao Cheng, Yapeng Tian, Xiangliang Zhang&lt;/li&gt;&lt;li&gt;Tags: machine unlearning, multimodal models, CLIP, data privacy, model forgetting&lt;/li&gt;&lt;li&gt;Relevance Score: 4/5&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2410.23330</guid><pubDate>Mon, 09 Jun 2025 00:00:00 +0000</pubDate></item><item><title>PoisonBench: Assessing Large Language Model Vulnerability to Data Poisoning</title><link>https://arxiv.org/abs/2410.08811</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces PoisonBench, a benchmark for evaluating LLM vulnerability to data poisoning during preference learning.&lt;/li&gt;&lt;li&gt;Assesses 21 widely-used models across eight realistic poisoning scenarios using two distinct attack types.&lt;/li&gt;&lt;li&gt;Finds that larger model size does not guarantee increased resilience to poisoning, and that attack effects generalize beyond poisoned data.&lt;/li&gt;&lt;li&gt;Highlights the need for more robust defenses against data poisoning and malicious manipulation in LLMs.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Quality Assessment&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Clarity: 5/5&lt;/li&gt;&lt;li&gt;Novelty: 4/5&lt;/li&gt;&lt;li&gt;Significance: 4/5&lt;/li&gt;&lt;li&gt;Try-worthiness: Yes&lt;/li&gt;&lt;li&gt;Justification: The abstract is well-written, clearly outlining the motivation, methodology, and key findings of the paper. The introduction of PoisonBench as a benchmark for evaluating LLM vulnerability to data poisoning in preference learning is a novel and timely contribution, especially given the increasing reliance on LLMs and the importance of alignment. The findings—such as the lack of increased resilience with larger models and the generalization of poisoning effects—are significant for both researchers and practitioners concerned with LLM safety. Although the paper is very recent and has no citations yet, its relevance and the urgency of the problem it addresses make it worth experimenting with. No code repository is provided in the metadata.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: Tingchen Fu, Mrinank Sharma, Philip Torr, Shay B. Cohen, David Krueger, Fazl Barez&lt;/li&gt;&lt;li&gt;Tags: data poisoning, LLM security, preference learning, robustness, benchmarking&lt;/li&gt;&lt;li&gt;Relevance Score: 5/5&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2410.08811</guid><pubDate>Mon, 09 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Certification for Differentially Private Prediction in Gradient-Based Training</title><link>https://arxiv.org/abs/2406.13433</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a novel method for certifying differential privacy in model predictions by computing dataset-specific sensitivity bounds.&lt;/li&gt;&lt;li&gt;Utilizes convex relaxation and bound propagation to obtain tighter sensitivity estimates compared to traditional global sensitivity approaches.&lt;/li&gt;&lt;li&gt;Demonstrates improved privacy-utility trade-offs in private prediction tasks across medical image classification and NLP datasets.&lt;/li&gt;&lt;li&gt;Supports the development of more effective privacy-preserving technologies for AI systems.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research Paper&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Quality Assessment&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Clarity: 4/5&lt;/li&gt;&lt;li&gt;Novelty: 5/5&lt;/li&gt;&lt;li&gt;Significance: 4/5&lt;/li&gt;&lt;li&gt;Try-worthiness: Yes&lt;/li&gt;&lt;li&gt;Justification: The abstract is well-written and clearly explains the motivation, method, and results, though there is a minor grammatical error ('are can be orders of magnitude'). The approach is novel, introducing dataset-specific sensitivity bounds for differentially private prediction using convex relaxation and bound propagation, which is a significant improvement over global sensitivity methods. The work is significant due to its potential impact on privacy-preserving machine learning, especially in sensitive domains like medical imaging and NLP. While it is a preprint and has no citations yet (expected for a very recent paper), the technical contribution and experimental validation make it worth trying for researchers and practitioners interested in differential privacy. No code repository is provided in the metadata.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: Matthew Wicker, Philip Sosnin, Igor Shilov, Adrianna Janik, Mark N. M\"uller, Yves-Alexandre de Montjoye, Adrian Weller, Calvin Tsay&lt;/li&gt;&lt;li&gt;Tags: differential privacy, privacy-preserving AI, model sensitivity, privacy certification&lt;/li&gt;&lt;li&gt;Relevance Score: 4/5&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2406.13433</guid><pubDate>Mon, 09 Jun 2025 00:00:00 +0000</pubDate></item><item><title>LlavaGuard: An Open VLM-based Framework for Safeguarding Vision Datasets and Models</title><link>https://arxiv.org/abs/2406.05113</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces LlavaGuard, an open framework for safeguarding vision datasets and models using Vision-Language Models (VLMs).&lt;/li&gt;&lt;li&gt;Presents a customizable safety taxonomy and a multimodal safety dataset with expert annotations for training and evaluation.&lt;/li&gt;&lt;li&gt;Demonstrates the framework's effectiveness in evaluating and enforcing safety compliance of visual content against flexible policies.&lt;/li&gt;&lt;li&gt;Shows real-world applications in large-scale dataset annotation and moderation of text-to-image models.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research Paper&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Quality Assessment&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Clarity: 5/5&lt;/li&gt;&lt;li&gt;Novelty: 4/5&lt;/li&gt;&lt;li&gt;Significance: 4/5&lt;/li&gt;&lt;li&gt;Try-worthiness: Yes&lt;/li&gt;&lt;li&gt;Justification: The abstract is well-written and clearly outlines the motivation, contributions, and results of the paper, including the introduction of a new open framework (LlavaGuard) for vision dataset and model safety. The work appears novel in its comprehensive approach—combining a customizable taxonomy, a new multimodal safety dataset with expert annotations, and advanced augmentations for context-specific safety assessment. The significance is high given the increasing importance of safety in large-scale vision models and datasets, and the open release of models, data, and code increases its impact. While the paper is very recent and has no citations yet, its comprehensive scope and practical applications (dataset annotation, model moderation) make it worth trying for researchers and practitioners in the field. No code repository URL is provided in the metadata, but the abstract claims the code and data are released.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: Lukas Helff, Felix Friedrich, Manuel Brack, Kristian Kersting, Patrick Schramowski&lt;/li&gt;&lt;li&gt;Tags: AI safety, content moderation, vision-language models, dataset safeguarding, safety evaluation&lt;/li&gt;&lt;li&gt;Relevance Score: 5/5&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2406.05113</guid><pubDate>Mon, 09 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Rethinking Machine Unlearning in Image Generation Models</title><link>https://arxiv.org/abs/2506.02761</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Addresses machine unlearning in image generation models, a key aspect of data privacy and content safety.&lt;/li&gt;&lt;li&gt;Identifies critical flaws in current unlearning algorithms and evaluation standards for image generation models.&lt;/li&gt;&lt;li&gt;Proposes CatIGMU, a hierarchical task categorization framework, and EvalIGMU, a comprehensive evaluation framework for unlearning.&lt;/li&gt;&lt;li&gt;Introduces DataIGM, a dataset for benchmarking and evaluating unlearning algorithms in image generation.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Quality Assessment&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Clarity: 5/5&lt;/li&gt;&lt;li&gt;Novelty: 5/5&lt;/li&gt;&lt;li&gt;Significance: 4/5&lt;/li&gt;&lt;li&gt;Try-worthiness: Yes&lt;/li&gt;&lt;li&gt;Justification: The abstract is well-written, clearly outlining the motivation, gaps in current research, and the specific contributions (CatIGMU, EvalIGMU, DataIGM). The work is highly novel, as it addresses fundamental issues in machine unlearning for image generation models, introduces new frameworks and datasets, and provides a comprehensive evaluation. While the paper is a preprint and very recent (hence no citations yet), the topic is timely and significant given the growing concerns around data privacy and content safety in generative models. The release of code and datasets further increases its practical value and try-worthiness for researchers and practitioners interested in unlearning. The code repository is provided, making it easy to experiment with the proposed methods.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: Renyang Liu, Wenjie Feng, Tianwei Zhang, Wei Zhou, Xueqi Cheng, See-Kiong Ng&lt;/li&gt;&lt;li&gt;Tags: machine unlearning, data privacy, content safety, evaluation frameworks, image generation models&lt;/li&gt;&lt;li&gt;Relevance Score: 4/5&lt;/li&gt;&lt;li&gt;Code Repository: &lt;a href='https://github.com/ryliu68/IGMU'&gt;https://github.com/ryliu68/IGMU&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.02761</guid><pubDate>Mon, 09 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Vehicle: Bridging the Embedding Gap in the Verification of Neuro-Symbolic Programs</title><link>https://arxiv.org/abs/2401.06379</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces Vehicle, an intermediate language for specifying and verifying properties of neuro-symbolic programs.&lt;/li&gt;&lt;li&gt;Addresses the challenge of verifying programs that combine neural network and symbolic components, focusing on the 'embedding gap' between them.&lt;/li&gt;&lt;li&gt;Demonstrates formal verification of safety properties for an autonomous car controlled by a neural network in a stochastic environment.&lt;/li&gt;&lt;li&gt;Facilitates safe compilation of specifications to various verification tools, supporting robust safety evaluation.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research Paper&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: Matthew L. Daggitt, Wen Kokke, Robert Atkey, Ekaterina Komendantskaya, Natalia Slusarz, Luca Arnaboldi&lt;/li&gt;&lt;li&gt;Tags: AI safety, formal verification, neuro-symbolic systems, autonomous systems, robustness&lt;/li&gt;&lt;li&gt;Relevance Score: 4/5&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2401.06379</guid><pubDate>Mon, 09 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Distillation Robustifies Unlearning</title><link>https://arxiv.org/abs/2506.06278</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Investigates the robustness of current LLM unlearning methods and finds them vulnerable to reversion via finetuning.&lt;/li&gt;&lt;li&gt;Proposes a new method, UNDO, that uses distillation to robustify unlearning and prevent the recovery of undesired capabilities.&lt;/li&gt;&lt;li&gt;Demonstrates that UNDO achieves robustness comparable to retraining from scratch, but with significantly reduced compute and data requirements.&lt;/li&gt;&lt;li&gt;Validates the approach on synthetic tasks and the Weapons of Mass Destruction Proxy (WMDP) benchmark, showing improved security in capability removal.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research Paper&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Quality Assessment&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Clarity: 5/5&lt;/li&gt;&lt;li&gt;Novelty: 5/5&lt;/li&gt;&lt;li&gt;Significance: 4/5&lt;/li&gt;&lt;li&gt;Try-worthiness: Yes&lt;/li&gt;&lt;li&gt;Justification: The abstract is very clearly written, concisely explaining the problem (lack of robustness in LLM unlearning), the key insight (distillation robustifies unlearning), and the proposed method (UNDO). The work appears highly novel, introducing a new approach to robust unlearning via distillation and noise, and establishing a new Pareto frontier on relevant tasks. While the paper is very recent and thus has no citations yet, the problem addressed is significant for both research and practical deployment of LLMs, especially given the increasing importance of data removal and model editing. The method is described as scalable and practical, making it worth trying for those interested in robust unlearning. No code repository is provided in the metadata or abstract.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: Bruce W. Lee, Addie Foote, Alex Infanger, Leni Shor, Harish Kamath, Jacob Goldman-Wetzler, Bryce Woodworth, Alex Cloud, Alexander Matt Turner&lt;/li&gt;&lt;li&gt;Tags: AI unlearning, model robustness, capability removal, LLM security, distillation&lt;/li&gt;&lt;li&gt;Relevance Score: 5/5&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.06278</guid><pubDate>Mon, 09 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Joint-GCG: Unified Gradient-Based Poisoning Attacks on Retrieval-Augmented Generation Systems</title><link>https://arxiv.org/abs/2506.06151</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes Joint-GCG, a unified gradient-based poisoning attack targeting both retriever and generator components in Retrieval-Augmented Generation (RAG) systems.&lt;/li&gt;&lt;li&gt;Introduces novel techniques for aligning embedding spaces and synchronizing gradient signals across models.&lt;/li&gt;&lt;li&gt;Demonstrates significantly higher attack success rates compared to previous poisoning methods, with strong transferability to unseen models.&lt;/li&gt;&lt;li&gt;Highlights fundamental vulnerabilities in RAG systems due to their reliance on external knowledge sources.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research Paper&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Quality Assessment&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Clarity: 5/5&lt;/li&gt;&lt;li&gt;Novelty: 5/5&lt;/li&gt;&lt;li&gt;Significance: 4/5&lt;/li&gt;&lt;li&gt;Try-worthiness: Yes&lt;/li&gt;&lt;li&gt;Justification: The abstract is well-written, clearly explaining the motivation, contributions, and results of the work. The paper introduces a novel unified gradient-based poisoning attack framework (Joint-GCG) for RAG systems, which is a timely and important topic given the increasing adoption of RAG architectures. The three technical innovations are clearly described and the reported improvements over prior work are substantial. While the paper is very new and thus has no citations yet, its significance is high due to the growing importance of RAG security. The availability of code further increases its try-worthiness for researchers and practitioners interested in adversarial robustness of LLMs.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: Haowei Wang, Rupeng Zhang, Junjie Wang, Mingyang Li, Yuekai Huang, Dandan Wang, Qing Wang&lt;/li&gt;&lt;li&gt;Tags: data poisoning, retrieval-augmented generation, AI security, adversarial attacks, model robustness&lt;/li&gt;&lt;li&gt;Relevance Score: 5/5&lt;/li&gt;&lt;li&gt;Code Repository: &lt;a href='https://github.com/NicerWang/Joint-GCG'&gt;https://github.com/NicerWang/Joint-GCG&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.06151</guid><pubDate>Mon, 09 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Towards Lifecycle Unlearning Commitment Management: Measuring Sample-level Unlearning Completeness</title><link>https://arxiv.org/abs/2506.06112</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a new framework (IAM) for measuring the completeness of machine unlearning at the sample level.&lt;/li&gt;&lt;li&gt;Addresses limitations of current Membership Inference Attacks (MIAs) in evaluating unlearning, particularly their computational cost and lack of granularity.&lt;/li&gt;&lt;li&gt;Demonstrates that IAM can efficiently and effectively assess both exact and approximate unlearning, including in large language models.&lt;/li&gt;&lt;li&gt;Highlights security and privacy risks such as over-unlearning and under-unlearning, emphasizing the need for robust safeguards in unlearning systems.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research Paper&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Quality Assessment&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Clarity: 5/5&lt;/li&gt;&lt;li&gt;Novelty: 5/5&lt;/li&gt;&lt;li&gt;Significance: 4/5&lt;/li&gt;&lt;li&gt;Try-worthiness: Yes&lt;/li&gt;&lt;li&gt;Justification: The abstract is well-written, clearly outlining the motivation, limitations of existing methods, and the proposed solution (IAM). The work is novel, introducing a new framework specifically designed for unlearning inference, which addresses both computational and granularity limitations of current Membership Inference Attacks. The significance is high given the growing importance of machine unlearning for privacy and compliance, and the paper's applicability to large language models (LLMs) is particularly timely. While the paper is very recent and has no citations yet, its relevance and the availability of code make it worth experimenting with.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: Cheng-Long Wang, Qi Li, Zihang Xiang, Yinzhi Cao, Di Wang&lt;/li&gt;&lt;li&gt;Tags: machine unlearning, privacy attacks, membership inference, AI security, model auditing&lt;/li&gt;&lt;li&gt;Relevance Score: 5/5&lt;/li&gt;&lt;li&gt;Code Repository: &lt;a href='https://github.com/Happy2Git/Unlearning_Inference_IAM'&gt;https://github.com/Happy2Git/Unlearning_Inference_IAM&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.06112</guid><pubDate>Mon, 09 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Simple Yet Effective: Extracting Private Data Across Clients in Federated Fine-Tuning of Large Language Models</title><link>https://arxiv.org/abs/2506.06060</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces new extraction attack algorithms targeting federated fine-tuning of large language models (FedLLMs).&lt;/li&gt;&lt;li&gt;Demonstrates that attackers with access to a single client's data can extract personally identifiable information (PII) from other clients.&lt;/li&gt;&lt;li&gt;Proposes new evaluation metrics and a benchmark dataset with PII annotations aligned with privacy regulations (CPIS, GDPR, CCPA).&lt;/li&gt;&lt;li&gt;Finds significant vulnerability in FedLLMs, highlighting the need for improved privacy-preserving techniques.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research Paper&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Quality Assessment&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Clarity: 5/5&lt;/li&gt;&lt;li&gt;Novelty: 4/5&lt;/li&gt;&lt;li&gt;Significance: 4/5&lt;/li&gt;&lt;li&gt;Try-worthiness: Yes&lt;/li&gt;&lt;li&gt;Justification: The abstract is well-written, clearly outlining the motivation, methodology, and findings. The work is novel in that it proposes new extraction attack algorithms tailored for federated fine-tuning of LLMs under a more realistic threat model (single-client access), which is a step beyond prior verbatim attacks. The introduction of new metrics and a benchmark dataset with PII annotations further adds to its novelty and utility. While the paper is very recent and thus has no citations yet, the topic is highly significant given the growing use of federated learning and privacy concerns in LLMs. The results (e.g., extraction of up to 56.57% of victim-exclusive PII) are impactful and highlight urgent privacy risks, making the work worth experimenting with for both researchers and practitioners in privacy-preserving ML. No code repository is provided in the metadata.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: Yingqi Hu, Zhuo Zhang, Jingyuan Zhang, Lizhen Qu, Zenglin Xu&lt;/li&gt;&lt;li&gt;Tags: privacy attacks, federated learning, data extraction, large language models, PII leakage&lt;/li&gt;&lt;li&gt;Relevance Score: 5/5&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.06060</guid><pubDate>Mon, 09 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Hey, That's My Data! Label-Only Dataset Inference in Large Language Models</title><link>https://arxiv.org/abs/2506.06057</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces CatShift, a label-only dataset inference framework for LLMs that does not require access to model logits.&lt;/li&gt;&lt;li&gt;Leverages catastrophic forgetting to detect whether a suspicious dataset was used in the model's training data.&lt;/li&gt;&lt;li&gt;Demonstrates effectiveness of the approach on both open-source and API-based LLMs, addressing scenarios where internal model signals are inaccessible.&lt;/li&gt;&lt;li&gt;Provides a practical method for identifying unauthorized use of proprietary data in LLM training.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research Paper&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Quality Assessment&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Clarity: 5/5&lt;/li&gt;&lt;li&gt;Novelty: 5/5&lt;/li&gt;&lt;li&gt;Significance: 4/5&lt;/li&gt;&lt;li&gt;Try-worthiness: Yes&lt;/li&gt;&lt;li&gt;Justification: The paper is very clearly written, with a well-structured abstract that explains both the motivation and the proposed method (CatShift). The novelty is high: it addresses a pressing and under-explored problem—dataset membership inference for LLMs when logit access is unavailable—by leveraging catastrophic forgetting, a creative and timely approach. The significance is strong given the increasing importance of data provenance and copyright in LLMs, though as a preprint with no citations yet, its impact is still to be seen. The method is practical and relevant for both researchers and practitioners concerned with data misuse in LLMs, making it worth trying. No code repository is provided in the metadata.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: Chen Xiong, Zihao Wang, Rui Zhu, Tsung-Yi Ho, Pin-Yu Chen, Jingwei Xiong, Haixu Tang, Lucila Ohno-Machado&lt;/li&gt;&lt;li&gt;Tags: membership inference, privacy attacks, dataset inference, large language models, data protection&lt;/li&gt;&lt;li&gt;Relevance Score: 5/5&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.06057</guid><pubDate>Mon, 09 Jun 2025 00:00:00 +0000</pubDate></item><item><title>When to Trust Context: Self-Reflective Debates for Context Reliability</title><link>https://arxiv.org/abs/2506.06020</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces SR-DCR, a framework using self-reflective debate among LLM agents to assess the reliability of contextual information.&lt;/li&gt;&lt;li&gt;Combines token-level self-confidence with multi-agent debate to resolve conflicts between model knowledge and provided context.&lt;/li&gt;&lt;li&gt;Aims to enhance robustness against misleading or adversarial context, reducing factual inconsistencies and hallucinations.&lt;/li&gt;&lt;li&gt;Demonstrates improved performance on the ClashEval benchmark, outperforming existing debate and confidence-based methods.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research Paper&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Quality Assessment&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Clarity: 5/5&lt;/li&gt;&lt;li&gt;Novelty: 5/5&lt;/li&gt;&lt;li&gt;Significance: 4/5&lt;/li&gt;&lt;li&gt;Try-worthiness: Yes&lt;/li&gt;&lt;li&gt;Justification: The abstract is well-written, clearly outlining the problem (conflicts between parametric knowledge and context in LLMs), the proposed solution (SR-DCR framework with self-confidence and asymmetric debate), and the results (improved robustness and accuracy on ClashEval). The approach appears novel, combining token-level self-confidence with a structured debate mechanism for context reliability, which is not standard in current literature. While the paper is very recent and has no citations yet, the problem addressed is significant for LLM reliability, and the reported improvements over strong baselines suggest practical value. The availability of code further increases its try-worthiness.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: Zeqi Zhou, Fang Wu, Shayan Talaei, Haokai Zhao, Cheng Meixin, Tinson Xu, Amin Saberi, Yejin Choi&lt;/li&gt;&lt;li&gt;Tags: context reliability, robustness, adversarial prompting, AI safety&lt;/li&gt;&lt;li&gt;Relevance Score: 4/5&lt;/li&gt;&lt;li&gt;Code Repository: &lt;a href='https://github.com/smiles724/Self-Reflective-Debates'&gt;https://github.com/smiles724/Self-Reflective-Debates&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.06020</guid><pubDate>Mon, 09 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Optimization-Free Universal Watermark Forgery with Regenerative Diffusion Models</title><link>https://arxiv.org/abs/2506.06018</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces an optimization-free attack (PnP) that forges watermarks onto images generated by diffusion models.&lt;/li&gt;&lt;li&gt;Demonstrates that the attack works universally, regardless of the watermarking model or image origin.&lt;/li&gt;&lt;li&gt;Shows that the attack achieves high watermark detectability and user attribution, challenging the reliability of current watermarking schemes.&lt;/li&gt;&lt;li&gt;Highlights significant security risks for watermark-based provenance and governance in synthetic image generation.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research Paper&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Quality Assessment&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Clarity: 4/5&lt;/li&gt;&lt;li&gt;Novelty: 5/5&lt;/li&gt;&lt;li&gt;Significance: 4/5&lt;/li&gt;&lt;li&gt;Try-worthiness: Yes&lt;/li&gt;&lt;li&gt;Justification: The abstract is well-written, clearly outlining the motivation, method (PnP), and results of the work. The paper introduces an optimization-free, universal watermark forgery attack leveraging regenerative diffusion models, which appears highly novel compared to prior adversarial optimization-based attacks. The significance is high given the implications for the security of watermarking in AI-generated images, though the impact is yet to be measured due to its recent release and preprint status. The method's broad applicability and strong reported results make it worth experimenting with, especially for researchers in AI security and watermarking. No code repository is provided in the metadata.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: Chaoyi Zhu, Zaitang Li, Renyi Yang, Robert Birke, Pin-Yu Chen, Tsung-Yi Ho, Lydia Y. Chen&lt;/li&gt;&lt;li&gt;Tags: watermark forgery, diffusion models, AI security, adversarial attacks, synthetic media provenance&lt;/li&gt;&lt;li&gt;Relevance Score: 5/5&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.06018</guid><pubDate>Mon, 09 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Quantifying Adversarial Uncertainty in Evidential Deep Learning using Conflict Resolution</title><link>https://arxiv.org/abs/2506.05937</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes Conflict-aware Evidential Deep Learning (C-EDL), a method to improve uncertainty quantification in deep learning models.&lt;/li&gt;&lt;li&gt;Addresses the vulnerability of Evidential Deep Learning (EDL) to adversarial and out-of-distribution (OOD) inputs.&lt;/li&gt;&lt;li&gt;C-EDL enhances robustness against adversarial attacks and OOD data by calibrating uncertainty estimates using representational disagreement.&lt;/li&gt;&lt;li&gt;Demonstrates significant improvements in detecting adversarial and OOD inputs across multiple datasets and attack types.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research Paper&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Quality Assessment&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Clarity: 5/5&lt;/li&gt;&lt;li&gt;Novelty: 4/5&lt;/li&gt;&lt;li&gt;Significance: 4/5&lt;/li&gt;&lt;li&gt;Try-worthiness: Yes&lt;/li&gt;&lt;li&gt;Justification: The abstract is well-written, clearly outlining the motivation, method (C-EDL), and results. The approach of post-hoc uncertainty calibration via conflict resolution in Evidential Deep Learning appears novel, especially as it avoids retraining and claims significant improvements in adversarial and OOD robustness. While the paper is very recent and has no citations yet, the problem addressed is highly significant for safety-critical applications. The described improvements over state-of-the-art methods and low computational overhead make it worth trying for practitioners interested in robust uncertainty quantification. No code repository is provided in the metadata or abstract.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: Charmaine Barker, Daniel Bethell, Simos Gerasimou&lt;/li&gt;&lt;li&gt;Tags: adversarial robustness, uncertainty quantification, out-of-distribution detection, deep learning security&lt;/li&gt;&lt;li&gt;Relevance Score: 5/5&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.05937</guid><pubDate>Mon, 09 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Small Models, Big Support: A Local LLM Framework for Teacher-Centric Content Creation and Assessment using RAG and CAG</title><link>https://arxiv.org/abs/2506.05925</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Presents an open-source framework for deploying small, locally hosted LLMs for educator support, emphasizing privacy and affordability.&lt;/li&gt;&lt;li&gt;Incorporates an auxiliary LLM verifier specifically designed to mitigate jailbreaking risks, enhancing the safety and reliability of generated content.&lt;/li&gt;&lt;li&gt;Utilizes Retrieval and Context Augmented Generation (RAG/CAG) to ensure factual accuracy and pedagogical relevance.&lt;/li&gt;&lt;li&gt;Demonstrates practical deployment and evaluation in a real-world educational setting, focusing on robust and safe AI system design.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research Paper&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Quality Assessment&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Clarity: 5/5&lt;/li&gt;&lt;li&gt;Novelty: 4/5&lt;/li&gt;&lt;li&gt;Significance: 4/5&lt;/li&gt;&lt;li&gt;Try-worthiness: Yes&lt;/li&gt;&lt;li&gt;Justification: The abstract is well-written, clearly outlining the motivation, approach, and contributions of the paper. The focus on small, locally deployable LLMs for teacher-centric content creation and assessment is a relatively novel direction, especially with the integration of RAG/CAG and an auxiliary verifier for safety. The significance is promising given the practical need for affordable, privacy-preserving educational tools, though the impact is yet to be seen due to the paper's recency and preprint status. The framework appears practical and addresses real-world constraints, making it worth experimenting with, especially for educational institutions with privacy or budget concerns. No code repository link is provided in the metadata.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: Zarreen Reza, Alexander Mazur, Michael T. Dugdale, Robin Ray-Chaudhuri&lt;/li&gt;&lt;li&gt;Tags: LLM jailbreaking mitigation, AI safety, Content filtering, On-premises deployment, Education AI security&lt;/li&gt;&lt;li&gt;Relevance Score: 4/5&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.05925</guid><pubDate>Mon, 09 Jun 2025 00:00:00 +0000</pubDate></item><item><title>DeepFake Doctor: Diagnosing and Treating Audio-Video Fake Detection</title><link>https://arxiv.org/abs/2506.05851</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Analyzes key challenges in benchmarking audio-video DeepFake detection, including dataset flaws and evaluation protocols.&lt;/li&gt;&lt;li&gt;Introduces a new evaluation protocol and benchmarks using the DeepSpeak v1 dataset.&lt;/li&gt;&lt;li&gt;Proposes a minimalistic multimodal baseline (SIMBA) for DeepFake detection.&lt;/li&gt;&lt;li&gt;Identifies and mitigates issues such as audio shortcuts in existing datasets.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Quality Assessment&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Clarity: 4/5&lt;/li&gt;&lt;li&gt;Novelty: 4/5&lt;/li&gt;&lt;li&gt;Significance: 4/5&lt;/li&gt;&lt;li&gt;Try-worthiness: Yes&lt;/li&gt;&lt;li&gt;Justification: The abstract is well-written and clearly outlines the motivation, challenges, and contributions of the paper, including the introduction of a new evaluation protocol, benchmarking with SOTA models, and a minimalistic baseline (SIMBA). The focus on reproducibility, dataset issues, and evaluation protocols addresses known gaps in the DeepFake detection literature, especially for multimodal (audio-video) scenarios. The work is novel in its critical analysis of benchmarking practices and its proposal of new protocols and baselines. While the paper is very recent and has no citations yet, the significance is high due to the importance of robust DeepFake detection and the practical impact of improved benchmarks and datasets. The paper appears worth implementing or experimenting with, especially for researchers or practitioners in multimedia security and AI ethics. No code repository is provided in the metadata.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: Marcel Klemt, Carlotta Segna, Anna Rohrbach&lt;/li&gt;&lt;li&gt;Tags: DeepFake detection, AI security, multimodal AI, benchmarking, dataset vulnerabilities&lt;/li&gt;&lt;li&gt;Relevance Score: 4/5&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.05851</guid><pubDate>Mon, 09 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Efficient Online RFT with Plug-and-Play LLM Judges: Unlocking State-of-the-Art Performance</title><link>https://arxiv.org/abs/2506.05748</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a cost-effective method for online reward model training in RLHF pipelines using a plug-and-play LLM judge with minimal parameter updates.&lt;/li&gt;&lt;li&gt;Demonstrates strong performance on benchmarks, including adversarial and safety-focused evaluation segments.&lt;/li&gt;&lt;li&gt;Introduces HH-Rationales dataset to examine interpretability and alignment with human justifications.&lt;/li&gt;&lt;li&gt;Highlights improvements in safety and adversarial robustness through prompt engineering and LoRA adapters.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research Paper&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: Rudransh Agnihotri, Ananya Pandey&lt;/li&gt;&lt;li&gt;Tags: RLHF, AI safety, adversarial robustness, reward modeling, alignment&lt;/li&gt;&lt;li&gt;Relevance Score: 4/5&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.05748</guid><pubDate>Mon, 09 Jun 2025 00:00:00 +0000</pubDate></item><item><title>When Better Features Mean Greater Risks: The Performance-Privacy Trade-Off in Contrastive Learning</title><link>https://arxiv.org/abs/2506.05743</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Systematically investigates privacy threats from membership inference attacks (MIAs) on encoder models in contrastive learning frameworks.&lt;/li&gt;&lt;li&gt;Finds that improved feature extraction performance in advanced encoder architectures increases the risk of privacy leakage.&lt;/li&gt;&lt;li&gt;Proposes a novel attack method (Embedding Lp-Norm Likelihood Attack, LpLA) that leverages feature vector norms to infer membership status.&lt;/li&gt;&lt;li&gt;Demonstrates that LpLA outperforms existing MIA methods, especially under limited attacker knowledge and queries.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research Paper&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Quality Assessment&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Clarity: 5/5&lt;/li&gt;&lt;li&gt;Novelty: 4/5&lt;/li&gt;&lt;li&gt;Significance: 4/5&lt;/li&gt;&lt;li&gt;Try-worthiness: Yes&lt;/li&gt;&lt;li&gt;Justification: The abstract is well-written, clearly outlining the motivation, methodology, and contributions of the paper. The work addresses a timely and important issue—the privacy risks in contrastive learning frameworks—by systematically analyzing membership inference attacks and introducing a novel attack method (LpLA) based on the p-norm of feature vectors. The novelty is strong, as the proposed attack appears to outperform existing methods, especially under realistic constraints. While the paper is a preprint and very recent (hence no citations yet), the topic is highly relevant to both privacy and representation learning communities, and the results could have significant implications for the design of future encoder models. The availability of code further increases the try-worthiness, making it practical for researchers to experiment with the proposed method.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: Ruining Sun, Hongsheng Hu, Wei Luo, Zhaoxi Zhang, Yanjun Zhang, Haizhuan Yuan, Leo Yu Zhang&lt;/li&gt;&lt;li&gt;Tags: membership inference attack, privacy attacks, contrastive learning, encoder models, data privacy&lt;/li&gt;&lt;li&gt;Relevance Score: 5/5&lt;/li&gt;&lt;li&gt;Code Repository: &lt;a href='https://github.com/SeroneySun/LpLA_code'&gt;https://github.com/SeroneySun/LpLA_code&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.05743</guid><pubDate>Mon, 09 Jun 2025 00:00:00 +0000</pubDate></item><item><title>To Protect the LLM Agent Against the Prompt Injection Attack with Polymorphic Prompt</title><link>https://arxiv.org/abs/2506.05739</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a novel defense mechanism (Polymorphic Prompt Assembling) to protect LLM agents from prompt injection attacks.&lt;/li&gt;&lt;li&gt;PPA works by dynamically varying the structure of system prompts, making it harder for attackers to predict and exploit prompt structure.&lt;/li&gt;&lt;li&gt;Experimental evaluation demonstrates the effectiveness of PPA against prompt injection compared to existing defenses.&lt;/li&gt;&lt;li&gt;The method is lightweight and introduces minimal overhead, aiming for practical deployment.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research Paper&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Quality Assessment&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Clarity: 4/5&lt;/li&gt;&lt;li&gt;Novelty: 4/5&lt;/li&gt;&lt;li&gt;Significance: 3/5&lt;/li&gt;&lt;li&gt;Try-worthiness: Yes&lt;/li&gt;&lt;li&gt;Justification: The abstract is clearly written, concisely outlining the problem (prompt injection attacks on LLM agents), the shortcomings of existing defenses, and the proposed solution (Polymorphic Prompt Assembling, PPA). The idea of dynamically varying system prompt structures to thwart attacks appears novel and lightweight compared to traditional methods. While the paper is very recent and has no citations yet, prompt injection is a significant and timely problem in LLM deployment, making the work potentially impactful. The approach seems practical and low-overhead, making it worth experimenting with. However, as it is only a preprint and lacks peer review or code, its significance is moderate for now.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: Zhilong Wang, Neha Nagaraja, Lan Zhang, Hayretdin Bahsi, Pawan Patil, Peng Liu&lt;/li&gt;&lt;li&gt;Tags: prompt injection, LLM security, adversarial prompting, defense mechanisms&lt;/li&gt;&lt;li&gt;Relevance Score: 5/5&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.05739</guid><pubDate>Mon, 09 Jun 2025 00:00:00 +0000</pubDate></item><item><title>SafeGenBench: A Benchmark Framework for Security Vulnerability Detection in LLM-Generated Code</title><link>https://arxiv.org/abs/2506.05692</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces SafeGenBench, a benchmark for detecting security vulnerabilities in code generated by large language models (LLMs).&lt;/li&gt;&lt;li&gt;Covers a wide range of software development scenarios and vulnerability types.&lt;/li&gt;&lt;li&gt;Proposes an automatic evaluation framework combining static application security testing (SAST) and LLM-based judging.&lt;/li&gt;&lt;li&gt;Empirically evaluates state-of-the-art LLMs, revealing deficiencies in generating secure code and providing insights for improving LLM security.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: Xinghang Li, Jingzhe Ding, Chao Peng, Bing Zhao, Xiang Gao, Hongwan Gao, Xinchen Gu&lt;/li&gt;&lt;li&gt;Tags: LLM security, code generation, vulnerability detection, benchmarking, secure AI&lt;/li&gt;&lt;li&gt;Relevance Score: 5/5&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.05692</guid><pubDate>Mon, 09 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Deployability-Centric Infrastructure-as-Code Generation: An LLM-based Iterative Framework</title><link>https://arxiv.org/abs/2506.05623</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes an LLM-based framework (IaCGen) for generating Infrastructure-as-Code (IaC) templates with a focus on deployability.&lt;/li&gt;&lt;li&gt;Introduces DPIaC-Eval, a benchmark evaluating IaC templates on syntax, deployment, user intent, and security compliance.&lt;/li&gt;&lt;li&gt;Finds that while iterative LLM prompting greatly improves deployability, security compliance remains a significant challenge (only 8.4% pass rate).&lt;/li&gt;&lt;li&gt;Highlights the need for further research in aligning LLM-generated IaC with security standards and user intent.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Quality Assessment&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Clarity: 4/5&lt;/li&gt;&lt;li&gt;Novelty: 5/5&lt;/li&gt;&lt;li&gt;Significance: 4/5&lt;/li&gt;&lt;li&gt;Try-worthiness: Yes&lt;/li&gt;&lt;li&gt;Justification: The abstract is well-written, clearly outlining the motivation, contributions, and results, though it is somewhat dense with technical terms. The work is highly novel, introducing both a deployability-centric LLM framework (IaCGen) and a new benchmark (DPIaC-Eval) for Infrastructure-as-Code generation, focusing on deployability rather than just syntax. The significance is high given the practical importance of IaC in cloud automation and the lack of prior deployability-focused evaluation, though the paper is very new and not yet peer-reviewed or cited. The dramatic improvement in deployment success rates and the identification of remaining challenges make this work worth trying for practitioners and researchers in the field. No code repository link is provided in the metadata.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: Tianyi Zhang, Shidong Pan, Zejun Zhang, Zhenchang Xing, Xiaoyu Sun&lt;/li&gt;&lt;li&gt;Tags: LLM security, infrastructure-as-code, security compliance, AI evaluation, cloud security&lt;/li&gt;&lt;li&gt;Relevance Score: 4/5&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.05623</guid><pubDate>Mon, 09 Jun 2025 00:00:00 +0000</pubDate></item><item><title>StealthInk: A Multi-bit and Stealthy Watermark for Large Language Models</title><link>https://arxiv.org/abs/2506.05502</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes StealthInk, a multi-bit watermarking scheme for LLM-generated text that embeds provenance data (e.g., userID, timestamp, modelID).&lt;/li&gt;&lt;li&gt;The watermark is designed to be stealthy, preserving the original text distribution and making detection difficult for adversaries.&lt;/li&gt;&lt;li&gt;The method enables traceability of AI-generated content without requiring access to the LLM's API or prompts.&lt;/li&gt;&lt;li&gt;Empirical evaluations demonstrate the watermark's stealthiness, detectability, and resilience against removal attempts.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research Paper&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Quality Assessment&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Clarity: 5/5&lt;/li&gt;&lt;li&gt;Novelty: 5/5&lt;/li&gt;&lt;li&gt;Significance: 4/5&lt;/li&gt;&lt;li&gt;Try-worthiness: Yes&lt;/li&gt;&lt;li&gt;Justification: The abstract is well-written, clearly outlining the motivation, limitations of prior work, and the contributions of StealthInk. The approach is novel in that it enables multi-bit watermarking (embedding provenance data) while preserving the original text distribution, addressing key limitations of prior watermarking methods for LLMs. The significance is high given the growing importance of provenance and traceability in AI-generated content, though the paper is very new and only on arXiv, so its impact is yet to be established. The method appears practical and broadly applicable, making it worth experimenting with. No code repository is provided in the metadata.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: Ya Jiang, Chuxiong Wu, Massieh Kordi Boroujeny, Brian Mark, Kai Zeng&lt;/li&gt;&lt;li&gt;Tags: LLM watermarking, AI provenance, AI security, Robustness, Traceability&lt;/li&gt;&lt;li&gt;Relevance Score: 4/5&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.05502</guid><pubDate>Mon, 09 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Interpretation Meets Safety: A Survey on Interpretation Methods and Tools for Improving LLM Safety</title><link>https://arxiv.org/abs/2506.05451</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Presents a comprehensive survey connecting interpretation methods with LLM safety.&lt;/li&gt;&lt;li&gt;Introduces a unified framework and taxonomy for safety-focused interpretation techniques.&lt;/li&gt;&lt;li&gt;Summarizes nearly 70 works at the intersection of interpretation and safety in LLMs.&lt;/li&gt;&lt;li&gt;Highlights open challenges and future directions for safer, more interpretable LLMs.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Survey/Review&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: Seongmin Lee, Aeree Cho, Grace C. Kim, ShengYun Peng, Mansi Phute, Duen Horng Chau&lt;/li&gt;&lt;li&gt;Tags: LLM safety, interpretability, safety evaluation, AI safety, survey&lt;/li&gt;&lt;li&gt;Relevance Score: 5/5&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.05451</guid><pubDate>Mon, 09 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Sentinel: SOTA model to protect against prompt injections</title><link>https://arxiv.org/abs/2506.05446</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces Sentinel, a detection model specifically designed to protect LLMs against prompt injection attacks.&lt;/li&gt;&lt;li&gt;Utilizes the ModernBERT-large architecture and is fine-tuned on a diverse dataset of prompt injection attacks and benign prompts.&lt;/li&gt;&lt;li&gt;Demonstrates state-of-the-art performance on both internal and public benchmarks, outperforming existing baselines.&lt;/li&gt;&lt;li&gt;Provides detailed methodology on dataset curation, model training, and evaluation for prompt injection detection.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research Paper&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Quality Assessment&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Clarity: 4/5&lt;/li&gt;&lt;li&gt;Novelty: 4/5&lt;/li&gt;&lt;li&gt;Significance: 4/5&lt;/li&gt;&lt;li&gt;Try-worthiness: Yes&lt;/li&gt;&lt;li&gt;Justification: The abstract is well-written and clearly outlines the motivation, methodology, and results of the paper, though some technical details are omitted (e.g., specifics of the dataset and architecture). The work appears novel, introducing a new model (Sentinel) for prompt injection detection, leveraging a modern architecture (ModernBERT-large) and a carefully curated dataset. The significance is high given the increasing importance of prompt injection attacks in LLMs, and the reported performance metrics (accuracy 0.987, F1 0.980) are strong, with the model outperforming established baselines. The paper is recent and on arXiv, so citation count is not yet meaningful, but the topic is timely and relevant. The mention of a model name (qualifire/prompt-injection-sentinel) suggests a possible code or model release, but no explicit code repository URL is provided in the metadata.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: Dror Ivry, Oran Nahum&lt;/li&gt;&lt;li&gt;Tags: prompt injection, LLM security, adversarial prompting, red teaming, robustness&lt;/li&gt;&lt;li&gt;Relevance Score: 5/5&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.05446</guid><pubDate>Mon, 09 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Efficient Robust Conformal Prediction via Lipschitz-Bounded Networks</title><link>https://arxiv.org/abs/2506.05434</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a new method for robust conformal prediction using Lipschitz-bounded neural networks.&lt;/li&gt;&lt;li&gt;Addresses the challenge of maintaining conformal prediction guarantees under adversarial attacks.&lt;/li&gt;&lt;li&gt;Demonstrates improved efficiency and robustness compared to state-of-the-art methods, especially on large-scale datasets.&lt;/li&gt;&lt;li&gt;Provides new theoretical bounds for conformal prediction coverage under adversarial scenarios.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research Paper&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: Thomas Massena (IRIT, DTIPG - SNCF, UT3), L\'eo and\'eol (IMT, DTIPG - SNCF, UT3), Thibaut Boissin (IRIT, UT3), Franck Mamalet (IRIT, UT3), Corentin Friedrich (IRIT, UT3), Mathieu Serrurier (IRIT, UT3), S\'ebastien Gerchinovitz (IMT)&lt;/li&gt;&lt;li&gt;Tags: robustness, adversarial attacks, conformal prediction, neural network security&lt;/li&gt;&lt;li&gt;Relevance Score: 5/5&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.05434</guid><pubDate>Mon, 09 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Robustness Evaluation for Video Models with Reinforcement Learning</title><link>https://arxiv.org/abs/2506.05431</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a multi-agent reinforcement learning approach to evaluate the robustness of video classification models.&lt;/li&gt;&lt;li&gt;Focuses on generating fine, visually imperceptible adversarial perturbations to induce misclassification.&lt;/li&gt;&lt;li&gt;Demonstrates improved performance over state-of-the-art methods in terms of perturbation size and query efficiency.&lt;/li&gt;&lt;li&gt;Evaluates the robustness of popular video action recognition models on standard datasets.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research Paper&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: Ashwin Ramesh Babu, Sajad Mousavi, Vineet Gundecha, Sahand Ghorbanpour, Avisek Naug, Antonio Guillen, Ricardo Luna Gutierrez, Soumyendu Sarkar&lt;/li&gt;&lt;li&gt;Tags: adversarial attacks, robustness evaluation, video models, reinforcement learning, AI security&lt;/li&gt;&lt;li&gt;Relevance Score: 4/5&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.05431</guid><pubDate>Mon, 09 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Explainer-guided Targeted Adversarial Attacks against Binary Code Similarity Detection Models</title><link>https://arxiv.org/abs/2506.05430</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a novel optimization for targeted adversarial attacks against binary code similarity detection (BCSD) models.&lt;/li&gt;&lt;li&gt;Utilizes model-agnostic explainers to identify critical code snippets for semantic-preserving perturbations.&lt;/li&gt;&lt;li&gt;Demonstrates improved attack success rate, efficiency, and transferability compared to state-of-the-art methods.&lt;/li&gt;&lt;li&gt;Highlights security implications for vulnerability detection and classification, emphasizing the need for more robust BCSD models.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research Paper&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Quality Assessment&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Clarity: 4/5&lt;/li&gt;&lt;li&gt;Novelty: 4/5&lt;/li&gt;&lt;li&gt;Significance: 3/5&lt;/li&gt;&lt;li&gt;Try-worthiness: Yes&lt;/li&gt;&lt;li&gt;Justification: The abstract is well-written and clearly explains the motivation, problem, and contributions of the paper, though some technical details are dense. The use of model-agnostic explainers for targeted adversarial attacks on binary code similarity detection (BCSD) models appears novel, especially in improving efficiency and transferability over prior heuristic or greedy approaches. The significance is moderate: BCSD is an important area, and the paper demonstrates real-world implications, but as an arXiv preprint with no citations yet, its impact is not fully established. The approach seems promising and worth experimenting with, especially for researchers interested in adversarial robustness in software engineering and security. No code repository is provided in the metadata.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: Mingjie Chen (Zhejiang University), Tiancheng Zhu (Huazhong University of Science,Technology), Mingxue Zhang (The State Key Laboratory of Blockchain,Data Security, Zhejiang University,Hangzhou High-Tech Zone), Yiling He (University College London), Minghao Lin (University of Southern California), Penghui Li (Columbia University), Kui Ren (The State Key Laboratory of Blockchain,Data Security, Zhejiang University)&lt;/li&gt;&lt;li&gt;Tags: adversarial attacks, model robustness, binary code similarity, AI security&lt;/li&gt;&lt;li&gt;Relevance Score: 5/5&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.05430</guid><pubDate>Mon, 09 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Coordinated Robustness Evaluation Framework for Vision-Language Models</title><link>https://arxiv.org/abs/2506.05429</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a coordinated robustness evaluation framework for vision-language models using adversarial perturbations in both image and text modalities.&lt;/li&gt;&lt;li&gt;Introduces a surrogate model to generate joint representations for crafting multi-modal adversarial attacks.&lt;/li&gt;&lt;li&gt;Demonstrates that the coordinated attack strategy is more effective than existing single-modality and multi-modal attacks.&lt;/li&gt;&lt;li&gt;Evaluates the approach on state-of-the-art vision-language models, highlighting vulnerabilities in their robustness.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research Paper&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Quality Assessment&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Clarity: 4/5&lt;/li&gt;&lt;li&gt;Novelty: 4/5&lt;/li&gt;&lt;li&gt;Significance: 3/5&lt;/li&gt;&lt;li&gt;Try-worthiness: Yes&lt;/li&gt;&lt;li&gt;Justification: The abstract is well-written and clearly explains the motivation, methodology, and results of the paper. The proposed coordinated robustness evaluation framework for vision-language models appears novel, especially in its use of a surrogate model to generate joint adversarial perturbations across both modalities. This is a timely and important topic as vision-language models are increasingly deployed in real-world applications. The significance is moderate given the preprint status and lack of citations, but the focus on state-of-the-art models and outperforming existing attacks suggests potential impact. The approach is worth experimenting with, especially for researchers interested in robustness and adversarial attacks in multi-modal AI. No code repository is provided in the metadata.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: Ashwin Ramesh Babu, Sajad Mousavi, Vineet Gundecha, Sahand Ghorbanpour, Avisek Naug, Antonio Guillen, Ricardo Luna Gutierrez, Soumyendu Sarkar&lt;/li&gt;&lt;li&gt;Tags: adversarial attacks, robustness evaluation, vision-language models, multi-modal security&lt;/li&gt;&lt;li&gt;Relevance Score: 5/5&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.05429</guid><pubDate>Mon, 09 Jun 2025 00:00:00 +0000</pubDate></item><item><title>FERRET: Private Deep Learning Faster And Better Than DPSGD</title><link>https://arxiv.org/abs/2506.05416</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces FERRET, a method for private deep learning using 1-bit gradient compression and mutual-information differential privacy (MI-DP).&lt;/li&gt;&lt;li&gt;Provides theoretical privacy guarantees, quantifying privacy loss per training step and group.&lt;/li&gt;&lt;li&gt;Demonstrates that FERRET achieves better utility (lower perplexity) and faster training compared to DPSGD, while maintaining strong privacy guarantees.&lt;/li&gt;&lt;li&gt;Evaluates FERRET on large language models, showing it can match non-private training utility in some cases while ensuring data privacy.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research Paper&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Quality Assessment&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Clarity: 5/5&lt;/li&gt;&lt;li&gt;Novelty: 5/5&lt;/li&gt;&lt;li&gt;Significance: 4/5&lt;/li&gt;&lt;li&gt;Try-worthiness: Yes&lt;/li&gt;&lt;li&gt;Justification: The abstract is exceptionally clear, well-structured, and provides concrete theoretical and empirical results. The work is highly novel, proposing a new approach (FERRET) that leverages sign-based updates and mutual-information differential privacy (MI-DP) without additive noise, which is a significant departure from standard DPSGD. The results are impressive, showing substantial improvements in both efficiency and utility while maintaining strong privacy guarantees. Although the paper is very new (hence no citations yet) and is currently a preprint, the potential impact is high given the importance of privacy-preserving deep learning. The method's strong empirical results and theoretical guarantees make it highly worth trying in practice. No code repository is provided in the metadata or abstract.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: David Zagardo&lt;/li&gt;&lt;li&gt;Tags: differential privacy, private deep learning, privacy-preserving machine learning, mutual-information differential privacy, secure model training&lt;/li&gt;&lt;li&gt;Relevance Score: 5/5&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.05416</guid><pubDate>Mon, 09 Jun 2025 00:00:00 +0000</pubDate></item><item><title>How stealthy is stealthy? Studying the Efficacy of Black-Box Adversarial Attacks in the Real World</title><link>https://arxiv.org/abs/2506.05382</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Investigates the efficacy of black-box adversarial attacks on deep learning systems in real-world scenarios.&lt;/li&gt;&lt;li&gt;Introduces three evaluation criteria for attacks: robustness to compression, stealthiness to automatic detection, and stealthiness to human inspection.&lt;/li&gt;&lt;li&gt;Proposes ECLIPSE, a novel attack method using Gaussian blurring and a local surrogate model.&lt;/li&gt;&lt;li&gt;Provides comprehensive experimental results demonstrating the trade-offs between attack stealthiness and effectiveness.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research Paper&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Quality Assessment&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Clarity: 4/5&lt;/li&gt;&lt;li&gt;Novelty: 4/5&lt;/li&gt;&lt;li&gt;Significance: 3/5&lt;/li&gt;&lt;li&gt;Try-worthiness: Yes&lt;/li&gt;&lt;li&gt;Justification: The abstract is well-written and clearly outlines the problem, motivation, and contributions, including the introduction of three evaluation properties and a new attack method (ECLIPSE). The focus on black-box adversarial attacks with realistic constraints (query-only access) and the explicit trade-off between robustness, stealthiness to detection, and human inspection is a novel angle. The significance is moderate given the preprint status and lack of citations, but the topic is important for security in deep learning. The proposed method appears practical and worth experimenting with, especially for researchers in adversarial machine learning. No code repository is provided in the metadata.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: Francesco Panebianco, Mario D'Onghia, Stefano Zanero aand Michele Carminati&lt;/li&gt;&lt;li&gt;Tags: adversarial attacks, black-box attacks, robustness, AI security, computer vision&lt;/li&gt;&lt;li&gt;Relevance Score: 5/5&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.05382</guid><pubDate>Mon, 09 Jun 2025 00:00:00 +0000</pubDate></item><item><title>A Red Teaming Roadmap Towards System-Level Safety</title><link>https://arxiv.org/abs/2506.05376</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Critiques current LLM red teaming research for not prioritizing system-level safety and realistic threat models.&lt;/li&gt;&lt;li&gt;Argues that red teaming should focus on concrete product safety specifications rather than abstract ethical concerns.&lt;/li&gt;&lt;li&gt;Emphasizes the importance of considering deployment context and system-level threats and mitigations.&lt;/li&gt;&lt;li&gt;Proposes a roadmap for advancing red teaming research to better address emerging AI threats.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Position Paper&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Quality Assessment&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Clarity: 4/5&lt;/li&gt;&lt;li&gt;Novelty: 4/5&lt;/li&gt;&lt;li&gt;Significance: 3/5&lt;/li&gt;&lt;li&gt;Try-worthiness: No&lt;/li&gt;&lt;li&gt;Justification: The abstract is well-written and clearly articulates the authors' position on the current state and future direction of red teaming research for LLM safety. The paper appears to offer a novel perspective by critiquing current research priorities and advocating for a shift towards system-level safety and realistic threat models. However, as a roadmap or position paper (rather than an empirical or technical contribution), its immediate significance is moderate, especially given its preprint status and lack of citations (which is expected for a new paper). There is no indication of an implementation or code contribution, so it is not directly try-worthy for experimentation. The paper is valuable for guiding future research directions rather than for direct application.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: Zifan Wang, Christina Q. Knight, Jeremy Kritz, Willow E. Primack, Julian Michael&lt;/li&gt;&lt;li&gt;Tags: LLM red teaming, system-level safety, AI security, threat modeling, AI safety&lt;/li&gt;&lt;li&gt;Relevance Score: 5/5&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.05376</guid><pubDate>Mon, 09 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Towards provable probabilistic safety for scalable embodied AI systems</title><link>https://arxiv.org/abs/2506.05171</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces the concept of provable probabilistic safety for embodied AI systems (e.g., robotics, autonomous vehicles).&lt;/li&gt;&lt;li&gt;Addresses the challenge of ensuring safety in complex, real-world environments where failures are rare but critical.&lt;/li&gt;&lt;li&gt;Proposes statistical methods to establish probabilistic safety boundaries, enabling scalable and practical safety guarantees.&lt;/li&gt;&lt;li&gt;Discusses how to define, prove, and achieve probabilistic safety for large-scale deployment in safety-critical domains.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research Paper&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Quality Assessment&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Clarity: 5/5&lt;/li&gt;&lt;li&gt;Novelty: 4/5&lt;/li&gt;&lt;li&gt;Significance: 4/5&lt;/li&gt;&lt;li&gt;Try-worthiness: Yes&lt;/li&gt;&lt;li&gt;Justification: The abstract is very well-written, clearly outlining the motivation, challenges, and the proposed paradigm of 'provable probabilistic safety' for embodied AI systems. The concept of moving from deterministic to probabilistic safety guarantees is a novel and pragmatic approach, especially for scalable, safety-critical applications where exhaustive verification is infeasible. While the paper is a preprint and has not yet been cited (which is expected given its recency), the topic is highly significant for real-world deployment of AI in domains like autonomous vehicles and robotics. The work appears to offer a new framework and methodology that could be impactful and is worth experimenting with, especially for researchers and practitioners in AI safety. No code repository is provided in the metadata.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: Linxuan He, Qing-Shan Jia, Ang Li, Hongyan Sang, Ling Wang, Jiwen Lu, Tao Zhang, Jie Zhou, Yi Zhang, Yisen Wang, Peng Wei, Zhongyuan Wang, Henry X. Liu, Shuo Feng&lt;/li&gt;&lt;li&gt;Tags: AI safety, probabilistic safety, embodied AI, risk assessment, safety-critical systems&lt;/li&gt;&lt;li&gt;Relevance Score: 5/5&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.05171</guid><pubDate>Mon, 09 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Preference Learning for AI Alignment: a Causal Perspective</title><link>https://arxiv.org/abs/2506.05967</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Frames reward modeling for LLM alignment as a causal inference problem, highlighting challenges such as causal misidentification and confounding.&lt;/li&gt;&lt;li&gt;Identifies key assumptions for reliable generalization in preference learning and contrasts them with current data collection practices.&lt;/li&gt;&lt;li&gt;Demonstrates failure modes of naive reward models and shows how causally-inspired methods can improve robustness.&lt;/li&gt;&lt;li&gt;Proposes future research directions and interventions to address limitations in observational data for AI alignment.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research Paper&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: Katarzyna Kobalczyk, Mihaela van der Schaar&lt;/li&gt;&lt;li&gt;Tags: AI alignment, preference learning, robustness, causal inference, reward modeling&lt;/li&gt;&lt;li&gt;Relevance Score: 4/5&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.05967</guid><pubDate>Mon, 09 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Constructive Symbolic Reinforcement Learning via Intuitionistic Logic and Goal-Chaining Inference</title><link>https://arxiv.org/abs/2506.05422</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a reinforcement learning framework based on constructive logical inference instead of reward-based optimization.&lt;/li&gt;&lt;li&gt;Ensures that actions and state transitions are only accepted if they are logically valid, preventing unsafe or invalid actions.&lt;/li&gt;&lt;li&gt;Demonstrates empirical results showing perfect safety and interpretable behavior in a symbolic gridworld environment.&lt;/li&gt;&lt;li&gt;Highlights the potential for this approach to enable safe planning and trustworthy AI by guaranteeing logical correctness of decisions.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research Paper&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Quality Assessment&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Clarity: 5/5&lt;/li&gt;&lt;li&gt;Novelty: 5/5&lt;/li&gt;&lt;li&gt;Significance: 4/5&lt;/li&gt;&lt;li&gt;Try-worthiness: Yes&lt;/li&gt;&lt;li&gt;Justification: The abstract is exceptionally clear, outlining both the motivation and the technical approach (constructive logic-based RL instead of reward-based optimization). The use of intuitionistic logic and proof theory for reinforcement learning is highly novel, especially as it replaces probabilistic trial-and-error with verifiable logical inference. While the paper is very new and only on arXiv (not yet peer-reviewed), the approach is significant for safe and interpretable AI, and the empirical comparison with Q-learning adds practical relevance. The method's promise of perfect safety and interpretable behavior makes it worth experimenting with, especially for researchers interested in symbolic AI or safe RL. No code repository is provided in the metadata.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: Andrei T. Patrascu&lt;/li&gt;&lt;li&gt;Tags: AI safety, Safe planning, Trustworthy AI, Symbolic reasoning&lt;/li&gt;&lt;li&gt;Relevance Score: 4/5&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.05422</guid><pubDate>Mon, 09 Jun 2025 00:00:00 +0000</pubDate></item></channel></rss>