<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>AI Security Paper Digest</title><link>https://kentaroh-toyoda.github.io/ai-security-paper-digest-rss/rss.xml</link><description>Curated papers on AI security from OpenAlex and ArXiv</description><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><lastBuildDate>Mon, 16 Jun 2025 22:29:55 +0000</lastBuildDate><item><title>CheXGenBench: A Unified Benchmark For Fidelity, Privacy and Utility of Synthetic Chest Radiographs</title><link>https://arxiv.org/abs/2505.10496</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces CheXGenBench, a benchmark for evaluating synthetic chest radiograph generation models.&lt;/li&gt;&lt;li&gt;Assesses fidelity, privacy risks (including potential privacy vulnerabilities), and clinical utility of generated images.&lt;/li&gt;&lt;li&gt;Provides a unified evaluation protocol with over 20 quantitative metrics, including privacy risk assessment.&lt;/li&gt;&lt;li&gt;Releases a large synthetic dataset (SynthCheX-75K) and evaluation framework to support reproducible research.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Quality Assessment&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Clarity: 5/5&lt;/li&gt;&lt;li&gt;Novelty: 5/5&lt;/li&gt;&lt;li&gt;Significance: 4/5&lt;/li&gt;&lt;li&gt;Try-worthiness: Yes&lt;/li&gt;&lt;li&gt;Justification: The abstract is very well-written, clearly outlining the motivation, contributions, and outcomes of the work. The paper introduces a unified benchmark (CheXGenBench) for evaluating synthetic chest radiograph generation, addressing fidelity, privacy, and utilityâ€”an area with previously fragmented evaluation standards. The novelty is high, as it provides a comprehensive, standardized protocol and a large synthetic dataset, filling a clear gap in medical AI evaluation. While the paper is too new to have citations, its significance is strong due to the importance of benchmarking in medical imaging and the release of both code and data. The work is highly try-worthy for researchers in medical imaging, generative models, and privacy, as it provides tools, data, and a reproducible framework. The code and dataset are available at the provided GitHub URL.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Raman Dutt', 'Pedro Sanchez', 'Yongchen Yao', 'Steven McDonagh', 'Sotirios A. Tsaftaris', 'Timothy Hospedales']&lt;/li&gt;&lt;li&gt;Tags: ['privacy risks', 'benchmarking', 'synthetic data', 'medical imaging', 'evaluation framework']&lt;/li&gt;&lt;li&gt;Code Repository: &lt;a href='https://raman1121.github.io/CheXGenBench/'&gt;https://raman1121.github.io/CheXGenBench/&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2505.10496</guid><pubDate>Mon, 16 Jun 2025 00:00:00 +0000</pubDate></item><item><title>GaussMarker: Robust Dual-Domain Watermark for Diffusion Models</title><link>https://arxiv.org/abs/2506.11444</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a robust watermarking method for images generated by diffusion models to address copyright and misuse concerns.&lt;/li&gt;&lt;li&gt;Introduces a dual-domain watermarking approach, embedding watermarks in both spatial and frequency domains.&lt;/li&gt;&lt;li&gt;Enhances robustness against image manipulations and advanced attacks using a learnable Gaussian Noise Restorer.&lt;/li&gt;&lt;li&gt;Demonstrates improved performance under various distortions and attacks, relevant for protecting AI-generated content.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research Paper&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Quality Assessment&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Clarity: 4/5&lt;/li&gt;&lt;li&gt;Novelty: 5/5&lt;/li&gt;&lt;li&gt;Significance: 4/5&lt;/li&gt;&lt;li&gt;Try-worthiness: Yes&lt;/li&gt;&lt;li&gt;Justification: The abstract is well-written and clearly explains the motivation, approach, and results of the paper, though some technical details are dense. The dual-domain watermarking method for diffusion models appears novel, especially with the introduction of a learnable Gaussian Noise Restorer and integration of spatial and frequency domain watermarks. The significance is high given the growing importance of watermarking in generative models, and the claimed state-of-the-art performance across multiple attacks and Stable Diffusion versions suggests practical impact. As a recent arXiv preprint, it has no citations yet, but the topic and results make it worth experimenting with. No code repository is provided in the metadata.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Kecen Li', 'Zhicong Huang', 'Xinwen Hou', 'Cheng Hong']&lt;/li&gt;&lt;li&gt;Tags: ['watermarking', 'diffusion models', 'robustness', 'copyright protection', 'adversarial attacks']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.11444</guid><pubDate>Mon, 16 Jun 2025 00:00:00 +0000</pubDate></item><item><title>FAME: A Lightweight Spatio-Temporal Network for Model Attribution of Face-Swap Deepfakes</title><link>https://arxiv.org/abs/2506.11477</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes FAME, a lightweight spatio-temporal network for attributing face-swap Deepfakes to their generative models.&lt;/li&gt;&lt;li&gt;Addresses the underexplored problem of model attribution, moving beyond simple Deepfake detection.&lt;/li&gt;&lt;li&gt;Demonstrates improved accuracy and efficiency over existing methods on multiple Deepfake datasets.&lt;/li&gt;&lt;li&gt;Highlights applications in digital forensics and information security.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research Paper&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Quality Assessment&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Clarity: 5/5&lt;/li&gt;&lt;li&gt;Novelty: 4/5&lt;/li&gt;&lt;li&gt;Significance: 4/5&lt;/li&gt;&lt;li&gt;Try-worthiness: Yes&lt;/li&gt;&lt;li&gt;Justification: The abstract is well-written, clearly outlining the motivation, contribution, and results of the paper. The focus on model attribution for face-swap Deepfakes, rather than just binary detection, is a relatively novel and underexplored area. The proposed FAME framework claims both improved accuracy and efficiency, which is significant for real-world deployment. While the paper is very new and has no citations yet, its evaluation on multiple challenging datasets and outperforming existing methods suggest it is a meaningful contribution. The lack of a code repository is a minor drawback, but the method appears promising and worth experimenting with.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Wasim Ahmad', 'Yan-Tsung Peng', 'Yuan-Hao Chang']&lt;/li&gt;&lt;li&gt;Tags: ['deepfake detection', 'model attribution', 'digital forensics', 'media integrity', 'information security']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.11477</guid><pubDate>Mon, 16 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Auditing Data Provenance in Real-world Text-to-Image Diffusion Models for Privacy and Copyright Protection</title><link>https://arxiv.org/abs/2506.11434</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a black-box auditing framework (FSCA) for tracing data provenance in text-to-image diffusion models.&lt;/li&gt;&lt;li&gt;Addresses privacy and copyright concerns by enabling detection of whether specific user data was used in model training.&lt;/li&gt;&lt;li&gt;Demonstrates effectiveness of FSCA on real-world datasets, outperforming existing state-of-the-art methods.&lt;/li&gt;&lt;li&gt;Introduces strategies to improve user-level auditing accuracy in practical scenarios.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research Paper&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Quality Assessment&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Clarity: 4/5&lt;/li&gt;&lt;li&gt;Novelty: 4/5&lt;/li&gt;&lt;li&gt;Significance: 3/5&lt;/li&gt;&lt;li&gt;Try-worthiness: Yes&lt;/li&gt;&lt;li&gt;Justification: The paper is clearly written, with a well-structured abstract that outlines the motivation, problem, proposed method (FSCA), and experimental results. The novelty is reasonably high, as it addresses a practical and under-explored problem (black-box auditing for data provenance in diffusion models) and claims to outperform existing methods without requiring internal model access. The significance is moderate: while the topic is important for privacy and copyright, the paper is a recent arXiv preprint without peer review or citations yet, so its impact is still to be determined. The presence of a code repository and promising results make it worth trying for practitioners or researchers interested in model auditing and data provenance.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jie Zhu', 'Leye Wang']&lt;/li&gt;&lt;li&gt;Tags: ['data provenance', 'privacy', 'copyright', 'auditing', 'diffusion models']&lt;/li&gt;&lt;li&gt;Code Repository: &lt;a href='https://github.com/JiePKU/FSCA'&gt;https://github.com/JiePKU/FSCA&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.11434</guid><pubDate>Mon, 16 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Stop learning it all to mitigate visual hallucination, Focus on the hallucination target</title><link>https://arxiv.org/abs/2506.11417</link><description>&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Other&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Quality Assessment&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Clarity: 4/5&lt;/li&gt;&lt;li&gt;Novelty: 4/5&lt;/li&gt;&lt;li&gt;Significance: 3/5&lt;/li&gt;&lt;li&gt;Try-worthiness: Yes&lt;/li&gt;&lt;li&gt;Justification: The abstract is well-written and clearly explains the motivation, method, and results, though some details (such as the exact implementation of \mymethod\) are left for the full paper. The approach of targeted preference learning to mitigate hallucinations in MLLMs appears novel, especially with the construction of a specialized dataset focusing on hallucination targets. While the significance is promising given the importance of hallucination mitigation in vision-language models, the paper is a very recent arXiv preprint with no citations yet, so its impact is not yet established. The method seems practical and potentially impactful, making it worth trying for those working on reliable MLLMs. No code repository is provided in the metadata.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Dokyoon Yoon', 'Youngsook Song', 'Woomyong Park']&lt;/li&gt;&lt;li&gt;Tags: []&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.11417</guid><pubDate>Mon, 16 Jun 2025 00:00:00 +0000</pubDate></item><item><title>A Watermark for Auto-Regressive Image Generation Models</title><link>https://arxiv.org/abs/2506.11371</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a novel watermarking method (C-reweight) for auto-regressive image generation models.&lt;/li&gt;&lt;li&gt;Addresses the challenge of retokenization mismatch in watermarking image models.&lt;/li&gt;&lt;li&gt;Aims to improve authenticity verification and mitigate misuse such as deepfakes and image-based phishing.&lt;/li&gt;&lt;li&gt;Demonstrates improved detectability and image fidelity compared to existing watermarking techniques.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research Paper&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Quality Assessment&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Clarity: 4/5&lt;/li&gt;&lt;li&gt;Novelty: 5/5&lt;/li&gt;&lt;li&gt;Significance: 4/5&lt;/li&gt;&lt;li&gt;Try-worthiness: Yes&lt;/li&gt;&lt;li&gt;Justification: The abstract is well-written and clearly explains the motivation, challenge (retokenization mismatch), and the proposed solution (C-reweight) for watermarking in autoregressive image generation models. The work appears highly novel, introducing a clustering-based, distortion-free watermarking method specifically tailored for image models, addressing a gap where language model techniques fail. The significance is high given the growing importance of authenticity in AI-generated images, though as a very recent arXiv preprint, it has not yet accumulated citations or peer-reviewed validation. The method's claimed improvements in detectability and fidelity make it worth experimenting with, especially for researchers or practitioners concerned with image provenance and security. No code repository is provided in the metadata.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yihan Wu', 'Xuehao Cui', 'Ruibo Chen', 'Georgios Milis', 'Heng Huang']&lt;/li&gt;&lt;li&gt;Tags: ['watermarking', 'image generation', 'authenticity verification', 'deepfake mitigation', 'AI security']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.11371</guid><pubDate>Mon, 16 Jun 2025 00:00:00 +0000</pubDate></item><item><title>RedDebate: Safer Responses through Multi-Agent Red Teaming Debates</title><link>https://arxiv.org/abs/2506.11083</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces RedDebate, a multi-agent debate framework for automated red teaming among LLMs.&lt;/li&gt;&lt;li&gt;Enables LLMs to identify and mitigate unsafe behaviors through adversarial argumentation.&lt;/li&gt;&lt;li&gt;Incorporates long-term memory modules to retain safety insights and improve over time.&lt;/li&gt;&lt;li&gt;Demonstrates effectiveness on safety benchmarks, reducing unsafe behaviors without direct human intervention.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research Paper&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Quality Assessment&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Clarity: 4/5&lt;/li&gt;&lt;li&gt;Novelty: 5/5&lt;/li&gt;&lt;li&gt;Significance: 4/5&lt;/li&gt;&lt;li&gt;Try-worthiness: Yes&lt;/li&gt;&lt;li&gt;Justification: The abstract is well-written and clearly explains the motivation, methodology, and results of the work, though some technical details are left for the full paper. The approachâ€”multi-agent debate with automated red-teaming and long-term memory for AI safetyâ€”is highly novel and addresses a pressing challenge in scalable AI safety. While the paper is a preprint and very new (hence no citations yet), it tackles a significant problem and demonstrates measurable improvements on established benchmarks. The availability of a code repository further increases its try-worthiness for practitioners and researchers interested in AI safety.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Ali Asad', 'Stephen Obadinma', 'Radin Shayanfar', 'Xiaodan Zhu']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'AI safety', 'multi-agent systems', 'automated safety evaluation', 'unsafe behavior mitigation']&lt;/li&gt;&lt;li&gt;Code Repository: &lt;a href='https://github.com/aliasad059/RedDebate'&gt;https://github.com/aliasad059/RedDebate&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.11083</guid><pubDate>Mon, 16 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Deontological Keyword Bias: The Impact of Modal Expressions on Normative Judgments of Language Models</title><link>https://arxiv.org/abs/2506.11068</link><description>&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Other&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Quality Assessment&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Clarity: 5/5&lt;/li&gt;&lt;li&gt;Novelty: 4/5&lt;/li&gt;&lt;li&gt;Significance: 4/5&lt;/li&gt;&lt;li&gt;Try-worthiness: Yes&lt;/li&gt;&lt;li&gt;Justification: The abstract is well-written and clearly explains the problem, findings, and proposed mitigation, earning a high clarity score. The identification and naming of 'Deontological Keyword Bias' (DKB) in LLMs is a novel contribution, as this specific framing effect has not been widely discussed in prior literature, though related work on LLM bias exists. The significance is notable because understanding and mitigating such biases is crucial for the safe deployment of LLMs in ethical and normative contexts, and the work appears to generalize across models and tasks. The proposed mitigation strategy (few-shot examples with reasoning prompts) is actionable and worth experimenting with, especially for those working on LLM alignment or safety. No code repository is provided in the metadata.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Bumjin Park', 'Jinsil Lee', 'Jaesik Choi']&lt;/li&gt;&lt;li&gt;Tags: []&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.11068</guid><pubDate>Mon, 16 Jun 2025 00:00:00 +0000</pubDate></item><item><title>VeriLeaky: Navigating IP Protection vs Utility in Fine-Tuning for LLM-Driven Verilog Coding</title><link>https://arxiv.org/abs/2503.13116</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Investigates the risk of proprietary IP leakage when fine-tuning LLMs for Verilog code generation.&lt;/li&gt;&lt;li&gt;Quantifies IP leakage by measuring structural and functional similarities between generated and original IP code.&lt;/li&gt;&lt;li&gt;Evaluates a defense mechanism (logic locking) to protect IP, noting its trade-off with model utility and performance.&lt;/li&gt;&lt;li&gt;Highlights the need for new strategies to balance IP protection and fine-tuning utility in LLM-driven coding applications.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research Paper&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Quality Assessment&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Clarity: 4/5&lt;/li&gt;&lt;li&gt;Novelty: 5/5&lt;/li&gt;&lt;li&gt;Significance: 4/5&lt;/li&gt;&lt;li&gt;Try-worthiness: Yes&lt;/li&gt;&lt;li&gt;Justification: The abstract is well-written and clearly outlines the motivation, methodology, and findings, though it is somewhat dense with technical terms. The work is highly novel, being the first to systematically study the trade-off between IP protection and utility in fine-tuning LLMs for Verilog coding, and it introduces rigorous leakage assessment methods. The significance is high given the growing interest in LLMs for code generation in specialized domains and the real-world implications for design houses, though the impact is yet to be seen due to its recent publication. The paper is worth experimenting with, especially for those interested in secure LLM deployment in hardware design, as it highlights both the risks and current limitations of existing defenses. No code repository is provided in the metadata.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Zeng Wang', 'Minghao Shao', 'Mohammed Nabeel', 'Prithwish Basu Roy', 'Likhitha Mankali', 'Jitendra Bhandari', 'Ramesh Karri', 'Ozgur Sinanoglu', 'Muhammad Shafique', 'Johann Knechtel']&lt;/li&gt;&lt;li&gt;Tags: ['IP leakage', 'fine-tuning security', 'LLM security', 'code generation', 'defense mechanisms']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2503.13116</guid><pubDate>Mon, 16 Jun 2025 00:00:00 +0000</pubDate></item><item><title>PANDAS: Improving Many-shot Jailbreaking via Positive Affirmation, Negative Demonstration, and Adaptive Sampling</title><link>https://arxiv.org/abs/2502.01925</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces PANDAS, a new technique to improve many-shot jailbreaking attacks against LLMs by using positive affirmations, negative demonstrations, and adaptive sampling.&lt;/li&gt;&lt;li&gt;Demonstrates that PANDAS significantly outperforms existing many-shot jailbreaking methods in long-context scenarios.&lt;/li&gt;&lt;li&gt;Presents ManyHarm, a dataset of harmful question-answer pairs used to facilitate and evaluate jailbreaking.&lt;/li&gt;&lt;li&gt;Provides analysis of how long-context vulnerabilities in LLMs are exploited by advanced jailbreaking techniques.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research Paper&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Quality Assessment&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Clarity: 4/5&lt;/li&gt;&lt;li&gt;Novelty: 4/5&lt;/li&gt;&lt;li&gt;Significance: 4/5&lt;/li&gt;&lt;li&gt;Try-worthiness: Yes&lt;/li&gt;&lt;li&gt;Justification: The abstract is well-written and clearly explains the problem (many-shot jailbreaking), the proposed method (PANDAS), and the contributions (a new technique, a dataset, and empirical results). The approachâ€”combining positive affirmation, negative demonstration, and adaptive samplingâ€”appears novel and tailored to a current, important challenge in LLM safety. The introduction of the ManyHarm dataset and attention analysis further strengthen the work's contribution. While the paper is very recent and has no citations yet, the topic is highly relevant, and the results claim significant improvements over baselines, making it worth experimenting with. No code repository is provided in the metadata.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Avery Ma', 'Yangchen Pan', 'Amir-massoud Farahmand']&lt;/li&gt;&lt;li&gt;Tags: ['LLM jailbreaking', 'adversarial prompting', 'AI security', 'long-context vulnerabilities', 'dataset']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2502.01925</guid><pubDate>Mon, 16 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Preempting Text Sanitization Utility in Resource-Constrained Privacy-Preserving LLM Interactions</title><link>https://arxiv.org/abs/2411.11521</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Addresses privacy concerns in LLM interactions by focusing on prompt sanitization using Differential Privacy.&lt;/li&gt;&lt;li&gt;Proposes a middleware architecture that predicts the utility of sanitized prompts before sending them to the LLM, aiming to prevent resource waste.&lt;/li&gt;&lt;li&gt;Evaluates the approach on summarization and translation tasks, demonstrating practical impact on resource usage.&lt;/li&gt;&lt;li&gt;Reproduces and critiques prior work on text sanitization, highlighting overlooked implementation choices affecting privacy and utility.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research Paper&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Quality Assessment&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Clarity: 4/5&lt;/li&gt;&lt;li&gt;Novelty: 4/5&lt;/li&gt;&lt;li&gt;Significance: 3/5&lt;/li&gt;&lt;li&gt;Try-worthiness: Yes&lt;/li&gt;&lt;li&gt;Justification: The abstract is well-written and clearly outlines the problem (privacy in LLM interactions, unpredictable utility after text sanitization), the proposed solution (middleware with a Small Language Model to predict utility), and the experimental results (resource savings on summarization and translation tasks). The novelty is reasonably high, as the use of a predictive middleware to preemptively assess the utility of sanitized prompts in LLM workflows is not widely explored. The significance is moderate: while the problem is important and the solution practical, the paper is a recent preprint on arXiv and has not yet been peer-reviewed or cited, so its impact is still to be determined. The approach is practical and could be valuable for those deploying LLMs in privacy-sensitive, resource-constrained settings, making it worth trying. No code repository is provided in the metadata.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Robin Carpentier', 'Benjamin Zi Hao Zhao', 'Hassan Jameel Asghar', 'Dali Kaafar']&lt;/li&gt;&lt;li&gt;Tags: ['privacy', 'differential privacy', 'LLM security', 'prompt sanitization', 'middleware']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2411.11521</guid><pubDate>Mon, 16 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Black-Box Adversarial Attacks on LLM-Based Code Completion</title><link>https://arxiv.org/abs/2408.02509</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces INSEC, a black-box adversarial attack that biases LLM-based code completion engines to generate insecure code.&lt;/li&gt;&lt;li&gt;Demonstrates the attack's effectiveness across multiple open-source and commercial LLM code completion services, including OpenAI API and GitHub Copilot.&lt;/li&gt;&lt;li&gt;Shows that the attack can increase insecure code generation rates by over 50% while preserving code functionality.&lt;/li&gt;&lt;li&gt;Presents a practical, low-cost implementation and demonstrates real-world deployability via an IDE plug-in.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research Paper&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Quality Assessment&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Clarity: 5/5&lt;/li&gt;&lt;li&gt;Novelty: 5/5&lt;/li&gt;&lt;li&gt;Significance: 4/5&lt;/li&gt;&lt;li&gt;Try-worthiness: Yes&lt;/li&gt;&lt;li&gt;Justification: The abstract is very clearly written, outlining the motivation, method (INSEC), evaluation, and practical implications. The work is highly novel, being the first to demonstrate a black-box adversarial attack that stealthily biases LLM-based code completion engines towards insecure code via comment injection. The significance is high given the widespread use of LLMs in code completion and the real-world demonstration (IDE plugin), though as a preprint with no citations yet, its impact is still emerging. The attack's practicality and low resource requirements make it worth experimenting with, especially for those interested in LLM security. No code repository is provided in the metadata.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Slobodan Jenko', 'Niels M\\"undler', 'Jingxuan He', 'Mark Vero', 'Martin Vechev']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial attacks', 'LLM security', 'code completion', 'prompt injection', 'software security']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2408.02509</guid><pubDate>Mon, 16 Jun 2025 00:00:00 +0000</pubDate></item><item><title>LoByITFL: Low Communication Secure and Private Federated Learning</title><link>https://arxiv.org/abs/2405.19217</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes LoByITFL, a federated learning scheme that is both secure against Byzantine adversaries and information-theoretically private.&lt;/li&gt;&lt;li&gt;Addresses the challenge of maintaining strong privacy guarantees without sacrificing security in federated learning.&lt;/li&gt;&lt;li&gt;Introduces a communication-efficient protocol using a modified FLTrust algorithm and a trusted third party for initialization.&lt;/li&gt;&lt;li&gt;Provides theoretical analysis and experimental validation of privacy and security guarantees.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research Paper&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Quality Assessment&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Clarity: 4/5&lt;/li&gt;&lt;li&gt;Novelty: 5/5&lt;/li&gt;&lt;li&gt;Significance: 4/5&lt;/li&gt;&lt;li&gt;Try-worthiness: Yes&lt;/li&gt;&lt;li&gt;Justification: The abstract is well-written and clearly outlines the motivation, contributions, and key components of the proposed LoByITFL scheme. The work claims to be the first to achieve communication-efficient, information-theoretic privacy and Byzantine security in federated learning without sacrificing privacy guarantees, which is a strong novelty claim. The significance is high due to the importance of privacy and security in federated learning, though the impact is yet to be seen as the paper is very recent and only on arXiv. The theoretical guarantees and experimental results make it worth trying for researchers in secure federated learning. No code repository is provided in the metadata.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yue Xia', 'Maximilian Egger', 'Christoph Hofmeister', 'Rawad Bitar']&lt;/li&gt;&lt;li&gt;Tags: ['federated learning', 'privacy', 'Byzantine security', 'secure aggregation', 'AI security']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2405.19217</guid><pubDate>Mon, 16 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Byzantine-Resilient Secure Aggregation for Federated Learning Without Privacy Compromises</title><link>https://arxiv.org/abs/2405.08698</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes ByITFL, a federated learning scheme resilient to Byzantine (malicious) users.&lt;/li&gt;&lt;li&gt;Ensures privacy of user data from both the federator and other users.&lt;/li&gt;&lt;li&gt;Combines techniques like Lagrange coded computing, verifiable secret sharing, and re-randomization for secure aggregation.&lt;/li&gt;&lt;li&gt;Achieves information-theoretic privacy while defending against adversarial participants.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research Paper&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Quality Assessment&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Clarity: 4/5&lt;/li&gt;&lt;li&gt;Novelty: 5/5&lt;/li&gt;&lt;li&gt;Significance: 4/5&lt;/li&gt;&lt;li&gt;Try-worthiness: Yes&lt;/li&gt;&lt;li&gt;Justification: The abstract is well-written and clearly outlines the motivation, approach, and contributions of the paper, though some technical details (e.g., the polynomial approximation of ReLU) could be clearer for a broader audience. The work appears highly novel, claiming to be the first Byzantine-resilient federated learning scheme with full information-theoretic privacy, and introduces a new combination of techniques (Lagrange coded computing, verifiable secret sharing, re-randomization). The significance is high given the importance of privacy and security in federated learning, though as a very recent preprint, it has not yet been cited or peer-reviewed. The approach is worth trying for researchers in secure federated learning, especially given the claimed privacy guarantees. No code repository is provided in the metadata.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yue Xia', 'Christoph Hofmeister', 'Maximilian Egger', 'Rawad Bitar']&lt;/li&gt;&lt;li&gt;Tags: ['federated learning', 'Byzantine resilience', 'secure aggregation', 'privacy', 'adversarial robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2405.08698</guid><pubDate>Mon, 16 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Differentially Private Relational Learning with Entity-level Privacy Guarantees</title><link>https://arxiv.org/abs/2506.08347</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a framework for relational learning with formal entity-level differential privacy (DP) guarantees.&lt;/li&gt;&lt;li&gt;Introduces adaptive gradient clipping to address sensitivity issues in relational data.&lt;/li&gt;&lt;li&gt;Extends privacy amplification analysis to certain coupled sampling procedures in relational learning.&lt;/li&gt;&lt;li&gt;Demonstrates the approach on fine-tuning text encoders over network-structured, text-attributed data.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research Paper&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Quality Assessment&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Clarity: 5/5&lt;/li&gt;&lt;li&gt;Novelty: 5/5&lt;/li&gt;&lt;li&gt;Significance: 4/5&lt;/li&gt;&lt;li&gt;Try-worthiness: Yes&lt;/li&gt;&lt;li&gt;Justification: The abstract is well-written, clearly outlining the motivation, challenges, and contributions of the paper. The work addresses a novel and important problem: providing entity-level differential privacy guarantees in relational learning, which is non-trivial due to the complex dependencies in relational data. The introduction of adaptive gradient clipping and an extension of privacy amplification to coupled sampling are both innovative. While the paper is very recent and has no citations yet, the topic is highly significant for privacy-preserving machine learning, especially in sensitive domains. The availability of code further increases its try-worthiness for practitioners and researchers.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yinan Huang', 'Haoteng Yin', 'Eli Chien', 'Rongzhe Wei', 'Pan Li']&lt;/li&gt;&lt;li&gt;Tags: ['differential privacy', 'entity-level privacy', 'relational learning', 'privacy-preserving machine learning', 'text encoders']&lt;/li&gt;&lt;li&gt;Code Repository: &lt;a href='https://github.com/Graph-COM/Node_DP'&gt;https://github.com/Graph-COM/Node_DP&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.08347</guid><pubDate>Mon, 16 Jun 2025 00:00:00 +0000</pubDate></item><item><title>On the Stability of Graph Convolutional Neural Networks: A Probabilistic Perspective</title><link>https://arxiv.org/abs/2506.01213</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Analyzes the stability of Graph Convolutional Neural Networks (GCNNs) under perturbations in graph topology.&lt;/li&gt;&lt;li&gt;Proposes a probabilistic framework to assess how changes in graph structure affect model outputs.&lt;/li&gt;&lt;li&gt;Includes evaluation of model robustness against adversarial attacks on downstream tasks.&lt;/li&gt;&lt;li&gt;Highlights the importance of considering data distribution in stability and robustness analysis.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research Paper&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Quality Assessment&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Clarity: 4/5&lt;/li&gt;&lt;li&gt;Novelty: 5/5&lt;/li&gt;&lt;li&gt;Significance: 4/5&lt;/li&gt;&lt;li&gt;Try-worthiness: Yes&lt;/li&gt;&lt;li&gt;Justification: The abstract is well-written and clearly outlines the motivation, problem, and contributions of the paper, though some technical details are left for the full text. The work is highly novel, introducing a probabilistic, distribution-aware framework for analyzing GCNN stability, which is a step beyond prior worst-case analyses. The significance is high given the importance of robustness in graph neural networks, though the impact is yet to be seen due to the paper's recency and preprint status. The approach appears promising and is supported by both theoretical and experimental results, making it worth trying for researchers interested in robust GNNs. No code repository is provided in the metadata.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Ning Zhang', 'Henry Kenlay', 'Li Zhang', 'Mihai Cucuringu', 'Xiaowen Dong']&lt;/li&gt;&lt;li&gt;Tags: ['robustness', 'adversarial attacks', 'graph neural networks', 'stability analysis']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.01213</guid><pubDate>Mon, 16 Jun 2025 00:00:00 +0000</pubDate></item><item><title>A Few Large Shifts: Layer-Inconsistency Based Minimal Overhead Adversarial Example Detection</title><link>https://arxiv.org/abs/2505.12586</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a lightweight adversarial example detection framework based on internal layer-wise inconsistencies in neural networks.&lt;/li&gt;&lt;li&gt;Introduces the 'A Few Large Shifts Assumption' to identify adversarial perturbations via representation shifts in a small subset of layers.&lt;/li&gt;&lt;li&gt;Presents two detection strategies (Recovery Testing and Logit-layer Testing) that require only benign data for calibration.&lt;/li&gt;&lt;li&gt;Demonstrates state-of-the-art detection performance on standard image datasets under various threat models with minimal computational overhead.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research Paper&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Quality Assessment&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Clarity: 5/5&lt;/li&gt;&lt;li&gt;Novelty: 4/5&lt;/li&gt;&lt;li&gt;Significance: 4/5&lt;/li&gt;&lt;li&gt;Try-worthiness: Yes&lt;/li&gt;&lt;li&gt;Justification: The abstract is very well-written, clearly outlining the motivation, approach, and results. The proposed method is novel in that it leverages internal layer-wise inconsistencies and introduces the 'A Few Large Shifts Assumption' for adversarial detection, which appears to be a new perspective. The method is lightweight, does not require adversarial data or complex augmentations, and achieves state-of-the-art results on standard benchmarks, making it significant for practical deployment. While the paper is very recent and has no citations yet, its claims and the availability of code make it worth trying for researchers and practitioners interested in adversarial robustness.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Sanggeon Yun', 'Ryozo Masukawa', 'Hyunwoo Oh', 'Nathaniel D. Bastian', 'Mohsen Imani']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial example detection', 'robustness', 'deep neural networks', 'security evaluation', 'image classification']&lt;/li&gt;&lt;li&gt;Code Repository: &lt;a href='https://github.com/c0510gy/AFLS-AED'&gt;https://github.com/c0510gy/AFLS-AED&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2505.12586</guid><pubDate>Mon, 16 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Federated Learning Nodes Can Reconstruct Peers' Image Data</title><link>https://arxiv.org/abs/2410.04661</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Demonstrates that nodes in federated learning can reconstruct peers' private image data using gradient inversion attacks.&lt;/li&gt;&lt;li&gt;Shows that not only central servers, but also honest-but-curious clients, can silently perform these privacy attacks.&lt;/li&gt;&lt;li&gt;Utilizes diffusion models to improve the quality and recognizability of reconstructed images, highlighting semantic-level privacy risks.&lt;/li&gt;&lt;li&gt;Emphasizes the need for stronger privacy-preserving mechanisms in federated learning to defend against client-side attacks.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research Paper&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Quality Assessment&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Clarity: 5/5&lt;/li&gt;&lt;li&gt;Novelty: 4/5&lt;/li&gt;&lt;li&gt;Significance: 4/5&lt;/li&gt;&lt;li&gt;Try-worthiness: Yes&lt;/li&gt;&lt;li&gt;Justification: The abstract is well-written, clearly outlining the problem, methodology, and implications. The work is novel in that it shifts the focus from server-side to client-side privacy attacks in federated learning, demonstrating that even honest-but-curious clients can reconstruct peers' image data using gradient inversion and diffusion models. This is a meaningful extension of prior work and highlights a new threat vector. The significance is high given the widespread adoption of federated learning and the critical importance of privacy in such systems. While the paper is very recent and has no citations yet, the topic and findings are impactful and likely to influence future research and practical deployments. Experimenting with or implementing the described attack would be valuable for both researchers and practitioners to understand and mitigate these risks. No code repository is provided in the metadata.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Ethan Wilson', 'Kai Yue', 'Chau-Wai Wong', 'Huaiyu Dai']&lt;/li&gt;&lt;li&gt;Tags: ['federated learning', 'privacy attack', 'gradient inversion', 'image reconstruction', 'data leakage']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2410.04661</guid><pubDate>Mon, 16 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Banded Square Root Matrix Factorization for Differentially Private Model Training</title><link>https://arxiv.org/abs/2405.13763</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a new matrix factorization method (BSR) to improve efficiency in differentially private model training.&lt;/li&gt;&lt;li&gt;Addresses computational bottlenecks in current state-of-the-art differentially private training methods.&lt;/li&gt;&lt;li&gt;Provides analytical solutions for common training scenarios, reducing computational overhead.&lt;/li&gt;&lt;li&gt;Includes theoretical bounds and empirical results in both centralized and federated learning settings.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research Paper&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Quality Assessment&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Clarity: 4/5&lt;/li&gt;&lt;li&gt;Novelty: 4/5&lt;/li&gt;&lt;li&gt;Significance: 4/5&lt;/li&gt;&lt;li&gt;Try-worthiness: Yes&lt;/li&gt;&lt;li&gt;Justification: The abstract is well-written and clearly explains the motivation, method, and results, though some technical terms may require background knowledge. The proposed BSR method appears novel, addressing a known computational bottleneck in differentially private model training by leveraging matrix square root properties and providing analytical solutions for common scenarios. The significance is promising, as the work claims to match state-of-the-art performance while greatly reducing computational overhead, which is a key practical concern. The paper is very recent and on arXiv, so citation count is not yet meaningful, but the topic is timely and relevant. The approach seems worth trying, especially for practitioners interested in efficient privacy-preserving machine learning. No code repository is provided in the metadata.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Nikita P. Kalinin', 'Christoph Lampert']&lt;/li&gt;&lt;li&gt;Tags: ['differential privacy', 'model training', 'privacy-preserving machine learning', 'federated learning']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2405.13763</guid><pubDate>Mon, 16 Jun 2025 00:00:00 +0000</pubDate></item><item><title>SecONNds: Secure Outsourced Neural Network Inference on ImageNet</title><link>https://arxiv.org/abs/2506.11586</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes SecONNds, a secure outsourced neural network inference framework for privacy-preserving computation on untrusted servers.&lt;/li&gt;&lt;li&gt;Introduces a novel Boolean GMW protocol and optimizations for secure comparison and efficient secure computation.&lt;/li&gt;&lt;li&gt;Demonstrates significant speedups and reduced communication overhead for large-scale image models (ImageNet).&lt;/li&gt;&lt;li&gt;Addresses privacy and security concerns in deploying neural network inference in resource-constrained and sensitive environments.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research Paper&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Quality Assessment&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Clarity: 5/5&lt;/li&gt;&lt;li&gt;Novelty: 5/5&lt;/li&gt;&lt;li&gt;Significance: 4/5&lt;/li&gt;&lt;li&gt;Try-worthiness: Yes&lt;/li&gt;&lt;li&gt;Justification: The abstract is very well-written, clearly outlining the motivation, technical contributions, and results. The work is highly novel, introducing a new secure inference framework (SecONNds) with a novel Boolean GMW protocol and significant performance improvements over state-of-the-art, especially for large-scale models like those on ImageNet. The significance is high given the practical speedups and reduced communication, though as a recent preprint, it has not yet accumulated citations or peer-reviewed validation. The open-source code and strong empirical results make it highly worth trying for researchers or practitioners in secure ML inference.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Shashank Balla']&lt;/li&gt;&lt;li&gt;Tags: ['secure inference', 'privacy-preserving AI', 'homomorphic encryption', 'neural network security', 'outsourced computation']&lt;/li&gt;&lt;li&gt;Code Repository: &lt;a href='https://github.com/shashankballa/SecONNds'&gt;https://github.com/shashankballa/SecONNds&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.11586</guid><pubDate>Mon, 16 Jun 2025 00:00:00 +0000</pubDate></item><item><title>On the Natural Robustness of Vision-Language Models Against Visual Perception Attacks in Autonomous Driving</title><link>https://arxiv.org/abs/2506.11472</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Investigates the robustness of vision-language models (V2LMs) against adversarial attacks in autonomous vehicle perception tasks.&lt;/li&gt;&lt;li&gt;Demonstrates that V2LMs maintain significantly higher accuracy under adversarial conditions compared to traditional DNNs.&lt;/li&gt;&lt;li&gt;Evaluates different deployment strategies (Solo Mode and Tandem Mode) for V2LMs in AV systems.&lt;/li&gt;&lt;li&gt;Suggests that integrating V2LMs can enhance the security and resilience of AV perception systems against adversarial threats.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research Paper&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Quality Assessment&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Clarity: 5/5&lt;/li&gt;&lt;li&gt;Novelty: 4/5&lt;/li&gt;&lt;li&gt;Significance: 4/5&lt;/li&gt;&lt;li&gt;Try-worthiness: Yes&lt;/li&gt;&lt;li&gt;Justification: The abstract is well-written, clearly outlining the motivation, methodology, and key findings. The work is novel in its application of fine-tuned vision-language models (V2LMs) to autonomous vehicle perception robustness, especially highlighting their natural resistance to adversarial attacks without adversarial training. While the venue is arXiv (preprint), the topic is highly relevant and timely, and the results (significantly improved robustness with minimal accuracy loss) are promising for the AV field. The paper is recent, so citation count is not yet meaningful, but the approach and results make it worth experimenting with. No code repository is provided.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Pedram MohajerAnsari (Clemson University', 'Clemson', 'SC', 'USA)', 'Amir Salarpour (Clemson University', 'Clemson', 'SC', 'USA)', 'Michael K\\"uhr (Technical University of Munich', 'Munich', 'Germany)', 'Siyu Huang (Clemson University', 'Clemson', 'SC', 'USA)', 'Mohammad Hamad (Technical University of Munich', 'Munich', 'Germany)', 'Sebastian Steinhorst (Technical University of Munich', 'Munich', 'Germany)', 'Habeeb Olufowobi (University of Texas at Arlington', 'Arlington', 'TX', 'USA)', "Mert D. Pes\\'e (Clemson University", 'Clemson', 'SC', 'USA)']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial robustness', 'autonomous vehicles', 'vision-language models', 'AI security', 'perception attacks']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.11472</guid><pubDate>Mon, 16 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Lifting Data-Tracing Machine Unlearning to Knowledge-Tracing for Foundation Models</title><link>https://arxiv.org/abs/2506.11253</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a shift from data-tracing machine unlearning to knowledge-tracing unlearning for foundation models.&lt;/li&gt;&lt;li&gt;Addresses practical challenges in fulfilling unlearning requests when direct access to training data is unavailable.&lt;/li&gt;&lt;li&gt;Highlights regulatory and user-driven needs for removing specific knowledge or capabilities from foundation models.&lt;/li&gt;&lt;li&gt;Provides a case study on applying knowledge-tracing unlearning to a vision-language foundation model.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Position Paper&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Quality Assessment&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Clarity: 4/5&lt;/li&gt;&lt;li&gt;Novelty: 5/5&lt;/li&gt;&lt;li&gt;Significance: 4/5&lt;/li&gt;&lt;li&gt;Try-worthiness: Yes&lt;/li&gt;&lt;li&gt;Justification: The paper is clearly written for a position paper, with a well-motivated abstract that explains the limitations of current data-tracing unlearning and proposes a novel shift to knowledge-tracing unlearning for foundation models. This is a new and timely idea, especially as regulatory and practical demands for unlearning in large models grow. The concept is significant because it addresses a real gap in current machine unlearning approaches, and the cognitive analogy strengthens the argument. The inclusion of a concrete case study adds practical value, making it worth experimenting with, especially for researchers or practitioners interested in model unlearning or compliance. However, as a position paper, it may lack detailed algorithms or empirical results, so implementation would require further development. No code repository is provided.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yuwen Tan', 'Boqing Gong']&lt;/li&gt;&lt;li&gt;Tags: ['machine unlearning', 'foundation models', 'regulatory compliance', 'knowledge removal', 'AI safety']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.11253</guid><pubDate>Mon, 16 Jun 2025 00:00:00 +0000</pubDate></item><item><title>CodeMirage: A Multi-Lingual Benchmark for Detecting AI-Generated and Paraphrased Source Code from Production-Level LLMs</title><link>https://arxiv.org/abs/2506.11059</link><description>&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Other&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Hanxi Guo', 'Siyuan Cheng', 'Kaiyuan Zhang', 'Guangyu Shen', 'Xiangyu Zhang']&lt;/li&gt;&lt;li&gt;Tags: []&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.11059</guid><pubDate>Mon, 16 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Compression Aware Certified Training</title><link>https://arxiv.org/abs/2506.11992</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes CACTUS, a framework for jointly optimizing neural network compression and certified robustness.&lt;/li&gt;&lt;li&gt;Addresses the challenge of deploying robust models in safety-critical, resource-constrained environments.&lt;/li&gt;&lt;li&gt;Demonstrates that models trained with CACTUS maintain high certified accuracy even after pruning and quantization.&lt;/li&gt;&lt;li&gt;Achieves state-of-the-art results in both efficiency and certifiable robustness across multiple datasets.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research Paper&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Quality Assessment&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Clarity: 4/5&lt;/li&gt;&lt;li&gt;Novelty: 4/5&lt;/li&gt;&lt;li&gt;Significance: 3/5&lt;/li&gt;&lt;li&gt;Try-worthiness: Yes&lt;/li&gt;&lt;li&gt;Justification: The abstract is well-written and clearly states the motivation, problem, and proposed solution (CACTUS). The work appears novel in its unified approach to compression and certified robustness, which are typically treated separately. While the significance is hard to fully assess due to its recent publication and lack of citations, the problem addressed is important for deploying neural networks in safety-critical, resource-constrained environments. The claim of state-of-the-art results and general applicability to both pruning and quantization further increases its try-worthiness. No code repository is provided in the metadata.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Changming Xu', 'Gagandeep Singh']&lt;/li&gt;&lt;li&gt;Tags: ['certified robustness', 'model compression', 'robustness', 'safety-critical AI', 'pruning and quantization']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.11992</guid><pubDate>Mon, 16 Jun 2025 00:00:00 +0000</pubDate></item><item><title>SEC-bench: Automated Benchmarking of LLM Agents on Real-World Software Security Tasks</title><link>https://arxiv.org/abs/2506.11791</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces SEC-bench, an automated benchmarking framework for evaluating LLM agents on real-world software security tasks.&lt;/li&gt;&lt;li&gt;Focuses on authentic security engineering tasks, including proof-of-concept (PoC) generation and vulnerability patching.&lt;/li&gt;&lt;li&gt;Benchmarks LLM agents using automatically generated, high-quality vulnerability datasets and reproducible artifacts.&lt;/li&gt;&lt;li&gt;Finds significant performance gaps in current LLM agents, highlighting the need for improved AI security capabilities.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Quality Assessment&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Clarity: 5/5&lt;/li&gt;&lt;li&gt;Novelty: 5/5&lt;/li&gt;&lt;li&gt;Significance: 4/5&lt;/li&gt;&lt;li&gt;Try-worthiness: Yes&lt;/li&gt;&lt;li&gt;Justification: The abstract is very well-written, clearly outlining the motivation, methodology, and key findings of the work. The novelty is high, as SEC-bench claims to be the first fully automated benchmarking framework for LLM agents on real-world software security tasks, moving beyond synthetic datasets. The significance is strong given the importance of security in LLM deployment, though as a very recent preprint, it has not yet accumulated citations or peer-reviewed validation. The results (low success rates for current LLMs) are impactful and suggest a real gap in current capabilities, making this work highly relevant for further experimentation and implementation. No code repository link is provided in the metadata or abstract.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Hwiwon Lee', 'Ziqi Zhang', 'Hanxiao Lu', 'Lingming Zhang']&lt;/li&gt;&lt;li&gt;Tags: ['LLM security evaluation', 'benchmarking', 'software vulnerability', 'vulnerability patching', 'proof-of-concept generation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.11791</guid><pubDate>Mon, 16 Jun 2025 00:00:00 +0000</pubDate></item><item><title>KCES: Training-Free Defense for Robust Graph Neural Networks via Kernel Complexity</title><link>https://arxiv.org/abs/2506.11611</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a training-free, model-agnostic defense framework (KCES) for Graph Neural Networks (GNNs) against adversarial attacks.&lt;/li&gt;&lt;li&gt;Introduces a novel metric, Graph Kernel Complexity (GKC), to characterize GNN generalization and detect adversarial perturbations.&lt;/li&gt;&lt;li&gt;Removes edges with high KC scores to mitigate the impact of adversarial attacks and improve robustness.&lt;/li&gt;&lt;li&gt;Demonstrates through theoretical analysis and experiments that KCES enhances GNN security and can be integrated with other defenses.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research Paper&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Quality Assessment&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Clarity: 4/5&lt;/li&gt;&lt;li&gt;Novelty: 5/5&lt;/li&gt;&lt;li&gt;Significance: 4/5&lt;/li&gt;&lt;li&gt;Try-worthiness: Yes&lt;/li&gt;&lt;li&gt;Justification: The abstract is well-written and clearly explains the motivation, method, and contributions of the paper, though some technical terms (e.g., 'Graph Kernel Complexity') may require background knowledge. The proposed KCES framework introduces a novel, training-free, and model-agnostic defense for GNNs based on a new metric (GKC), which appears to be a significant conceptual advance over heuristic or attack-specific defenses. While the paper is very recent and has no citations yet, the problem addressed (robustness of GNNs) is important, and the claimed improvements over state-of-the-art methods suggest high potential significance. The method's plug-and-play nature and lack of training requirements make it practical and worth trying for researchers and practitioners in robust GNNs. No code repository is provided in the metadata.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yaning Jia', 'Shenyang Deng', 'Chiyu Ma', 'Yaoqing Yang', 'Soroush Vosoughi']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial robustness', 'graph neural networks', 'defense methods', 'AI security']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.11611</guid><pubDate>Mon, 16 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Bias Amplification in RAG: Poisoning Knowledge Retrieval to Steer LLMs</title><link>https://arxiv.org/abs/2506.11415</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Investigates how poisoning attacks in Retrieval-Augmented Generation (RAG) systems can amplify biases in large language models.&lt;/li&gt;&lt;li&gt;Proposes a Bias Retrieval and Reward Attack (BRRA) framework to systematically manipulate RAG retrieval to steer LLM outputs toward biased responses.&lt;/li&gt;&lt;li&gt;Demonstrates the effectiveness of the attack using adversarial document generation, subspace projection, and cyclic feedback mechanisms.&lt;/li&gt;&lt;li&gt;Explores a dual-stage defense mechanism to mitigate the impact of such attacks and discusses the intersection of RAG security and model fairness.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research Paper&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Quality Assessment&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Clarity: 4/5&lt;/li&gt;&lt;li&gt;Novelty: 5/5&lt;/li&gt;&lt;li&gt;Significance: 4/5&lt;/li&gt;&lt;li&gt;Try-worthiness: Yes&lt;/li&gt;&lt;li&gt;Justification: The abstract is well-written and clearly explains the motivation, methodology, and findings of the paper, though some technical details are dense. The work is highly novel, as it identifies and systematically studies a previously underexplored security risk in RAG systems: the amplification of bias via poisoning attacks. This is a new angle compared to most prior work, which focuses on output quality rather than fairness or bias. The significance is high given the increasing deployment of RAG systems and the importance of fairness in AI, though the impact is yet to be seen due to the paper's recency and preprint status. The proposed BRRA framework and dual-stage defense mechanism make the work actionable and worth experimenting with, especially for researchers and practitioners concerned with LLM security and fairness. No code repository is provided in the metadata or abstract.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Linlin Wang', 'Tianqing Zhu', 'Laiqiao Qin', 'Longxiang Gao', 'Wanlei Zhou']&lt;/li&gt;&lt;li&gt;Tags: ['RAG security', 'data poisoning', 'bias amplification', 'adversarial attacks', 'defense mechanisms']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.11415</guid><pubDate>Mon, 16 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Byzantine Outside, Curious Inside: Reconstructing Data Through Malicious Updates</title><link>https://arxiv.org/abs/2506.11413</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces a novel threat model in federated learning: the 'maliciously curious client' that manipulates its own gradients to reconstruct private data from peers.&lt;/li&gt;&lt;li&gt;Demonstrates both theoretically and empirically that this attack can successfully reconstruct sensitive data during federated learning training.&lt;/li&gt;&lt;li&gt;Shows that existing server-side and client-side defenses may not only fail to prevent this attack but can sometimes amplify data leakage.&lt;/li&gt;&lt;li&gt;Highlights a critical blind spot in current federated learning security and privacy mechanisms.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research Paper&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Quality Assessment&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Clarity: 5/5&lt;/li&gt;&lt;li&gt;Novelty: 5/5&lt;/li&gt;&lt;li&gt;Significance: 4/5&lt;/li&gt;&lt;li&gt;Try-worthiness: Yes&lt;/li&gt;&lt;li&gt;Justification: The abstract is very well-written, clearly outlining the motivation, threat model, methodology, and key findings. The introduction of a 'maliciously curious client' as a new threat model in federated learning is highly novel, especially as it repurposes Byzantine adversarial strategies for privacy attacks rather than robustness. The significance is high given the critical blind spot identified in current FL defenses, though as a recent preprint, its impact is not yet reflected in citations. The work is worth experimenting with, as it exposes a practical and underexplored vulnerability in FL systems. No code repository is provided in the metadata.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Kai Yue', 'Richeng Jin', 'Chau-Wai Wong', 'Huaiyu Dai']&lt;/li&gt;&lt;li&gt;Tags: ['federated learning', 'privacy attack', 'gradient inversion', 'data reconstruction', 'Byzantine adversary']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.11413</guid><pubDate>Mon, 16 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Evaluating Privacy-Utility Tradeoffs in Synthetic Smart Grid Data</title><link>https://arxiv.org/abs/2506.11026</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Compares four synthetic data generation methods for smart grid data with a focus on privacy and utility tradeoffs.&lt;/li&gt;&lt;li&gt;Evaluates privacy leakage, specifically resistance to reconstruction attacks, for each method.&lt;/li&gt;&lt;li&gt;Finds that CTGAN offers the strongest privacy protection, while diffusion models provide the best utility.&lt;/li&gt;&lt;li&gt;Highlights implications for privacy-preserving data sharing in energy systems.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Quality Assessment&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Clarity: 5/5&lt;/li&gt;&lt;li&gt;Novelty: 4/5&lt;/li&gt;&lt;li&gt;Significance: 4/5&lt;/li&gt;&lt;li&gt;Try-worthiness: Yes&lt;/li&gt;&lt;li&gt;Justification: The paper is clearly written, with a well-structured abstract that concisely explains the motivation, methods, and key findings. The comparative evaluation of modern generative models (WGAN, CTGAN, Diffusion Models) for privacy-utility tradeoffs in synthetic smart grid data is timely and relevant, especially given the increasing importance of privacy in energy data. While the use of these models is not entirely novel, their application to this specific domain and the systematic comparison (including privacy leakage and utility) adds value. The results (e.g., diffusion models achieving high utility, CTGAN resisting reconstruction attacks) are actionable and informative for practitioners. The work is significant for researchers and engineers working on privacy-preserving data generation in energy systems. No code repository is provided, but the methodology is likely reproducible given the popularity of the models used.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Andre Catarino', 'Rui Melo', 'Rui Abreu', 'Luis Cruz']&lt;/li&gt;&lt;li&gt;Tags: ['privacy attacks', 'synthetic data', 'reconstruction attacks', 'privacy-preserving AI']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.11026</guid><pubDate>Mon, 16 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Evidential Spectrum-Aware Contrastive Learning for OOD Detection in Dynamic Graphs</title><link>https://arxiv.org/abs/2506.07417</link><description>&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Other&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Quality Assessment&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Clarity: 4/5&lt;/li&gt;&lt;li&gt;Novelty: 4/5&lt;/li&gt;&lt;li&gt;Significance: 3/5&lt;/li&gt;&lt;li&gt;Try-worthiness: Yes&lt;/li&gt;&lt;li&gt;Justification: The paper is clearly written, with a well-structured abstract that outlines the motivation, challenges, and proposed solution (EviSEC) for OOD detection in dynamic graphs. The use of Evidential Deep Learning and spectrum-aware contrastive learning for this specific problem appears novel, especially as most prior work focuses on static graphs. The significance is moderate: while OOD detection in dynamic graphs is important, the paper is a recent arXiv preprint with no citations yet, so its impact is not yet established. However, the approach is promising and addresses real challenges, making it worth trying for researchers in graph learning or OOD detection. No code repository is provided in the metadata.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Nan Sun', 'Xixun Lin', 'Zhiheng Zhou', 'Yanmin Shang', 'Zhenlin Cheng', 'Yanan Cao']&lt;/li&gt;&lt;li&gt;Tags: []&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.07417</guid><pubDate>Mon, 16 Jun 2025 00:00:00 +0000</pubDate></item><item><title>DURA-CPS: A Multi-Role Orchestrator for Dependability Assurance in LLM-Enabled Cyber-Physical Systems</title><link>https://arxiv.org/abs/2506.06381</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces DURA-CPS, a framework for dependability assurance in LLM-enabled cyber-physical systems.&lt;/li&gt;&lt;li&gt;Automates verification and validation by assigning roles such as safety monitoring, security assessment, and fault injection to agents.&lt;/li&gt;&lt;li&gt;Continuously evaluates AI behavior for safety and security vulnerabilities in critical CPS applications.&lt;/li&gt;&lt;li&gt;Demonstrates effectiveness through a case study with autonomous vehicles, showing detection of vulnerabilities and support for adaptive recovery.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research Paper&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Quality Assessment&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Clarity: 4/5&lt;/li&gt;&lt;li&gt;Novelty: 5/5&lt;/li&gt;&lt;li&gt;Significance: 4/5&lt;/li&gt;&lt;li&gt;Try-worthiness: Yes&lt;/li&gt;&lt;li&gt;Justification: The abstract is well-written and clearly explains the motivation, approach, and results of the paper, though some technical details are left for the full text. The concept of a multi-role orchestrator for dependability assurance in LLM-enabled CPS appears novel, especially with the explicit assignment of roles like safety monitoring and recovery planning to agents. The significance is high given the growing importance of AI in safety-critical CPS, though as a recent preprint, it has not yet accumulated citations or been peer-reviewed. The framework's demonstration on an autonomous vehicle scenario and its extensibility make it worth trying for researchers or practitioners in AI safety and CPS. No code repository is provided in the metadata.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Trisanth Srinivasan', 'Santosh Patapati', 'Himani Musku', 'Idhant Gode', 'Aditya Arora', 'Samvit Bhattacharya', 'Abubakr Nazriev', 'Sanika Hirave', 'Zaryab Kanjiani', 'Srinjoy Ghose']&lt;/li&gt;&lt;li&gt;Tags: ['AI safety', 'AI security', 'cyber-physical systems', 'verification and validation', 'autonomous vehicles']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.06381</guid><pubDate>Mon, 16 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Training RL Agents for Multi-Objective Network Defense Tasks</title><link>https://arxiv.org/abs/2505.22531</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a training approach for reinforcement learning agents to act as autonomous network defenders in cybersecurity settings.&lt;/li&gt;&lt;li&gt;Applies open-ended learning (OEL) principles to improve robustness and generalization of AI agents in network defense tasks.&lt;/li&gt;&lt;li&gt;Addresses technical challenges in representing diverse cyber defense tasks with consistent interfaces for goals, rewards, and actions.&lt;/li&gt;&lt;li&gt;Aims to influence the development of benchmarks and gyms for AI-based cyber defense research.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research Paper&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Quality Assessment&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Clarity: 4/5&lt;/li&gt;&lt;li&gt;Novelty: 4/5&lt;/li&gt;&lt;li&gt;Significance: 3/5&lt;/li&gt;&lt;li&gt;Try-worthiness: Yes&lt;/li&gt;&lt;li&gt;Justification: The abstract is well-written and clearly communicates the motivation, approach, and potential impact of the work, though some technical details are left for the full paper. The application of open-ended learning (OEL) principles to multi-objective network defense in RL is a novel direction, especially in the context of cybersecurity, which is less explored compared to other RL domains. The significance is moderate at this stage: while the venue is arXiv (preprint), the topic is timely and relevant, but the lack of peer review and citations (due to recency) limits its immediate impact. The approach appears promising and worth experimenting with, especially for researchers in AI for cybersecurity. No code repository is provided in the metadata.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Andres Molina-Markham', 'Luis Robaina', 'Sean Steinle', 'Akash Trivedi', 'Derek Tsui', 'Nicholas Potteiger', 'Lauren Brandt', 'Ransom Winder', 'Ahmad Ridley']&lt;/li&gt;&lt;li&gt;Tags: ['AI security', 'cyber defense', 'reinforcement learning', 'robustness', 'autonomous agents']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2505.22531</guid><pubDate>Mon, 16 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Safer or Luckier? LLMs as Safety Evaluators Are Not Robust to Artifacts</title><link>https://arxiv.org/abs/2503.09347</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Evaluates the robustness of LLMs when used as automated safety evaluators for generated content.&lt;/li&gt;&lt;li&gt;Finds that LLM judges are susceptible to input artifacts (e.g., apologetic or verbose phrasing), which can significantly bias safety assessments.&lt;/li&gt;&lt;li&gt;Shows that model size does not guarantee increased robustness to such artifacts.&lt;/li&gt;&lt;li&gt;Explores jury-based aggregation of multiple LLMs to improve robustness, but artifact sensitivity remains an issue.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research Paper&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Quality Assessment&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Clarity: 5/5&lt;/li&gt;&lt;li&gt;Novelty: 4/5&lt;/li&gt;&lt;li&gt;Significance: 4/5&lt;/li&gt;&lt;li&gt;Try-worthiness: Yes&lt;/li&gt;&lt;li&gt;Justification: The abstract is well-written, clearly outlining the motivation, methodology, and key findings. The paper addresses a timely and important issue: the robustness of LLMs as safety evaluators, especially in the presence of input artifacts. The finding that apologetic language can drastically skew safety judgments is novel and impactful, as is the observation that larger models are not always more robust. The exploration of jury-based aggregation is a practical contribution. While the paper is a preprint and very recent (hence no citations yet), the topic is highly significant for both research and deployment of LLMs in safety-critical settings. The work is worth experimenting with, especially for those developing or relying on LLM-based evaluation pipelines. No code repository is provided in the metadata.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Hongyu Chen', 'Seraphina Goldfarb-Tarrant']&lt;/li&gt;&lt;li&gt;Tags: ['LLM safety evaluation', 'robustness', 'adversarial artifacts', 'AI safety', 'evaluation bias']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2503.09347</guid><pubDate>Mon, 16 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Unsafe LLM-Based Search: Quantitative Analysis and Mitigation of Safety Risks in AI Web Search</title><link>https://arxiv.org/abs/2502.04951</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Quantitatively analyzes safety risks in LLM-based AI-powered search engines (AIPSEs), focusing on the dissemination of malicious or harmful content.&lt;/li&gt;&lt;li&gt;Defines threat models and risk types, and evaluates responses of seven production AIPSEs to various query types using real-world malicious URL datasets.&lt;/li&gt;&lt;li&gt;Demonstrates that AIPSEs can be deceived into quoting or citing malicious content, with case studies on document spoofing and phishing.&lt;/li&gt;&lt;li&gt;Proposes and evaluates an agent-based defense using GPT-4.1 for content refinement and URL detection, showing effective risk mitigation.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research Paper&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Quality Assessment&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Clarity: 5/5&lt;/li&gt;&lt;li&gt;Novelty: 4/5&lt;/li&gt;&lt;li&gt;Significance: 4/5&lt;/li&gt;&lt;li&gt;Try-worthiness: Yes&lt;/li&gt;&lt;li&gt;Justification: The abstract is well-written, clearly outlining the motivation, methodology, findings, and contributions of the paper. The work is novel in that it provides the first quantitative safety risk analysis of LLM-based search engines, a timely and important topic as such systems become more prevalent. The significance is high due to the practical implications for AI-powered search safety, though the impact is yet to be measured given the paper's recency and preprint status. The proposed mitigation strategy (agent-based defense with GPT-4.1 and a URL detector) is actionable and worth experimenting with, especially for those developing or deploying AI search systems. No code repository is provided in the metadata.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Zeren Luo', 'Zifan Peng', 'Yule Liu', 'Zhen Sun', 'Mingchen Li', 'Jingyi Zheng', 'Xinlei He']&lt;/li&gt;&lt;li&gt;Tags: ['LLM safety', 'AI web search', 'malicious content detection', 'risk mitigation', 'phishing']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2502.04951</guid><pubDate>Mon, 16 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Graph of Attacks with Pruning: Optimizing Stealthy Jailbreak Prompt Generation for Enhanced LLM Content Moderation</title><link>https://arxiv.org/abs/2501.18638</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces the GAP (Graph of Attacks with Pruning) framework for generating stealthy jailbreak prompts targeting LLMs.&lt;/li&gt;&lt;li&gt;Proposes a graph-based approach to improve over tree-based jailbreak methods, enabling knowledge sharing across attack paths.&lt;/li&gt;&lt;li&gt;Demonstrates significant improvements in attack success rates and efficiency for both open and closed LLMs.&lt;/li&gt;&lt;li&gt;Shows that GAP-generated prompts can enhance content moderation systems when used for fine-tuning.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research Paper&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Quality Assessment&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Clarity: 4/5&lt;/li&gt;&lt;li&gt;Novelty: 5/5&lt;/li&gt;&lt;li&gt;Significance: 4/5&lt;/li&gt;&lt;li&gt;Try-worthiness: Yes&lt;/li&gt;&lt;li&gt;Justification: The abstract is well-written and clearly explains the motivation, methodology, and results of the paper, though some technical details are omitted (clarity: 4). The introduction of a graph-based approach (GAP) for jailbreak prompt generation, as opposed to tree-based methods, and the inclusion of automated and multimodal variants, represent a novel contribution (novelty: 5). The reported improvements in attack success rates and moderation system performance are substantial, indicating potential significance, though the paper is very new and not yet peer-reviewed or cited (significance: 4). The availability of code and strong reported results make this paper worth trying for researchers and practitioners in LLM safety and adversarial robustness (try_worthiness: true).&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Daniel Schwartz', 'Dmitriy Bespalov', 'Zhe Wang', 'Ninad Kulkarni', 'Yanjun Qi']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'jailbreak prompts', 'adversarial prompting', 'content moderation', 'multimodal attacks']&lt;/li&gt;&lt;li&gt;Code Repository: &lt;a href='https://github.com/dsbuddy/GAP-LLM-Safety'&gt;https://github.com/dsbuddy/GAP-LLM-Safety&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2501.18638</guid><pubDate>Mon, 16 Jun 2025 00:00:00 +0000</pubDate></item><item><title>How Well Do Large Language Models Serve as End-to-End Secure Code Agents for Python?</title><link>https://arxiv.org/abs/2408.10495</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Systematically investigates the ability of large language models (LLMs) to generate secure Python code and repair vulnerabilities.&lt;/li&gt;&lt;li&gt;Benchmarks GPT-3.5, GPT-4, Code Llama, and CodeGeeX2 on their capacity to identify and fix security flaws in code.&lt;/li&gt;&lt;li&gt;Finds that LLMs frequently generate vulnerable code and struggle to identify or repair their own security issues.&lt;/li&gt;&lt;li&gt;Proposes and evaluates an iterative repair tool that leverages LLMs and semantic analysis to improve code security.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research Paper&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Quality Assessment&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Clarity: 4/5&lt;/li&gt;&lt;li&gt;Novelty: 4/5&lt;/li&gt;&lt;li&gt;Significance: 4/5&lt;/li&gt;&lt;li&gt;Try-worthiness: Yes&lt;/li&gt;&lt;li&gt;Justification: The paper is clearly written, with a well-structured abstract that outlines the motivation, methodology, and key findings. The focus on the security of LLM-generated code, especially in an end-to-end context and with iterative repair, is timely and novel given the rapid adoption of LLMs in software engineering. The systematic evaluation across multiple LLMs and the development of a lightweight iterative repair tool add practical value. While the paper is a preprint and has not yet accrued citations, its topic is highly significant for both research and industry. The results suggest actionable insights and a tool that could be worth experimenting with for those interested in secure code generation. No code repository link is provided in the metadata.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jianian Gong', 'Nachuan Duan', 'Ziheng Tao', 'Zhaohui Gong', 'Yuan Yuan', 'Minlie Huang']&lt;/li&gt;&lt;li&gt;Tags: ['LLM security', 'code vulnerability', 'secure code generation', 'automated code repair', 'benchmarking']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2408.10495</guid><pubDate>Mon, 16 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Self-interpreting Adversarial Images</title><link>https://arxiv.org/abs/2407.08970</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces a novel cross-modal injection attack on visual language models using 'self-interpreting images' that embed hidden meta-instructions.&lt;/li&gt;&lt;li&gt;Demonstrates that these adversarial images can control model outputs to express adversary-chosen styles, sentiments, or viewpoints.&lt;/li&gt;&lt;li&gt;Shows that the attacks are effective across various models and can be used for harmful purposes such as misinformation or spam.&lt;/li&gt;&lt;li&gt;Discusses potential defenses against these attacks.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research Paper&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Quality Assessment&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Clarity: 5/5&lt;/li&gt;&lt;li&gt;Novelty: 5/5&lt;/li&gt;&lt;li&gt;Significance: 4/5&lt;/li&gt;&lt;li&gt;Try-worthiness: Yes&lt;/li&gt;&lt;li&gt;Justification: The abstract is well-written, clearly explaining the concept of 'self-interpreting images' and their implications for visual language models. The idea of cross-modal, indirect prompt injection via images is highly novel and timely, especially as multimodal models become more prevalent. While the paper is very recent and thus has no citations yet, the topic is significant due to the potential security and societal risks described. The work is worth experimenting with, both to understand the attack and to develop defenses. No code repository is provided in the metadata.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Tingwei Zhang', 'Collin Zhang', 'John X. Morris', 'Eugene Bagdasarian', 'Vitaly Shmatikov']&lt;/li&gt;&lt;li&gt;Tags: ['prompt injection', 'adversarial attacks', 'visual language models', 'cross-modal security', 'AI safety']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2407.08970</guid><pubDate>Mon, 16 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Evaluating Implicit Bias in Large Language Models by Attacking From a Psychometric Perspective</title><link>https://arxiv.org/abs/2406.14023</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes novel attack methods (Disguise, Deception, Teaching) to elicit and evaluate implicit bias in LLMs.&lt;/li&gt;&lt;li&gt;Builds two benchmarks for systematically testing LLM susceptibility to implicit bias attacks.&lt;/li&gt;&lt;li&gt;Demonstrates that these attack strategies can more effectively reveal LLM biases compared to existing baselines.&lt;/li&gt;&lt;li&gt;Contributes tools and datasets for assessing ethical and safety risks in LLM deployment.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research Paper&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Quality Assessment&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Clarity: 5/5&lt;/li&gt;&lt;li&gt;Novelty: 4/5&lt;/li&gt;&lt;li&gt;Significance: 4/5&lt;/li&gt;&lt;li&gt;Try-worthiness: Yes&lt;/li&gt;&lt;li&gt;Justification: The abstract is well-written, clearly outlining the motivation, methodology, and contributions of the paper. The approach of attacking LLMs from a psychometric perspective to elicit implicit bias is novel and leverages principles from cognitive and social psychology, which is not commonly seen in LLM bias evaluation. The creation of two new benchmarks, including a large-scale and bilingual dataset, adds to the significance and utility of the work. Although the paper is very recent and has no citations yet, the topic is highly relevant and the resources (code, data, benchmarks) are openly available, making it worth trying for researchers and practitioners interested in LLM ethics and bias evaluation.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yuchen Wen', 'Keping Bi', 'Wei Chen', 'Jiafeng Guo', 'Xueqi Cheng']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'adversarial prompting', 'AI safety evaluation', 'bias assessment', 'ethical risk']&lt;/li&gt;&lt;li&gt;Code Repository: &lt;a href='https://yuchenwen1.github.io/ImplicitBiasEvaluation/'&gt;https://yuchenwen1.github.io/ImplicitBiasEvaluation/&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2406.14023</guid><pubDate>Mon, 16 Jun 2025 00:00:00 +0000</pubDate></item><item><title>PLeak: Prompt Leaking Attacks against Large Language Model Applications</title><link>https://arxiv.org/abs/2405.06823</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces PLeak, a novel closed-box attack framework for leaking system prompts from LLM applications.&lt;/li&gt;&lt;li&gt;Formulates prompt leaking as an optimization problem and uses a gradient-based method to generate adversarial queries.&lt;/li&gt;&lt;li&gt;Demonstrates the effectiveness of PLeak in both offline and real-world LLM application settings, outperforming existing prompt leaking and jailbreaking baselines.&lt;/li&gt;&lt;li&gt;Highlights the security risk of prompt leakage, which can compromise intellectual property of LLM application developers.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research Paper&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Quality Assessment&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Clarity: 5/5&lt;/li&gt;&lt;li&gt;Novelty: 4/5&lt;/li&gt;&lt;li&gt;Significance: 4/5&lt;/li&gt;&lt;li&gt;Try-worthiness: Yes&lt;/li&gt;&lt;li&gt;Justification: The abstract is well-written, clearly explaining the motivation, methodology, and results of the work. The paper introduces a novel, optimization-based prompt leaking attack (PLeak) that goes beyond manually crafted queries, which is a meaningful advancement in the area of LLM security. While the paper is very recent and thus has no citations yet, the topic is highly relevant given the rapid deployment of LLM applications and the importance of prompt confidentiality. The evaluation on real-world platforms and the availability of code make it highly try-worthy for researchers and practitioners interested in LLM security.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Bo Hui', 'Haolin Yuan', 'Neil Gong', 'Philippe Burlina', 'Yinzhi Cao']&lt;/li&gt;&lt;li&gt;Tags: ['prompt leaking', 'adversarial attacks', 'LLM security', 'intellectual property', 'prompt extraction']&lt;/li&gt;&lt;li&gt;Code Repository: &lt;a href='https://github.com/BHui97/PLeak'&gt;https://github.com/BHui97/PLeak&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2405.06823</guid><pubDate>Mon, 16 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Manipulating Feature Visualizations with Gradient Slingshots</title><link>https://arxiv.org/abs/2401.06122</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces a method (Gradient Slingshots) to manipulate feature visualizations in deep neural networks without altering model architecture.&lt;/li&gt;&lt;li&gt;Demonstrates that feature visualizations can be coerced to produce arbitrary, misleading explanations.&lt;/li&gt;&lt;li&gt;Highlights a security vulnerability where auditors may be deceived by manipulated visualizations.&lt;/li&gt;&lt;li&gt;Proposes and evaluates a defense mechanism to mitigate this risk.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research Paper&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Quality Assessment&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Clarity: 4/5&lt;/li&gt;&lt;li&gt;Novelty: 5/5&lt;/li&gt;&lt;li&gt;Significance: 4/5&lt;/li&gt;&lt;li&gt;Try-worthiness: Yes&lt;/li&gt;&lt;li&gt;Justification: The abstract is well-written and clearly explains the motivation, method, and implications of the work, though some technical terms may require background knowledge (clarity: 4). The proposed 'Gradient Slingshots' method for manipulating feature visualizations appears novel, especially in exposing vulnerabilities in a widely used interpretability technique (novelty: 5). The significance is high, as it highlights a critical security/robustness issue in model interpretability, which is a major concern in trustworthy AI, though the impact is yet to be seen due to the paper's recency and preprint status (significance: 4). The work is worth trying for researchers in interpretability and security, as it both demonstrates a new attack and proposes a defense (try_worthiness: true). No code repository is provided in the metadata.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Dilyara Bareeva', 'Marina M. -C. H\\"ohne', 'Alexander Warnecke', 'Lukas Pirch', 'Klaus-Robert M\\"uller', 'Konrad Rieck', 'Sebastian Lapuschkin', 'Kirill Bykov']&lt;/li&gt;&lt;li&gt;Tags: ['model manipulation', 'explainability security', 'adversarial attacks', 'AI safety', 'defenses']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2401.06122</guid><pubDate>Mon, 16 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Epistemic Artificial Intelligence is Essential for Machine Learning Models to Truly `Know When They Do Not Know'</title><link>https://arxiv.org/abs/2505.04950</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Argues for a paradigm shift towards epistemic AI to better manage uncertainty in machine learning models.&lt;/li&gt;&lt;li&gt;Highlights the limitations of current uncertainty quantification methods in handling unfamiliar or adversarial data.&lt;/li&gt;&lt;li&gt;Proposes the use of second-order uncertainty measures to improve the robustness and resilience of AI systems.&lt;/li&gt;&lt;li&gt;Focuses on enabling AI models to recognize and communicate their own ignorance, which is critical for safe deployment.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Position Paper&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Quality Assessment&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Clarity: 4/5&lt;/li&gt;&lt;li&gt;Novelty: 4/5&lt;/li&gt;&lt;li&gt;Significance: 3/5&lt;/li&gt;&lt;li&gt;Try-worthiness: No&lt;/li&gt;&lt;li&gt;Justification: The abstract is well-written and clearly articulates the motivation and proposed paradigm shift towards 'epistemic artificial intelligence' using second-order uncertainty measures. The idea of explicitly modeling epistemic uncertainty is not entirely new, but the emphasis on a paradigm shift and the use of second-order measures adds a degree of novelty. However, as a position paper (rather than an empirical or technical contribution), its significance is more conceptual than practical at this stage. There is no code repository provided, and the paper does not appear to present a concrete method or implementation to try out. Thus, while the ideas are interesting and potentially impactful, the paper is not immediately try-worthy for implementation or experimentation.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Shireen Kudukkil Manchingal', 'Andrew Bradley', 'Julian F. P. Kooij', 'Keivan Shariatmadar', 'Neil Yorke-Smith', 'Fabio Cuzzolin']&lt;/li&gt;&lt;li&gt;Tags: ['uncertainty quantification', 'robustness', 'AI safety', 'epistemic AI', 'adversarial robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2505.04950</guid><pubDate>Mon, 16 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Tell Me What You Don't Know: Enhancing Refusal Capabilities of Role-Playing Agents via Representation Space Analysis and Editing</title><link>https://arxiv.org/abs/2409.16913</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Develops a benchmark to evaluate role-playing agents' (RPAs) ability to refuse answering conflicting or inappropriate requests.&lt;/li&gt;&lt;li&gt;Analyzes the internal representation space of RPAs to identify regions associated with refusal and direct response behaviors.&lt;/li&gt;&lt;li&gt;Proposes a representation editing method to enhance RPAs' refusal accuracy for conflicting requests.&lt;/li&gt;&lt;li&gt;Demonstrates improved refusal capabilities without degrading general performance.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research Paper&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Quality Assessment&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Clarity: 4/5&lt;/li&gt;&lt;li&gt;Novelty: 4/5&lt;/li&gt;&lt;li&gt;Significance: 3/5&lt;/li&gt;&lt;li&gt;Try-worthiness: Yes&lt;/li&gt;&lt;li&gt;Justification: The paper is clearly written, with a well-structured abstract that outlines the problem, methodology, and contributions. The focus on refusal capabilities in Role-Playing Agents (RPAs) via representation space analysis and editing is a novel angle, especially the identification and manipulation of 'rejection regions' in the model's representation space. While the work is new (no citations yet, as expected for a recent preprint), the problem addressed is important for safety and reliability in conversational AI. The proposed lightweight editing method appears practical and could be valuable for researchers or practitioners working on safe AI agents, making the paper worth trying out. However, as it is an arXiv preprint, its significance is moderate until peer-reviewed validation or broader adoption. No code repository is provided in the metadata.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Wenhao Liu', 'Siyu An', 'Junru Lu', 'Muling Wu', 'Tianlong Li', 'Xiaohua Wang', 'Changze lv', 'Xiaoqing Zheng', 'Di Yin', 'Xing Sun', 'Xuanjing Huang']&lt;/li&gt;&lt;li&gt;Tags: ['refusal', 'harmful output prevention', 'AI safety', 'role-playing agents', 'representation editing']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2409.16913</guid><pubDate>Mon, 16 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Technical Evaluation of a Disruptive Approach in Homomorphic AI</title><link>https://arxiv.org/abs/2506.11954</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Evaluates a novel cryptographic approach (HbHAI) enabling AI algorithms to process encrypted data without modification.&lt;/li&gt;&lt;li&gt;Assesses the security, operability, and performance of HbHAI compared to existing homomorphic encryption schemes.&lt;/li&gt;&lt;li&gt;Presents technical results from independent analysis using standard AI algorithms on HbHAI-protected datasets.&lt;/li&gt;&lt;li&gt;Focuses on data security and privacy in AI workflows.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research Paper&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Quality Assessment&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Clarity: 3/5&lt;/li&gt;&lt;li&gt;Novelty: 4/5&lt;/li&gt;&lt;li&gt;Significance: 3/5&lt;/li&gt;&lt;li&gt;Try-worthiness: No&lt;/li&gt;&lt;li&gt;Justification: The abstract is somewhat clear in describing the main contribution (HbHAI, a hash-based homomorphic AI approach), but suffers from awkward phrasing and some grammatical errors, which reduce clarity. The novelty appears high, as the approach claims to enable native AI algorithms to operate on encrypted data without modification, which is a significant departure from traditional homomorphic encryption schemes. However, the lack of technical detail and the fact that the datasets are not public limit the ability to assess or reproduce the results. The significance is moderate: while the problem is important, the paper is a preprint with no citations yet, and the claims are not independently verified. Try-worthiness is rated as false for now, due to the absence of code, non-public datasets, and limited technical detail, making it difficult for others to experiment or implement the approach. No code repository is provided.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Eric Filiol']&lt;/li&gt;&lt;li&gt;Tags: ['homomorphic encryption', 'AI security', 'privacy-preserving AI', 'cryptography']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.11954</guid><pubDate>Mon, 16 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Improving Large Language Model Safety with Contrastive Representation Learning</title><link>https://arxiv.org/abs/2506.11938</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a defense framework for LLMs using contrastive representation learning to improve safety.&lt;/li&gt;&lt;li&gt;Finetunes models with a triplet-based loss and adversarial hard negative mining to separate benign and harmful representations.&lt;/li&gt;&lt;li&gt;Demonstrates improved robustness against both input-level and embedding-space adversarial attacks.&lt;/li&gt;&lt;li&gt;Shows that the method outperforms prior representation engineering-based defenses without sacrificing standard performance.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research Paper&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Quality Assessment&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Clarity: 4/5&lt;/li&gt;&lt;li&gt;Novelty: 4/5&lt;/li&gt;&lt;li&gt;Significance: 3/5&lt;/li&gt;&lt;li&gt;Try-worthiness: Yes&lt;/li&gt;&lt;li&gt;Justification: The abstract is well-written and clearly explains the motivation, method, and results of the paper. The use of contrastive representation learning (CRL) for LLM safety is a novel application, especially with the integration of triplet-based loss and adversarial hard negative mining. While the paper is a preprint and very recent (hence no citations yet), the topic is highly relevant given the increasing deployment of LLMs and the need for robust defenses. The reported improvements over prior representation engineering-based defenses suggest practical significance. The availability of code further increases the paper's try-worthiness for practitioners and researchers interested in LLM safety.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Samuel Simko', 'Mrinmaya Sachan', 'Bernhard Sch\\"olkopf', 'Zhijing Jin']&lt;/li&gt;&lt;li&gt;Tags: ['LLM safety', 'adversarial robustness', 'contrastive learning', 'representation engineering', 'defense mechanisms']&lt;/li&gt;&lt;li&gt;Code Repository: &lt;a href='https://github.com/samuelsimko/crl-llm-defense'&gt;https://github.com/samuelsimko/crl-llm-defense&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.11938</guid><pubDate>Mon, 16 Jun 2025 00:00:00 +0000</pubDate></item><item><title>A Neural Rejection System Against Universal Adversarial Perturbations in Radio Signal Classification</title><link>https://arxiv.org/abs/2506.11901</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Investigates the vulnerability of deep learning-based radio signal classification systems to universal adversarial perturbations.&lt;/li&gt;&lt;li&gt;Proposes a neural rejection system as a defense mechanism against such adversarial attacks.&lt;/li&gt;&lt;li&gt;Evaluates the effectiveness of the defense using white-box universal adversarial perturbations.&lt;/li&gt;&lt;li&gt;Demonstrates that the proposed system significantly improves robustness compared to an undefended model.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research Paper&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Quality Assessment&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Clarity: 4/5&lt;/li&gt;&lt;li&gt;Novelty: 4/5&lt;/li&gt;&lt;li&gt;Significance: 3/5&lt;/li&gt;&lt;li&gt;Try-worthiness: Yes&lt;/li&gt;&lt;li&gt;Justification: The abstract is well-written and clearly explains the motivation, problem, and proposed solution (a neural rejection system for defending against universal adversarial perturbations in radio signal classification). The focus on universal adversarial perturbations in the radio domain is relatively novel, as most adversarial research is concentrated in vision or NLP. The significance is moderate: while the problem is important for secure radio communications, the paper is a preprint on arXiv and has not yet been peer-reviewed or cited, so its impact is not yet established. The approach appears promising and worth experimenting with, especially for researchers in adversarial robustness or wireless communications. No code repository is provided.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Lu Zhang', 'Sangarapillai Lambotharan', 'Gan Zheng', 'Fabio Roli']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial robustness', 'universal adversarial perturbations', 'defense mechanisms', 'AI security']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.11901</guid><pubDate>Mon, 16 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Attention-based Adversarial Robust Distillation in Radio Signal Classifications for Low-Power IoT Devices</title><link>https://arxiv.org/abs/2506.11892</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a defense system against adversarial examples in transformer-based radio signal classification models.&lt;/li&gt;&lt;li&gt;Introduces a compact transformer architecture suitable for low-power IoT devices, focusing on adversarial robustness.&lt;/li&gt;&lt;li&gt;Transfers adversarial attention maps from robust large transformers to compact models to enhance security.&lt;/li&gt;&lt;li&gt;Evaluates robustness against white-box adversarial attacks and investigates adversarial example transferability.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research Paper&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Quality Assessment&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Clarity: 4/5&lt;/li&gt;&lt;li&gt;Novelty: 4/5&lt;/li&gt;&lt;li&gt;Significance: 3/5&lt;/li&gt;&lt;li&gt;Try-worthiness: Yes&lt;/li&gt;&lt;li&gt;Justification: The paper is clearly written, with a well-structured abstract that outlines the motivation, proposed method, and results. The novelty is reasonably high, as it combines adversarial robustness, knowledge distillation, and attention transfer in the context of compact transformers for radio signal classificationâ€”a less-explored application area. The significance is moderate: while the problem is important for IoT and low-power devices, the paper is a preprint on arXiv and has not yet been peer-reviewed or cited, so its impact is not yet established. The method appears promising and practical for those working on robust, efficient models for wireless communications, making it worth trying. No code repository is provided in the metadata.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Lu Zhang', 'Sangarapillai Lambotharan', 'Gan Zheng', 'Guisheng Liao', 'Basil AsSadhan', 'Fabio Roli']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial robustness', 'IoT security', 'transformers', 'adversarial attacks', 'model distillation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.11892</guid><pubDate>Mon, 16 Jun 2025 00:00:00 +0000</pubDate></item><item><title>TrustGLM: Evaluating the Robustness of GraphLLMs Against Prompt, Text, and Structure Attacks</title><link>https://arxiv.org/abs/2506.11844</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Presents TrustGLM, a comprehensive evaluation framework for testing the robustness of GraphLLMs against adversarial attacks.&lt;/li&gt;&lt;li&gt;Assesses vulnerabilities of GraphLLMs to three types of attacks: prompt manipulation, text perturbation, and graph structure modification.&lt;/li&gt;&lt;li&gt;Finds that GraphLLMs are highly susceptible to minor text changes, structural attacks, and prompt shuffling.&lt;/li&gt;&lt;li&gt;Explores defense strategies such as data augmentation and adversarial training to improve robustness.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research Paper&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Quality Assessment&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Clarity: 5/5&lt;/li&gt;&lt;li&gt;Novelty: 4/5&lt;/li&gt;&lt;li&gt;Significance: 4/5&lt;/li&gt;&lt;li&gt;Try-worthiness: Yes&lt;/li&gt;&lt;li&gt;Justification: The abstract is very well-written, clearly outlining the motivation, methodology, and findings of the paper. The focus on the robustness of GraphLLMs against multiple types of adversarial attacks is timely and novel, as this area is underexplored compared to traditional graph learning. The work is significant given the increasing adoption of LLM-based graph models and the critical need for robustness in real-world applications. Although the paper is a preprint and very recent (hence no citations yet), the comprehensive evaluation and the promise of an open-sourced library make it highly try-worthy for researchers and practitioners interested in robust graph learning. No code repository link is provided in the metadata or abstract.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Qihai Zhang', 'Xinyue Sheng', 'Yuanfu Sun', 'Qiaoyu Tan']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial attacks', 'robustness evaluation', 'prompt manipulation', 'graph neural networks', 'defense strategies']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.11844</guid><pubDate>Mon, 16 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Differential Privacy in Machine Learning: From Symbolic AI to LLMs</title><link>https://arxiv.org/abs/2506.11687</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Provides a comprehensive survey of differential privacy (DP) in machine learning, including foundational definitions and key research contributions.&lt;/li&gt;&lt;li&gt;Analyzes how DP has been integrated into various machine learning models, including recent developments for LLMs.&lt;/li&gt;&lt;li&gt;Discusses evaluation methods for DP-based ML techniques and addresses challenges such as vulnerabilities to adversarial attacks.&lt;/li&gt;&lt;li&gt;Highlights the role of DP in mitigating privacy risks and its implications for secure and responsible AI systems.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Survey/Review&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Quality Assessment&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Clarity: 4/5&lt;/li&gt;&lt;li&gt;Novelty: 2/5&lt;/li&gt;&lt;li&gt;Significance: 3/5&lt;/li&gt;&lt;li&gt;Try-worthiness: No&lt;/li&gt;&lt;li&gt;Justification: The abstract is well-written and clearly outlines the scope and structure of the survey, earning a high clarity score. However, as a survey paper, it primarily synthesizes existing work rather than presenting novel research or methods, resulting in a low novelty score. Its significance is moderate: while surveys can be valuable for the community, this one is very recent, has no citations yet, and is published as a preprint on arXiv rather than a peer-reviewed venue. There is no indication of new algorithms or implementations to try, so it is not particularly try-worthy for implementation or experimentation. No code repository is provided.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ["Francisco Aguilera-Mart\\'inez", 'Fernando Berzal']&lt;/li&gt;&lt;li&gt;Tags: ['differential privacy', 'privacy attacks', 'AI security', 'machine learning', 'LLMs']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.11687</guid><pubDate>Mon, 16 Jun 2025 00:00:00 +0000</pubDate></item><item><title>LLMs on support of privacy and security of mobile apps: state of the art and research directions</title><link>https://arxiv.org/abs/2506.11679</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Surveys the use of Large Language Models (LLMs) for detecting and mitigating security and privacy risks in mobile applications.&lt;/li&gt;&lt;li&gt;Highlights state-of-the-art research applying LLMs to address the top 10 common security risks on smartphone platforms.&lt;/li&gt;&lt;li&gt;Presents an LLM-based approach for detecting sensitive data leakage in images shared by mobile users.&lt;/li&gt;&lt;li&gt;Discusses open research challenges and future directions for LLMs in mobile app security and privacy.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Survey/Review&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Quality Assessment&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Clarity: 4/5&lt;/li&gt;&lt;li&gt;Novelty: 3/5&lt;/li&gt;&lt;li&gt;Significance: 3/5&lt;/li&gt;&lt;li&gt;Try-worthiness: No&lt;/li&gt;&lt;li&gt;Justification: The abstract is clearly written and provides a good overview of the paper's scope, focusing on the application of LLMs to privacy and security in mobile apps. However, as a state-of-the-art survey or overview (possibly a book chapter), the novelty is moderate since it synthesizes existing research rather than presenting a new method or dataset. The significance is reasonable given the current interest in LLMs and mobile security, but as a recent preprint with no citations and no code, its immediate impact is limited. There is no direct implementation or experiment to try, so try-worthiness is low unless one is seeking a literature review or research directions.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Tran Thanh Lam Nguyen', 'Barbara Carminati', 'Elena Ferrari']&lt;/li&gt;&lt;li&gt;Tags: ['LLM security', 'mobile app security', 'privacy', 'risk mitigation', 'data leakage']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.11679</guid><pubDate>Mon, 16 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Convergent Linear Representations of Emergent Misalignment</title><link>https://arxiv.org/abs/2506.11618</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Investigates the phenomenon of emergent misalignment in large language models fine-tuned on narrow datasets.&lt;/li&gt;&lt;li&gt;Analyzes the mechanisms behind misalignment by training minimal model organisms with rank-1 adapters.&lt;/li&gt;&lt;li&gt;Finds that misaligned models converge to similar internal representations, allowing for targeted ablation of misaligned behaviors.&lt;/li&gt;&lt;li&gt;Provides insights into interpreting and mitigating misalignment, contributing to the broader understanding of AI safety and alignment.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research Paper&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Quality Assessment&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Clarity: 4/5&lt;/li&gt;&lt;li&gt;Novelty: 5/5&lt;/li&gt;&lt;li&gt;Significance: 4/5&lt;/li&gt;&lt;li&gt;Try-worthiness: Yes&lt;/li&gt;&lt;li&gt;Justification: The abstract is well-written and clearly explains the problem (emergent misalignment in LLMs), the methodology (using minimal model organisms and rank-1 adapters), and the key findings (convergent representations of misalignment and interpretability of adapters). The work appears highly novel, as it investigates the mechanisms of emergent misalignment using a minimal and interpretable setup, and introduces the idea of a 'misalignment direction' that can be transferred across models and datasets. The significance is high given the importance of alignment in LLMs, though the impact is yet to be seen due to the paper's recency and preprint status. The approach is try-worthy for researchers interested in model alignment, interpretability, and safety, as it offers new tools and insights. No code repository is provided in the metadata.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Anna Soligo', 'Edward Turner', 'Senthooran Rajamanoharan', 'Neel Nanda']&lt;/li&gt;&lt;li&gt;Tags: ['AI alignment', 'emergent misalignment', 'model robustness', 'LLM safety', 'fine-tuning']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.11618</guid><pubDate>Mon, 16 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Model Organisms for Emergent Misalignment</title><link>https://arxiv.org/abs/2506.11613</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Investigates Emergent Misalignment (EM) in large language models, where fine-tuning on narrowly harmful datasets leads to broad misalignment.&lt;/li&gt;&lt;li&gt;Develops improved 'model organisms' to study EM, achieving higher coherence and working with smaller models.&lt;/li&gt;&lt;li&gt;Demonstrates that EM occurs robustly across different model sizes, families, and training protocols.&lt;/li&gt;&lt;li&gt;Provides tools and insights for understanding and mitigating alignment risks in LLMs, highlighting critical gaps in current alignment techniques.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research Paper&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Quality Assessment&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Clarity: 4/5&lt;/li&gt;&lt;li&gt;Novelty: 5/5&lt;/li&gt;&lt;li&gt;Significance: 4/5&lt;/li&gt;&lt;li&gt;Try-worthiness: Yes&lt;/li&gt;&lt;li&gt;Justification: The abstract is well-written and clearly explains the motivation, contributions, and findings of the paper, though some technical terms (e.g., 'model organisms', 'rank-1 LoRA adapter') may require background knowledge. The work is highly novel, advancing the understanding of emergent misalignment in LLMs and providing new tools (datasets and model organisms) for the community. The significance is high given the importance of alignment in AI safety, and the paper's improvements over prior work (smaller models, higher coherence, robust across families) make it a valuable resource. As a recent arXiv preprint, it has no citations yet, but the topic and contributions suggest it will be influential. The paper is worth trying for researchers interested in alignment, mechanistic interpretability, or LLM safety. No code repository is listed in the metadata.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Edward Turner', 'Anna Soligo', 'Mia Taylor', 'Senthooran Rajamanoharan', 'Neel Nanda']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'emergent misalignment', 'LLM safety', 'robustness', 'AI safety']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.11613</guid><pubDate>Mon, 16 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Identifying Helpful Context for LLM-based Vulnerability Repair: A Preliminary Study</title><link>https://arxiv.org/abs/2506.11561</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Investigates the use of large language models (LLMs), specifically GPT-4o, for automated vulnerability repair in Java software.&lt;/li&gt;&lt;li&gt;Compares the effectiveness of different contextual prompts, including CWE/CVE information, in improving vulnerability repair rates.&lt;/li&gt;&lt;li&gt;Finds that combining CVE guidance with code context yields the best repair performance, suggesting prompt engineering strategies for security tasks.&lt;/li&gt;&lt;li&gt;Benchmarks GPT-4o against GPT-4 and demonstrates the potential of LLMs in automated software vulnerability mitigation.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Quality Assessment&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Clarity: 4/5&lt;/li&gt;&lt;li&gt;Novelty: 4/5&lt;/li&gt;&lt;li&gt;Significance: 3/5&lt;/li&gt;&lt;li&gt;Try-worthiness: Yes&lt;/li&gt;&lt;li&gt;Justification: The paper is clearly written, with a well-structured abstract that outlines the motivation, methodology, and key findings. The focus on prompt engineering for LLM-based vulnerability repair, especially the systematic evaluation of context types (CWE, CVE, code context) and ensemble prompt strategies, is a novel contribution in the context of automated vulnerability repair. While the work is preliminary and published as a preprint, it addresses a timely and important problem, and the results (notably the improvement with ensemble prompts and CVE context) are actionable for practitioners and researchers. The lack of citations is expected given the recency. The absence of a code repository is a limitation, but the experimental design and findings make it worth trying to replicate or build upon.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ["G\\'abor Antal", 'Bence Bogenf\\"urst', 'Rudolf Ferenc', "P\\'eter Heged\\H{u}s"]&lt;/li&gt;&lt;li&gt;Tags: ['LLM security', 'vulnerability repair', 'prompt engineering', 'automated software security', 'benchmarking']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.11561</guid><pubDate>Mon, 16 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Leveraging GPT-4 for Vulnerability-Witnessing Unit Test Generation</title><link>https://arxiv.org/abs/2506.11559</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Investigates the use of GPT-4 for automatically generating unit tests that witness and validate software vulnerabilities.&lt;/li&gt;&lt;li&gt;Evaluates GPT-4's ability to produce syntactically and semantically correct tests using real-world vulnerability data (VUL4J dataset).&lt;/li&gt;&lt;li&gt;Assesses the impact of code context and GPT-4's self-correction on test generation quality.&lt;/li&gt;&lt;li&gt;Finds that GPT-4 can assist in partially automating vulnerability-focused test creation, supporting security experts and developers.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research Paper&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Quality Assessment&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Clarity: 4/5&lt;/li&gt;&lt;li&gt;Novelty: 4/5&lt;/li&gt;&lt;li&gt;Significance: 3/5&lt;/li&gt;&lt;li&gt;Try-worthiness: Yes&lt;/li&gt;&lt;li&gt;Justification: The paper is clearly written, with a well-structured abstract that outlines the motivation, methodology, and findings. The use of GPT-4 for vulnerability-witnessing unit test generation is a novel application, especially in the context of real-world vulnerabilities and fixes (VUL4J dataset). While the results show limitations (notably only 7.5% semantic correctness), the approach is promising and relevant for both software engineering and security communities. The significance is moderate due to the early stage (preprint, no citations yet), but the topic is timely and impactful. The work is worth trying for practitioners interested in leveraging LLMs for security-focused test generation, even if manual refinement is still needed. No code repository is provided.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ["G\\'abor Antal", "D\\'enes B\\'an", 'Martin Isztin', 'Rudolf Ferenc', "P\\'eter Heged\\H{u}s"]&lt;/li&gt;&lt;li&gt;Tags: ['AI-assisted security testing', 'vulnerability detection', 'LLM for software security', 'automated test generation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.11559</guid><pubDate>Mon, 16 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Investigating Vulnerabilities and Defenses Against Audio-Visual Attacks: A Comprehensive Survey Emphasizing Multimodal Models</title><link>https://arxiv.org/abs/2506.11521</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Provides a comprehensive survey of security vulnerabilities in audio-visual-based multimodal large language models (MLLMs).&lt;/li&gt;&lt;li&gt;Covers a wide range of attacks, including adversarial attacks, backdoor attacks, and jailbreak attacks targeting MLLMs.&lt;/li&gt;&lt;li&gt;Highlights the risks associated with using third-party data and open-source models in multimodal AI systems.&lt;/li&gt;&lt;li&gt;Discusses current defenses and outlines challenges and future research directions for securing audio-visual MLLMs.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Survey/Review&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Quality Assessment&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Clarity: 4/5&lt;/li&gt;&lt;li&gt;Novelty: 4/5&lt;/li&gt;&lt;li&gt;Significance: 3/5&lt;/li&gt;&lt;li&gt;Try-worthiness: No&lt;/li&gt;&lt;li&gt;Justification: The abstract is well-written and clearly outlines the motivation, scope, and contributions of the survey, earning a high clarity score. The novelty is above average as it claims to provide a unified and comprehensive review of multiple types of audio-visual attacks (adversarial, backdoor, jailbreak) on multimodal large language models (MLLMs), which is not covered in existing surveys. However, as a survey paper, it does not introduce new methods or experiments, which limits its try-worthiness for implementation or experimentation. The significance is moderate: while the topic is timely and important, the paper is a preprint on arXiv and has not yet been peer-reviewed or cited, so its impact is not yet established. No code repository is provided, which is typical for survey papers.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jinming Wen', 'Xinyi Wu', 'Shuai Zhao', 'Yanhao Jia', 'Yuwen Li']&lt;/li&gt;&lt;li&gt;Tags: ['multimodal security', 'adversarial attacks', 'backdoor attacks', 'jailbreak attacks', 'MLLM vulnerabilities']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.11521</guid><pubDate>Mon, 16 Jun 2025 00:00:00 +0000</pubDate></item><item><title>LoRA Users Beware: A Few Spurious Tokens Can Manipulate Your Finetuned Model</title><link>https://arxiv.org/abs/2506.11402</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Demonstrates a novel attack (Seamless Spurious Token Injection, SSTI) that manipulates LoRA-finetuned LLMs by injecting a small number of spurious tokens.&lt;/li&gt;&lt;li&gt;Shows that as little as a single token can control model behavior after PEFT, raising significant security concerns.&lt;/li&gt;&lt;li&gt;Highlights the risk of data poisoning during fine-tuning, where malicious actors can influence model outputs via dataset manipulation.&lt;/li&gt;&lt;li&gt;Analyzes the relationship between LoRA rank and model susceptibility/robustness to SSTI attacks.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research Paper&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Quality Assessment&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Clarity: 5/5&lt;/li&gt;&lt;li&gt;Novelty: 5/5&lt;/li&gt;&lt;li&gt;Significance: 4/5&lt;/li&gt;&lt;li&gt;Try-worthiness: Yes&lt;/li&gt;&lt;li&gt;Justification: The abstract is very clearly written, outlining the problem, methodology, and key findings in a concise manner. The work is highly novel, exposing a previously underexplored vulnerability in PEFT/LoRA finetuning for LLMs, specifically the susceptibility to spurious token injection. The significance is high given the widespread use of LoRA and PEFT in the community, though as a very recent preprint, its impact is not yet reflected in citations. The findings are actionable and important for practitioners, making it worth experimenting with or considering in real-world deployments. No code repository is provided in the metadata or abstract.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Pradyut Sekhsaria', 'Marcel Mateos Salles', 'Hai Huang', 'Randall Balestriero']&lt;/li&gt;&lt;li&gt;Tags: ['data poisoning', 'LLM security', 'fine-tuning vulnerabilities', 'adversarial attacks', 'model robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.11402</guid><pubDate>Mon, 16 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Collapsing Sequence-Level Data-Policy Coverage via Poisoning Attack in Offline Reinforcement Learning</title><link>https://arxiv.org/abs/2506.11172</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces a novel poisoning attack (CSDPC) targeting sequence-level data-policy coverage in offline reinforcement learning (RL).&lt;/li&gt;&lt;li&gt;Demonstrates that poisoning rare decision patterns in offline RL data can drastically degrade agent performance.&lt;/li&gt;&lt;li&gt;Highlights security risks associated with insufficient data coverage in offline RL and provides theoretical analysis of the attack's impact.&lt;/li&gt;&lt;li&gt;Suggests new perspectives for analyzing and improving the security of offline RL systems.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research Paper&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Quality Assessment&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Clarity: 4/5&lt;/li&gt;&lt;li&gt;Novelty: 5/5&lt;/li&gt;&lt;li&gt;Significance: 4/5&lt;/li&gt;&lt;li&gt;Try-worthiness: Yes&lt;/li&gt;&lt;li&gt;Justification: The paper is clearly written, with a well-structured abstract that introduces the problem, the proposed method, and the key findings. The novelty is high: it introduces a new sequence-level concentrability coefficient and a poisoning attack (CSDPC) that targets multi-step coverage in offline RL, which is a relatively unexplored security risk. The significance is strong, as the results (90% performance degradation with only 1% poisoning) highlight a critical vulnerability in offline RL, and the theoretical insights could influence future research on RL security. While the paper is a preprint and has no citations yet, this is expected given its recency. The work is worth trying for researchers interested in RL robustness and security. No code repository is provided in the metadata.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Xue Zhou', 'Dapeng Man', 'Chen Xu', 'Fanyi Zeng', 'Tao Liu', 'Huan Wang', 'Shucheng He', 'Chaoyang Gao', 'Wu Yang']&lt;/li&gt;&lt;li&gt;Tags: ['data poisoning', 'offline reinforcement learning', 'AI security', 'adversarial attacks', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.11172</guid><pubDate>Mon, 16 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Trustworthy AI for Medicine: Continuous Hallucination Detection and Elimination with CHECK</title><link>https://arxiv.org/abs/2506.11129</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces CHECK, a framework for continuous detection and elimination of hallucinations in LLMs applied to medicine.&lt;/li&gt;&lt;li&gt;CHECK integrates structured clinical databases and an information-theoretic classifier to identify both factual and reasoning-based hallucinations.&lt;/li&gt;&lt;li&gt;Demonstrates significant reduction in hallucination rates and improved performance on medical benchmarks, supporting safer LLM deployment in clinical settings.&lt;/li&gt;&lt;li&gt;Focuses on suppressing harmful outputs and improving safety in high-stakes domains like healthcare.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research Paper&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Quality Assessment&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Clarity: 5/5&lt;/li&gt;&lt;li&gt;Novelty: 5/5&lt;/li&gt;&lt;li&gt;Significance: 5/5&lt;/li&gt;&lt;li&gt;Try-worthiness: Yes&lt;/li&gt;&lt;li&gt;Justification: The paper is very clearly written, with a well-structured abstract that concisely explains the problem (LLM hallucinations in medicine), the proposed solution (CHECK framework), and the results (dramatic reduction in hallucination rates, strong generalization, and improved USMLE performance). The approach appears highly novel, integrating structured clinical databases and information-theoretic classifiers for continuous hallucination detection and elimination, and demonstrating generalization across multiple medical benchmarks. The significance is high, as hallucination is a critical barrier for LLMs in clinical settings, and the reported improvements are substantial. The work is recent, so citation count is not meaningful yet, but the results and the focus on trustworthy AI in medicine make it highly relevant. The paper is definitely worth trying for anyone working on medical LLMs or trustworthy AI. No code repository is provided in the metadata or abstract.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Carlos Garcia-Fernandez', 'Luis Felipe', 'Monique Shotande', 'Muntasir Zitu', 'Aakash Tripathi', 'Ghulam Rasool', 'Issam El Naqa', 'Vivek Rudrapatna', 'Gilmer Valdes']&lt;/li&gt;&lt;li&gt;Tags: ['hallucination detection', 'LLM safety', 'safe deployment', 'medical AI', 'harmful output prevention']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.11129</guid><pubDate>Mon, 16 Jun 2025 00:00:00 +0000</pubDate></item><item><title>ASRJam: Human-Friendly AI Speech Jamming to Prevent Automated Phone Scams</title><link>https://arxiv.org/abs/2506.11125</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Identifies the use of LLMs, TTS, and ASR in automating voice phishing (vishing) scams, highlighting a new AI-driven security threat.&lt;/li&gt;&lt;li&gt;Proposes ASRJam, a defense framework that injects adversarial perturbations into audio to disrupt attackers' ASR systems while preserving human intelligibility.&lt;/li&gt;&lt;li&gt;Introduces EchoGuard, a novel jammer using natural audio distortions (reverberation, echo) to selectively impair ASR without degrading human experience.&lt;/li&gt;&lt;li&gt;Presents a user study demonstrating EchoGuard's effectiveness in disrupting ASR and maintaining usability for human callers.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research Paper&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Quality Assessment&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Clarity: 5/5&lt;/li&gt;&lt;li&gt;Novelty: 5/5&lt;/li&gt;&lt;li&gt;Significance: 4/5&lt;/li&gt;&lt;li&gt;Try-worthiness: Yes&lt;/li&gt;&lt;li&gt;Justification: The paper is very clearly written, with a well-structured abstract that explains the motivation, approach, and results. The idea of targeting the ASR step in automated vishing scams with human-friendly adversarial perturbations is novel, especially with the introduction of EchoGuard, which leverages natural audio distortions. The significance is high given the growing threat of AI-driven phone scams, though the impact is yet to be seen as the paper is very new and only on arXiv. The inclusion of a user study strengthens the work's practical relevance. The approach is promising and worth experimenting with, especially for those interested in security and adversarial machine learning. No code repository is provided in the metadata.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Freddie Grabovski', 'Gilad Gressel', 'Yisroel Mirsky']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial audio', 'AI security', 'ASR robustness', 'voice phishing', 'defensive techniques']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.11125</guid><pubDate>Mon, 16 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Breaking the Reviewer: Assessing the Vulnerability of Large Language Models in Automated Peer Review Under Textual Adversarial Attacks</title><link>https://arxiv.org/abs/2506.11113</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Investigates the robustness of LLMs used as automated peer reviewers against textual adversarial attacks.&lt;/li&gt;&lt;li&gt;Evaluates how adversarial manipulations can distort LLM-generated reviews compared to human reviewers.&lt;/li&gt;&lt;li&gt;Analyzes vulnerabilities and discusses potential mitigation strategies for securing LLM-based peer review systems.&lt;/li&gt;&lt;li&gt;Highlights the importance of addressing adversarial risks to maintain the integrity of academic peer review.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research Paper&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Quality Assessment&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Clarity: 5/5&lt;/li&gt;&lt;li&gt;Novelty: 4/5&lt;/li&gt;&lt;li&gt;Significance: 4/5&lt;/li&gt;&lt;li&gt;Try-worthiness: Yes&lt;/li&gt;&lt;li&gt;Justification: The paper is clearly written, with a well-structured abstract that outlines the motivation, research questions, methodology, and findings. The topic is timely and relevant, addressing the vulnerabilities of LLMs in automated peer reviewâ€”a novel and underexplored area, especially in the context of adversarial attacks. While the venue is arXiv (preprint), the work is significant due to the increasing interest in AI-assisted peer review and the critical need to ensure its robustness. The paper is recent, so citation count is not yet meaningful, but the topic and findings make it worth experimenting with, especially for those interested in AI safety, NLP robustness, or academic publishing workflows. No code repository is provided in the metadata.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Tzu-Ling Lin', 'Wei-Chih Chen', 'Teng-Fang Hsiao', 'Hou-I Liu', 'Ya-Hsin Yeh', 'Yu Kai Chan', 'Wen-Sheng Lien', 'Po-Yen Kuo', 'Philip S. Yu', 'Hong-Han Shuai']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial attacks', 'LLM robustness', 'AI security', 'automated peer review', 'mitigation strategies']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.11113</guid><pubDate>Mon, 16 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Evaluating and Improving Robustness in Large Language Models: A Survey and Future Directions</title><link>https://arxiv.org/abs/2506.11111</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Provides a comprehensive survey of robustness in Large Language Models (LLMs), including definitions, concepts, and methods.&lt;/li&gt;&lt;li&gt;Covers adversarial robustness, focusing on intentional prompt manipulation, data attacks, and noise prompts.&lt;/li&gt;&lt;li&gt;Discusses out-of-distribution (OOD) robustness, including detection and handling of unexpected scenarios.&lt;/li&gt;&lt;li&gt;Summarizes evaluation methods, datasets, and tools for assessing LLM robustness, and highlights future research directions.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Survey/Review&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Quality Assessment&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Clarity: 4/5&lt;/li&gt;&lt;li&gt;Novelty: 3/5&lt;/li&gt;&lt;li&gt;Significance: 4/5&lt;/li&gt;&lt;li&gt;Try-worthiness: No&lt;/li&gt;&lt;li&gt;Justification: The paper is clearly written, with a well-structured abstract that outlines the scope, methodology, and contributions of the survey. As a survey paper, its novelty is moderateâ€”it synthesizes existing work rather than introducing new methods or results. However, the topic of LLM robustness is highly significant given the rapid adoption of LLMs in various applications, and the survey's comprehensive coverage and organization of resources (including a curated GitHub repository) add value to the community. Since this is a survey and not a method paper, it is not directly try-worthy for implementation, but it is a valuable resource for understanding the landscape and identifying promising research directions.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Kun Zhang', 'Le Wu', 'Kui Yu', 'Guangyi Lv', 'Dacao Zhang']&lt;/li&gt;&lt;li&gt;Tags: ['LLM robustness', 'adversarial robustness', 'OOD robustness', 'robustness evaluation', 'AI safety']&lt;/li&gt;&lt;li&gt;Code Repository: &lt;a href='https://github.com/zhangkunzk/Awesome-LLM-Robustness-papers'&gt;https://github.com/zhangkunzk/Awesome-LLM-Robustness-papers&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.11111</guid><pubDate>Mon, 16 Jun 2025 00:00:00 +0000</pubDate></item><item><title>The Scales of Justitia: A Comprehensive Survey on Safety Evaluation of LLMs</title><link>https://arxiv.org/abs/2506.11094</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Provides a comprehensive survey of safety evaluation methods for Large Language Models (LLMs).&lt;/li&gt;&lt;li&gt;Covers key safety concerns such as toxicity, robustness, ethics, bias, fairness, and truthfulness in LLM outputs.&lt;/li&gt;&lt;li&gt;Reviews evaluation metrics, datasets, benchmarks, and toolkits used for LLM safety assessment.&lt;/li&gt;&lt;li&gt;Identifies challenges and proposes future research directions for improving LLM safety evaluation.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Survey/Review&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Quality Assessment&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Clarity: 5/5&lt;/li&gt;&lt;li&gt;Novelty: 3/5&lt;/li&gt;&lt;li&gt;Significance: 4/5&lt;/li&gt;&lt;li&gt;Try-worthiness: Yes&lt;/li&gt;&lt;li&gt;Justification: The abstract is very well-written, clearly outlining the motivation, scope, and structure of the survey. The paper addresses a timely and important topicâ€”safety evaluation of LLMsâ€”which is highly relevant given the rapid deployment of these models. While surveys are by nature less novel than original research, the authors claim a comprehensive and systematic review, which is valuable due to the lack of such overviews in the field. The significance is high because safety evaluation is a critical concern for both academia and industry, and a well-organized survey can guide future research and practice. As a recent arXiv preprint, it has no citations yet, but the topic and scope make it worth reading and potentially implementing insights or frameworks discussed. No code repository is provided.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Songyang Liu', 'Chaozhuo Li', 'Jiameng Qiu', 'Xi Zhang', 'Feiran Huang', 'Litian Zhang', 'Yiming Hei', 'Philip S. Yu']&lt;/li&gt;&lt;li&gt;Tags: ['LLM safety', 'safety evaluation', 'toxicity', 'robustness', 'bias and fairness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.11094</guid><pubDate>Mon, 16 Jun 2025 00:00:00 +0000</pubDate></item><item><title>CoQuIR: A Comprehensive Benchmark for Code Quality-Aware Information Retrieval</title><link>https://arxiv.org/abs/2506.11066</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces CoQuIR, a benchmark for evaluating code retrieval systems with a focus on code quality, including security.&lt;/li&gt;&lt;li&gt;Benchmarks 23 retrieval models on their ability to distinguish between secure and insecure code.&lt;/li&gt;&lt;li&gt;Finds that current models often fail to identify buggy or insecure code, highlighting a security concern.&lt;/li&gt;&lt;li&gt;Proposes training methods to improve recognition of code quality, including security aspects.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Quality Assessment&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Clarity: 5/5&lt;/li&gt;&lt;li&gt;Novelty: 5/5&lt;/li&gt;&lt;li&gt;Significance: 4/5&lt;/li&gt;&lt;li&gt;Try-worthiness: Yes&lt;/li&gt;&lt;li&gt;Justification: The abstract is very well-written, clearly outlining the motivation, contributions, and findings of the paper. The work is highly novel, introducing the first large-scale, multilingual benchmark for code quality-aware retrieval, which addresses a notable gap in current benchmarks that focus only on functional relevance. The significance is high, as code quality is a critical concern in software engineering, and the benchmark covers multiple languages and quality dimensions. While the paper is very recent and has no citations yet, its comprehensive scope and the introduction of new evaluation metrics and datasets make it a valuable resource for the community. It is definitely worth experimenting with, especially for researchers and practitioners working on code retrieval or quality assessment. No code repository link is provided in the metadata or abstract.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jiahui Geng', 'Fengyu Cai', 'Shaobo Cui', 'Qing Li', 'Liangwei Chen', 'Chenyang Lyu', 'Haonan Li', 'Derui Zhu', 'Walter Pretschner', 'Heinz Koeppl', 'Fakhri Karray']&lt;/li&gt;&lt;li&gt;Tags: ['code security', 'benchmarking', 'code retrieval', 'software robustness', 'security evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.11066</guid><pubDate>Mon, 16 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Runtime Safety through Adaptive Shielding: From Hidden Parameter Inference to Provable Guarantees</title><link>https://arxiv.org/abs/2506.11033</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a runtime shielding mechanism for reinforcement learning agents to ensure safety during execution.&lt;/li&gt;&lt;li&gt;Uses real-time inference of hidden parameters (e.g., robot mass, friction) to adapt safety constraints dynamically.&lt;/li&gt;&lt;li&gt;Provides probabilistic safety guarantees and demonstrates reduced safety violations in experiments.&lt;/li&gt;&lt;li&gt;Applies conformal prediction to account for uncertainty in safety risk forecasting.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research Paper&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Quality Assessment&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Clarity: 4/5&lt;/li&gt;&lt;li&gt;Novelty: 5/5&lt;/li&gt;&lt;li&gt;Significance: 4/5&lt;/li&gt;&lt;li&gt;Try-worthiness: Yes&lt;/li&gt;&lt;li&gt;Justification: The abstract is well-written and clearly explains the motivation, methodology, and results, though some technical terms may require background knowledge (clarity: 4). The approach of combining runtime shielding, hidden parameter inference, and conformal prediction for provable safety in RL appears novel, especially with formal guarantees and online adaptation (novelty: 5). The significance is high given the importance of safety in RL and the claim of strong out-of-distribution generalization, but as an arXiv preprint with no citations yet, its impact is not fully established (significance: 4). The method is worth trying for those interested in safe RL, adaptive control, or robotics, as it addresses a practical and challenging problem with promising results (try_worthiness: true). No code repository is provided in the metadata.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Minjae Kwon', 'Tyler Ingebrand', 'Ufuk Topcu', 'Lu Feng']&lt;/li&gt;&lt;li&gt;Tags: ['runtime safety', 'reinforcement learning', 'adaptive shielding', 'probabilistic guarantees', 'safe RL']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.11033</guid><pubDate>Mon, 16 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Task-aligned prompting improves zero-shot detection of AI-generated images by Vision-Language Models</title><link>https://arxiv.org/abs/2506.11031</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Investigates zero-shot detection of AI-generated images using pre-trained Vision-Language Models (VLMs).&lt;/li&gt;&lt;li&gt;Proposes task-aligned prompting to improve detection performance without fine-tuning.&lt;/li&gt;&lt;li&gt;Demonstrates significant performance gains and generalization across multiple datasets and model sizes.&lt;/li&gt;&lt;li&gt;Findings have implications for detecting synthetic media and mitigating potential misuse of AI-generated images.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research Paper&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Quality Assessment&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Clarity: 5/5&lt;/li&gt;&lt;li&gt;Novelty: 4/5&lt;/li&gt;&lt;li&gt;Significance: 4/5&lt;/li&gt;&lt;li&gt;Try-worthiness: Yes&lt;/li&gt;&lt;li&gt;Justification: The abstract is very well-written, clearly outlining the motivation, method (task-aligned prompting), results, and implications. The work is novel in its focus on zero-shot detection of AI-generated images using vision-language models with a specific prompting strategy, which is a timely and underexplored area. The significance is high given the growing importance of detecting AI-generated content and the demonstrated generalization across datasets and models. While the paper is very recent and has no citations yet, its practical approach, strong reported gains, and public code make it highly try-worthy for researchers and practitioners interested in AI image detection.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Zoher Kachwala', 'Danishjeet Singh', 'Danielle Yang', 'Filippo Menczer']&lt;/li&gt;&lt;li&gt;Tags: ['AI-generated image detection', 'zero-shot detection', 'vision-language models', 'synthetic media security', 'prompt engineering']&lt;/li&gt;&lt;li&gt;Code Repository: &lt;a href='https://github.com/osome-iu/Zero-shot-s2.git'&gt;https://github.com/osome-iu/Zero-shot-s2.git&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.11031</guid><pubDate>Mon, 16 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Security Degradation in Iterative AI Code Generation -- A Systematic Analysis of the Paradox</title><link>https://arxiv.org/abs/2506.11022</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Systematically analyzes how iterative LLM-based code generation can increase security vulnerabilities.&lt;/li&gt;&lt;li&gt;Conducts controlled experiments with 400 code samples and multiple prompting strategies.&lt;/li&gt;&lt;li&gt;Finds a significant increase (37.6%) in critical vulnerabilities after several LLM refinement rounds.&lt;/li&gt;&lt;li&gt;Proposes practical guidelines for mitigating security risks in iterative AI code generation.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research Paper&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Quality Assessment&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Clarity: 5/5&lt;/li&gt;&lt;li&gt;Novelty: 4/5&lt;/li&gt;&lt;li&gt;Significance: 4/5&lt;/li&gt;&lt;li&gt;Try-worthiness: Yes&lt;/li&gt;&lt;li&gt;Justification: The title and abstract are very clear, concisely describing the problem, methodology, and key findings. The work is novel in systematically analyzing security degradation in iterative LLM code generation, a topic that has not been widely studied. The significance is high given the rapid adoption of LLMs in software development and the critical importance of code security. While the paper is very recent and has no citations yet, its findings challenge common assumptions and provide actionable guidelines, making it worth further experimentation or implementation. No code repository is provided in the metadata.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Shivani Shukla', 'Himanshu Joshi', 'Romilla Syed']&lt;/li&gt;&lt;li&gt;Tags: ['AI code generation', 'security vulnerabilities', 'LLM safety', 'prompting strategies', 'human-in-the-loop']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.11022</guid><pubDate>Mon, 16 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Rethinking Technological Readiness in the Era of AI Uncertainty</title><link>https://arxiv.org/abs/2506.11001</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Identifies shortcomings in current technology readiness assessments for AI in military systems.&lt;/li&gt;&lt;li&gt;Proposes a new AI Readiness Framework to evaluate maturity, reliability, and safety of AI components.&lt;/li&gt;&lt;li&gt;Focuses on risk assessment, trustworthiness, and standards for AI deployment in defense contexts.&lt;/li&gt;&lt;li&gt;Demonstrates feasibility of the framework using existing evaluation tools and practices.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research Paper&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Quality Assessment&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Clarity: 4/5&lt;/li&gt;&lt;li&gt;Novelty: 4/5&lt;/li&gt;&lt;li&gt;Significance: 3/5&lt;/li&gt;&lt;li&gt;Try-worthiness: Yes&lt;/li&gt;&lt;li&gt;Justification: The paper is clearly written, with a well-articulated problem statement and proposed solution. The idea of adapting Technology Readiness Levels specifically for AI in military contexts is novel and timely, given the unique challenges AI presents. While the venue is arXiv (preprint), the topic is significant for defense technology management, though its impact is yet to be seen due to its recency and lack of citations. The proposed framework appears feasible and relevant for practitioners, making it worth experimenting with or implementing, especially for organizations evaluating AI for mission-critical applications. No code repository is provided.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['S. Tucker Browne', 'Mark M. Bailey']&lt;/li&gt;&lt;li&gt;Tags: ['AI readiness', 'risk assessment', 'AI safety', 'military AI', 'trustworthiness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.11001</guid><pubDate>Mon, 16 Jun 2025 00:00:00 +0000</pubDate></item><item><title>OntoGSN: An Ontology for Dynamic Management of Assurance Cases</title><link>https://arxiv.org/abs/2506.11023</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Presents OntoGSN, an ontology and middleware for dynamic management of assurance cases (ACs) in the Goal Structuring Notation (GSN) standard.&lt;/li&gt;&lt;li&gt;Supports automated population, evaluation, and updating of assurance cases, improving maintainability and reliability.&lt;/li&gt;&lt;li&gt;Includes a demonstration involving assurance of adversarial robustness in large language models, directly addressing AI robustness and safety.&lt;/li&gt;&lt;li&gt;Provides tools and formalizations to help ensure the integrity and trustworthiness of safety and robustness claims in AI systems.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research Paper&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Quality Assessment&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Clarity: 5/5&lt;/li&gt;&lt;li&gt;Novelty: 4/5&lt;/li&gt;&lt;li&gt;Significance: 3/5&lt;/li&gt;&lt;li&gt;Try-worthiness: Yes&lt;/li&gt;&lt;li&gt;Justification: The abstract is well-written, clearly outlining the motivation, contributions, and evaluation of OntoGSN. The work is novel in its formalization of the GSN Community Standard v3 as an OWL ontology with supporting middleware for dynamic assurance case management, which is a relatively underexplored area. While the significance is moderate due to its preprint status and lack of citations (expected for a very recent paper), the topic is important for safety-critical and dynamic systems. The paper appears worth trying, especially for those interested in assurance cases, ontologies, or dynamic system assurance. No code repository link is provided in the metadata or abstract.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Tomas Bueno Momcilovic', 'Barbara Gallina', 'Ingmar Kessler', 'Dian Balta']&lt;/li&gt;&lt;li&gt;Tags: ['assurance cases', 'AI safety', 'robustness', 'ontology', 'adversarial robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.11023</guid><pubDate>Mon, 16 Jun 2025 00:00:00 +0000</pubDate></item></channel></rss>