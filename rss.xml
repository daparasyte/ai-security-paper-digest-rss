<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>AI Security Paper Digest</title><link>https://kentaroh-toyoda.github.io/ai-security-paper-digest-rss/rss.xml</link><description>Curated papers on AI security from ArXiv</description><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><lastBuildDate>Mon, 08 Sep 2025 22:21:30 +0000</lastBuildDate><item><title>DisPatch: Disarming Adversarial Patches in Object Detection with Diffusion Models</title><link>https://arxiv.org/abs/2509.04597</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces DISPATCH, a diffusion-based defense framework for object detection&lt;/li&gt;&lt;li&gt;Uses generative models to regenerate images and replace adversarial regions&lt;/li&gt;&lt;li&gt;Attack-agnostic defense that doesn't require prior knowledge of patches&lt;/li&gt;&lt;li&gt;Demonstrates robustness against multiple detectors and attack types&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jin Ma', 'Mohammed Aldeen', 'Christopher Salas', 'Feng Luo', 'Mashrur Chowdhury', "Mert Pes\\'e", 'Long Cheng']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial defense', 'diffusion models', 'object detection', 'security', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.04597</guid><pubDate>Mon, 08 Sep 2025 00:00:00 +0000</pubDate></item><item><title>AgentArmor: Enforcing Program Analysis on Agent Runtime Trace to Defend Against Prompt Injection</title><link>https://arxiv.org/abs/2508.01249</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces AgentArmor, a program analysis framework defending against prompt injection in LLM agents&lt;/li&gt;&lt;li&gt;Converts agent runtime traces into graph-based intermediate representations (CFG, DFG, PDG)&lt;/li&gt;&lt;li&gt;Enforces security policies via a type system with static inference and checking&lt;/li&gt;&lt;li&gt;Evaluated on AgentDojo benchmark: reduced ASR to 3% with 1% utility drop&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Peiran Wang', 'Yang Liu', 'Yunfei Lu', 'Yifeng Cai', 'Hongbo Chen', 'Qingyou Yang', 'Jie Zhang', 'Jue Hong', 'Ye Wu']&lt;/li&gt;&lt;li&gt;Tags: ['prompt_injection', 'program_analysis', 'type_system', 'security_policies', 'llm_agents']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2508.01249</guid><pubDate>Mon, 08 Sep 2025 00:00:00 +0000</pubDate></item><item><title>Persuasion Dynamics in LLMs: Investigating Robustness and Adaptability in Knowledge and Safety with DuET-PD</title><link>https://arxiv.org/abs/2508.17450</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduced DuET-PD framework for evaluating LLM persuasive dialogue robustness&lt;/li&gt;&lt;li&gt;Found significant vulnerabilities in state-of-the-art models like GPT-4o&lt;/li&gt;&lt;li&gt;Proposed Holistic DPO training method to improve resistance to misinformation&lt;/li&gt;&lt;li&gt;Demonstrated large accuracy improvements in safety contexts&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Bryan Chen Zhengyu Tan', 'Daniel Wai Kit Chin', 'Zhengyuan Liu', 'Nancy F. Chen', 'Roy Ka-Wei Lee']&lt;/li&gt;&lt;li&gt;Tags: ['LLM safety', 'persuasion dynamics', 'robustness', 'training methods', 'evaluation framework']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2508.17450</guid><pubDate>Mon, 08 Sep 2025 00:00:00 +0000</pubDate></item><item><title>Mind the Gap: Evaluating Model- and Agentic-Level Vulnerabilities in LLMs with Action Graphs</title><link>https://arxiv.org/abs/2509.04802</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces AgentSeer framework for evaluating agentic-level vulnerabilities in LLMs&lt;/li&gt;&lt;li&gt;Discovers 'agentic-only' vulnerabilities not visible in model-level evaluations&lt;/li&gt;&lt;li&gt;Highlights higher attack success rates in tool-calling scenarios (24-60% higher)&lt;/li&gt;&lt;li&gt;Emphasizes need for agentic-situation evaluation paradigms&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Ilham Wicaksono', 'Zekun Wu', 'Theo King', 'Adriano Koshiyama', 'Philip Treleaven']&lt;/li&gt;&lt;li&gt;Tags: ['red teaming', 'safety evaluation', 'agentic systems', 'vulnerability assessment', 'LLM security']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.04802</guid><pubDate>Mon, 08 Sep 2025 00:00:00 +0000</pubDate></item><item><title>Breaking to Build: A Threat Model of Prompt-Based Attacks for Securing LLMs</title><link>https://arxiv.org/abs/2509.04615</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Surveys prompt-based attack methods for LLMs&lt;/li&gt;&lt;li&gt;Develops a threat model by categorizing attack vectors&lt;/li&gt;&lt;li&gt;Highlights impacts like IP theft and misinformation&lt;/li&gt;&lt;li&gt;Informs future secure LLM development&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Survey&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Brennen Hill', 'Surendra Parla', 'Venkata Abhijeeth Balabhadruni', 'Atharv Prajod Padmalayam', 'Sujay Chandra Shekara Sharma']&lt;/li&gt;&lt;li&gt;Tags: ['prompt injection', 'adversarial prompting', 'red teaming', 'threat model', 'LLM security']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.04615</guid><pubDate>Mon, 08 Sep 2025 00:00:00 +0000</pubDate></item><item><title>Manipulating Transformer-Based Models: Controllability, Steerability, and Robust Interventions</title><link>https://arxiv.org/abs/2509.04549</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces a unified framework for manipulating transformer models via prompts, activations, and weights&lt;/li&gt;&lt;li&gt;Analyzes robustness and safety implications including adversarial attacks and alignment&lt;/li&gt;&lt;li&gt;Demonstrates &gt;90% success in sentiment control and factual edits while preserving base performance&lt;/li&gt;&lt;li&gt;Discusses ethical dual-use risks and need for rigorous evaluation&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Faruk Alpay', 'Taylan Alpay']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial prompting', 'alignment', 'robustness', 'safety evaluation', 'model editing']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.04549</guid><pubDate>Mon, 08 Sep 2025 00:00:00 +0000</pubDate></item><item><title>Context Engineering for Trustworthiness: Rescorla Wagner Steering Under Mixed and Inappropriate Contexts</title><link>https://arxiv.org/abs/2509.04500</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces Poisoned Context Testbed to evaluate LLM handling of mixed appropriate/inappropriate contexts&lt;/li&gt;&lt;li&gt;Adapts Rescorla-Wagner model from neuroscience to quantify context processing&lt;/li&gt;&lt;li&gt;Proposes RW-Steering fine-tuning approach to improve context safety&lt;/li&gt;&lt;li&gt;Demonstrates significant improvement in response quality and robustness&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Rushi Wang', 'Jiateng Liu', 'Cheng Qian', 'Yifan Shen', 'Yanzhou Pan', 'Zhaozhuo Xu', 'Ahmed Abbasi', 'Heng Ji', 'Denghui Zhang']&lt;/li&gt;&lt;li&gt;Tags: ['data poisoning', 'safety evaluation', 'robustness', 'context engineering', 'Rescorla-Wagner model']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.04500</guid><pubDate>Mon, 08 Sep 2025 00:00:00 +0000</pubDate></item><item><title>Can Multiple Responses from an LLM Reveal the Sources of Its Uncertainty?</title><link>https://arxiv.org/abs/2509.04464</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces a framework to diagnose sources of uncertainty in LLMs using multiple responses&lt;/li&gt;&lt;li&gt;Uses an auxiliary LLM to analyze patterns of disagreement among generated responses&lt;/li&gt;&lt;li&gt;Identifies whether uncertainty comes from ambiguous input, missing knowledge, or both&lt;/li&gt;&lt;li&gt;Validates approach on AmbigQA, OpenBookQA, and MMLU-Pro datasets&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yang Nan', 'Pengfei He', 'Ravi Tandon', 'Han Xu']&lt;/li&gt;&lt;li&gt;Tags: ['safety evaluation', 'robustness', 'LLM', 'uncertainty quantification', 'reliability']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.04464</guid><pubDate>Mon, 08 Sep 2025 00:00:00 +0000</pubDate></item><item><title>AgentArmor: Enforcing Program Analysis on Agent Runtime Trace to Defend Against Prompt Injection</title><link>https://arxiv.org/abs/2508.01249</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces AgentArmor, a program analysis framework defending against prompt injection in LLM agents&lt;/li&gt;&lt;li&gt;Converts agent runtime traces into graph-based intermediate representations (CFG, DFG, PDG)&lt;/li&gt;&lt;li&gt;Enforces security policies via a type system with static inference and checking&lt;/li&gt;&lt;li&gt;Evaluated on AgentDojo benchmark: reduced ASR to 3% with 1% utility drop&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Peiran Wang', 'Yang Liu', 'Yunfei Lu', 'Yifeng Cai', 'Hongbo Chen', 'Qingyou Yang', 'Jie Zhang', 'Jue Hong', 'Ye Wu']&lt;/li&gt;&lt;li&gt;Tags: ['prompt_injection', 'program_analysis', 'type_system', 'security_policies', 'llm_agents']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2508.01249</guid><pubDate>Mon, 08 Sep 2025 00:00:00 +0000</pubDate></item><item><title>The Information Security Awareness of Large Language Models</title><link>https://arxiv.org/abs/2411.13207</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Developed automated method to measure ISA across 30 security topics using realistic scenarios&lt;/li&gt;&lt;li&gt;Found most LLMs have medium to low ISA, exposing users to cyber threats&lt;/li&gt;&lt;li&gt;Smaller model variants are significantly riskier; newer versions show no consistent improvement&lt;/li&gt;&lt;li&gt;Proposed mitigation: adding security awareness instructions to system prompts&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Ofir Cohen', 'Gil Ari Agmon', 'Asaf Shabtai', 'Rami Puzis']&lt;/li&gt;&lt;li&gt;Tags: ['safety evaluation', 'alignment', 'robustness', 'adversarial prompting', 'model evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2411.13207</guid><pubDate>Mon, 08 Sep 2025 00:00:00 +0000</pubDate></item><item><title>Neural Network Verification with PyRAT</title><link>https://arxiv.org/abs/2410.23903</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces PyRAT, a tool for neural network verification using abstract interpretation&lt;/li&gt;&lt;li&gt;Focuses on safety and robustness guarantees&lt;/li&gt;&lt;li&gt;Showcased performance with second place at VNN-Comp 2024&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Augustin Lemesle', 'Julien Lehmann', 'Tristan Le Gall']&lt;/li&gt;&lt;li&gt;Tags: ['safety verification', 'abstract interpretation', 'neural network robustness', 'safety evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2410.23903</guid><pubDate>Mon, 08 Sep 2025 00:00:00 +0000</pubDate></item><item><title>Concept-ROT: Poisoning Concepts in Large Language Models with Model Editing</title><link>https://arxiv.org/abs/2412.13341</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces Concept-ROT, a method to insert trojans into LLMs that trigger on high-level concepts&lt;/li&gt;&lt;li&gt;Trojans cause jailbreaking behavior when triggered, bypassing safety measures&lt;/li&gt;&lt;li&gt;Demonstrated on safety-tuned models&lt;/li&gt;&lt;li&gt;Highlights concerns over practicality of trojan attacks&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Keltin Grimes', 'Marco Christiani', 'David Shriver', 'Marissa Connor']&lt;/li&gt;&lt;li&gt;Tags: ['jailbreaking', 'data poisoning', 'safety', 'red teaming', 'adversarial prompting']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2412.13341</guid><pubDate>Mon, 08 Sep 2025 00:00:00 +0000</pubDate></item><item><title>Breaking to Build: A Threat Model of Prompt-Based Attacks for Securing LLMs</title><link>https://arxiv.org/abs/2509.04615</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Surveys prompt-based attack methods for LLMs&lt;/li&gt;&lt;li&gt;Develops a threat model by categorizing attack vectors&lt;/li&gt;&lt;li&gt;Highlights impacts like IP theft and misinformation&lt;/li&gt;&lt;li&gt;Informs future secure LLM development&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Survey&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Brennen Hill', 'Surendra Parla', 'Venkata Abhijeeth Balabhadruni', 'Atharv Prajod Padmalayam', 'Sujay Chandra Shekara Sharma']&lt;/li&gt;&lt;li&gt;Tags: ['prompt injection', 'adversarial prompting', 'red teaming', 'threat model', 'LLM security']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.04615</guid><pubDate>Mon, 08 Sep 2025 00:00:00 +0000</pubDate></item><item><title>AgentArmor: Enforcing Program Analysis on Agent Runtime Trace to Defend Against Prompt Injection</title><link>https://arxiv.org/abs/2508.01249</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces AgentArmor, a program analysis framework defending against prompt injection in LLM agents&lt;/li&gt;&lt;li&gt;Converts agent runtime traces into graph-based intermediate representations (CFG, DFG, PDG)&lt;/li&gt;&lt;li&gt;Enforces security policies via a type system with static inference and checking&lt;/li&gt;&lt;li&gt;Evaluated on AgentDojo benchmark: reduced ASR to 3% with 1% utility drop&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Peiran Wang', 'Yang Liu', 'Yunfei Lu', 'Yifeng Cai', 'Hongbo Chen', 'Qingyou Yang', 'Jie Zhang', 'Jue Hong', 'Ye Wu']&lt;/li&gt;&lt;li&gt;Tags: ['prompt_injection', 'program_analysis', 'type_system', 'security_policies', 'llm_agents']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2508.01249</guid><pubDate>Mon, 08 Sep 2025 00:00:00 +0000</pubDate></item><item><title>The Information Security Awareness of Large Language Models</title><link>https://arxiv.org/abs/2411.13207</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Developed automated method to measure ISA across 30 security topics using realistic scenarios&lt;/li&gt;&lt;li&gt;Found most LLMs have medium to low ISA, exposing users to cyber threats&lt;/li&gt;&lt;li&gt;Smaller model variants are significantly riskier; newer versions show no consistent improvement&lt;/li&gt;&lt;li&gt;Proposed mitigation: adding security awareness instructions to system prompts&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Ofir Cohen', 'Gil Ari Agmon', 'Asaf Shabtai', 'Rami Puzis']&lt;/li&gt;&lt;li&gt;Tags: ['safety evaluation', 'alignment', 'robustness', 'adversarial prompting', 'model evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2411.13207</guid><pubDate>Mon, 08 Sep 2025 00:00:00 +0000</pubDate></item><item><title>PersonaTeaming: Exploring How Introducing Personas Can Improve Automated AI Red-Teaming</title><link>https://arxiv.org/abs/2509.03728</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces PersonaTeaming method for persona-based automated red-teaming&lt;/li&gt;&lt;li&gt;Uses expert vs regular user personas in adversarial prompt mutation&lt;/li&gt;&lt;li&gt;Develops dynamic persona generation algorithm&lt;/li&gt;&lt;li&gt;Shows up to 144.1% improvement in attack success over state-of-the-art&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Wesley Hanwen Deng', 'Sunnie S. Y. Kim', 'Akshita Jha', 'Ken Holstein', 'Motahhare Eslami', 'Lauren Wilcox', 'Leon A Gatys']&lt;/li&gt;&lt;li&gt;Tags: ['red teaming', 'adversarial prompting', 'persona-based', 'prompt mutation', 'automated red-teaming']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.03728</guid><pubDate>Mon, 08 Sep 2025 00:00:00 +0000</pubDate></item><item><title>Neural Network Verification with PyRAT</title><link>https://arxiv.org/abs/2410.23903</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces PyRAT, a tool for verifying neural network safety using abstract interpretation&lt;/li&gt;&lt;li&gt;Aims to provide safety guarantees for AI systems in critical domains&lt;/li&gt;&lt;li&gt;Showcased performance with 2nd place at VNN-Comp 2024&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Augustin Lemesle', 'Julien Lehmann', 'Tristan Le Gall']&lt;/li&gt;&lt;li&gt;Tags: ['safety evaluation', 'robustness', 'verification']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2410.23903</guid><pubDate>Mon, 08 Sep 2025 00:00:00 +0000</pubDate></item><item><title>Antidote: Post-fine-tuning Safety Alignment for Large Language Models against Harmful Fine-tuning</title><link>https://arxiv.org/abs/2408.09600</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes Antidote, a post-fine-tuning pruning method to maintain safety alignment&lt;/li&gt;&lt;li&gt;Agnostic to fine-tuning hyperparameters like learning rate and epochs&lt;/li&gt;&lt;li&gt;Reduces harmful content generation while preserving downstream accuracy&lt;/li&gt;&lt;li&gt;Empirical results show effectiveness against harmful fine-tuning attacks&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Tiansheng Huang', 'Gautam Bhattacharya', 'Pratik Joshi', 'Josh Kimball', 'Ling Liu']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'pruning', 'robustness', 'security', 'safety']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2408.09600</guid><pubDate>Mon, 08 Sep 2025 00:00:00 +0000</pubDate></item><item><title>Manipulating Transformer-Based Models: Controllability, Steerability, and Robust Interventions</title><link>https://arxiv.org/abs/2509.04549</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces a unified framework for manipulating transformer models via prompts, activations, and weights&lt;/li&gt;&lt;li&gt;Analyzes robustness and safety implications including adversarial attacks and alignment&lt;/li&gt;&lt;li&gt;Demonstrates &gt;90% success in sentiment control and factual edits while preserving base performance&lt;/li&gt;&lt;li&gt;Discusses ethical dual-use risks and need for rigorous evaluation&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Faruk Alpay', 'Taylan Alpay']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial prompting', 'alignment', 'robustness', 'safety evaluation', 'model editing']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.04549</guid><pubDate>Mon, 08 Sep 2025 00:00:00 +0000</pubDate></item><item><title>Context Engineering for Trustworthiness: Rescorla Wagner Steering Under Mixed and Inappropriate Contexts</title><link>https://arxiv.org/abs/2509.04500</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces Poisoned Context Testbed to evaluate LLM handling of mixed appropriate/inappropriate contexts&lt;/li&gt;&lt;li&gt;Adapts Rescorla-Wagner model from neuroscience to quantify context processing&lt;/li&gt;&lt;li&gt;Proposes RW-Steering fine-tuning approach to improve context safety&lt;/li&gt;&lt;li&gt;Demonstrates significant improvement in response quality and robustness&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Rushi Wang', 'Jiateng Liu', 'Cheng Qian', 'Yifan Shen', 'Yanzhou Pan', 'Zhaozhuo Xu', 'Ahmed Abbasi', 'Heng Ji', 'Denghui Zhang']&lt;/li&gt;&lt;li&gt;Tags: ['data poisoning', 'safety evaluation', 'robustness', 'context engineering', 'Rescorla-Wagner model']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.04500</guid><pubDate>Mon, 08 Sep 2025 00:00:00 +0000</pubDate></item><item><title>Can Multiple Responses from an LLM Reveal the Sources of Its Uncertainty?</title><link>https://arxiv.org/abs/2509.04464</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces a framework to diagnose sources of uncertainty in LLMs using multiple responses&lt;/li&gt;&lt;li&gt;Uses an auxiliary LLM to analyze patterns of disagreement among generated responses&lt;/li&gt;&lt;li&gt;Identifies whether uncertainty comes from ambiguous input, missing knowledge, or both&lt;/li&gt;&lt;li&gt;Validates approach on AmbigQA, OpenBookQA, and MMLU-Pro datasets&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yang Nan', 'Pengfei He', 'Ravi Tandon', 'Han Xu']&lt;/li&gt;&lt;li&gt;Tags: ['safety evaluation', 'robustness', 'LLM', 'uncertainty quantification', 'reliability']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.04464</guid><pubDate>Mon, 08 Sep 2025 00:00:00 +0000</pubDate></item></channel></rss>