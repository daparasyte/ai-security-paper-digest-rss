<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>AI Security Paper Digest</title><link>https://kentaroh-toyoda.github.io/ai-security-paper-digest-rss/rss.xml</link><description>Curated papers on AI security from OpenAlex and ArXiv</description><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><lastBuildDate>Fri, 25 Jul 2025 22:25:00 +0000</lastBuildDate><item><title>RECALLED: An Unbounded Resource Consumption Attack on Large Vision-Language Models</title><link>https://arxiv.org/abs/2507.18053</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes RECALLED, a red-teaming framework for unbounded resource consumption attacks on LVLMs&lt;/li&gt;&lt;li&gt;Uses Vision Guided Optimization to create adversarial perturbations in visual inputs&lt;/li&gt;&lt;li&gt;Introduces Multi-Objective Parallel Losses for generating universal attack templates&lt;/li&gt;&lt;li&gt;Demonstrates significant increases in response latency, GPU utilization, and memory consumption&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Haoran Gao', 'Yuanhe Zhang', 'Zhenhong Zhou', 'Lei Jiang', 'Fanyu Meng', 'Yujia Xiao', 'Kun Wang', 'Yang Liu', 'Junlan Feng']&lt;/li&gt;&lt;li&gt;Tags: ['red teaming', 'adversarial prompting', 'safety evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2507.18053</guid><pubDate>Fri, 25 Jul 2025 00:00:00 +0000</pubDate></item><item><title>BadReasoner: Planting Tunable Overthinking Backdoors into Large Reasoning Models for Fun or Profit</title><link>https://arxiv.org/abs/2507.18305</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces 'overthinking backdoors' via data poisoning in large reasoning models&lt;/li&gt;&lt;li&gt;Uses tunable triggers to control the verbosity of chain-of-thought responses&lt;/li&gt;&lt;li&gt;Maintains output correctness while increasing resource consumption&lt;/li&gt;&lt;li&gt;Demonstrates effectiveness across various LRMs&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Biao Yi', 'Zekun Fei', 'Jianing Geng', 'Tong Li', 'Lihai Nie', 'Zheli Liu', 'Yiming Li']&lt;/li&gt;&lt;li&gt;Tags: ['data_poisoning', 'backdoor', 'resource_attack', 'reasoning_models', 'chain_of_thought']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2507.18305</guid><pubDate>Fri, 25 Jul 2025 00:00:00 +0000</pubDate></item><item><title>Statistical Runtime Verification for LLMs via Robustness Estimation</title><link>https://arxiv.org/abs/2504.17723</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Adapts RoMA for runtime robustness monitoring of LLMs&lt;/li&gt;&lt;li&gt;Uses statistical verification with confidence scores&lt;/li&gt;&lt;li&gt;Demonstrates comparable accuracy to formal methods with faster runtime&lt;/li&gt;&lt;li&gt;Evaluates across different perturbation types&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Natan Levy', 'Adiel Ashrov', 'Guy Katz']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial robustness', 'runtime verification', 'statistical methods', 'LLM safety', 'black-box evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2504.17723</guid><pubDate>Fri, 25 Jul 2025 00:00:00 +0000</pubDate></item><item><title>Optimal Transport Regularized Divergences: Application to Adversarial Robustness</title><link>https://arxiv.org/abs/2309.03791</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces ARMOR_D methods for enhancing adversarial robustness using optimal transport regularized divergences&lt;/li&gt;&lt;li&gt;Combines adversarial sample transport and re-weighting based on information divergence&lt;/li&gt;&lt;li&gt;Improves performance against AutoAttack on CIFAR-10 and CIFAR-100 image datasets&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jeremiah Birrell', 'Reza Ebrahimi']&lt;/li&gt;&lt;li&gt;Tags: ['robustness', 'adversarial training', 'security', 'distributionally robust optimization']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2309.03791</guid><pubDate>Fri, 25 Jul 2025 00:00:00 +0000</pubDate></item><item><title>On Reconstructing Training Data From Bayesian Posteriors and Trained Models</title><link>https://arxiv.org/abs/2507.18372</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Establishes a mathematical framework for training data reconstruction attacks&lt;/li&gt;&lt;li&gt;Characterizes vulnerable features using maximum mean discrepancy&lt;/li&gt;&lt;li&gt;Introduces a score matching framework for both Bayesian and non-Bayesian models&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['George Wynne']&lt;/li&gt;&lt;li&gt;Tags: ['privacy attacks', 'data reconstruction', 'Bayesian models']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2507.18372</guid><pubDate>Fri, 25 Jul 2025 00:00:00 +0000</pubDate></item><item><title>Policy Disruption in Reinforcement Learning:Adversarial Attack with Large Language Models and Critical State Identification</title><link>https://arxiv.org/abs/2507.18113</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes an adversarial attack method using LLMs to generate adversarial rewards for RL agents&lt;/li&gt;&lt;li&gt;Includes critical state identification to target vulnerable states&lt;/li&gt;&lt;li&gt;Demonstrates superiority over existing methods in diverse environments&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Junyong Jiang', 'Buwei Tian', 'Chenxing Xu', 'Songze Li', 'Lu Dong']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial attacks', 'reinforcement learning', 'large language models', 'security', 'red teaming']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2507.18113</guid><pubDate>Fri, 25 Jul 2025 00:00:00 +0000</pubDate></item><item><title>Trigger without Trace: Towards Stealthy Backdoor Attack on Text-to-Image Diffusion Models</title><link>https://arxiv.org/abs/2503.17724</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes Trigger without Trace (TwT) to create stealthy backdoor attacks on text-to-image models&lt;/li&gt;&lt;li&gt;Mitigates semantic and attention consistencies that reveal backdoor samples&lt;/li&gt;&lt;li&gt;Uses syntactic triggers and KMMD regularization for better evasion&lt;/li&gt;&lt;li&gt;Achieves 97.5% attack success rate and bypasses state-of-the-art defenses&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jie Zhang', 'Zhongqi Wang', 'Shiguang Shan', 'Xilin Chen']&lt;/li&gt;&lt;li&gt;Tags: ['backdoor attacks', 'text-to-image', 'stealthy attacks', 'adversarial prompting', 'detection evasion']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2503.17724</guid><pubDate>Fri, 25 Jul 2025 00:00:00 +0000</pubDate></item><item><title>HPS: Hard Preference Sampling for Human Preference Alignment</title><link>https://arxiv.org/abs/2502.14400</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces Hard Preference Sampling (HPS) framework for human preference alignment&lt;/li&gt;&lt;li&gt;Emphasizes handling of 'hard' dispreferred responses to improve rejection capabilities&lt;/li&gt;&lt;li&gt;Reduces computational overhead with single-sample Monte Carlo strategy&lt;/li&gt;&lt;li&gt;Validates effectiveness in reducing harmful content generation on safety datasets&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Xiandong Zou', 'Wanyu Lin', 'Yuchen Li', 'Pan Zhou']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'safety', 'robustness', 'preference-learning']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2502.14400</guid><pubDate>Fri, 25 Jul 2025 00:00:00 +0000</pubDate></item><item><title>LoRA-Leak: Membership Inference Attacks Against LoRA Fine-tuned Language Models</title><link>https://arxiv.org/abs/2507.18302</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces LoRA-Leak framework for evaluating membership inference attacks on LoRA fine-tuned LMs&lt;/li&gt;&lt;li&gt;Demonstrates significant vulnerability (0.775 AUC) under conservative settings&lt;/li&gt;&lt;li&gt;Evaluates four defenses, finding dropout and layer exclusion effective&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Delong Ran', 'Xinlei He', 'Tianshuo Cong', 'Anyu Wang', 'Qi Li', 'Xiaoyun Wang']&lt;/li&gt;&lt;li&gt;Tags: ['privacy attacks', 'membership inference', 'LoRA', 'fine-tuning', 'red teaming']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2507.18302</guid><pubDate>Fri, 25 Jul 2025 00:00:00 +0000</pubDate></item><item><title>Safeguarding RAG Pipelines with GMTP: A Gradient-based Masked Token Probability Method for Poisoned Document Detection</title><link>https://arxiv.org/abs/2507.18202</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Addresses data poisoning in RAG pipelines by detecting adversarial documents&lt;/li&gt;&lt;li&gt;Introduces GMTP method using gradient-based token analysis and MLM probability checks&lt;/li&gt;&lt;li&gt;Achieves over 90% poisoning elimination while retaining relevant documents&lt;/li&gt;&lt;li&gt;Maintains robust retrieval and generation performance across datasets&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['San Kim', 'Jonghwi Kim', 'Yejin Jeon', 'Gary Geunbae Lee']&lt;/li&gt;&lt;li&gt;Tags: ['data poisoning', 'security', 'robustness', 'RAG', 'adversarial attacks']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2507.18202</guid><pubDate>Fri, 25 Jul 2025 00:00:00 +0000</pubDate></item><item><title>Evaluating the Performance of AI Text Detectors, Few-Shot and Chain-of-Thought Prompting Using DeepSeek Generated Text</title><link>https://arxiv.org/abs/2507.17944</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Evaluates AI text detectors against DeepSeek-generated text with adversarial attacks&lt;/li&gt;&lt;li&gt;Tests paraphrasing and humanization attacks on six detection tools&lt;/li&gt;&lt;li&gt;Analyzes robustness and accuracy impact of adversarial techniques&lt;/li&gt;&lt;li&gt;Compares few-shot and chain-of-thought prompting for detection&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Hulayyil Alshammari', 'Praveen Rao']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial prompting', 'robustness', 'safety evaluation', 'red teaming', 'text detection']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2507.17944</guid><pubDate>Fri, 25 Jul 2025 00:00:00 +0000</pubDate></item><item><title>SafeWork-R1: Coevolving Safety and Intelligence under the AI-45$^{\circ}$ Law</title><link>https://arxiv.org/abs/2507.18576</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces SafeWork-R1, a multimodal reasoning model with coevolved safety and intelligence&lt;/li&gt;&lt;li&gt;Developed using SafeLadder framework with safety-oriented reinforcement learning&lt;/li&gt;&lt;li&gt;Achieves 46.54% average improvement on safety benchmarks over base model&lt;/li&gt;&lt;li&gt;Incorporates inference-time interventions and deliberative search for step-level verification&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Shanghai AI Lab', ':', 'Yicheng Bao', 'Guanxu Chen', 'Mingkang Chen', 'Yunhao Chen', 'Chiyu Chen', 'Lingjie Chen', 'Sirui Chen', 'Xinquan Chen', 'Jie Cheng', 'Yu Cheng', 'Dengke Deng', 'Yizhuo Ding', 'Dan Ding', 'Xiaoshan Ding', 'Yi Ding', 'Zhichen Dong', 'Lingxiao Du', 'Yuyu Fan', 'Xinshun Feng', 'Yanwei Fu', 'Yuxuan Gao', 'Ruijun Ge', 'Tianle Gu', 'Lujun Gui', 'Jiaxuan Guo', 'Qianxi He', 'Yuenan Hou', 'Xuhao Hu', 'Hong Huang', 'Kaichen Huang', 'Shiyang Huang', 'Yuxian Jiang', 'Shanzhe Lei', 'Jie Li', 'Lijun Li', 'Hao Li', 'Juncheng Li', 'Xiangtian Li', 'Yafu Li', 'Lingyu Li', 'Xueyan Li', 'Haotian Liang', 'Dongrui Liu', 'Qihua Liu', 'Zhixuan Liu', 'Bangwei Liu', 'Huacan Liu', 'Yuexiao Liu', 'Zongkai Liu', 'Chaochao Lu', 'Yudong Lu', 'Xiaoya Lu', 'Zhenghao Lu', 'Qitan Lv', 'Caoyuan Ma', 'Jiachen Ma', 'Xiaoya Ma', 'Zhongtian Ma', 'Lingyu Meng', 'Ziqi Miao', 'Yazhe Niu', 'Yuezhang Peng', 'Yuan Pu', 'Han Qi', 'Chen Qian', 'Xingge Qiao', 'Jingjing Qu', 'Jiashu Qu', 'Wanying Qu', 'Wenwen Qu', 'Xiaoye Qu', 'Qihan Ren', 'Qingnan Ren', 'Qingyu Ren', 'Jing Shao', 'Wenqi Shao', 'Shuai Shao', 'Dongxing Shi', 'Xin Song', 'Xinhao Song', 'Yan Teng', 'Xuan Tong', 'Yingchun Wang', 'Xuhong Wang', 'Shujie Wang', 'Xin Wang', 'Yige Wang', 'Yixu Wang', 'Yuanfu Wang', 'Futing Wang', 'Ruofan Wang', 'Wenjie Wang', 'Yajie Wang', 'Muhao Wei', 'Xiaoyu Wen', 'Fenghua Weng', 'Yuqi Wu', 'Yingtong Xiong', 'Xingcheng Xu', 'Chao Yang', 'Yue Yang', 'Yang Yao', 'Yulei Ye', 'Zhenyun Yin', 'Yi Yu', 'Bo Zhang', 'Qiaosheng Zhang', 'Jinxuan Zhang', 'Yexin Zhang', 'Yinqiang Zheng', 'Hefeng Zhou', 'Zhanhui Zhou', 'Pengyu Zhu', 'Qingzi Zhu', 'Yubo Zhu', 'Bowen Zhou']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'safety evaluation', 'robustness', 'multimodal', 'reinforcement learning']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2507.18576</guid><pubDate>Fri, 25 Jul 2025 00:00:00 +0000</pubDate></item></channel></rss>