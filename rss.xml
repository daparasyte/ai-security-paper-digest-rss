<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>AI Security Paper Digest</title><link>https://kentaroh-toyoda.github.io/ai-security-paper-digest-rss/rss.xml</link><description>Summarized AI security papers from arXiv</description><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><lastBuildDate>Tue, 20 May 2025 23:03:33 +0000</lastBuildDate><item><title>FL-PLAS: Federated Learning with Partial Layer Aggregation for Backdoor Defense Against High-Ratio Malicious Clients</title><link>https://arxiv.org/abs/2505.12019</link><description>• This paper proposes FL-PLAS, a defense algorithm for federated learning that mitigates backdoor attacks, even when a high proportion of clients are malicious.&lt;br/&gt;• The method involves partial layer aggregation, separating feature extractor and classifier layers to prevent backdoor label propagation.&lt;br/&gt;• The approach is evaluated against state-of-the-art backdoor attacks and compared to existing defenses, showing effectiveness without requiring auxiliary datasets.&lt;br/&gt;&lt;br/&gt;Tags: federated learning, backdoor attacks, defense, robustness, AI security&lt;br/&gt;Authors: Jianyi Zhang, Ziyin Zhou, Yilong Li, Qichao Jin&lt;br/&gt;Relevance: 5 / 5&lt;br/&gt;&lt;br/&gt;&lt;a href='https://arxiv.org/abs/2505.12019'&gt;Read on arXiv&lt;/a&gt;</description><pubDate>Tue, 20 May 2025 23:03:33 +0000</pubDate></item><item><title>TechniqueRAG: Retrieval Augmented Generation for Adversarial Technique Annotation in Cyber Threat Intelligence Text</title><link>https://arxiv.org/abs/2505.11988</link><description>• The paper introduces TechniqueRAG, a retrieval-augmented generation framework designed to annotate adversarial techniques in cyber threat intelligence texts. It focuses on improving the identification of adversarial techniques using LLMs and retrieval methods, addressing challenges in data scarcity and domain specificity. The work is directly related to AI security as it deals with the detection and annotation of adversarial techniques, which are central to understanding and defending against AI-driven cyber threats.&lt;br/&gt;&lt;br/&gt;Tags: adversarial techniques, cyber threat intelligence, retrieval-augmented generation, LLM, AI security&lt;br/&gt;Authors: Ahmed Lekssays, Utsav Shukla, Husrev Taha Sencar, Md Rizwan Parvez&lt;br/&gt;Relevance: 4 / 5&lt;br/&gt;&lt;br/&gt;&lt;a href='https://arxiv.org/abs/2505.11988'&gt;Read on arXiv&lt;/a&gt;</description><pubDate>Tue, 20 May 2025 23:03:33 +0000</pubDate></item><item><title>Co-Evolutionary Defence of Active Directory Attack Graphs via GNN-Approximated Dynamic Programming</title><link>https://arxiv.org/abs/2505.11710</link><description>• The paper addresses the security of Active Directory (AD) systems against adaptive adversaries by modeling attacker-defender interactions as a Stackelberg game. It proposes a co-evolutionary defense framework using Graph Neural Networks and dynamic programming to anticipate and counteract attacks on AD attack graphs, focusing on robust defense strategies against evolving threats.&lt;br/&gt;&lt;br/&gt;Tags: AI security, adversarial attacks, defense strategies, graph neural networks, enterprise security&lt;br/&gt;Authors: Diksha Goel, Hussain Ahmad, Kristen Moore, Mingyu Guo&lt;br/&gt;Relevance: 5 / 5&lt;br/&gt;&lt;br/&gt;&lt;a href='https://arxiv.org/abs/2505.11710'&gt;Read on arXiv&lt;/a&gt;</description><pubDate>Tue, 20 May 2025 23:03:33 +0000</pubDate></item><item><title>Unveiling the Black Box: A Multi-Layer Framework for Explaining Reinforcement Learning-Based Cyber Agents</title><link>https://arxiv.org/abs/2505.11708</link><description>• This paper introduces a multi-layer explainability framework for reinforcement learning (RL) agents used in simulating cyberattacks. The framework aims to make the decision-making processes of RL-based attacker agents more transparent, which is crucial for trust, debugging, and defensive preparedness in cybersecurity. The approach is evaluated in CyberBattleSim environments and is designed to support use cases such as red-team simulation and RL policy debugging, directly addressing the analysis and anticipation of autonomous cyber threats.&lt;br/&gt;&lt;br/&gt;Tags: AI security, explainability, reinforcement learning, cybersecurity, red teaming, autonomous agents&lt;br/&gt;Authors: Diksha Goel, Kristen Moore, Jeff Wang, Minjune Kim, Thanh Thi Nguyen&lt;br/&gt;Relevance: 4 / 5&lt;br/&gt;&lt;br/&gt;&lt;a href='https://arxiv.org/abs/2505.11708'&gt;Read on arXiv&lt;/a&gt;</description><pubDate>Tue, 20 May 2025 23:03:33 +0000</pubDate></item><item><title>The Ripple Effect: On Unforeseen Complications of Backdoor Attacks</title><link>https://arxiv.org/abs/2505.11586</link><description>• This paper investigates the unforeseen complications of backdoor attacks in pre-trained language models (PTLMs), specifically how backdoored models adapted to new tasks can exhibit suspicious behaviors that compromise attack stealthiness.&lt;br/&gt;• It provides a comprehensive quantification of these complications and proposes a mitigation method using multi-task learning.&lt;br/&gt;• The work directly addresses AI security concerns related to backdoor attacks, model robustness, and stealthiness of adversarial manipulations.&lt;br/&gt;&lt;br/&gt;Tags: backdoor attacks, PTLMs, model robustness, adversarial attacks, AI security&lt;br/&gt;Authors: Rui Zhang, Yun Shen, Hongwei Li, Wenbo Jiang, Hanxiao Chen, Yuan Zhang, Guowen Xu, Yang Zhang&lt;br/&gt;Relevance: 5 / 5&lt;br/&gt;&lt;br/&gt;&lt;a href='https://arxiv.org/abs/2505.11586'&gt;Read on arXiv&lt;/a&gt;</description><pubDate>Tue, 20 May 2025 23:03:33 +0000</pubDate></item><item><title>ACSE-Eval: Can LLMs threat model real-world cloud infrastructure?</title><link>https://arxiv.org/abs/2505.11565</link><description>• The paper introduces a dataset (ACSE-Eval) for evaluating the ability of large language models (LLMs) to perform threat modeling in real-world cloud infrastructure scenarios.&lt;br/&gt;• It assesses LLMs' capabilities to identify security risks, analyze attack vectors, and propose mitigations in cloud environments.&lt;br/&gt;• The work directly addresses the use of LLMs for cybersecurity tasks, including threat identification and modeling, which are core aspects of AI security.&lt;br/&gt;&lt;br/&gt;Tags: LLM security, threat modeling, cloud security, cybersecurity, AI robustness&lt;br/&gt;Authors: Sarthak Munshi, Swapnil Pathak, Sonam Ghatode, Thenuga Priyadarshini, Dhivya Chandramouleeswaran, Ashutosh Rana&lt;br/&gt;Relevance: 4 / 5&lt;br/&gt;&lt;br/&gt;&lt;a href='https://arxiv.org/abs/2505.11565'&gt;Read on arXiv&lt;/a&gt;</description><pubDate>Tue, 20 May 2025 23:03:33 +0000</pubDate></item><item><title>One Shot Dominance: Knowledge Poisoning Attack on Retrieval-Augmented Generation Systems</title><link>https://arxiv.org/abs/2505.11548</link><description>• This paper introduces AuthChain, a novel knowledge poisoning attack targeting Retrieval-Augmented Generation (RAG) systems. The attack enables effective single-document poisoning by leveraging chain-of-evidence theory and authority effects, making the malicious content more convincing and stealthy. The work demonstrates higher attack success rates and improved stealthiness against existing RAG defense mechanisms, highlighting significant security vulnerabilities in LLM-based RAG systems.&lt;br/&gt;&lt;br/&gt;Tags: knowledge poisoning, RAG, adversarial attacks, LLM security, robustness, AI security&lt;br/&gt;Authors: Zhiyuan Chang, Xiaojun Jia, Mingyang Li, Junjie Wang, Yuekai Huang, Qing Wang, Ziyou Jiang, Yang Liu&lt;br/&gt;Relevance: 5 / 5&lt;br/&gt;&lt;br/&gt;&lt;a href='https://arxiv.org/abs/2505.11548'&gt;Read on arXiv&lt;/a&gt;</description><pubDate>Tue, 20 May 2025 23:03:33 +0000</pubDate></item><item><title>Why Not Act on What You Know? Unleashing Safety Potential of LLMs via Self-Aware Guard Enhancement</title><link>https://arxiv.org/abs/2505.12060</link><description>• This paper addresses the vulnerability of Large Language Models (LLMs) to jailbreak attacks, where adversaries bypass safety mechanisms to elicit unsafe outputs.&lt;br/&gt;• It proposes SAGE, a defense strategy that enhances LLMs' ability to consistently refuse unsafe requests by aligning their detection and generation behaviors.&lt;br/&gt;• The work includes experiments demonstrating improved robustness against sophisticated jailbreak attempts and analyzes the mechanisms behind LLMs' safety gaps.&lt;br/&gt;&lt;br/&gt;Tags: jailbreak, LLM security, adversarial attacks, robustness, defense mechanisms&lt;br/&gt;Authors: Peng Ding, Jun Kuang, Zongyu Wang, Xuezhi Cao, Xunliang Cai, Jiajun Chen, Shujian Huang&lt;br/&gt;Relevance: 5 / 5&lt;br/&gt;&lt;br/&gt;&lt;a href='https://arxiv.org/abs/2505.12060'&gt;Read on arXiv&lt;/a&gt;</description><pubDate>Tue, 20 May 2025 23:03:33 +0000</pubDate></item><item><title>Multilingual Collaborative Defense for Large Language Models</title><link>https://arxiv.org/abs/2505.11835</link><description>• The paper addresses the security of large language models (LLMs) against multilingual jailbreak attacks, where adversaries bypass safeguards by using rare or underrepresented languages.&lt;br/&gt;• It proposes a new defense method (Multilingual Collaborative Defense) to improve LLM robustness and safety across multiple languages.&lt;br/&gt;• The work includes the creation and use of multilingual adversarial benchmarks to evaluate defense effectiveness.&lt;br/&gt;&lt;br/&gt;Tags: LLM security, jailbreak attacks, multilingual robustness, adversarial attacks, AI safety&lt;br/&gt;Authors: Hongliang Li, Jinan Xu, Gengping Cui, Changhao Guan, Fengran Mo, Kaiyu Huang&lt;br/&gt;Relevance: 5 / 5&lt;br/&gt;&lt;br/&gt;&lt;a href='https://arxiv.org/abs/2505.11835'&gt;Read on arXiv&lt;/a&gt;</description><pubDate>Tue, 20 May 2025 23:03:33 +0000</pubDate></item><item><title>EnvInjection: Environmental Prompt Injection Attack to Multi-modal Web Agents</title><link>https://arxiv.org/abs/2505.11717</link><description>• The paper introduces EnvInjection, a novel environmental prompt injection attack targeting multi-modal large language model (MLLM)-based web agents.&lt;br/&gt;• The attack manipulates webpage environments at the pixel level to induce specific, attacker-chosen actions by the agent.&lt;br/&gt;• The method is formulated as an optimization problem and overcomes technical challenges related to non-differentiable mappings.&lt;br/&gt;• Extensive evaluations demonstrate the attack's effectiveness and stealthiness compared to existing methods.&lt;br/&gt;&lt;br/&gt;Tags: prompt injection, adversarial attacks, multi-modal models, web agents, AI security&lt;br/&gt;Authors: Xilong Wang, John Bloch, Zedian Shao, Yuepeng Hu, Shuyan Zhou, Neil Zhenqiang Gong&lt;br/&gt;Relevance: 5 / 5&lt;br/&gt;&lt;br/&gt;&lt;a href='https://arxiv.org/abs/2505.11717'&gt;Read on arXiv&lt;/a&gt;</description><pubDate>Tue, 20 May 2025 23:03:33 +0000</pubDate></item></channel></rss>