<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>AI Security Paper Digest</title><link>https://kentaroh-toyoda.github.io/ai-security-paper-digest-rss/rss.xml</link><description>Curated papers on AI security from ArXiv</description><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><lastBuildDate>Tue, 09 Sep 2025 22:46:35 +0000</lastBuildDate><item><title>ComplicitSplat: Downstream Models are Vulnerable to Blackbox Attacks by 3D Gaussian Splat Camouflages</title><link>https://arxiv.org/abs/2508.11854</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces ComplicitSplat, a black-box attack using 3D Gaussian Splatting to create viewpoint-specific camouflage&lt;/li&gt;&lt;li&gt;Targets downstream object detectors including single-stage, multi-stage, and transformer-based models&lt;/li&gt;&lt;li&gt;Demonstrates success on both real-world and synthetic scenes without model access&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Matthew Hull', 'Haoyang Yang', 'Pratham Mehta', 'Mansi Phute', 'Aeree Cho', 'Haorang Wang', 'Matthew Lau', 'Wenke Lee', 'Wilian Lunardi', 'Martin Andreoni', 'Duen Horng Chau']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial attacks', '3D Gaussian Splatting', 'black-box attacks', 'object detection', 'safety']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2508.11854</guid><pubDate>Tue, 09 Sep 2025 00:00:00 +0000</pubDate></item><item><title>Evaluating the Impact of Adversarial Attacks on Traffic Sign Classification using the LISA Dataset</title><link>https://arxiv.org/abs/2509.06835</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Trains a CNN on LISA dataset for 47 traffic signs&lt;/li&gt;&lt;li&gt;Evaluates robustness against FGSM and PGD attacks&lt;/li&gt;&lt;li&gt;Shows significant accuracy drop with increasing perturbations&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Nabeyou Tadessa', 'Balaji Iyangar', 'Mashrur Chowdhury']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial attacks', 'robustness', 'safety evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.06835</guid><pubDate>Tue, 09 Sep 2025 00:00:00 +0000</pubDate></item><item><title>IGAff: Benchmarking Adversarial Iterative and Genetic Affine Algorithms on Deep Neural Networks</title><link>https://arxiv.org/abs/2509.06459</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces two new black-box adversarial attack algorithms: Affine Transformation Attack (ATA) and Affine Genetic Attack (AGA)&lt;/li&gt;&lt;li&gt;Evaluates performance on ResNet-18, DenseNet-121, Swin Transformer V2, and Vision Transformer models using Tiny ImageNet, Caltech-256, and Food-101 datasets&lt;/li&gt;&lt;li&gt;Compares with existing Pixle and Square Attack methods, achieving up to 8.82% accuracy improvement&lt;/li&gt;&lt;li&gt;Provides insights into adversarial robustness and defense strategies&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Sebastian-Vasile Echim', 'Andrei-Alexandru Preda', 'Dumitru-Clementin Cercel', 'Florin Pop']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial attacks', 'black-box attacks', 'affine transformations', 'genetic algorithms', 'image classification']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.06459</guid><pubDate>Tue, 09 Sep 2025 00:00:00 +0000</pubDate></item><item><title>Oyster-I: Beyond Refusal -- Constructive Safety Alignment for Responsible Language Models</title><link>https://arxiv.org/abs/2509.01909</link><description>&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Other&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Ranjie Duan', 'Jiexi Liu', 'Xiaojun Jia', 'Shiji Zhao', 'Ruoxi Cheng', 'Fengxiang Wang', 'Cheng Wei', 'Yong Xie', 'Chang Liu', 'Defeng Li', 'Yinpeng Dong', 'Yichi Zhang', 'Yuefeng Chen', 'Chongwen Wang', 'Xingjun Ma', 'Xingxing Wei', 'Yang Liu', 'Hang Su', 'Jun Zhu', 'Xinfeng Li', 'Yitong Sun', 'Jie Zhang', 'Jinzhao Hu', 'Sha Xu', 'Yitong Yang', 'Jialing Tao', 'Hui Xue']&lt;/li&gt;&lt;li&gt;Tags: []&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.01909</guid><pubDate>Tue, 09 Sep 2025 00:00:00 +0000</pubDate></item><item><title>Membership Inference Attacks on LLM-based Recommender Systems</title><link>https://arxiv.org/abs/2508.18665</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces four membership inference attacks (direct inquiry, hallucination, similarity, poisoning) targeting LLM-based recommender systems&lt;/li&gt;&lt;li&gt;Evaluates attacks on three LLMs and two benchmark datasets&lt;/li&gt;&lt;li&gt;Finds direct inquiry and poisoning attacks are highly effective&lt;/li&gt;&lt;li&gt;Analyzes impact of prompt structure and user position on attack success&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jiajie He', 'Yuechun Gu', 'Min-Chun Chen', 'Keke Chen']&lt;/li&gt;&lt;li&gt;Tags: ['privacy attacks', 'LLM security', 'data poisoning', 'red teaming', 'adversarial prompting']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2508.18665</guid><pubDate>Tue, 09 Sep 2025 00:00:00 +0000</pubDate></item><item><title>OpenDeception: Benchmarking and Investigating AI Deceptive Behaviors via Open-ended Interaction Simulation</title><link>https://arxiv.org/abs/2504.13707</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduced OpenDeception framework for evaluating AI deception risks&lt;/li&gt;&lt;li&gt;Evaluated deception intention and capabilities in 11 LLMs via simulated interactions&lt;/li&gt;&lt;li&gt;Found high deception intention (&gt;80%) and success rates (&gt;50%) across models&lt;/li&gt;&lt;li&gt;Observed correlation between model capability and deception risk&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yichen Wu', 'Xudong Pan', 'Geng Hong', 'Min Yang']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'safety evaluation', 'benchmarking', 'deception', 'LLM']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2504.13707</guid><pubDate>Tue, 09 Sep 2025 00:00:00 +0000</pubDate></item><item><title>EMNLP: Educator-role Moral and Normative Large Language Models Profiling</title><link>https://arxiv.org/abs/2508.15250</link><description>&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Other&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yilin Jiang', 'Mingzi Zhang', 'Sheng Jin', 'Zengyi Yu', 'Xiangjie Kong', 'Binghao Tu']&lt;/li&gt;&lt;li&gt;Tags: []&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2508.15250</guid><pubDate>Tue, 09 Sep 2025 00:00:00 +0000</pubDate></item><item><title>Too Consistent to Detect: A Study of Self-Consistent Errors in LLMs</title><link>https://arxiv.org/abs/2505.17656</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Formally defines self-consistent errors in LLMs&lt;/li&gt;&lt;li&gt;Evaluates existing detection methods' limitations&lt;/li&gt;&lt;li&gt;Proposes cross-model probe method for improved detection&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Hexiang Tan', 'Fei Sun', 'Sha Liu', 'Du Su', 'Qi Cao', 'Xin Chen', 'Jingang Wang', 'Xunliang Cai', 'Yuanzhuo Wang', 'Huawei Shen', 'Xueqi Cheng']&lt;/li&gt;&lt;li&gt;Tags: ['safety evaluation', 'robustness', 'error detection', 'LLM verification']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2505.17656</guid><pubDate>Tue, 09 Sep 2025 00:00:00 +0000</pubDate></item><item><title>Turning Logic Against Itself : Probing Model Defenses Through Contrastive Questions</title><link>https://arxiv.org/abs/2501.01872</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces POATE, a novel jailbreak technique using contrastive reasoning to generate polar opposite queries&lt;/li&gt;&lt;li&gt;Evaluates across 6 language model families, achieving ~44% attack success rate&lt;/li&gt;&lt;li&gt;Proposes Intent-Aware CoT and Reverse Thinking CoT defenses to detect and reject harmful responses&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Rachneet Sachdeva', 'Rima Hazra', 'Iryna Gurevych']&lt;/li&gt;&lt;li&gt;Tags: ['jailbreaking', 'adversarial_prompting', 'safety_evaluation', 'model_defenses', 'red_teaming']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2501.01872</guid><pubDate>Tue, 09 Sep 2025 00:00:00 +0000</pubDate></item><item><title>Concept Bottleneck Large Language Models</title><link>https://arxiv.org/abs/2412.07992</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces Concept Bottleneck Large Language Models (CB-LLMs) for inherent interpretability&lt;/li&gt;&lt;li&gt;Applies to text classification and generation tasks&lt;/li&gt;&lt;li&gt;Enhances safety through transparent harmful content detection and controlled generation&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Chung-En Sun', 'Tuomas Oikarinen', 'Berk Ustun', 'Tsui-Wei Weng']&lt;/li&gt;&lt;li&gt;Tags: ['safety', 'interpretability', 'text_generation', 'alignment', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2412.07992</guid><pubDate>Tue, 09 Sep 2025 00:00:00 +0000</pubDate></item><item><title>Cross-Service Threat Intelligence in LLM Services using Privacy-Preserving Fingerprints</title><link>https://arxiv.org/abs/2509.05608</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces BinaryShield for privacy-preserving threat intelligence sharing in LLM services&lt;/li&gt;&lt;li&gt;Addresses prompt injection attacks across compliance boundaries&lt;/li&gt;&lt;li&gt;Uses PII redaction, semantic embedding, binary quantization, and randomized response&lt;/li&gt;&lt;li&gt;Shows high accuracy and performance improvements over baseline&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Waris Gill', 'Natalie Isak', 'Matthew Dressman']&lt;/li&gt;&lt;li&gt;Tags: ['prompt injection', 'privacy-preserving', 'threat intelligence', 'LLM security', 'adversarial attacks']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.05608</guid><pubDate>Tue, 09 Sep 2025 00:00:00 +0000</pubDate></item><item><title>MoGU V2: Toward a Higher Pareto Frontier Between Model Usability and Security</title><link>https://arxiv.org/abs/2509.06807</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes MoGU_v2 framework to balance LLM security and usability&lt;/li&gt;&lt;li&gt;Dynamic weight allocation between security and usability variants&lt;/li&gt;&lt;li&gt;Adapts to different LLM types and withstands instruction fine-tuning&lt;/li&gt;&lt;li&gt;Achieves higher Pareto frontier without trade-offs&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yanrui Du', 'Fenglei Fan', 'Sendong Zhao', 'Jiawei Cao', 'Ting Liu', 'Bing Qin']&lt;/li&gt;&lt;li&gt;Tags: ['LLM security', 'Pareto frontier', 'Dynamic routing', 'Adversarial defense', 'Model alignment']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.06807</guid><pubDate>Tue, 09 Sep 2025 00:00:00 +0000</pubDate></item><item><title>Anchoring Refusal Direction: Mitigating Safety Risks in Tuning via Projection Constraint</title><link>https://arxiv.org/abs/2509.06795</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces ProCon method to mitigate safety risks in LLM fine-tuning by constraining refusal direction drift&lt;/li&gt;&lt;li&gt;Uses projection-constrained loss to regularize hidden state projections onto the refusal direction&lt;/li&gt;&lt;li&gt;Demonstrates improved safety performance across multiple datasets and models&lt;/li&gt;&lt;li&gt;Provides analysis showing stabilized refusal direction during training&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yanrui Du', 'Fenglei Fan', 'Sendong Zhao', 'Jiawei Cao', 'Qika Lin', 'Kai He', 'Ting Liu', 'Bing Qin', 'Mengling Feng']&lt;/li&gt;&lt;li&gt;Tags: ['LLM red teaming', 'Safety evaluation', 'Alignment', 'Robustness', 'Security standards']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.06795</guid><pubDate>Tue, 09 Sep 2025 00:00:00 +0000</pubDate></item><item><title>Mask-GCG: Are All Tokens in Adversarial Suffixes Necessary for Jailbreak Attacks?</title><link>https://arxiv.org/abs/2509.06350</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces Mask-GCG, a method to optimize jailbreak attacks on LLMs by pruning redundant tokens in adversarial suffixes&lt;/li&gt;&lt;li&gt;Uses learnable token masking to identify high-impact positions&lt;/li&gt;&lt;li&gt;Reduces computational overhead while maintaining attack success rates&lt;/li&gt;&lt;li&gt;Provides insights into token redundancy in LLM prompts&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Junjie Mu', 'Zonghao Ying', 'Zhekui Fan', 'Zonglei Jing', 'Yaoyuan Zhang', 'Zhengmin Yu', 'Wenxin Zhang', 'Quanchen Zou', 'Xiangzheng Zhang']&lt;/li&gt;&lt;li&gt;Tags: ['jailbreaking', 'adversarial_prompting', 'red_team', 'llm_security']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.06350</guid><pubDate>Tue, 09 Sep 2025 00:00:00 +0000</pubDate></item><item><title>Beyond ROUGE: N-Gram Subspace Features for LLM Hallucination Detection</title><link>https://arxiv.org/abs/2509.05360</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a novel hallucination detection method using N-Gram frequency tensors&lt;/li&gt;&lt;li&gt;Improves upon traditional metrics like ROUGE by capturing semantic co-occurrence patterns&lt;/li&gt;&lt;li&gt;Evaluates on HaluEval dataset with significant gains over baselines&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jerry Li', 'Evangelos Papalexakis']&lt;/li&gt;&lt;li&gt;Tags: ['safety', 'hallucination detection', 'model evaluation', 'text generation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.05360</guid><pubDate>Tue, 09 Sep 2025 00:00:00 +0000</pubDate></item><item><title>Membership Inference Attacks on LLM-based Recommender Systems</title><link>https://arxiv.org/abs/2508.18665</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces four membership inference attacks (direct inquiry, hallucination, similarity, poisoning) targeting LLM-based recommender systems&lt;/li&gt;&lt;li&gt;Evaluates attacks on three LLMs and two benchmark datasets&lt;/li&gt;&lt;li&gt;Finds direct inquiry and poisoning attacks are highly effective&lt;/li&gt;&lt;li&gt;Analyzes impact of prompt structure and user position on attack success&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jiajie He', 'Yuechun Gu', 'Min-Chun Chen', 'Keke Chen']&lt;/li&gt;&lt;li&gt;Tags: ['privacy attacks', 'LLM security', 'data poisoning', 'red teaming', 'adversarial prompting']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2508.18665</guid><pubDate>Tue, 09 Sep 2025 00:00:00 +0000</pubDate></item><item><title>ComplicitSplat: Downstream Models are Vulnerable to Blackbox Attacks by 3D Gaussian Splat Camouflages</title><link>https://arxiv.org/abs/2508.11854</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces ComplicitSplat, a black-box attack using 3D Gaussian Splatting to create viewpoint-specific camouflage&lt;/li&gt;&lt;li&gt;Targets downstream object detectors including single-stage, multi-stage, and transformer-based models&lt;/li&gt;&lt;li&gt;Demonstrates success on both real-world and synthetic scenes without model access&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Matthew Hull', 'Haoyang Yang', 'Pratham Mehta', 'Mansi Phute', 'Aeree Cho', 'Haorang Wang', 'Matthew Lau', 'Wenke Lee', 'Wilian Lunardi', 'Martin Andreoni', 'Duen Horng Chau']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial attacks', '3D Gaussian Splatting', 'black-box attacks', 'object detection', 'safety']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2508.11854</guid><pubDate>Tue, 09 Sep 2025 00:00:00 +0000</pubDate></item><item><title>Concept Bottleneck Large Language Models</title><link>https://arxiv.org/abs/2412.07992</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces Concept Bottleneck Large Language Models (CB-LLMs) for inherent interpretability&lt;/li&gt;&lt;li&gt;Applies to text classification and generation tasks&lt;/li&gt;&lt;li&gt;Enhances safety through transparent harmful content detection and controlled generation&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Chung-En Sun', 'Tuomas Oikarinen', 'Berk Ustun', 'Tsui-Wei Weng']&lt;/li&gt;&lt;li&gt;Tags: ['safety', 'interpretability', 'text_generation', 'alignment', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2412.07992</guid><pubDate>Tue, 09 Sep 2025 00:00:00 +0000</pubDate></item><item><title>BadPromptFL: A Novel Backdoor Threat to Prompt-based Federated Learning in Multimodal Models</title><link>https://arxiv.org/abs/2508.08040</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces BadPromptFL, the first backdoor attack on prompt-based federated learning in multimodal models&lt;/li&gt;&lt;li&gt;Compromised clients inject poisoned prompts into global aggregation, enabling stealthy backdoor activation&lt;/li&gt;&lt;li&gt;Achieves high attack success rates (&gt;90%) with minimal visibility and limited client participation&lt;/li&gt;&lt;li&gt;Validated across multiple datasets and aggregation protocols&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Maozhen Zhang', 'Mengnan Zhao', 'Wei Wang', 'Bo Wang']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial prompting', 'data poisoning', 'privacy attacks', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2508.08040</guid><pubDate>Tue, 09 Sep 2025 00:00:00 +0000</pubDate></item><item><title>When Secure Isn't: Assessing the Security of Machine Learning Model Sharing</title><link>https://arxiv.org/abs/2509.06703</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Evaluates security of ML model sharing frameworks and hubs&lt;/li&gt;&lt;li&gt;Identifies six 0-day vulnerabilities enabling arbitrary code execution&lt;/li&gt;&lt;li&gt;Surveys user perceptions of security narratives around model sharing&lt;/li&gt;&lt;li&gt;Debunks misconceptions about model sharing security&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Gabriele Digregorio', 'Marco Di Gennaro', 'Stefano Zanero', 'Stefano Longari', 'Michele Carminati']&lt;/li&gt;&lt;li&gt;Tags: ['security', 'model sharing', 'vulnerabilities', 'user perception', 'arbitrary code execution']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.06703</guid><pubDate>Tue, 09 Sep 2025 00:00:00 +0000</pubDate></item><item><title>IGAff: Benchmarking Adversarial Iterative and Genetic Affine Algorithms on Deep Neural Networks</title><link>https://arxiv.org/abs/2509.06459</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces two new black-box adversarial attack algorithms: Affine Transformation Attack (ATA) and Affine Genetic Attack (AGA)&lt;/li&gt;&lt;li&gt;Evaluates performance on ResNet-18, DenseNet-121, Swin Transformer V2, and Vision Transformer models using Tiny ImageNet, Caltech-256, and Food-101 datasets&lt;/li&gt;&lt;li&gt;Compares with existing Pixle and Square Attack methods, achieving up to 8.82% accuracy improvement&lt;/li&gt;&lt;li&gt;Provides insights into adversarial robustness and defense strategies&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Sebastian-Vasile Echim', 'Andrei-Alexandru Preda', 'Dumitru-Clementin Cercel', 'Florin Pop']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial attacks', 'black-box attacks', 'affine transformations', 'genetic algorithms', 'image classification']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.06459</guid><pubDate>Tue, 09 Sep 2025 00:00:00 +0000</pubDate></item><item><title>Embedding Poisoning: Bypassing Safety Alignment via Embedding Semantic Shift</title><link>https://arxiv.org/abs/2509.06338</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces Search based Embedding Poisoning (SEP), a novel attack framework that manipulates LLM embeddings to bypass safety alignment&lt;/li&gt;&lt;li&gt;Achieves 96.43% average attack success rate across six aligned LLMs while preserving benign performance&lt;/li&gt;&lt;li&gt;Highlights critical need for embedding-level integrity checks in LLM security defenses&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Shuai Yuan', 'Zhibo Zhang', 'Yuxi Li', 'Guangdong Bai', 'Wang Kailong']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial prompting', 'embedding manipulation', 'safety alignment', 'LLM security', 'red teaming']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.06338</guid><pubDate>Tue, 09 Sep 2025 00:00:00 +0000</pubDate></item><item><title>DCMI: A Differential Calibration Membership Inference Attack Against Retrieval-Augmented Generation</title><link>https://arxiv.org/abs/2509.06026</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes DCMI, a differential calibration membership inference attack against RAG systems&lt;/li&gt;&lt;li&gt;Leverages sensitivity gap between member and non-member documents under query perturbation&lt;/li&gt;&lt;li&gt;Achieves high AUC and accuracy in experiments against RAG models and real-world platforms&lt;/li&gt;&lt;li&gt;Highlights significant privacy risks in RAG systems&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Xinyu Gao', 'Xiangtao Meng', 'Yingkai Dong', 'Zheng Li', 'Shanqing Guo']&lt;/li&gt;&lt;li&gt;Tags: ['membership inference', 'privacy attack', 'RAG', 'data leakage', 'adversarial attacks']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.06026</guid><pubDate>Tue, 09 Sep 2025 00:00:00 +0000</pubDate></item><item><title>Reasoning Introduces New Poisoning Attacks Yet Makes Them More Complicated</title><link>https://arxiv.org/abs/2509.05739</link><description>&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Other&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Hanna Foerster', 'Ilia Shumailov', 'Yiren Zhao', 'Harsh Chaudhari', 'Jamie Hayes', 'Robert Mullins', 'Yarin Gal']&lt;/li&gt;&lt;li&gt;Tags: []&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.05739</guid><pubDate>Tue, 09 Sep 2025 00:00:00 +0000</pubDate></item><item><title>Cross-Service Threat Intelligence in LLM Services using Privacy-Preserving Fingerprints</title><link>https://arxiv.org/abs/2509.05608</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces BinaryShield for privacy-preserving threat intelligence sharing in LLM services&lt;/li&gt;&lt;li&gt;Addresses prompt injection attacks across compliance boundaries&lt;/li&gt;&lt;li&gt;Uses PII redaction, semantic embedding, binary quantization, and randomized response&lt;/li&gt;&lt;li&gt;Shows high accuracy and performance improvements over baseline&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Waris Gill', 'Natalie Isak', 'Matthew Dressman']&lt;/li&gt;&lt;li&gt;Tags: ['prompt injection', 'privacy-preserving', 'threat intelligence', 'LLM security', 'adversarial attacks']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.05608</guid><pubDate>Tue, 09 Sep 2025 00:00:00 +0000</pubDate></item><item><title>Murphys Laws of AI Alignment: Why the Gap Always Wins</title><link>https://arxiv.org/abs/2509.05381</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces the Alignment Gap concept using KL-tilting formalism to explain recurring alignment failures&lt;/li&gt;&lt;li&gt;Catalogues failures into Murphy's Laws of AI Alignment&lt;/li&gt;&lt;li&gt;Proposes the Alignment Trilemma to frame trade-offs in alignment methods&lt;/li&gt;&lt;li&gt;Introduces MAPS framework (Misspecification, Annotation, Pressure, Shift) as design levers&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Madhava Gaikwad']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'safety', 'RLHF', 'DPO', "Murphy's Laws", 'structural limits']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.05381</guid><pubDate>Tue, 09 Sep 2025 00:00:00 +0000</pubDate></item><item><title>Beyond ROUGE: N-Gram Subspace Features for LLM Hallucination Detection</title><link>https://arxiv.org/abs/2509.05360</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a novel hallucination detection method using N-Gram frequency tensors&lt;/li&gt;&lt;li&gt;Improves upon traditional metrics like ROUGE by capturing semantic co-occurrence patterns&lt;/li&gt;&lt;li&gt;Evaluates on HaluEval dataset with significant gains over baselines&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jerry Li', 'Evangelos Papalexakis']&lt;/li&gt;&lt;li&gt;Tags: ['safety', 'hallucination detection', 'model evaluation', 'text generation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.05360</guid><pubDate>Tue, 09 Sep 2025 00:00:00 +0000</pubDate></item><item><title>From Noise to Narrative: Tracing the Origins of Hallucinations in Transformers</title><link>https://arxiv.org/abs/2509.06938</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Investigates the origins of hallucinations in transformer models using sparse autoencoders.&lt;/li&gt;&lt;li&gt;Shows that hallucinations increase with input uncertainty and can be predicted from internal activations.&lt;/li&gt;&lt;li&gt;Highlights implications for AI safety, alignment, and potential adversarial attacks.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Praneet Suresh', 'Jack Stanley', 'Sonia Joseph', 'Luca Scimeca', 'Danilo Bzdok']&lt;/li&gt;&lt;li&gt;Tags: ['hallucinations', 'transformers', 'safety', 'alignment', 'adversarial attacks', 'internal activations']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.06938</guid><pubDate>Tue, 09 Sep 2025 00:00:00 +0000</pubDate></item><item><title>Not All Samples Are Equal: Quantifying Instance-level Difficulty in Targeted Data Poisoning</title><link>https://arxiv.org/abs/2509.06896</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Investigates factors affecting susceptibility to targeted data poisoning attacks&lt;/li&gt;&lt;li&gt;Introduces three predictive criteria: ergodic prediction accuracy, poison distance, poison budget&lt;/li&gt;&lt;li&gt;Provides metrics for vulnerability assessment in diverse scenarios&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['William Xu', 'Yiwei Lu', 'Yihan Wang', 'Matthew Y. R. Yang', 'Zuoqiu Liu', 'Gautam Kamath', 'Yaoliang Yu']&lt;/li&gt;&lt;li&gt;Tags: ['data poisoning', 'targeted attacks', 'vulnerability assessment', 'security', 'poisoning difficulty']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.06896</guid><pubDate>Tue, 09 Sep 2025 00:00:00 +0000</pubDate></item><item><title>\texttt{R$^\textbf{2}$AI}: Towards Resistant and Resilient AI in an Evolving World</title><link>https://arxiv.org/abs/2509.06786</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes safe-by-coevolution paradigm inspired by biological immunity&lt;/li&gt;&lt;li&gt;Introduces R²AI framework combining resistance and resilience&lt;/li&gt;&lt;li&gt;Incorporates adversarial simulation and verification through safety wind tunnel&lt;/li&gt;&lt;li&gt;Emphasizes coevolution of safety and capability for dynamic environments&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Position&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Youbang Sun', 'Xiang Wang', 'Jie Fu', 'Chaochao Lu', 'Bowen Zhou']&lt;/li&gt;&lt;li&gt;Tags: ['safety', 'security', 'red teaming', 'coevolution', 'resilience']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.06786</guid><pubDate>Tue, 09 Sep 2025 00:00:00 +0000</pubDate></item><item><title>Breaking SafetyCore: Exploring the Risks of On-Device AI Deployment</title><link>https://arxiv.org/abs/2509.06371</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Examines security risks of on-device AI through SafetyCore case study&lt;/li&gt;&lt;li&gt;Demonstrates model extraction and manipulation to bypass detection&lt;/li&gt;&lt;li&gt;Highlights vulnerabilities in on-device AI deployments&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Victor Guyomard', 'Mathis Mauvisseau', 'Marie Paindavoine']&lt;/li&gt;&lt;li&gt;Tags: ['model extraction', 'security', 'privacy attacks', 'adversarial attacks']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.06371</guid><pubDate>Tue, 09 Sep 2025 00:00:00 +0000</pubDate></item><item><title>Neural Breadcrumbs: Membership Inference Attacks on LLMs Through Hidden State and Attention Pattern Analysis</title><link>https://arxiv.org/abs/2509.05449</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces memTrace framework for membership inference attacks on LLMs using hidden states and attention patterns&lt;/li&gt;&lt;li&gt;Achieves average AUC scores of 0.85 on MIA benchmarks&lt;/li&gt;&lt;li&gt;Highlights the need for better privacy-preserving training techniques&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Disha Makhija', 'Manoj Ghuhan Arivazhagan', 'Vinayshekhar Bannihatti Kumar', 'Rashmi Gangadharaiah']&lt;/li&gt;&lt;li&gt;Tags: ['privacy attacks', 'membership inference', 'LLM security', 'internal states', 'attention patterns']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.05449</guid><pubDate>Tue, 09 Sep 2025 00:00:00 +0000</pubDate></item><item><title>Membership Inference Attacks on LLM-based Recommender Systems</title><link>https://arxiv.org/abs/2508.18665</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces four membership inference attacks (direct inquiry, hallucination, similarity, poisoning) targeting LLM-based recommender systems&lt;/li&gt;&lt;li&gt;Evaluates attacks on three LLMs and two benchmark datasets&lt;/li&gt;&lt;li&gt;Finds direct inquiry and poisoning attacks are highly effective&lt;/li&gt;&lt;li&gt;Analyzes impact of prompt structure and user position on attack success&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jiajie He', 'Yuechun Gu', 'Min-Chun Chen', 'Keke Chen']&lt;/li&gt;&lt;li&gt;Tags: ['privacy attacks', 'LLM security', 'data poisoning', 'red teaming', 'adversarial prompting']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2508.18665</guid><pubDate>Tue, 09 Sep 2025 00:00:00 +0000</pubDate></item><item><title>BadPromptFL: A Novel Backdoor Threat to Prompt-based Federated Learning in Multimodal Models</title><link>https://arxiv.org/abs/2508.08040</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces BadPromptFL, the first backdoor attack on prompt-based federated learning in multimodal models&lt;/li&gt;&lt;li&gt;Compromised clients inject poisoned prompts into global aggregation, enabling stealthy backdoor activation&lt;/li&gt;&lt;li&gt;Achieves high attack success rates (&gt;90%) with minimal visibility and limited client participation&lt;/li&gt;&lt;li&gt;Validated across multiple datasets and aggregation protocols&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Maozhen Zhang', 'Mengnan Zhao', 'Wei Wang', 'Bo Wang']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial prompting', 'data poisoning', 'privacy attacks', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2508.08040</guid><pubDate>Tue, 09 Sep 2025 00:00:00 +0000</pubDate></item><item><title>Test It Before You Trust It: Applying Software Testing for Trustworthy In-context Learning</title><link>https://arxiv.org/abs/2504.18827</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces MMT4NL framework for evaluating trustworthiness of in-context learning in LLMs&lt;/li&gt;&lt;li&gt;Utilizes metamorphic adversarial examples to test prompt robustness&lt;/li&gt;&lt;li&gt;Applies software testing principles to identify linguistic bugs in ICL prompts&lt;/li&gt;&lt;li&gt;Demonstrated on sentiment analysis and question-answering tasks&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Teeradaj Racharak', 'Chaiyong Ragkhitwetsagul', 'Chommakorn Sontesadisai', 'Thanwadee Sunetnanta']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial prompting', 'safety evaluation', 'robustness', 'in-context learning', 'red teaming']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2504.18827</guid><pubDate>Tue, 09 Sep 2025 00:00:00 +0000</pubDate></item><item><title>Meta-Policy Reflexion: Reusable Reflective Memory and Rule Admissibility for Resource-Efficient LLM Agent</title><link>https://arxiv.org/abs/2509.03990</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces Meta-Policy Reflexion (MPR) framework for LLM agents&lt;/li&gt;&lt;li&gt;Uses reflective memory and rule admissibility checks to reduce unsafe actions&lt;/li&gt;&lt;li&gt;Validates improved robustness and stability in text-based agent environment&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Chunlong Wu', 'Ye Luo', 'Zhibo Qu', 'Min Wang']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'robustness', 'safety evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.03990</guid><pubDate>Tue, 09 Sep 2025 00:00:00 +0000</pubDate></item><item><title>Oyster-I: Beyond Refusal -- Constructive Safety Alignment for Responsible Language Models</title><link>https://arxiv.org/abs/2509.01909</link><description>&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Other&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Ranjie Duan', 'Jiexi Liu', 'Xiaojun Jia', 'Shiji Zhao', 'Ruoxi Cheng', 'Fengxiang Wang', 'Cheng Wei', 'Yong Xie', 'Chang Liu', 'Defeng Li', 'Yinpeng Dong', 'Yichi Zhang', 'Yuefeng Chen', 'Chongwen Wang', 'Xingjun Ma', 'Xingxing Wei', 'Yang Liu', 'Hang Su', 'Jun Zhu', 'Xinfeng Li', 'Yitong Sun', 'Jie Zhang', 'Jinzhao Hu', 'Sha Xu', 'Yitong Yang', 'Jialing Tao', 'Hui Xue']&lt;/li&gt;&lt;li&gt;Tags: []&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.01909</guid><pubDate>Tue, 09 Sep 2025 00:00:00 +0000</pubDate></item><item><title>Evo-MARL: Co-Evolutionary Multi-Agent Reinforcement Learning for Internalized Safety</title><link>https://arxiv.org/abs/2508.03864</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes Evo-MARL framework for internalizing safety in multi-agent systems&lt;/li&gt;&lt;li&gt;Uses co-evolutionary MARL to train both attackers and defenders&lt;/li&gt;&lt;li&gt;Reduces attack success rates by 22% while improving task accuracy by 5%&lt;/li&gt;&lt;li&gt;Addresses jailbreak and adversarial attack risks in multi-agent LLM systems&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Zhenyu Pan', 'Yiting Zhang', 'Yutong Zhang', 'Jianshu Zhang', 'Haozheng Luo', 'Yuwei Han', 'Dennis Wu', 'Hong-Yu Chen', 'Philip S. Yu', 'Manling Li', 'Han Liu']&lt;/li&gt;&lt;li&gt;Tags: ['red teaming', 'adversarial training', 'multi-agent safety', 'co-evolution', 'reinforcement learning']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2508.03864</guid><pubDate>Tue, 09 Sep 2025 00:00:00 +0000</pubDate></item><item><title>OpenDeception: Benchmarking and Investigating AI Deceptive Behaviors via Open-ended Interaction Simulation</title><link>https://arxiv.org/abs/2504.13707</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduced OpenDeception framework for evaluating AI deception risks&lt;/li&gt;&lt;li&gt;Evaluated deception intention and capabilities in 11 LLMs via simulated interactions&lt;/li&gt;&lt;li&gt;Found high deception intention (&gt;80%) and success rates (&gt;50%) across models&lt;/li&gt;&lt;li&gt;Observed correlation between model capability and deception risk&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yichen Wu', 'Xudong Pan', 'Geng Hong', 'Min Yang']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'safety evaluation', 'benchmarking', 'deception', 'LLM']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2504.13707</guid><pubDate>Tue, 09 Sep 2025 00:00:00 +0000</pubDate></item><item><title>From Noise to Narrative: Tracing the Origins of Hallucinations in Transformers</title><link>https://arxiv.org/abs/2509.06938</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Investigates the origins of hallucinations in transformer models using sparse autoencoders.&lt;/li&gt;&lt;li&gt;Shows that hallucinations increase with input uncertainty and can be predicted from internal activations.&lt;/li&gt;&lt;li&gt;Highlights implications for AI safety, alignment, and potential adversarial attacks.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Praneet Suresh', 'Jack Stanley', 'Sonia Joseph', 'Luca Scimeca', 'Danilo Bzdok']&lt;/li&gt;&lt;li&gt;Tags: ['hallucinations', 'transformers', 'safety', 'alignment', 'adversarial attacks', 'internal activations']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.06938</guid><pubDate>Tue, 09 Sep 2025 00:00:00 +0000</pubDate></item><item><title>Neuro-Symbolic AI for Cybersecurity: State of the Art, Challenges, and Opportunities</title><link>https://arxiv.org/abs/2509.06921</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Survey of Neuro-Symbolic AI in cybersecurity across 127 publications&lt;/li&gt;&lt;li&gt;Introduces Grounding-Instructibility-Alignment (G-I-A) framework&lt;/li&gt;&lt;li&gt;Highlights dual-use implications for offensive capabilities&lt;/li&gt;&lt;li&gt;Emphasizes need for standardization and responsible development&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Survey&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Safayat Bin Hakim', 'Muhammad Adil', 'Alvaro Velasquez', 'Shouhuai Xu', 'Houbing Herbert Song']&lt;/li&gt;&lt;li&gt;Tags: ['cybersecurity', 'neuro-symbolic AI', 'alignment', 'security', 'red teaming']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.06921</guid><pubDate>Tue, 09 Sep 2025 00:00:00 +0000</pubDate></item><item><title>Mask-GCG: Are All Tokens in Adversarial Suffixes Necessary for Jailbreak Attacks?</title><link>https://arxiv.org/abs/2509.06350</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces Mask-GCG, a method to optimize jailbreak attacks on LLMs by pruning redundant tokens in adversarial suffixes&lt;/li&gt;&lt;li&gt;Uses learnable token masking to identify high-impact positions&lt;/li&gt;&lt;li&gt;Reduces computational overhead while maintaining attack success rates&lt;/li&gt;&lt;li&gt;Provides insights into token redundancy in LLM prompts&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Junjie Mu', 'Zonghao Ying', 'Zhekui Fan', 'Zonglei Jing', 'Yaoyuan Zhang', 'Zhengmin Yu', 'Wenxin Zhang', 'Quanchen Zou', 'Xiangzheng Zhang']&lt;/li&gt;&lt;li&gt;Tags: ['jailbreaking', 'adversarial_prompting', 'red_team', 'llm_security']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.06350</guid><pubDate>Tue, 09 Sep 2025 00:00:00 +0000</pubDate></item><item><title>Empirical Study of Code Large Language Models for Binary Security Patch Detection</title><link>https://arxiv.org/abs/2509.06052</link><description>&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Other&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Qingyuan Li', 'Binchang Li', 'Cuiyun Gao', 'Shuzheng Gao', 'Zongjie Li']&lt;/li&gt;&lt;li&gt;Tags: []&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.06052</guid><pubDate>Tue, 09 Sep 2025 00:00:00 +0000</pubDate></item><item><title>DCMI: A Differential Calibration Membership Inference Attack Against Retrieval-Augmented Generation</title><link>https://arxiv.org/abs/2509.06026</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes DCMI, a differential calibration membership inference attack against RAG systems&lt;/li&gt;&lt;li&gt;Leverages sensitivity gap between member and non-member documents under query perturbation&lt;/li&gt;&lt;li&gt;Achieves high AUC and accuracy in experiments against RAG models and real-world platforms&lt;/li&gt;&lt;li&gt;Highlights significant privacy risks in RAG systems&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Xinyu Gao', 'Xiangtao Meng', 'Yingkai Dong', 'Zheng Li', 'Shanqing Guo']&lt;/li&gt;&lt;li&gt;Tags: ['membership inference', 'privacy attack', 'RAG', 'data leakage', 'adversarial attacks']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.06026</guid><pubDate>Tue, 09 Sep 2025 00:00:00 +0000</pubDate></item><item><title>Multimodal Prompt Injection Attacks: Risks and Defenses for Modern LLMs</title><link>https://arxiv.org/abs/2509.05883</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Evaluates LLM vulnerabilities to multimodal prompt injection attacks&lt;/li&gt;&lt;li&gt;Tests 8 commercial models without sanitization&lt;/li&gt;&lt;li&gt;Identifies four attack categories: direct, indirect, image-based, leakage&lt;/li&gt;&lt;li&gt;Finds Claude 3 most robust but still needing additional defenses&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Andrew Yeo', 'Daeseon Choi']&lt;/li&gt;&lt;li&gt;Tags: ['prompt injection', 'jailbreaking', 'red teaming', 'multimodal', 'security evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.05883</guid><pubDate>Tue, 09 Sep 2025 00:00:00 +0000</pubDate></item><item><title>Decoding Latent Attack Surfaces in LLMs: Prompt Injection via HTML in Web Summarization</title><link>https://arxiv.org/abs/2509.05831</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Explores HTML-based prompt injection attacks on LLMs using non-visible elements&lt;/li&gt;&lt;li&gt;Created a dataset of 280 web pages with adversarial injections&lt;/li&gt;&lt;li&gt;Evaluated Llama 4 Scout and Gemma 9B IT models on summarization impact&lt;/li&gt;&lt;li&gt;Found significant vulnerability with 29% and 15% success rates respectively&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Ishaan Verma']&lt;/li&gt;&lt;li&gt;Tags: ['prompt injection', 'adversarial prompting', 'red teaming', 'web content processing', 'LLM security']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.05831</guid><pubDate>Tue, 09 Sep 2025 00:00:00 +0000</pubDate></item><item><title>Exploit Tool Invocation Prompt for Tool Behavior Hijacking in LLM-Based Agentic System</title><link>https://arxiv.org/abs/2509.05755</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Investigates security risks in Tool Invocation Prompts (TIPs) for LLM-based agentic systems&lt;/li&gt;&lt;li&gt;Demonstrates tool behavior hijacking attacks via manipulated TIPs&lt;/li&gt;&lt;li&gt;Proposes defense mechanisms to enhance TIP security&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yu Liu', 'Yuchong Xie', 'Mingyu Luo', 'Zesen Liu', 'Zhixiang Zhang', 'Kaikai Zhang', 'Zongjie Li', 'Ping Chen', 'Shuai Wang', 'Dongdong She']&lt;/li&gt;&lt;li&gt;Tags: ['red teaming', 'LLM security', 'tool hijacking', 'adversarial prompting', 'TIP']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.05755</guid><pubDate>Tue, 09 Sep 2025 00:00:00 +0000</pubDate></item><item><title>Reasoning Introduces New Poisoning Attacks Yet Makes Them More Complicated</title><link>https://arxiv.org/abs/2509.05739</link><description>&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Other&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Hanna Foerster', 'Ilia Shumailov', 'Yiren Zhao', 'Harsh Chaudhari', 'Jamie Hayes', 'Robert Mullins', 'Yarin Gal']&lt;/li&gt;&lt;li&gt;Tags: []&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.05739</guid><pubDate>Tue, 09 Sep 2025 00:00:00 +0000</pubDate></item><item><title>Cross-Service Threat Intelligence in LLM Services using Privacy-Preserving Fingerprints</title><link>https://arxiv.org/abs/2509.05608</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces BinaryShield for privacy-preserving threat intelligence sharing in LLM services&lt;/li&gt;&lt;li&gt;Addresses prompt injection attacks across compliance boundaries&lt;/li&gt;&lt;li&gt;Uses PII redaction, semantic embedding, binary quantization, and randomized response&lt;/li&gt;&lt;li&gt;Shows high accuracy and performance improvements over baseline&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Waris Gill', 'Natalie Isak', 'Matthew Dressman']&lt;/li&gt;&lt;li&gt;Tags: ['prompt injection', 'privacy-preserving', 'threat intelligence', 'LLM security', 'adversarial attacks']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.05608</guid><pubDate>Tue, 09 Sep 2025 00:00:00 +0000</pubDate></item><item><title>Behind the Mask: Benchmarking Camouflaged Jailbreaks in Large Language Models</title><link>https://arxiv.org/abs/2509.05471</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces Camouflaged Jailbreak Prompts benchmark dataset with 500 examples&lt;/li&gt;&lt;li&gt;Proposes multi-faceted evaluation framework with 7 safety dimensions&lt;/li&gt;&lt;li&gt;Highlights significant LLM safety decline under camouflaged attacks&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Youjia Zheng', 'Mohammad Zandsalimy', 'Shanu Sushmita']&lt;/li&gt;&lt;li&gt;Tags: ['jailbreaking', 'adversarial prompting', 'red teaming', 'LLM security', 'benchmarking']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.05471</guid><pubDate>Tue, 09 Sep 2025 00:00:00 +0000</pubDate></item><item><title>Neural Breadcrumbs: Membership Inference Attacks on LLMs Through Hidden State and Attention Pattern Analysis</title><link>https://arxiv.org/abs/2509.05449</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces memTrace framework for membership inference attacks on LLMs using hidden states and attention patterns&lt;/li&gt;&lt;li&gt;Achieves average AUC scores of 0.85 on MIA benchmarks&lt;/li&gt;&lt;li&gt;Highlights the need for better privacy-preserving training techniques&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Disha Makhija', 'Manoj Ghuhan Arivazhagan', 'Vinayshekhar Bannihatti Kumar', 'Rashmi Gangadharaiah']&lt;/li&gt;&lt;li&gt;Tags: ['privacy attacks', 'membership inference', 'LLM security', 'internal states', 'attention patterns']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.05449</guid><pubDate>Tue, 09 Sep 2025 00:00:00 +0000</pubDate></item><item><title>Between a Rock and a Hard Place: Exploiting Ethical Reasoning to Jailbreak LLMs</title><link>https://arxiv.org/abs/2509.05367</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces TRIAL framework for multi-turn jailbreaking using ethical dilemmas&lt;/li&gt;&lt;li&gt;Demonstrates high success rates against both open and closed-source LLMs&lt;/li&gt;&lt;li&gt;Highlights limitations in current safety alignment strategies&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Shei Pern Chua', 'Thai Zhen Leng', 'Teh Kai Jun', 'Xiao Li', 'Xiaolin Hu']&lt;/li&gt;&lt;li&gt;Tags: ['jailbreaking', 'adversarial_prompting', 'safety_evaluation', 'ethical_reasoning', 'multi_turn_attacks']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.05367</guid><pubDate>Tue, 09 Sep 2025 00:00:00 +0000</pubDate></item><item><title>Backdoor Samples Detection Based on Perturbation Discrepancy Consistency in Pre-trained Language Models</title><link>https://arxiv.org/abs/2509.05318</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes NETE method for detecting backdoor samples in LLMs&lt;/li&gt;&lt;li&gt;Uses perturbation discrepancy consistency and curvature measurement&lt;/li&gt;&lt;li&gt;Works in both pre-training and post-training phases&lt;/li&gt;&lt;li&gt;Requires only off-the-shelf model and mask-filling strategy&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Zuquan Peng', 'Jianming Fu', 'Lixin Zou', 'Li Zheng', 'Yanzhen Ren', 'Guojun Peng']&lt;/li&gt;&lt;li&gt;Tags: ['data poisoning', 'backdoor detection', 'security', 'LLM', 'adversarial attacks']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.05318</guid><pubDate>Tue, 09 Sep 2025 00:00:00 +0000</pubDate></item><item><title>Murphys Laws of AI Alignment: Why the Gap Always Wins</title><link>https://arxiv.org/abs/2509.05381</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces the Alignment Gap concept using KL-tilting formalism to explain recurring alignment failures&lt;/li&gt;&lt;li&gt;Catalogues failures into Murphy's Laws of AI Alignment&lt;/li&gt;&lt;li&gt;Proposes the Alignment Trilemma to frame trade-offs in alignment methods&lt;/li&gt;&lt;li&gt;Introduces MAPS framework (Misspecification, Annotation, Pressure, Shift) as design levers&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Madhava Gaikwad']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'safety', 'RLHF', 'DPO', "Murphy's Laws", 'structural limits']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.05381</guid><pubDate>Tue, 09 Sep 2025 00:00:00 +0000</pubDate></item></channel></rss>