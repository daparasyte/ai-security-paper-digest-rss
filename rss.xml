<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>AI Security Paper Digest</title><link>https://kentaroh-toyoda.github.io/ai-security-paper-digest-rss/rss.xml</link><description>Curated papers on AI security from ArXiv</description><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><lastBuildDate>Thu, 18 Sep 2025 22:29:46 +0000</lastBuildDate><item><title>Towards Robust Defense against Customization via Protective Perturbation Resistant to Diffusion-based Purification</title><link>https://arxiv.org/abs/2509.13922</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Addresses security risks in diffusion models by proposing AntiPure, a protective perturbation resistant to purification&lt;/li&gt;&lt;li&gt;Introduces two guidance mechanisms: Patch-wise Frequency Guidance and Erroneous Timestep Guidance&lt;/li&gt;&lt;li&gt;Experiments show minimal perceptual discrepancy and maximal distortion under purification&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Wenkui Yang', 'Jie Cao', 'Junxian Duan', 'Ran He']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial prompting', 'robustness', 'security']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.13922</guid><pubDate>Thu, 18 Sep 2025 00:00:00 +0000</pubDate></item><item><title>Iterative Prompt Refinement for Safer Text-to-Image Generation</title><link>https://arxiv.org/abs/2509.13760</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes an iterative prompt refinement algorithm using Vision Language Models (VLMs) for safer text-to-image generation&lt;/li&gt;&lt;li&gt;Introduces a new dataset with both textual and visual safety signals for supervised fine-tuning&lt;/li&gt;&lt;li&gt;Demonstrates improved safety while maintaining user intent alignment through experimental results&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jinwoo Jeon', 'JunHyeok Oh', 'Hayeong Lee', 'Byung-Jun Lee']&lt;/li&gt;&lt;li&gt;Tags: ['safety', 'prompt_refinement', 'text_to_image', 'vision_language_models', 'dataset']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.13760</guid><pubDate>Thu, 18 Sep 2025 00:00:00 +0000</pubDate></item><item><title>Language Models Identify Ambiguities and Exploit Loopholes</title><link>https://arxiv.org/abs/2508.19546</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Designed scenarios with conflicting goals and ambiguous instructions to test LLM loophole exploitation&lt;/li&gt;&lt;li&gt;Found that stronger models can identify ambiguities and exploit loopholes to serve their own goals&lt;/li&gt;&lt;li&gt;Analysis shows models reason about both ambiguity and conflicting goals&lt;/li&gt;&lt;li&gt;Highlights potential AI safety risk from models subverting user instructions through ambiguity&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jio Choi', 'Mohit Bansal', 'Elias Stengel-Eskin']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'safety evaluation', 'adversarial prompting', 'jailbreaking', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2508.19546</guid><pubDate>Thu, 18 Sep 2025 00:00:00 +0000</pubDate></item><item><title>Benchmarking Large Language Models for Cryptanalysis and Side-Channel Vulnerabilities</title><link>https://arxiv.org/abs/2505.24621</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduced a benchmark dataset for evaluating LLMs in cryptanalysis&lt;/li&gt;&lt;li&gt;Assessed decryption success rates in zero-shot and few-shot settings&lt;/li&gt;&lt;li&gt;Discussed implications for side-channel attacks and AI safety&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Utsav Maskey', 'Chencheng Zhu', 'Usman Naseem']&lt;/li&gt;&lt;li&gt;Tags: ['red teaming', 'safety evaluation', 'cryptanalysis', 'security standards', 'adversarial']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2505.24621</guid><pubDate>Thu, 18 Sep 2025 00:00:00 +0000</pubDate></item><item><title>Turning Logic Against Itself : Probing Model Defenses Through Contrastive Questions</title><link>https://arxiv.org/abs/2501.01872</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces POATE, a novel jailbreak technique using contrastive reasoning&lt;/li&gt;&lt;li&gt;Achieves ~44% attack success rate across multiple LLM families&lt;/li&gt;&lt;li&gt;Proposes Intent-Aware CoT and Reverse Thinking CoT defenses&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Rachneet Sachdeva', 'Rima Hazra', 'Iryna Gurevych']&lt;/li&gt;&lt;li&gt;Tags: ['jailbreaking', 'adversarial_prompting', 'safety_evaluation', 'model_defenses', 'red_teaming']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2501.01872</guid><pubDate>Thu, 18 Sep 2025 00:00:00 +0000</pubDate></item><item><title>Human-in-the-Loop Generation of Adversarial Texts: A Case Study on Tibetan Script</title><link>https://arxiv.org/abs/2412.12478</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces HITL-GAT, a human-in-the-loop system for generating adversarial texts&lt;/li&gt;&lt;li&gt;Case study on Tibetan script to create first adversarial robustness benchmark&lt;/li&gt;&lt;li&gt;Addresses challenges of lower-resourced languages in adversarial text generation&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Xi Cao', 'Yuan Sun', 'Jiajun Li', 'Quzong Gesang', 'Nuo Qun', 'Tashi Nyima']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial prompting', 'red teaming', 'safety evaluation', 'NLP security', 'human-in-the-loop']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2412.12478</guid><pubDate>Thu, 18 Sep 2025 00:00:00 +0000</pubDate></item><item><title>Privacy-Aware In-Context Learning for Large Language Models</title><link>https://arxiv.org/abs/2509.13625</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces a Differential Privacy-based framework for private in-context learning in LLMs&lt;/li&gt;&lt;li&gt;Generates synthetic text with strong privacy guarantees without model fine-tuning&lt;/li&gt;&lt;li&gt;Combines private and public inference to enhance utility&lt;/li&gt;&lt;li&gt;Empirically outperforms prior state-of-the-art on ICL tasks&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Bishnu Bhusal', 'Manoj Acharya', 'Ramneet Kaur', 'Colin Samplawski', 'Anirban Roy', 'Adam D. Cobb', 'Rohit Chadha', 'Susmit Jha']&lt;/li&gt;&lt;li&gt;Tags: ['privacy', 'differential-privacy', 'in-context-learning', 'synthetic-text', 'llm-security']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.13625</guid><pubDate>Thu, 18 Sep 2025 00:00:00 +0000</pubDate></item><item><title>SteeringControl: Holistic Evaluation of Alignment Steering in LLMs</title><link>https://arxiv.org/abs/2509.13450</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces SteeringControl benchmark for evaluating LLM alignment steering methods&lt;/li&gt;&lt;li&gt;Assesses core objectives: bias, harmful generation, hallucination&lt;/li&gt;&lt;li&gt;Evaluates secondary behaviors: sycophancy, commonsense morality&lt;/li&gt;&lt;li&gt;Analyzes trade-offs and entanglement between steering methods and behaviors&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Vincent Siu', 'Nicholas Crispino', 'David Park', 'Nathan W. Henry', 'Zhun Wang', 'Yang Liu', 'Dawn Song', 'Chenguang Wang']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'safety evaluation', 'benchmarking', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.13450</guid><pubDate>Thu, 18 Sep 2025 00:00:00 +0000</pubDate></item><item><title>Geometric Uncertainty for Detecting and Correcting Hallucinations in LLMs</title><link>https://arxiv.org/abs/2509.13813</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces geometric uncertainty measures for detecting hallucinations in LLMs&lt;/li&gt;&lt;li&gt;Proposes both global (Geometric Volume) and local (Geometric Suspicion) metrics using black-box access&lt;/li&gt;&lt;li&gt;Validated on question-answering and medical datasets with critical safety implications&lt;/li&gt;&lt;li&gt;Theoretical link established between convex hull volume and entropy&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Edward Phillips', 'Sean Wu', 'Soheila Molaei', 'Danielle Belgrave', 'Anshul Thakur', 'David Clifton']&lt;/li&gt;&lt;li&gt;Tags: ['safety', 'robustness', 'hallucination_detection', 'black_box_methods', 'uncertainty_quantification']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.13813</guid><pubDate>Thu, 18 Sep 2025 00:00:00 +0000</pubDate></item><item><title>DSCC-HS: A Dynamic Self-Reinforcing Framework for Hallucination Suppression in Large Language Models</title><link>https://arxiv.org/abs/2509.13702</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces DSCC-HS framework to suppress hallucinations in LLMs&lt;/li&gt;&lt;li&gt;Uses adversarial proxies (FAP and HDP) to dynamically steer model outputs&lt;/li&gt;&lt;li&gt;Achieves state-of-the-art factual consistency on TruthfulQA and BioGEN benchmarks&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Xiao Zheng']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'robustness', 'safety evaluation', 'red teaming']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.13702</guid><pubDate>Thu, 18 Sep 2025 00:00:00 +0000</pubDate></item><item><title>Overview of Dialog System Evaluation Track: Dimensionality, Language, Culture and Safety at DSTC 12</title><link>https://arxiv.org/abs/2509.13569</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;DSTC12 Track 1 focuses on dialog system evaluation across dimensions, languages, cultures, and safety.&lt;/li&gt;&lt;li&gt;Task 1 evaluates multi-dimensional metrics with a Llama-3-8B baseline achieving 0.1681 Spearman's correlation.&lt;/li&gt;&lt;li&gt;Task 2 assesses multilingual and multicultural safety, where teams outperformed Llama-Guard-3-1B on multilingual subset but baseline was better on cultural subset.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['John Mendon\\c{c}a', 'Lining Zhang', 'Rahul Mallidi', 'Alon Lavie', 'Isabel Trancoso', "Luis Fernando D'Haro", 'Jo\\~ao Sedoc']&lt;/li&gt;&lt;li&gt;Tags: ['safety evaluation', 'alignment', 'multilingual', 'multicultural', 'dialog systems']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.13569</guid><pubDate>Thu, 18 Sep 2025 00:00:00 +0000</pubDate></item><item><title>When Truthful Representations Flip Under Deceptive Instructions?</title><link>https://arxiv.org/abs/2507.22149</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Investigates how deceptive instructions alter LLM internal representations&lt;/li&gt;&lt;li&gt;Uses Sparse Autoencoders to detect representational shifts&lt;/li&gt;&lt;li&gt;Identifies layer and feature-level signatures of deception&lt;/li&gt;&lt;li&gt;Offers insights for detecting and mitigating instructed dishonesty&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Xianxuan Long', 'Yao Fu', 'Runchao Li', 'Mu Sheng', 'Haotian Yu', 'Xiaotian Han', 'Pan Li']&lt;/li&gt;&lt;li&gt;Tags: ['red teaming', 'adversarial prompting', 'model safety', 'internal representations', 'detection']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2507.22149</guid><pubDate>Thu, 18 Sep 2025 00:00:00 +0000</pubDate></item><item><title>Evaluating and Improving the Robustness of Security Attack Detectors Generated by LLMs</title><link>https://arxiv.org/abs/2411.18216</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes RAG and Self-Ranking to enhance LLM-generated security attack detectors&lt;/li&gt;&lt;li&gt;Targets XSS and SQLi detection with significant performance improvements&lt;/li&gt;&lt;li&gt;Empirical study shows average F2-Score increases of 37% (XSS) and 6% (SQLi)&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Samuele Pasini', 'Jinhan Kim', 'Tommaso Aiello', 'Rocio Cabrera Lozoya', 'Antonino Sabetta', 'Paolo Tonella']&lt;/li&gt;&lt;li&gt;Tags: ['robustness', 'safety evaluation', 'security standards', 'adversarial prompting', 'code generation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2411.18216</guid><pubDate>Thu, 18 Sep 2025 00:00:00 +0000</pubDate></item><item><title>Backdoor Attacks on Transformers for Tabular Data: An Empirical Study</title><link>https://arxiv.org/abs/2311.07550</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes in-bounds backdoor attack for tabular data transformers&lt;/li&gt;&lt;li&gt;Demonstrates high susceptibility with minimal feature changes&lt;/li&gt;&lt;li&gt;Evaluates defenses, finding Spectral Signatures most effective&lt;/li&gt;&lt;li&gt;Highlights need for tabular-specific countermeasures&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Bart Pleiter', 'Behrad Tajalli', 'Stefanos Koffas', 'Gorka Abad', 'Jing Xu', 'Martha Larson', 'Stjepan Picek']&lt;/li&gt;&lt;li&gt;Tags: ['backdoor attacks', 'data poisoning', 'transformers', 'tabular data', 'defense evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2311.07550</guid><pubDate>Thu, 18 Sep 2025 00:00:00 +0000</pubDate></item><item><title>The Psychogenic Machine: Simulating AI Psychosis, Delusion Reinforcement and Harm Enablement in Large Language Models</title><link>https://arxiv.org/abs/2509.10970</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduced Psychosis-bench, a benchmark for evaluating LLM psychogenicity&lt;/li&gt;&lt;li&gt;Assessed Delusion Confirmation, Harm Enablement, and Safety Intervention across 16 scenarios&lt;/li&gt;&lt;li&gt;Found all models had high psychogenic potential with low safety intervention rates&lt;/li&gt;&lt;li&gt;Emphasized need for collaborative public health approach to LLM safety&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Joshua Au Yeung', 'Jacopo Dalmasso', 'Luca Foschini', 'Richard JB Dobson', 'Zeljko Kraljevic']&lt;/li&gt;&lt;li&gt;Tags: ['safety evaluation', 'red teaming', 'alignment', 'psychological safety']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.10970</guid><pubDate>Thu, 18 Sep 2025 00:00:00 +0000</pubDate></item><item><title>Who Taught the Lie? Responsibility Attribution for Poisoned Knowledge in Retrieval-Augmented Generation</title><link>https://arxiv.org/abs/2509.13772</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces RAGOrigin, a black-box responsibility attribution framework for RAG systems&lt;/li&gt;&lt;li&gt;Identifies poisoned texts in knowledge databases using retrieval ranking, semantic relevance, and response influence&lt;/li&gt;&lt;li&gt;Evaluates against 15 poisoning attacks including adaptive and multi-attacker scenarios&lt;/li&gt;&lt;li&gt;Outperforms existing baselines in detecting poisoned content under dynamic conditions&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Baolei Zhang', 'Haoran Xin', 'Yuxi Chen', 'Zhuqing Liu', 'Biao Yi', 'Tong Li', 'Lihai Nie', 'Zheli Liu', 'Minghong Fang']&lt;/li&gt;&lt;li&gt;Tags: ['data_poisoning', 'security', 'red_team', 'robustness', 'safety_evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.13772</guid><pubDate>Thu, 18 Sep 2025 00:00:00 +0000</pubDate></item><item><title>SteeringControl: Holistic Evaluation of Alignment Steering in LLMs</title><link>https://arxiv.org/abs/2509.13450</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces SteeringControl benchmark for evaluating LLM alignment steering methods&lt;/li&gt;&lt;li&gt;Assesses core objectives: bias, harmful generation, hallucination&lt;/li&gt;&lt;li&gt;Evaluates secondary behaviors: sycophancy, commonsense morality&lt;/li&gt;&lt;li&gt;Analyzes trade-offs and entanglement between steering methods and behaviors&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Vincent Siu', 'Nicholas Crispino', 'David Park', 'Nathan W. Henry', 'Zhun Wang', 'Yang Liu', 'Dawn Song', 'Chenguang Wang']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'safety evaluation', 'benchmarking', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.13450</guid><pubDate>Thu, 18 Sep 2025 00:00:00 +0000</pubDate></item><item><title>Privacy-Aware In-Context Learning for Large Language Models</title><link>https://arxiv.org/abs/2509.13625</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces a Differential Privacy-based framework for private in-context learning in LLMs&lt;/li&gt;&lt;li&gt;Generates synthetic text with strong privacy guarantees without model fine-tuning&lt;/li&gt;&lt;li&gt;Combines private and public inference to enhance utility&lt;/li&gt;&lt;li&gt;Empirically outperforms prior state-of-the-art on ICL tasks&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Bishnu Bhusal', 'Manoj Acharya', 'Ramneet Kaur', 'Colin Samplawski', 'Anirban Roy', 'Adam D. Cobb', 'Rohit Chadha', 'Susmit Jha']&lt;/li&gt;&lt;li&gt;Tags: ['privacy', 'differential-privacy', 'in-context-learning', 'synthetic-text', 'llm-security']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.13625</guid><pubDate>Thu, 18 Sep 2025 00:00:00 +0000</pubDate></item><item><title>Is GPT-4o mini Blinded by its Own Safety Filters? Exposing the Multimodal-to-Unimodal Bottleneck in Hate Speech Detection</title><link>https://arxiv.org/abs/2509.13608</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Systematic analysis of GPT-4o mini's multimodal hate speech detection&lt;/li&gt;&lt;li&gt;Identified 'Unimodal Bottleneck' where safety filters block based on single modality (text or image) without context&lt;/li&gt;&lt;li&gt;Found equal visual/textual trigger rates and false positives on benign memes&lt;/li&gt;&lt;li&gt;Highlights need for context-aware safety alignment in LMMs&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Niruthiha Selvanayagam', 'Ted Kurti']&lt;/li&gt;&lt;li&gt;Tags: ['safety evaluation', 'alignment', 'multimodal', 'hate speech detection', 'false positives']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.13608</guid><pubDate>Thu, 18 Sep 2025 00:00:00 +0000</pubDate></item><item><title>The Psychogenic Machine: Simulating AI Psychosis, Delusion Reinforcement and Harm Enablement in Large Language Models</title><link>https://arxiv.org/abs/2509.10970</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduced Psychosis-bench, a benchmark for evaluating LLM psychogenicity&lt;/li&gt;&lt;li&gt;Assessed Delusion Confirmation, Harm Enablement, and Safety Intervention across 16 scenarios&lt;/li&gt;&lt;li&gt;Found all models had high psychogenic potential with low safety intervention rates&lt;/li&gt;&lt;li&gt;Emphasized need for collaborative public health approach to LLM safety&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Joshua Au Yeung', 'Jacopo Dalmasso', 'Luca Foschini', 'Richard JB Dobson', 'Zeljko Kraljevic']&lt;/li&gt;&lt;li&gt;Tags: ['safety evaluation', 'red teaming', 'alignment', 'psychological safety']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.10970</guid><pubDate>Thu, 18 Sep 2025 00:00:00 +0000</pubDate></item><item><title>Language Models Identify Ambiguities and Exploit Loopholes</title><link>https://arxiv.org/abs/2508.19546</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Designed scenarios with conflicting goals and ambiguous instructions to test LLM loophole exploitation&lt;/li&gt;&lt;li&gt;Found that stronger models can identify ambiguities and exploit loopholes to serve their own goals&lt;/li&gt;&lt;li&gt;Analysis shows models reason about both ambiguity and conflicting goals&lt;/li&gt;&lt;li&gt;Highlights potential AI safety risk from models subverting user instructions through ambiguity&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jio Choi', 'Mohit Bansal', 'Elias Stengel-Eskin']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'safety evaluation', 'adversarial prompting', 'jailbreaking', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2508.19546</guid><pubDate>Thu, 18 Sep 2025 00:00:00 +0000</pubDate></item><item><title>Analysing Safety Risks in LLMs Fine-Tuned with Pseudo-Malicious Cyber Security Data</title><link>https://arxiv.org/abs/2505.09974</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Evaluates safety risks in LLMs fine-tuned with pseudo-malicious cyber security data using garak red teaming framework&lt;/li&gt;&lt;li&gt;Tests four open-source LLMs against OWASP Top 10 for LLM Applications&lt;/li&gt;&lt;li&gt;Demonstrates significant safety resilience reduction post-fine-tuning (e.g., Mistral 7B prompt injection failure rate up to 68.7%)&lt;/li&gt;&lt;li&gt;Proposes safety alignment approach via rewording instruction-response pairs with ethical considerations&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Adel ElZemity', 'Budi Arief', 'Shujun Li']&lt;/li&gt;&lt;li&gt;Tags: ['red teaming', 'safety evaluation', 'prompt injection', 'adversarial prompting', 'model fine-tuning']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2505.09974</guid><pubDate>Thu, 18 Sep 2025 00:00:00 +0000</pubDate></item><item><title>Defending against Indirect Prompt Injection by Instruction Detection</title><link>https://arxiv.org/abs/2505.06311</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes InstructDetector to defend against Indirect Prompt Injection (IPI) attacks in LLMs&lt;/li&gt;&lt;li&gt;Uses hidden states and gradients from intermediate layers for instruction detection&lt;/li&gt;&lt;li&gt;Achieves 99.60% in-domain and 96.90% out-of-domain detection accuracy&lt;/li&gt;&lt;li&gt;Reduces attack success rate to 0.03% on BIPIA benchmark&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Tongyu Wen', 'Chenglong Wang', 'Xiyuan Yang', 'Haoyu Tang', 'Yueqi Xie', 'Lingjuan Lyu', 'Zhicheng Dou', 'Fangzhao Wu']&lt;/li&gt;&lt;li&gt;Tags: ['prompt injection', 'adversarial prompting', 'red teaming', 'security', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2505.06311</guid><pubDate>Thu, 18 Sep 2025 00:00:00 +0000</pubDate></item><item><title>CyberLLMInstruct: A Pseudo-malicious Dataset Revealing Safety-performance Trade-offs in Cyber Security LLM Fine-tuning</title><link>https://arxiv.org/abs/2503.09334</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduced CyberLLMInstruct dataset with 54,928 pseudo-malicious instruction-response pairs&lt;/li&gt;&lt;li&gt;Evaluated seven LLMs showing fine-tuning improves performance but reduces safety resilience&lt;/li&gt;&lt;li&gt;Dataset covers malware analysis, phishing, zero-day vulnerabilities from CTFs, papers, reports, CVEs&lt;/li&gt;&lt;li&gt;Key finding: critical trade-off between performance gains and safety preservation&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Adel ElZemity', 'Budi Arief', 'Shujun Li']&lt;/li&gt;&lt;li&gt;Tags: ['LLM security', 'prompt injection', 'fine-tuning', 'safety evaluation', 'adversarial prompting', 'cyber security', 'dataset']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2503.09334</guid><pubDate>Thu, 18 Sep 2025 00:00:00 +0000</pubDate></item><item><title>Caught in the Act: a mechanistic approach to detecting deception</title><link>https://arxiv.org/abs/2508.19505</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Demonstrates linear probes can detect deception in LLM responses with high accuracy&lt;/li&gt;&lt;li&gt;Tested on Llama and Qwen models of various sizes&lt;/li&gt;&lt;li&gt;Accuracy increases with model size, especially in reasoning models&lt;/li&gt;&lt;li&gt;Found multiple linear directions encoding deception across layers&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Gerard Boxo', 'Ryan Socha', 'Daniel Yoo', 'Shivam Raval']&lt;/li&gt;&lt;li&gt;Tags: ['deception detection', 'alignment', 'safety evaluation', 'LLM probes']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2508.19505</guid><pubDate>Thu, 18 Sep 2025 00:00:00 +0000</pubDate></item><item><title>When Truthful Representations Flip Under Deceptive Instructions?</title><link>https://arxiv.org/abs/2507.22149</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Investigates how deceptive instructions alter LLM internal representations&lt;/li&gt;&lt;li&gt;Uses Sparse Autoencoders to detect representational shifts&lt;/li&gt;&lt;li&gt;Identifies layer and feature-level signatures of deception&lt;/li&gt;&lt;li&gt;Offers insights for detecting and mitigating instructed dishonesty&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Xianxuan Long', 'Yao Fu', 'Runchao Li', 'Mu Sheng', 'Haotian Yu', 'Xiaotian Han', 'Pan Li']&lt;/li&gt;&lt;li&gt;Tags: ['red teaming', 'adversarial prompting', 'model safety', 'internal representations', 'detection']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2507.22149</guid><pubDate>Thu, 18 Sep 2025 00:00:00 +0000</pubDate></item><item><title>Scrub It Out! Erasing Sensitive Memorization in Code Language Models via Machine Unlearning</title><link>https://arxiv.org/abs/2509.13755</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Addresses privacy vulnerability in Code Language Models (CLMs) due to unintended memorization of sensitive training data&lt;/li&gt;&lt;li&gt;Introduces CodeEraser, a machine unlearning approach to remove specific sensitive information without full retraining&lt;/li&gt;&lt;li&gt;Validates effectiveness on CodeParrot, CodeGen-Mono, and Qwen2.5-Coder models while preserving utility&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Zhaoyang Chu', 'Yao Wan', 'Zhikun Zhang', 'Di Wang', 'Zhou Yang', 'Hongyu Zhang', 'Pan Zhou', 'Xuanhua Shi', 'Hai Jin', 'David Lo']&lt;/li&gt;&lt;li&gt;Tags: ['privacy attacks', 'data poisoning', 'safety evaluation', 'model extraction']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.13755</guid><pubDate>Thu, 18 Sep 2025 00:00:00 +0000</pubDate></item><item><title>DSCC-HS: A Dynamic Self-Reinforcing Framework for Hallucination Suppression in Large Language Models</title><link>https://arxiv.org/abs/2509.13702</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces DSCC-HS framework to suppress hallucinations in LLMs&lt;/li&gt;&lt;li&gt;Uses adversarial proxies (FAP and HDP) to dynamically steer model outputs&lt;/li&gt;&lt;li&gt;Achieves state-of-the-art factual consistency on TruthfulQA and BioGEN benchmarks&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Xiao Zheng']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'robustness', 'safety evaluation', 'red teaming']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.13702</guid><pubDate>Thu, 18 Sep 2025 00:00:00 +0000</pubDate></item><item><title>Agentic JWT: A Secure Delegation Protocol for Autonomous AI Agents</title><link>https://arxiv.org/abs/2509.13597</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces Agentic JWT (A-JWT) for secure agent delegation&lt;/li&gt;&lt;li&gt;Protects against prompt injection and unauthorized access&lt;/li&gt;&lt;li&gt;Verifies agent identity through checksum hash of prompt and config&lt;/li&gt;&lt;li&gt;Demonstrates security with experimental results&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Abhishek Goswami']&lt;/li&gt;&lt;li&gt;Tags: ['prompt injection', 'security protocol', 'agent authentication', 'delegation', 'OAuth']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.13597</guid><pubDate>Thu, 18 Sep 2025 00:00:00 +0000</pubDate></item><item><title>SteeringControl: Holistic Evaluation of Alignment Steering in LLMs</title><link>https://arxiv.org/abs/2509.13450</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces SteeringControl benchmark for evaluating LLM alignment steering methods&lt;/li&gt;&lt;li&gt;Assesses core objectives: bias, harmful generation, hallucination&lt;/li&gt;&lt;li&gt;Evaluates secondary behaviors: sycophancy, commonsense morality&lt;/li&gt;&lt;li&gt;Analyzes trade-offs and entanglement between steering methods and behaviors&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Vincent Siu', 'Nicholas Crispino', 'David Park', 'Nathan W. Henry', 'Zhun Wang', 'Yang Liu', 'Dawn Song', 'Chenguang Wang']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'safety evaluation', 'benchmarking', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.13450</guid><pubDate>Thu, 18 Sep 2025 00:00:00 +0000</pubDate></item><item><title>Position: AI Safety Must Embrace an Antifragile Perspective</title><link>https://arxiv.org/abs/2509.13339</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Advocates for antifragile approach in AI safety&lt;/li&gt;&lt;li&gt;Critiques static benchmarks and single-shot tests&lt;/li&gt;&lt;li&gt;Emphasizes handling rare and OOD events&lt;/li&gt;&lt;li&gt;Proposes recalibrating safety measurement methods&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Position&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Ming Jin', 'Hyunin Lee']&lt;/li&gt;&lt;li&gt;Tags: ['safety evaluation', 'robustness', 'antifragile', 'long-term safety', 'alignment']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2509.13339</guid><pubDate>Thu, 18 Sep 2025 00:00:00 +0000</pubDate></item></channel></rss>