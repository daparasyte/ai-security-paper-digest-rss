<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>AI Security Paper Digest</title><link>https://kentaroh-toyoda.github.io/ai-security-paper-digest-rss/rss.xml</link><description>Curated papers on AI security from OpenAlex and ArXiv</description><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><lastBuildDate>Mon, 14 Jul 2025 22:19:49 +0000</lastBuildDate><item><title>Entangled Threats: A Unified Kill Chain Model for Quantum Machine Learning Security</title><link>https://arxiv.org/abs/2507.08623</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a kill chain model for Quantum Machine Learning (QML) security&lt;/li&gt;&lt;li&gt;Maps attack vectors to stages in the QML pipeline&lt;/li&gt;&lt;li&gt;Highlights interdependencies between physical, data, and privacy threats&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Pascal Debus', 'Maximilian Wendlinger', 'Kilian Tscharke', 'Daniel Herr', 'Cedric Br\\"ugmann', 'Daniel Ohl de Mello', 'Juris Ulmanis', 'Alexander Erhard', 'Arthur Schmidt', 'Fabian Petsch']&lt;/li&gt;&lt;li&gt;Tags: ['quantum machine learning security', 'kill chain model', 'adversarial attacks', 'model extraction', 'data poisoning']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2507.08623</guid><pubDate>Mon, 14 Jul 2025 00:00:00 +0000</pubDate></item><item><title>Admissibility of Stein Shrinkage for Batch Normalization in the Presence of Adversarial Attacks</title><link>https://arxiv.org/abs/2507.08261</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;The paper proposes using Stein shrinkage in batch normalization to improve robustness against adversarial attacks&lt;/li&gt;&lt;li&gt;Theoretical proofs show dominance over sample estimators under sub-Gaussian attack models&lt;/li&gt;&lt;li&gt;Empirical results demonstrate SOTA performance in image classification and segmentation with adversarial attacks&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Sofia Ivolgina', 'P. Thomas Fletcher', 'Baba C. Vemuri']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial attacks', 'batch normalization', 'robustness', 'Stein shrinkage', 'image classification']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2507.08261</guid><pubDate>Mon, 14 Jul 2025 00:00:00 +0000</pubDate></item><item><title>Simple Mechanistic Explanations for Out-Of-Context Reasoning</title><link>https://arxiv.org/abs/2507.08218</link><description>&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Other&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Atticus Wang', 'Joshua Engels', 'Oliver Clive-Griffin']&lt;/li&gt;&lt;li&gt;Tags: []&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2507.08218</guid><pubDate>Mon, 14 Jul 2025 00:00:00 +0000</pubDate></item><item><title>One Token to Fool LLM-as-a-Judge</title><link>https://arxiv.org/abs/2507.08794</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Demonstrates vulnerability of LLM-based reward models to simple prompt manipulations&lt;/li&gt;&lt;li&gt;Shows widespread impact across models, datasets, and prompt formats&lt;/li&gt;&lt;li&gt;Introduces data augmentation strategy to improve robustness&lt;/li&gt;&lt;li&gt;Releases robust reward model and synthetic training data&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yulai Zhao', 'Haolin Liu', 'Dian Yu', 'S. Y. Kung', 'Haitao Mi', 'Dong Yu']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial prompting', 'robustness', 'safety evaluation', 'model extraction', 'red teaming']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2507.08794</guid><pubDate>Mon, 14 Jul 2025 00:00:00 +0000</pubDate></item><item><title>Red Teaming Large Language Models for Healthcare</title><link>https://arxiv.org/abs/2505.00467</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Workshop focused on red teaming LLMs for healthcare to identify clinical harm vulnerabilities&lt;/li&gt;&lt;li&gt;Involved clinicians and computational experts in vulnerability discovery&lt;/li&gt;&lt;li&gt;Categorized vulnerabilities and replicated findings across multiple LLMs&lt;/li&gt;&lt;li&gt;Emphasizes importance of clinical expertise in red teaming LLMs&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Vahid Balazadeh', 'Michael Cooper', 'David Pellow', 'Atousa Assadi', 'Jennifer Bell', 'Mark Coatsworth', 'Kaivalya Deshpande', 'Jim Fackler', 'Gabriel Funingana', 'Spencer Gable-Cook', 'Anirudh Gangadhar', 'Abhishek Jaiswal', 'Sumanth Kaja', 'Christopher Khoury', 'Amrit Krishnan', 'Randy Lin', 'Kaden McKeen', 'Sara Naimimohasses', 'Khashayar Namdar', 'Aviraj Newatia', 'Allan Pang', 'Anshul Pattoo', 'Sameer Peesapati', 'Diana Prepelita', 'Bogdana Rakova', 'Saba Sadatamin', 'Rafael Schulman', 'Ajay Shah', 'Syed Azhar Shah', 'Syed Ahmar Shah', 'Babak Taati', 'Balagopal Unnikrishnan', 'I\\~nigo Urteaga', 'Stephanie Williams', 'Rahul G Krishnan']&lt;/li&gt;&lt;li&gt;Tags: ['red teaming', 'LLM', 'healthcare', 'adversarial prompting', 'safety evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2505.00467</guid><pubDate>Mon, 14 Jul 2025 00:00:00 +0000</pubDate></item><item><title>Lightweight Safety Guardrails via Synthetic Data and RL-guided Adversarial Training</title><link>https://arxiv.org/abs/2507.08284</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces a safety guardrail framework using synthetic data and RL-guided adversarial training&lt;/li&gt;&lt;li&gt;Employs high-fidelity synthetic data generation through query augmentation and paraphrasing&lt;/li&gt;&lt;li&gt;Uses adversarial training to enhance resilience against harmful content&lt;/li&gt;&lt;li&gt;Demonstrates small models can outperform larger ones in content moderation tasks&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Aleksei Ilin', 'Gor Matevosyan', 'Xueying Ma', 'Vladimir Eremin', 'Suhaa Dada', 'Muqun Li', 'Riyaaz Shaik', 'Haluk Noyan Tokgozoglu']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial training', 'safety', 'synthetic data', 'content moderation', 'scalability']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2507.08284</guid><pubDate>Mon, 14 Jul 2025 00:00:00 +0000</pubDate></item><item><title>Circumventing Safety Alignment in Large Language Models Through Embedding Space Toxicity Attenuation</title><link>https://arxiv.org/abs/2507.08020</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces ETTA framework for embedding space toxicity attenuation&lt;/li&gt;&lt;li&gt;Bypasses safety alignment mechanisms in LLMs&lt;/li&gt;&lt;li&gt;High attack success rates (88.61%) across multiple models&lt;/li&gt;&lt;li&gt;Highlights critical vulnerability in current alignment strategies&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Zhibo Zhang', 'Yuxi Li', 'Kailong Wang', 'Shuai Yuan', 'Ling Shi', 'Haoyu Wang']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial prompting', 'safety evaluation', 'alignment', 'robustness']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2507.08020</guid><pubDate>Mon, 14 Jul 2025 00:00:00 +0000</pubDate></item><item><title>Mass-Scale Analysis of In-the-Wild Conversations Reveals Complexity Bounds on LLM Jailbreaking</title><link>https://arxiv.org/abs/2507.08014</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Mass-scale analysis of 2 million real-world conversations across diverse platforms&lt;/li&gt;&lt;li&gt;Examines jailbreak complexity using probabilistic, lexical, compression, and cognitive load metrics&lt;/li&gt;&lt;li&gt;Finds jailbreak attempts have similar complexity to normal conversations&lt;/li&gt;&lt;li&gt;Highlights implications for AI safety and defensive mechanisms&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Aldan Creo', 'Raul Castro Fernandez', 'Manuel Cebrian']&lt;/li&gt;&lt;li&gt;Tags: ['jailbreaking', 'LLM', 'red teaming', 'security', 'safety evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2507.08014</guid><pubDate>Mon, 14 Jul 2025 00:00:00 +0000</pubDate></item><item><title>Agent Safety Alignment via Reinforcement Learning</title><link>https://arxiv.org/abs/2507.08270</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Proposes a unified safety-alignment framework for tool-using LLM agents&lt;/li&gt;&lt;li&gt;Introduces a tri-modal taxonomy for user prompts and tool responses&lt;/li&gt;&lt;li&gt;Uses sandboxed reinforcement learning for threat handling&lt;/li&gt;&lt;li&gt;Demonstrates improved security through evaluations on multiple benchmarks&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Zeyang Sha', 'Hanling Tian', 'Zhuoer Xu', 'Shiwen Cui', 'Changhua Meng', 'Weiqiang Wang']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'safety', 'reinforcement_learning', 'sandbox', 'adversarial_prompts', 'tool_usage', 'security', 'evaluation_benchmarks']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2507.08270</guid><pubDate>Mon, 14 Jul 2025 00:00:00 +0000</pubDate></item><item><title>A Dynamic Stackelberg Game Framework for Agentic AI Defense Against LLM Jailbreaking</title><link>https://arxiv.org/abs/2507.08207</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces a dynamic Stackelberg game framework for LLM jailbreaking defense&lt;/li&gt;&lt;li&gt;Proposes the Purple Agent using RRT for adversarial exploration and defense&lt;/li&gt;&lt;li&gt;Models attacker-defender interactions as a sequential game&lt;/li&gt;&lt;li&gt;Aims to prevent harmful outputs by proactive intervention&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Zhengye Han', 'Quanyan Zhu']&lt;/li&gt;&lt;li&gt;Tags: ['Stackelberg game', 'jailbreaking', 'LLM security', 'adversarial AI', 'agentic AI']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2507.08207</guid><pubDate>Mon, 14 Jul 2025 00:00:00 +0000</pubDate></item></channel></rss>