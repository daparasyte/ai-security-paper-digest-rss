<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>AI Security Paper Digest</title><link>https://kentaroh-toyoda.github.io/ai-security-paper-digest-rss/rss.xml</link><description>Curated papers on AI security from OpenAlex and ArXiv</description><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><lastBuildDate>Thu, 24 Jul 2025 22:13:57 +0000</lastBuildDate><item><title>Gungnir: Exploiting Stylistic Features in Images for Backdoor Attacks on Diffusion Models</title><link>https://arxiv.org/abs/2502.20650</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces Gungnir, a novel backdoor attack method for diffusion models using stylistic features as triggers&lt;/li&gt;&lt;li&gt;Generates trigger-embedded images that are perceptually indistinguishable from clean images&lt;/li&gt;&lt;li&gt;Utilizes Reconstructing-Adversarial Noise (RAN) and Short-Term Timesteps-Retention (STTR) techniques&lt;/li&gt;&lt;li&gt;Achieves 0% backdoor detection rate against existing defense methods&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yu Pan', 'Jiahao Chen', 'Bingrong Dai', 'Lin Wang', 'Yi Du', 'Jiao Liu']&lt;/li&gt;&lt;li&gt;Tags: ['backdoor attacks', 'diffusion models', 'adversarial attacks', 'image generation', 'stealth attacks', 'evasion']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2502.20650</guid><pubDate>Thu, 24 Jul 2025 00:00:00 +0000</pubDate></item><item><title>Tab-MIA: A Benchmark Dataset for Membership Inference Attacks on Tabular Data in LLMs</title><link>https://arxiv.org/abs/2507.17259</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces Tab-MIA benchmark dataset for membership inference attacks on tabular data in LLMs&lt;/li&gt;&lt;li&gt;Evaluates state-of-the-art MIA methods across multiple encoding formats&lt;/li&gt;&lt;li&gt;Demonstrates high vulnerability of LLMs fine-tuned on tabular data with AUROC scores up to 90%&lt;/li&gt;&lt;li&gt;Provides foundation for developing privacy-preserving methods for tabular data in LLMs&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Eyal German', 'Sagiv Antebi', 'Daniel Samira', 'Asaf Shabtai', 'Yuval Elovici']&lt;/li&gt;&lt;li&gt;Tags: ['privacy attacks', 'membership inference', 'tabular data', 'LLMs', 'benchmarking']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2507.17259</guid><pubDate>Thu, 24 Jul 2025 00:00:00 +0000</pubDate></item><item><title>Harnessing RLHF for Robust Unanswerability Recognition and Trustworthy Response Generation in LLMs</title><link>https://arxiv.org/abs/2507.16951</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces SALU, an LLM with integrated unanswerability detection using multi-task learning&lt;/li&gt;&lt;li&gt;Employs confidence-score-guided RLHF to penalize hallucinations and reward appropriate abstentions&lt;/li&gt;&lt;li&gt;Outperforms baselines in accuracy and human evaluations for factuality and appropriate abstention&lt;/li&gt;&lt;li&gt;Demonstrates significant reduction in hallucinated responses&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Shuyuan Lin', 'Lei Duan', 'Philip Hughes', 'Yuxuan Sheng']&lt;/li&gt;&lt;li&gt;Tags: ['LLM safety', 'unanswerability detection', 'RLHF', 'hallucination reduction', 'robustness', 'reliability', 'human feedback']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2507.16951</guid><pubDate>Thu, 24 Jul 2025 00:00:00 +0000</pubDate></item><item><title>Risk In Context: Benchmarking Privacy Leakage of Foundation Models in Synthetic Tabular Data Generation</title><link>https://arxiv.org/abs/2507.17066</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Benchmarking privacy leakage in synthetic tabular data generation using foundation models&lt;/li&gt;&lt;li&gt;Evaluated models include GPT-4o-mini, LLaMA 3.3 70B, TabPFN v2&lt;/li&gt;&lt;li&gt;Found high membership inference leakage compared to baselines&lt;/li&gt;&lt;li&gt;Suggested prompt tweaks to reduce leakage&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jessup Byun', 'Xiaofeng Lin', 'Joshua Ward', 'Guang Cheng']&lt;/li&gt;&lt;li&gt;Tags: ['privacy attacks', 'synthetic data', 'membership inference', 'foundation models', 'tabular data']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2507.17066</guid><pubDate>Thu, 24 Jul 2025 00:00:00 +0000</pubDate></item><item><title>LoX: Low-Rank Extrapolation Robustifies LLM Safety Against Fine-tuning</title><link>https://arxiv.org/abs/2506.15606</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Demonstrates vulnerability of aligned LLMs to safety undermining via fine-tuning&lt;/li&gt;&lt;li&gt;Proposes Low-Rank Extrapolation (LoX) to enhance safety robustness without retraining&lt;/li&gt;&lt;li&gt;Shows 11-54% reduction in attack success rates against benign/malicious fine-tuning&lt;/li&gt;&lt;li&gt;Attributes effectiveness to flatter parameter landscape reducing sensitivity to perturbations&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Gabriel J. Perin', 'Runjin Chen', 'Xuxi Chen', 'Nina S. T. Hirata', 'Zhangyang Wang', 'Junyuan Hong']&lt;/li&gt;&lt;li&gt;Tags: ['alignment', 'robustness', 'safety', 'fine-tuning', 'adversarial']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2506.15606</guid><pubDate>Thu, 24 Jul 2025 00:00:00 +0000</pubDate></item><item><title>Parasite: A Steganography-based Backdoor Attack Framework for Diffusion Models</title><link>https://arxiv.org/abs/2504.05815</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Introduces Parasite, a steganography-based backdoor attack framework for diffusion models&lt;/li&gt;&lt;li&gt;Targets image-to-image tasks with hidden triggers for increased concealment&lt;/li&gt;&lt;li&gt;Achieves 0% detection rate against mainstream defense frameworks&lt;/li&gt;&lt;li&gt;Explores impact of hiding coefficients in ablation studies&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Jiahao Chen', 'Yu Pan', 'Yi Du', 'Chunkai Wu', 'Lin Wang']&lt;/li&gt;&lt;li&gt;Tags: ['backdoor attack', 'steganography', 'diffusion models', 'image-to-image', 'adversarial attacks']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2504.05815</guid><pubDate>Thu, 24 Jul 2025 00:00:00 +0000</pubDate></item><item><title>More is Less: The Pitfalls of Multi-Model Synthetic Preference Data in DPO Safety Alignment</title><link>https://arxiv.org/abs/2504.02193</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Multi-model synthetic data in DPO improves general performance but increases vulnerability to jailbreaking attacks&lt;/li&gt;&lt;li&gt;Using stronger models for generating chosen responses leads to higher attack success rates&lt;/li&gt;&lt;li&gt;Single-model generated data outperforms multi-model approaches in safety metrics&lt;/li&gt;&lt;li&gt;Linear separability in multi-model data allows models to exploit superficial cues instead of learning robust safety&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Yifan Wang', 'Runjin Chen', 'Bolian Li', 'David Cho', 'Yihe Deng', 'Ruqi Zhang', 'Tianlong Chen', 'Zhangyang Wang', 'Ananth Grama', 'Junyuan Hong']&lt;/li&gt;&lt;li&gt;Tags: ['jailbreaking', 'adversarial prompting', 'safety evaluation', 'direct preference optimization (DPO)', 'reward hacking']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2504.02193</guid><pubDate>Thu, 24 Jul 2025 00:00:00 +0000</pubDate></item><item><title>On the Interaction of Compressibility and Adversarial Robustness</title><link>https://arxiv.org/abs/2507.17725</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Develops a framework analyzing how compressibility affects adversarial robustness&lt;/li&gt;&lt;li&gt;Identifies vulnerabilities from compression-induced sensitive directions&lt;/li&gt;&lt;li&gt;Demonstrates persistence of vulnerabilities under adversarial training&lt;/li&gt;&lt;li&gt;Reveals tension between compression and robustness&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Melih Barsbey', 'Ant\\^onio H. Ribeiro', 'Umut \\c{S}im\\c{s}ekli', 'Tolga Birdal']&lt;/li&gt;&lt;li&gt;Tags: ['adversarial_robustness', 'compressibility', 'model_security', 'theoretical_analysis', 'empirical_evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2507.17725</guid><pubDate>Thu, 24 Jul 2025 00:00:00 +0000</pubDate></item><item><title>Revisiting Pre-trained Language Models for Vulnerability Detection</title><link>https://arxiv.org/abs/2507.16887</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Evaluates 17 PLMs for vulnerability detection using RevisitVD&lt;/li&gt;&lt;li&gt;Compares fine-tuning and prompt engineering&lt;/li&gt;&lt;li&gt;Finds code-specific PLMs outperform general ones&lt;/li&gt;&lt;li&gt;Highlights challenges in complex dependencies, code transformations, and context truncation&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Paper Type&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Research&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Additional Information&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Authors: ['Youpeng Li', 'Weiliang Qi', 'Xuyu Wang', 'Fuxun Yu', 'Xinda Wang']&lt;/li&gt;&lt;li&gt;Tags: ['vulnerability detection', 'robustness', 'safety evaluation', 'code transformations', 'model evaluation']&lt;/li&gt;&lt;/ul&gt;</description><guid isPermaLink="false">https://arxiv.org/abs/2507.16887</guid><pubDate>Thu, 24 Jul 2025 00:00:00 +0000</pubDate></item></channel></rss>