<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>AI Security Paper Digest</title><link>https://kentaroh-toyoda.github.io/ai-security-paper-digest-rss/rss.xml</link><description>Curated papers on AI security from OpenAlex and ArXiv</description><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><lastBuildDate>Tue, 03 Jun 2025 23:01:01 +0000</lastBuildDate><item><title>Not All Jokes Land: Evaluating Large Language Models Understanding of Workplace Humor</title><link>https://arxiv.org/abs/2506.01819</link><description>Investigates how well large language models (LLMs) understand and evaluate workplace humor.
Develops a dataset of professional humor statements with features assessing appropriateness.
Finds that LLMs often struggle to accurately judge the appropriateness of workplace humor.
Highlights the importance of aligning LLMs with human values, especially in sensitive workplace contexts.</description><guid isPermaLink="false">https://arxiv.org/abs/2506.01819</guid><pubDate>Tue, 03 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Falsification of Unconfoundedness by Testing Independence of Causal Mechanisms</title><link>https://arxiv.org/abs/2502.06231</link><description>Proposes an algorithm to falsify the assumption of no unmeasured confounding in observational studies using data from multiple environments.
Leverages the dependence of observed causal mechanisms as an indicator of unmeasured confounding.
Develops a two-stage procedure to detect these dependencies with high statistical power and controlled false positives.
Demonstrates the method's effectiveness on simulated and semi-synthetic data, even under violations of transportability.</description><guid isPermaLink="false">https://arxiv.org/abs/2502.06231</guid><pubDate>Tue, 03 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Adversarial bandit optimization for approximately linear functions</title><link>https://arxiv.org/abs/2505.20734</link><description>Presents new regret bounds for bandit optimization in adversarial settings with approximately linear functions.
Considers loss functions with arbitrary perturbations chosen after the player's action, modeling adversarial scenarios.
Provides both upper and lower bounds on regret, including high-probability results.
Results have implications for robustness in online learning and adversarial environments.</description><guid isPermaLink="false">https://arxiv.org/abs/2505.20734</guid><pubDate>Tue, 03 Jun 2025 00:00:00 +0000</pubDate></item><item><title>STSA: Federated Class-Incremental Learning via Spatial-Temporal Statistics Aggregation</title><link>https://arxiv.org/abs/2506.01327</link><description>Proposes a new method (STSA) for Federated Class-Incremental Learning (FCIL) that aggregates feature statistics spatially and temporally.
Addresses challenges of data heterogeneity and client drift in federated learning, which can impact model robustness and reliability.
Introduces a communication-efficient variant (STSA-E) with theoretical guarantees, reducing overhead while maintaining performance.
Demonstrates improved performance and efficiency over existing FCIL methods on multiple datasets.</description><guid isPermaLink="false">https://arxiv.org/abs/2506.01327</guid><pubDate>Tue, 03 Jun 2025 00:00:00 +0000</pubDate></item><item><title>BMIKE-53: Investigating Cross-Lingual Knowledge Editing with In-Context Learning</title><link>https://arxiv.org/abs/2406.17764</link><description>Introduces BMIKE-53, a benchmark for evaluating cross-lingual in-context knowledge editing (IKE) across 53 languages.
Unifies and extends three existing knowledge editing datasets to assess how knowledge edited in one language generalizes to others.
Finds that model scale, demonstration alignment, and linguistic properties (e.g., script type) significantly affect cross-lingual knowledge editing performance.
Highlights challenges in maintaining knowledge consistency and robustness across languages, especially for non-Latin scripts.</description><guid isPermaLink="false">https://arxiv.org/abs/2406.17764</guid><pubDate>Tue, 03 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Pattern Recognition or Medical Knowledge? The Problem with Multiple-Choice Questions in Medicine</title><link>https://arxiv.org/abs/2406.02394</link><description>Examines the limitations of using multiple-choice questions (MCQs) to evaluate large language models (LLMs) in the medical domain.
Finds that LLMs can exploit pattern recognition and test-taking heuristics rather than demonstrating true clinical reasoning.
Highlights the risk of overestimating LLMs' medical understanding due to shallow evaluation methods.
Calls for more robust and meaningful assessment techniques to ensure the safety and reliability of LLMs in clinical settings.</description><guid isPermaLink="false">https://arxiv.org/abs/2406.02394</guid><pubDate>Tue, 03 Jun 2025 00:00:00 +0000</pubDate></item><item><title>Assortment of Attention Heads: Accelerating Federated PEFT with Head Pruning and Strategic Client Selection</title><link>https://arxiv.org/abs/2506.00743</link><description>Proposes an efficient method for Parameter Efficient Fine-Tuning (PEFT) in Federated Learning (FL) for Multi-Head Attention (MHA) based language models.
Introduces head pruning and a head-specific weighted aggregation mechanism to reduce training complexity and improve model aggregation.
Addresses challenges in FL such as resource constraints and heterogeneous data distributions, which are relevant to privacy-preserving and secure AI deployment.
Demonstrates significant improvements in communication efficiency and computational reduction while maintaining model accuracy.</description><guid isPermaLink="false">https://arxiv.org/abs/2506.00743</guid><pubDate>Tue, 03 Jun 2025 00:00:00 +0000</pubDate></item><item><title>ChemAU: Harness the Reasoning of LLMs in Chemical Research with Adaptive Uncertainty Estimation</title><link>https://arxiv.org/abs/2506.01116</link><description>Introduces ChemAU, a framework that enhances LLM reasoning in chemistry by adaptively estimating uncertainty at each reasoning step.
Addresses the issue of hallucinations and reasoning errors in LLMs when dealing with complex, domain-specific chemistry problems.
ChemAU supplements LLMs with specialized domain models to correct flawed reasoning and improve accuracy.
Demonstrates improved reasoning accuracy and uncertainty estimation in experiments with multiple LLMs and chemistry datasets.</description><guid isPermaLink="false">https://arxiv.org/abs/2506.01116</guid><pubDate>Tue, 03 Jun 2025 00:00:00 +0000</pubDate></item></channel></rss>