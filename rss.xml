<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>AI Security Digest</title><link>https://yourdomain.com/rss.xml</link><description>Summarized AI security papers from arXiv</description><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><lastBuildDate>Tue, 20 May 2025 03:58:24 +0000</lastBuildDate><item><title>Agent Name Service (ANS): A Universal Directory for Secure AI Agent Discovery and Interoperability</title><link>https://arxiv.org/abs/2505.10609</link><description>&lt;![CDATA[• Proposes the Agent Name Service (ANS), a secure, protocol-agnostic directory for AI agent discovery and interoperability.&lt;br/&gt;• Leverages PKI certificates for verifiable agent identity and trust, with formalized registration and lifecycle management.&lt;br/&gt;• Includes a comprehensive threat analysis and secure resolution algorithms to address security challenges in multi-agent systems.&lt;br/&gt;&lt;br/&gt;Tags: multi-agent systems, identity, PKI, discovery, security, interoperability, threat analysis]]&gt;</description><pubDate>Tue, 20 May 2025 03:58:24 +0000</pubDate></item><item><title>Two Minds Better Than One: Collaborative Reward Modeling for LLM Alignment</title><link>https://arxiv.org/abs/2505.10597</link><description>&lt;![CDATA[• Analyzes how noisy human feedback can cause reward models for LLM alignment to misgeneralize, potentially leading to misaligned or insecure AI behavior.&lt;br/&gt;• Proposes a Collaborative Reward Modeling (CRM) framework that uses peer review and curriculum learning between two reward models to filter out noisy preferences and improve alignment robustness.&lt;br/&gt;• Demonstrates that CRM enhances reward model generalization and can be extended to other alignment methods, improving security and reliability in LLM alignment.&lt;br/&gt;&lt;br/&gt;Tags: LLM, alignment, reward modeling, robustness, human feedback, AI safety]]&gt;</description><pubDate>Tue, 20 May 2025 03:58:24 +0000</pubDate></item></channel></rss>